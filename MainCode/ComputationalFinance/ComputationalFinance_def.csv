Abstract
"peer-to-peer (p2p) lending has attracted scholarly attention because of its economic significance and potential to democratize access to finance. however, p2p lending platforms face many challenges and failures that we need to understand more clearly. we build a computational model to study how borrower default affects p2p platform lending. we show that borrower default disrupts the p2p network formation process and undermines platform stability. moreover, we find that defaults increase the inequality in accessing funding and provide a rationale for using curation rules, widely used in p2p platforms, in contrast to p2p insurance, which fosters cascading defaults. we also address a new trend in p2p lending platforms in which large companies (institutional investors) play an increasingly important role. we find that the presence of large companies creates a denser network (more loans) but generates a trade-off between making the platform more resilient to cascading defaults and more dependent on specific players. overall, we explain how borrower defaults affect platform stability and what makes a platform vulnerable, threatening its survival. we discuss research and managerial insights into platform stability and the economic effect of p2p lending platforms. © the author(s) 2024."
"generating multivariate normal distributions is widely used in various fields, including engineering, statistics, finance and machine learning. in this paper, simulating large multivariate normal distributions truncated on the intersection of a set of hyperplanes is investigated. specifically, the proposed methodology focuses on cases where the prior multivariate normal is extracted from a stationary gaussian process (gp). it is based on combining both karhunen-loève expansions (kle) and matheron’s update rules (mur). the kle requires the computation of the decomposition of the covariance matrix of the random variables, which can become expensive when the random vector is too large. to address this issue, the input domain is split in smallest subdomains where the eigendecomposition can be computed. due to the stationary property, only the eigendecomposition of the first subdomain is required. through this strategy, the computational complexity is drastically reduced. the mean-square truncation and block errors have been calculated. the efficiency of the proposed approach has been demonstrated through both synthetic and real data studies. © the author(s), under exclusive licence to springer-verlag gmbh germany, part of springer nature 2023."
"finding the dynamical law of observable quantities lies at the core of physics. within the particular field of statistical mechanics, the generalized langevin equation (gle) comprises a general model for the evolution of observables covering a great deal of physical systems with many degrees of freedom and an inherently stochastic nature. although formally exact, gle brings its own great challenges. it depends on the complete history of the observables under scrutiny, as well as the microscopic degrees of freedom, all of which are often inaccessible. we show that these drawbacks can be overcome by adopting elements of machine learning from empirical data, in particular coupling a multilayer perceptron (mlp) with the formal structure of gle and calibrating the mlp with the data. this yields a powerful computational tool capable of describing noisy complex systems beyond the realms of statistical mechanics. it is exemplified with a number of representative examples from different fields: from a single colloidal particle and particle chains in a thermal bath to climatology and finance, showing in all cases excellent agreement with the actual observable dynamics. the new framework offers an alternative perspective for the study of nonequilibrium processes opening also a new route for stochastic modeling.  © 2012 ieee."
"this study explores the impact of green finance and technological innovation on oil production and expulsion by comparing the expulsinator with traditional pyrolysis methods. the expulsinator introduces a novel approach to simulating compound production and expulsion in natural settings, utilizing hydrous decomposition in an open pass-on mode and lithostatic compression on an intact starting point disc with an undamaged mineral framework and chemical kerogen network. in contrast, traditional decomposition methods, while valuable for assessing production dynamics, are less suitable for studying primary emigration and expulsion due to sampling damage, improper pressure settings, closed-mode burning, or waterless pyrolysis. this study aims to assess the efficacy of energy production and expulsion emulation by the expulsinator, driven by green finance initiatives, and compare it with traditional decomposition methods. production and evacuation behaviors were evaluated using rock evaluation pyrolysis, hypy, and covered small container pyrolysis (csvp). the expulsinator exhibited higher asphalt discharge than csvp, attributed to increased bitumen cross-linking during traditional pyrolysis, which favors pyrobitumen formation. variations in alkane quantities and structures were observed due to ejection impacts and production dynamics, especially delayed ejection from chromatography. expulsinator hydrous csvp ratios remained at 65 %, while toc ratios exceeded 81 %. lower gas production was observed compared to csvp, with higher expulsinator toc conversion explained by the rapid removal of produced products in the open setup to prevent reformation. the expulsinator provides valuable data on oil and gas production suitable for computational modeling tasks, highlighting the role of green finance and technological innovation in advancing sustainable energy solutions. © 2024"
computational modeling is being used in more research and applications in accounting and finance. these approaches include simulations and computer-intensive methods that are different from traditional methods in most prior research. this chapter explains the underlying philosophy of these methods and discusses several research studies using them. inferences of the studies are discussed and both benefits and concerns with the methods are addressed. © 2024 world scientific publishing company. all rights reserved.
"multivariate functional data are prevalent in various fields such as biology, climatology, and finance. motivated by the world health data applications, in this study, we propose and examine a global test for assessing the equality of multiple mean functions in multivariate functional data. this test addresses the one-way functional multivariate analysis of variance (fmanova) problem, which is a fundamental issue in the analysis of multivariate functional data. while numerous analysis of variance tests have been proposed and studied for univariate functional data, only a limited number of methods have been developed for the one-way fmanova problem. furthermore, our global test has the ability to handle heteroscedasticity in the unknown covariance function matrices that underlie the multivariate functional data, which is not possible with existing methods. we establish the asymptotic null distribution of the test statistic as a chi-squared-type mixture, which depends on the eigenvalues of the covariance function matrices. to approximate the null distribution, we introduce a welch–satterthwaite type chi-squared-approximation with consistent parameter estimation. the proposed test exhibits root-n consistency, meaning it possesses nontrivial power against a local alternative. additionally, it offers superior computational efficiency compared to several permutation-based tests. through simulation studies and applications to the world health data, we highlight the advantages of our global test. © 2023 elsevier b.v."
"the quadratic knapsack problem (qkp) is a well-studied combinatorial optimization problem with practical applications in various fields such as finance, logistics, and telecommunications. despite its longstanding interest, the qkp remains challenging due to its strong np-hardness. moreover, recent studies have introduced new instances where all existing algorithms have failed to produce good-quality results. in this paper, we aim to address these challenging qkp instances by proposing a novel approach to enhance the regular value function used in dynamic programming (dp) literature. our proposed method considers the contribution of each item not only with respect to the items already selected, but also estimates its potential contribution with respect to items yet to be considered. additionally, we introduce a propagation technique and a “remove-and-fill-up” local search procedure to further improve the solution quality. through extensive computational experiments, our heuristic algorithm demonstrates superior performance compared to existing heuristics, producing optimal or near-optimal solutions for even the most demanding qkp instances. empirical evidence, supported by an automated instance space analysis using unbiased metrics, showcases the remarkable improvements achieved, with solutions surpassing on average the solution quality of existing algorithms by up to 98%, and up to 77% reduction of the computational time. © 2024 the author(s)"
"over the past two decades, language modeling (lm) has emerged as a primary methodology for language understanding and generation. this technology has become a cornerstone within the field of natural language processing (nlp). at its core, lm is designed to train models to predict the probability of the next word or token, thereby generating natural and fluent language. the advent of large language models (llms), such as bidirectional encoder representations from transformers and gpt-3, marks a significant milestone in the evolution of lm. these llms have left a profound impact on the field of artificial intelligence (ai) while also paving the way for advancements in other domains. this progression underscores the power and efficacy of ai, illustrating how the landscape of ai research has been reshaped by the rapid advancement of llms. this paper provides a comprehensive review of the evolution of llms, focusing on the technical architecture, model scale, training methods, optimization techniques, and evaluation metrics. language models have evolved significantly over time, starting from initial statistical language models, moving onto neural network-based models, and now embracing the era of advanced pre-trained language models. as the scale of these models has expanded, so has their performance in language understanding and generation. this has led to notable results across various sectors, including education, healthcare, finance, and industry. however, the application of llms also presents certain challenges, such as data quality, model generalization capabilities, and computational resources. this paper delves into these issues, providing an analysis of the strengths and limitations of llms. furthermore, the rise of llms has sparked a series of ethical, privacy, and security concerns. for instance, llms may generate discriminatory, false, or misleading information, infringe on personal privacy, or even be exploited for malicious activities such as cyber-attacks. to tackle these issues, this paper explores relevant technical measures, such as model interpretability, privacy protection, and security assessments. ultimately, the paper outlines potential future research trends of llms. with ongoing enhancements to model scale and efficiency, llms are expected to play an even greater role in multimodal processing and societal impact. for example, by integrating information from different modalities, such as images and sound, llms can better understand and generate language. additionally, they can be employed for societal impact assessment, providing support for policy formulation and decision-making. by thoroughly analyzing the current state of research and potential future directions, this paper aims to offer researchers valuable insights and inspiration regarding llms, thereby fostering further advancement in the field. © 2024 science press. all rights reserved."
"artificial intelligence (ai) is rapidly transforming the healthcare, finance and transportation industries. this paper presents a field-programmable gate array (fpga)-based neural network accelerator (nna) design for power allocation in downlink nonorthogonal multiple access (noma) networks. the proposed hardware accelerator effectively cuts computational costs while delivering performance on par with the highest sum capacity. numerical results show that this nna offers a remarkable computational speed increase of up to 99% compared to the conventional exhaustive search method. furthermore, the deep learning (dl) model achieved high accuracy (0.92 training, 0.93 testing), and the hardware accelerator design for this dl inference model was implemented on the pynq-z2 board-constrained edge device to predict power allocation coefficients in noma systems. © world scientific publishing company."
"price trend prediction is critical across sectors, including finance, energy, and supply chains, guiding strategic decision-making. machine learning's rise, fueled by abundant data and computational power, has transformed trend prediction. advanced algorithms enable precise forecasting. the brazilian energy commercialization platform shapes the nation's energy sector, influencing electricity contract trading. despite opportunities, the brazilian energy market faces liquidity challenges. this research introduces ensemble strategies utilizing diverse data sources and machine learning to enhance price trend prediction accuracy. one of the main contributions of this study is to investigate how the use of different data sources can improve the prediction of energy prices. the study considers close-to-close, intraday, and day ahead trends, incorporating technical indicators, water flow rates, energy storage, precipitation, and load data. employing stacking and performance-weighted averages, the proposed strategies aim to synergistically improve prediction accuracy and financial returns in the brazilian energy market. © 2024 elsevier ltd"
"in financial computation, field programmable gate arrays (fpgas) have emerged as a transformative technology, particularly in the domain of option pricing. this study presents the impact of field programmable gate arrays (fpgas) on computational methods in finance, with an emphasis on option pricing. our review examined 99 selected studies from an initial pool of 131, revealing how fpgas substantially enhance both the speed and energy efficiency of various financial models, particularly black–scholes and monte carlo simulations. notably, the performance gains—ranging from 270- to 5400-times faster than conventional cpu implementations—are highly dependent on the specific option pricing model employed. these findings illustrate fpgas’ capability to efficiently process complex financial computations while consuming less energy. despite these benefits, this paper highlights persistent challenges in fpga design optimization and programming complexity. this study not only emphasises the potential of fpgas to further innovate financial computing but also outlines the critical areas for future research to overcome existing barriers and fully leverage fpga technology in future financial applications. © 2024 by the authors."
"stochastic optimal control and games have a wide range of applications, from finance and economics to social sciences, robotics, and energy management. many real-world applications involve complex models that have driven the development of sophisticated numerical methods. recently, computational methods based on machine learning have been developed for solving stochastic control problems and games. in this review, we focus on deep learning methods that have unlocked the possibility of solving such problems, even in high dimensions or when the structure is very complex, beyond what traditional numerical methods can achieve. we consider mostly the continuous time and continuous space setting. many of the new approaches build on recent neural-network-based methods for solving high-dimensional partial differential equations or backward stochastic differential equations, or on model-free reinforcement learning for markov decision processes that have led to breakthrough results. this paper provides an introduction to these methods and summarizes the state-of-the-art works at the crossroad of machine learning and stochastic control and games. © 2024, american institute of mathematical sciences. all rights reserved."
"technology is constantly evolving, and machine learning is positioned to become a pivotal tool with the power to transform industries and revolutionize everyday life. this book underscores the urgency of leveraging the latest machine learning methodologies and theoretical advancements, all while harnessing a wealth of realistic data and affordable computational resources. machine learning is no longer confined to theoretical domains; it is now a vital component in healthcare, manufacturing, education, finance, law enforcement, and marketing, ushering in an era of data-driven decision-making. academic scholars seeking to unlock the potential of machine learning in the context of industry 5.0 and advanced iot applications will find that the groundbreaking book, methodologies, frameworks, and applications of machine learning, introduces an unmissable opportunity to delve into the forefront of modern research and application. this book offers a wealth of knowledge and practical insights across a wide array of topics, ranging from conceptual frameworks and methodological approaches to the application of probability theory, statistical techniques, and machine learning in domains as diverse as e-government, healthcare, cyber-physical systems, and sustainable development, this comprehensive guide equips you with the tools to navigate the complexities of industry 5.0 and the internet of things (iot). students, researchers, and practitioners will find that this book serves as a beacon of wisdom, guiding towards the future of machine learning and its innumerable applications. harness the power of machine learning to address real-world challenges and transform the way we perceive and engage with the world. © 2024 by igi global. all rights reserved."
"the mixed fractional brownian motion (mfbm) has gained popularity in finance because it can effectively model long-range dependence, self-similarity, and is arbitrage-free. this paper focuses on mfbm with jumps modeled by the poisson process and derives an analytical formula for valuing geometric asian options. additionally, approximate closed-form solutions for pricing arithmetic asian options and arithmetic asian power options are obtained. numerical examples are provided to demonstrate the accuracy of these formulas, which rely on a convenient approximation of the option strike price. the proposed approximation demonstrates significantly higher computational efficiency compared to monte carlo simulation. © 2024 international association for mathematics and computers in simulation (imacs)"
"portfolio management (pm) is a central subject of finance that presents many unresolved challenges. although solutions have been proposed since the 1950's, recent advances in machine learning are revolutionizing investments. whereas supervised paradigms such as regression and classification are established in this direction, emerging research suggests that the learning-to-rank (ltr) paradigm can be more effective. yet, these works are not many and overlook some important aspects. these aspects include the risk inherent in stocks, feature representation, portfolio cardinality and allocation. addressing these gaps, this paper presents stochastic-aware bootstrap ensemble ranking (saber), an ltr method that directly minimizes the uncertainty and improves ranking performance. in addition, we propose merged bootstrap selection for dynamic portfolio cardinality optimization. trained on macroeconomic, fundamental and market indicators, the proposed methods outperform a state-of-the-art ltr approach, reaching annual returns and volatility of 83.48% and 29.89% across 6 years. the former also considers renowned weight allocation methods and achieves sharpe and sortino ratios of 2.79 and 5.28 respectively. finally, the computational complexity of the proposed method is proven comparable to that of current methods. © 2024 elsevier ltd"
"this article introduces a groundbreaking method for accurately forecasting financial stock market returns. the approach utilizes a hybrid neuro-autoregressive model, combined with a multi-objective decision-making phase, to determine the optimal distribution, offering significant relevance in modern finance. the proposal harnesses the impressive capabilities of the long short-term memory (lstm) recurrent neural network, synergistically coupled with the autoregressive fractionally integrated moving-average (arfima) model across various distribution options. this synergy enables precise management of a wide range of both linear and nonlinear time series data. utilized on two prominent american stock market indices (dow jones industrial average (djia) and dow jones islamic market international titans 100 (imxl) between 1/2/2015 and 12/10/2020), the experimental findings unequivocally illustrate the hybrid model's supremacy over baseline models in accuracy and computational efficiency. notably, the forecasting experiments conducted in both tranquil and turbulent periods underscore the stability and robustness of this approach. the model's adaptability and resilience make it a promising tool for precise financial stock market return forecasts, particularly crucial in informing decision-making within the financial industry. furthermore, this proposed approach contributes to the expanding research on decision support systems for financial forecasting, potentially influencing policy and strategic financial management, particularly in addressing both stable and volatile market conditions. © 2024 elsevier inc."
"the article highlights the growing importance of computational mathematics in tackling intricate real-world phenomena governed by distributed order fractional dynamics. these dynamics, characterized by memory effects and non-local interactions, are pervasive in various fields such as physics, biology, finance, and engineering. despite their prevalence, analytical solutions for distributed order fractional differential equations remain difficult to find, necessitating the development of robust numerical methods. the paper explores the effectiveness of romanovski–jacobi spectral collocation techniques in this context and provides a thorough analysis of how to use them to solve distributed order fractional differential equations. by leveraging these schemes, researchers gain valuable insights into efficiently analyzing and simulating fractional systems, thereby advancing our understanding of complex dynamics across diverse domains. it provides precise numerical findings by using finite expansion to evaluate residuals. the precision of the approach is illustrated by numerical simulations, especially in distributed order fractional differential equations. additionally, we provide a few numerical tests to demonstrate the method's ability to preserve the underlying problem's non-smooth solution. © 2024 faculty of engineering, alexandria university"
"the weibull distribution is considered one of the type-i generalized extreme value (gev) distribution, and it plays a crucial role in modeling extreme events in various fields, such as hydrology, finance, and environmental sciences. bayesian methods play a strong, decisive role in estimating the parameters of the gev distribution due to their ability to incorporate prior knowledge and handle small sample sizes effectively. in this research, we compare several shrinkage bayesian estimation methods based on the squared error and the linear exponential loss functions. they were adopted and compared by the monte carlo simulation method. the performance of these methods is assessed based on their accuracy and computational efficiency in estimating the scale parameter of the weibull distribution. to evaluate their performance, we generate simulated datasets with different sample sizes and varying parameter values. a technique for pre-estimation shrinkage is suggested to enhance the precision of estimation. simulation experiments proved that the bayesian shrinkage estimator and shrinkage pre-estimation under the squared loss function method are better than the other methods because they give the least mean square error. overall, our findings highlight the advantages of shrinkage bayesian estimation methods for the proposed distribution. researchers and practitioners in fields reliant on extreme value analysis can benefit from these findings when selecting appropriate bayesian estimation techniques for modeling extreme events accurately and efficiently. © 2024 the authors"
"the central limit theorem justifies using a normal distribution when looking at sums of many terms. in a parallel way, extreme value distributions arise in the study of maxima of many terms. the goal of this paper is to briefly review the univariate theory of extremes based on the fisher-tippet-gnedenko theorem. we then state the basics of the multivariate theory, which is significantly more complicated because it requires a measure to define the distribution. some properties of these laws are explored, including a description of the support, an expression for the density when it exists, and some examples that illustrate possible joint dependence structures. this article is categorized under: statistical and graphical methods of data analysis > multivariate analysis applications of computational statistics > computational finance. © 2024 wiley periodicals llc."
"portfolio optimization is one of the essential fields of focus in finance. there has been an increasing demand for novel computational methods in this area to compute portfolios with better returns and lower risks in recent years. we present a novel computational method called representation portfolio selection by redefining the distance matrix of financial assets using representation learning and clustering algorithms for portfolio selection to increase diversification. rps proposes a heuristic for getting closer to the optimal subset of assets. using empirical results in this paper, we demonstrate that widely used portfolio optimization algorithms, such as mean-variance optimization, critical line algorithm, and hierarchical risk parity can benefit from our asset subset selection. © 2024 the author(s)"
"optimization problems, which involve finding the best solution from a set of possible solutions, are ubiquitous in various fields, from finance to engineering. traditional algorithms sometimes struggle with these problems, especially when the solution space is vast, or the landscape is filled with numerous local minima. quantum-inspired computing, which emulates quantum mechanical principles on classical hardware, emerges as a promising paradigm to address these challenges. this paper delves into two notable approaches: coherent ising machines (cim) and graphics processing unit (gpu)-accelerated simulated annealing. in essence, both methods offer innovative strategies to navigate the solution landscape, potentially bypassing the pitfalls of local optima and ensuring more efficient convergence to solutions. by harnessing the strengths of these quantum-inspired techniques, we can pave the way for enhanced computational capabilities in tackling complex optimization problems, even without a fault-tolerant quantum computer. © 2007-2011 ieee."
"non-technical summary. human societies are changing where and how water flows through the atmosphere. however, these changes in the atmospheric water cycle are not being managed, nor is there any real sense of where these changes might be headed in the future. thus, we develop a new economic theory of atmospheric water management, and explore this theory using creative story-based scenarios. these scenarios reveal surprising possibilities for the future of atmospheric water management, ranging from a stock market for transpiration to on-demand weather. we discuss these story-based futures in the context of research and policy priorities in the present day. technical summary. humanity is modifying the atmospheric water cycle, via land use, climate change, air pollution, and weather modification. historically, atmospheric water was implicitly considered a ‘public good’ since it was neither actively consumed nor controlled. however, given anthropogenic changes, atmospheric water can become a 'common-pool’ good (consumable) or a 'club’ good (controllable). moreover, advancements in weather modification presage water becoming a 'private’ good, meaning both consumable and controllable. given the implications, we designed a theoretical framing of atmospheric water as an economic good and used a combination of methods in order to explore possible future scenarios based on human modifications of the atmospheric water cycle. first, a systematic literature search of scholarly abstracts was used in a computational text analysis. second, the output of the text analysis was matched to different parts of an existing economic goods framework. then, a group of global water experts were trained and developed story-based scenarios. the resultant scenarios serve as creative investigations of the future of human modification of the atmospheric water cycle. we discuss how the scenarios can enhance anticipatory capacity in the context of both future research frontiers and potential policy pathways including transboundary governance, finance, and resource management. social media summary. story-based scenarios reveal novel future pathways for the management of the atmospheric water cycle. © the author(s), 2024. published by cambridge university press."
"under a two-factor stochastic volatility jump (2fsvj) model we obtain an exact decomposition formula for a plain vanilla option price and a second-order approximation of this formula, using itô calculus techniques. the 2fsvj model is a generalization of several models described in the literature such as heston (rev financ stud 6(2):327–343, 1993); bates (rev financ stud 9(1):69–107, 1996); kou (manag sci 48(8):1086–1101, 2002); christoffersen et al. (manag sci 55(12):1914–1932, 2009) models. thus, the aim of this study is to extend some approximate pricing formulas described in the literature, like formulas in alòs (finance stoch 16(3):403–422, 2012); merino et al. (int j theor appl finance 21(08):1850052, 2018); gulisashvili et al. (j comput finance 24(1), 2020), to pricing under the more general 2fsvj model. moreover, we provide numerical illustrations of our pricing method and its accuracy and computational advantage under double exponential and log-normal jumps. numerically, our pricing method performs very well compared to the fourier integral method. the performance is ideal for out-of-the-money options as well as for short maturities. © 2023, the author(s)."
"the objective of the current study is analyze linear and nonlinear time–space fractional black–scholes models via modified homotopy perturbation method (m-hpm). in current investigation, memory effects in financial markets are explored through fractional derivative in caputo sense. the effectiveness of proposed methodology is checked numerically by finding residual errors and presented in tables. these tables also provide a benchmark for the comparison with already existing results in literature. furthermore, solutions are graphically analyzed via 3d and contour plots across a range of parameters under varying market conditions. analysis confirms the efficiency of m-hpm for predicting solutions of time–space fractional black–scholes models. the current study can contribute in understanding the applications of fractional calculus in finance, and can be a valuable computational tool for pricing financial derivatives in fractional environments. © 2024 faculty of engineering, alexandria university"
"the stock market data analysis has received interest as a result of technological advancements and the investigation of new machine learning models, since these models provide a platform for traders and business people to choose gaining stocks. the business price prediction is a challenging and extremely complex process due to the impact of several factors on company prices. the numerous patterns that the stock market goes, they have been the focus of extensive research and analysis by numerous experts. there are several large data sets accessible, an artificial intelligence and machine learning techniques are developing quickly, and because of the machine's improved computational power, complex stock price prediction algorithms can be developed. this paper presents stock market index prediction based on market trend using long short-term memory (lstm). using built-in application programmable interface (api), yahoo finance offers a simple method to programmatically retrieve any historical stock prices of an organization using the ticker name. the standard and poor's 500 index (s&p 500 index) include the firms that have been taken into consideration here. utilizing the selected input variable, single-layer and multi-layer lstm models are implemented, and the measurement parameters of mean absolute error (mae), root mean square error (rmse), and correlation coefficient (r) are used to compare each performance. nearly all of the real closing price's curve and the prediction curve's closing price for test data overlap. a potential stock investor may benefit significantly from such a prediction by using it to make well-informed choices that would increase his earnings. © 2024 institute of advanced engineering and science. all rights reserved."
"this work proposes a novel modified (g’/g2)-expansion method to construct the exact travelling wave solutions of three famous nonlinear partial differential equations namely, nanoionic currents along microtubules equation, benney-luke equation and the generalized hirota-satsuma coupled kdv system. in the result, these solutions can be divided into different types i.e. trigonometric, rational and exponential solutions. the computational tool maple is used to arrive at these solutions. applying the novel modified (g’/g2)-expansion approach yields results that are trustworthy, easy to understand, and useful in a variety of disciplines including physics, biology, finance, engineering, statistics, and more. software is used to create the two- and three-dimensional graphs. the comparison of the obtained solutions is also given in the form of tables 1, 2 and 3, to show the novelty of our obtained solutions. © 2024 the author(s)"
"quasi-monte carlo (qmc) methods have been gaining popularity in computational finance as they are competitive alternatives to monte carlo methods that can accelerate numerical accuracy. this paper develops a new approach for reducing the effective dimension combined with a randomized qmc method. a distinctive feature of the proposed approach is its sample-based transformation that enables us to choose a flexible manipulation via regression. in the proposed approach, the first step is to perform a regression using the samples to estimate the parameters of the regression model. an optimal transformation is proposed based on the regression result to minimize the effective dimension. an advantage of this approach is that adopting a statistical approach allows greater flexibility in selecting the regression model. in addition to a linear model, this paper proposes a dimension reduction method based on a linear-quadratic model for regression. in numerical experiments, we focus on pricing different types of exotic options to test the effectiveness of the proposed approach. the numerical results show that different regression models are chosen depending on the underlying risk process and the type of derivative securities. in particular, we show several examples where the proposed method works while existing dimension reductions are ineffective. © 2024 the author(s)"
"computational finance combines machine learning with financial needs to provide more efficient solutions for investment analysis and automated trading. in previous studies, traditional online portfolio selection (olps) algorithms were found to be overly reliant on artificially designed, subjective financial features. to address this issue, we propose a new predictive price tracking algorithm based on deep sequence features and reversal information (dsf-ri-ppt) for olps, extending a hybrid stock prediction algorithm to a multi-asset trading algorithm. we respectively employ the complete ensemble empirical mode decomposition with adaptive noise (ceemdan), principal component analysis (pca) algorithms and long short-term memory (lstm) network to perform decomposition, feature extraction and prediction on financial data. further, we supplement the reversal information by modifying the predicted prices with a reversal indicator-rate of change (roc). finally, we introduce a fast error back-propagation algorithm to integrate the predictive information into the investment ratio using gradient projection. through empirical comparison and statistic analysis of the dsf-ri-ppt algorithm, price-tracking algorithms with similar prediction models, and nine classic olps algorithms in nine portfolio data sets under three financial indexes, it can be found that the dsf-ri-ppt algorithm is profitable and generalizable. © 2024 elsevier ltd"
"data segmentation a.k.a. multiple change point analysis has received considerable attention due to its importance in time series analysis and signal processing, with applications in a variety of fields including natural and social sciences, medicine, engineering and finance. the first part reviews the existing literature on the canonical data segmentation problem which aims at detecting and localising multiple change points in the mean of univariate time series. an overview of popular methodologies is provided on their computational complexity and theoretical properties. in particular, the theoretical discussion focuses on the separation rate relating to which change points are detectable by a given procedure, and the localisation rate quantifying the precision of corresponding change point estimators, and a distinction is made whether a homogeneous or multiscale viewpoint has been adopted in their derivation. it is further highlighted that the latter viewpoint provides the most general setting for investigating the optimality of data segmentation algorithms. arguably, the canonical segmentation problem has been the most popular framework to propose new data segmentation algorithms and study their efficiency in the last decades. the second part of this survey motivates the importance of attaining an in-depth understanding of strengths and weaknesses of methodologies for the change point problem in a simpler, univariate setting, as a stepping stone for the development of methodologies for more complex problems. this point is illustrated with a range of examples showcasing the connections between complex distributional changes and those in the mean. extensions towards high-dimensional change point problems are also discussed where it is demonstrated that the challenges arising from high dimensionality are orthogonal to those in dealing with multiple change points. © 2021 ecosta econometrics and statistics"
"individuals participate in the purchase and sale of securities affiliated with corporations on the stock market, which increases economic prosperity. the intricate interplay between economic factors, market dynamics, and investor psychology poses a significant challenge in accurately predicting outcomes within the field of finance. additionally, the presence of non-stationarity, non-linearity, and high volatility in stock price time series data exacerbates the challenge of making precise estimations about stock prices in the securities market. the use of conventional techniques has the capacity to augment the accuracy of predictive modeling. however, it is important to acknowledge that these approaches also include computational intricacies, which might result in a higher likelihood of errors in predicting. this research introduces a novel model that adeptly addresses several issues via the integration of the ant lion optimization methodology with the radial basis function method. the hybrid model showed greater effectiveness and performance in comparison to other models in the current study. the proposed model demonstrated a significant degree of effectiveness, characterized by optimum performance. the usefulness of a proposed predictive model for projecting stock prices was assessed by an analysis of data obtained from the nasdaq index. the data covered the time period from january 1, 2015, to june 29, 2023. the findings suggest that the suggested model demonstrates reliability and effectiveness in its ability to analyze and predict the time series of stock prices. the empirical results suggest that the suggested model has a higher level of predictive accuracy in comparison to the other approaches by having the highest value of 0.991 for the coefficient of determination. © the author(s) under exclusive licence to the society for reliability engineering, quality and operations management (sreqom), india and the division of operation and maintenance, lulea university of technology, sweden 2024."
"gaussian processes (gps), known for their flexibility as non-parametric models, have been widely used in practice involving sensitive data (e.g., healthcare, finance) from multiple sources. with the challenge of data isolation and the need for high-performance models, how to jointly develop privacy-preserving gp for multiple parties has emerged as a crucial topic. in this paper, we propose a new privacy-preserving gp algorithm, namely pp-gp, which employs secret sharing (ss) techniques. specifically, we introduce a new ss-based exponentiation operation (pp-exp) through confusion correction and an ss-based matrix inversion operation (pp-mi) based on cholesky decomposition. however, the advantages of the gp come with a great computational burden and space cost. to further enhance the efficiency, we propose an efficient split learning framework for privacy-preserving gp, named split-gp, which demonstrably improves performance on large-scale data. we leave the private data-related and smpc-hostile computations (i.e., random features) on data holders, and delegate the rest of smpc-friendly computations (i.e., low-rank approximation, model construction, and prediction) to semi-honest servers. the resulting algorithm significantly reduces computational and communication costs compared to pp-gp, making it well-suited for application to large-scale datasets. we provide a theoretical analysis in terms of the correctness and security of the proposed ss-based operations. extensive experiments show that our methods can achieve competitive performance and efficiency under the premise of preserving privacy. © 2024 elsevier b.v."
"entropic risk (erisk) is an established risk measure in finance, quantifying risk by an exponential re-weighting of rewards. we study erisk for the first time in the context of turn-based stochastic games with the total reward objective. this gives rise to an objective function that demands the control of systems in a risk-averse manner. we show that the resulting games are determined and, in particular, admit optimal memoryless deterministic strategies. this contrasts risk measures that previously have been considered in the special case of markov decision processes and that require randomization and/or memory. we provide several results on the decidability and the computational complexity of the threshold problem, i.e. whether the optimal value of erisk exceeds a given threshold. furthermore, an approximation algorithm for the optimal value of erisk is provided. © 2024 the author(s)"
"portfolio optimisation is a core problem in quantitative finance and scenario generation techniques play a crucial role in simulating the future behaviour of the assets that can be used in allocation strategies. in the literature, there are different approaches to generating scenarios, from historical observations to models that predict the volatility of assets. in this paper, we propose a new methodology to generate one-day-ahead discrete scenarios, which are then used as input in choosing the portfolio that optimises the conditional value at risk (cvar). our approach uses machine learning supervised algorithms as forecasting models to predict the realized variance and intraday kendall correlation of assets. with the predictions, we use an evt-copula approach to simulate the multivariate probability distribution of the assets. our computational experiments suggest that our approach could produce more accurate volatility and correlation forecasts, and lower risk portfolios than traditional literature baselines. © the author(s), under exclusive licence to springer science+business media, llc, part of springer nature 2023."
"in the realm of computer science and language, large language models (llms) stand out as remarkable tools of artificial intelligence (ai). proficient in deciphering intricate language nuances, llms offer sensible responses and find applications in natural language understanding, language translation, and question answering. this chapter delves into the history, creation, training, and multifaceted applications of llms. it explores the basics of generative ai, focusing on generative pre-trained transformers (gpt). examining the evolution of llms and their diverse applications in medicine, education, finance, and engineering, the chapter addresses real-world challenges, including ethical concerns, biases, comprehensibility, and computational requirements. it serves as an informative guide for researchers, practitioners, and enthusiasts, elucidating the potential, challenges, and future of llms in ai. © 2024, igi global. all rights reserved."
"purpose: this study aims to evaluate the effectiveness of machine learning models to yield profitability over the market benchmark, notably in periods of systemic instability, such as the ongoing war between russia and ukraine. design/methodology/approach: this study made computational experiments using support vector machine (svm) classifiers to predict stock price movements for three financial markets and construct profitable trading strategies to subsidize investors’ decision-making. findings: on average, machine learning models outperformed the market benchmarks during the more volatile period of the russia–ukraine war, but not during the period before the conflict. moreover, the hyperparameter combinations for which the profitability is superior were found to be highly sensitive to small variations during the model training process. practical implications: investors should proceed with caution when applying machine learning models for stock price forecasting and trading recommendations, as their superior performance for volatile periods – in terms of generating abnormal gains over the market – was not observed for a period of relative stability in the economy. originality/value: this paper’s approach to search for financial strategies that succeed in outperforming the market provides empirical evidence about the effectiveness of state-of-the-art machine learning techniques before and after the conflict deflagration, which is of potential value for researchers in quantitative finance and market professionals who operate in the financial segment. © 2024, yaohao peng and joão gabriel de moraes souza."
"dividend policy is one of the most discussed and controversial topics in corporate finance for decades. due to the increase of computational power, few scholars tried to find most informative determinants with the help of advanced heuristic methods, a.k.a. machine learning. however, some critiques need to be addressed regarding the metric scores, model selection or robustness of their approaches. this paper proposes a working methodology that deals with these critiques. a principal components analysis as well as numerous ml models, resample techniques and metric scores have been applied in order to better understand the dividend puzzle. the conclusions suggest that the size of the companies is the most informative determinant, larger and less risky firms being more likely to pay dividends. © the author(s), under exclusive licence to springer science+business media, llc, part of springer nature 2023."
"the article is devoted to the topical issues regarding the implementation of big data technologies in public finance management. the application of big data has the potential to enhance transparency and accountability in the use of budgetary resources, increase trust in government, improve the efficiency of budget resource utilization, better understand citizens' needs, and engage the public in public finance management. the purpose of the study is to explore theoretical, methodological, and practical aspects, as well as to develop recommendations for the implementation of big data processing and analysis technologies to enhance public participation in public financial management. the article examines traditional methods of civil engagement in the budgetary process, identifies their disadvantages, and explores big data technology potential based on computational linguistics and machine learning to strengthen public participation. developments in sentiment analysis and opinion mining have been adapted to the field of public finance. a generative model for analyzing public sentiment on social networks regarding public finance management has been constructed and tested. the approaches developed for using big data technologies can be implemented in the field of public finance to enhance public participation in their management as advisory tools for the realization of representative democracy and require further theoretical elaboration and practical application to improve the analysis of alternative sentiments, prevent manipulation of public opinion, and abuse within the network. © 2024 by the author(s)."
"we develop a fast monte carlo simulation (mcs) for pricing equity-linked securities (els) with time-dependent volatility and interest rate. in this paper, we extend a recently developed fast mcs for pricing els. in the previous model, both the volatility and interest rate were constant. however, in the real finance market, volatility and interest rate are time-dependent parameters. in this work, we approximate the time-dependent parameters by piecewise constant functions and apply brownian bridge technique. we present some numerical results of the proposed method. the computational results demonstrate the fastness of the proposed algorithm with equivalent accuracy with standard mcs. it is important for traders and hedgers considering derivatives to evaluate prices and risks quickly and accurately. therefore, our algorithm will be very useful to practitioners in the els market. © the author(s), under exclusive licence to springer science+business media, llc, part of springer nature 2023."
"with the advent of the concept of ‘big data’, the past couple of decades have witnessed a substantial rise in the application of probability models other than the multivariate normal model in dealing with multidimensional data. this phenomenon has been observed in various research fields ranging from finance to ecology, from gene expression to social media, etc. copula has been the prima facie alternative tool to the widely and often inappropriately used multivariate normal model. but with the inception of copula theory we have been presented with a variety of copula-based non-normal models. this naturally leads to the inevitable question of the goodness of fit tests of these non-normal models for a given dataset. in this work, we have tried to address this question by developing a novel data driven goodness of fit (gof) test for the farlie–gumbel–morgenstern copula-based bivariate distribution. this approach can also act as a template for other copula-based distributions. further, our proposed family of parametric bootstrap gof tests does not require the knowledge of any sampling distribution, and works through a set of computational steps which can be implemented easily. we have studied the performance of the proposed gof tests in terms of size and power. finally, we have applied the test on a real dataset from mekong river delta (in vietnam) pertaining to arsenic contamination in groundwater. © grace scientific publishing 2024."
"deep learning, a crucial technique for achieving artificial intelligence (ai), has been successfully applied in many fields. the gradual application of the latest architectures of deep learning in the field of time series forecasting (tsf), such as transformers, has shown excellent performance and results compared to traditional statistical methods. these applications are widely present in academia and in our daily lives, covering many areas including forecasting electricity consumption in power systems, meteorological rainfall, traffic flow, quantitative trading, risk control in finance, sales operations and price predictions for commercial companies, and pandemic prediction in the medical field. deep learning-based tsf tasks stand out as one of the most valuable ai scenarios for research, playing an important role in explaining complex real-world phenomena. however, deep learning models still face challenges: they need to deal with the challenge of large-scale data in the information age, achieve longer forecasting ranges, reduce excessively high computational complexity, etc. therefore, novel methods and more effective solutions are essential. in this paper, we review the latest developments in deep learning for tsf. we begin by introducing the recent development trends in the field of tsf and then propose a new taxonomy from the perspective of deep neural network models, comprehensively covering articles published over the past five years. we also organize commonly used experimental evaluation metrics and datasets. finally, we point out current issues with the existing solutions and suggest promising future directions in the field of deep learning combined with tsf. this paper is the most comprehensive review related to tsf in recent years and will provide a detailed index for researchers in this field and those who are just starting out. © 2024 by the authors."
"federated learning (fl) and bioinspired computing (bic), two distinct, yet complementary fields, have gained significant attention in the machine learning community due to their unique characteristics. fl enables decentralized machine learning by allowing models to be trained on data residing across multiple devices or servers without exchanging raw data, thus enhancing privacy and reducing communication overhead. conversely, bic draws inspiration from nature to develop robust and adaptive computational solutions for complex problems. this paper explores the state of the art in the integration of fl and bic, introducing bic techniques and discussing the motivations for their integration with fl. the convergence of these fields can lead to improved model accuracy, enhanced privacy, energy efficiency, and reduced communication overhead. this synergy addresses inherent challenges in fl, such as data heterogeneity and limited computational resources, and opens up new avenues for developing more efficient and autonomous learning systems. the integration of fl and bic holds promise for various application domains, including healthcare, finance, and smart cities, where privacy-preserving and efficient computation is paramount. this survey provides a systematic review of the current research landscape, identifies key challenges and opportunities, and suggests future directions for the successful integration of fl and bic. © 2024 by the authors."
"from finance and healthcare to transportation and beyond, effective time series modeling underpins a wide range of applications. while transformers have achieved success, their reliance on global context limits scalability for lengthy sequences due to the quadratic increase in computational cost with sequence length. recent research suggests linear models can achieve comparable performance with lower complexity. however, the heterogeneity and non-stationary characteristics of time series data continue to challenge single models’ ability to capture complex temporal dynamics, especially in long-term forecasting. this paper proposes mixmamba, a novel framework for time series modeling applicable across diverse domains. the framework leverages the content-based reasoning strengths of the mamba model by integrating it as an expert within a mixture-of-experts (moe) framework. this framework decomposes modeling into a pool of specialized experts, enabling the model to learn robust representations and capture the full spectrum of patterns present in time series data. furthermore, a dynamic gating network is introduced within the framework. this network adaptively allocates each data segment to the most suitable expert based on its characteristics. this is crucial in non-stationary time series, as it allows the model to adjust dynamically to temporal changes in the underlying data distribution. to prevent bias towards a limited subset of experts, a load balancing loss function is incorporated. extensive experiments on benchmark datasets demonstrate the effectiveness and robustness of our proposed method in various time series modeling tasks, including long-term and short-term forecasting, as well as classification. © 2024 elsevier b.v."
"in the rapidly evolving domain of cryptocurrency trading, accurate market data analysis is crucial for informed decision making. candlestick patterns, a cornerstone of technical analysis, serve as visual representations of market sentiment and potential price movements. however, the sheer volume and complexity of cryptocurrency price time-series data presents a significant challenge to traders and analysts alike. this paper introduces an innovative rule-based methodology for recognizing candlestick patterns in cryptocurrency markets using python. by focusing on ethereum, bitcoin, and litecoin, this study demonstrates the effectiveness of the proposed methodology in identifying key candlestick patterns associated with significant market movements. the structured approach simplifies the recognition process while enhancing the precision and reliability of market analysis. through rigorous testing, this study shows that the automated recognition of these patterns provides actionable insights for traders. this paper concludes with a discussion on the implications, limitations, and potential future research directions that contribute to the field of computational finance by offering a novel tool for automated analysis in the highly volatile cryptocurrency market. © 2024 by the authors."
"the scalability of blockchain technology remains a critical challenge, hindering its widespread adoption across various sectors. this study introduces an innovative approach to address this challenge by proposing the adaptive restructuring of merkle trees, fundamental components of blockchain architecture responsible for ensuring data integrity and facilitating efficient verification processes. unlike traditional static tree structures, our adaptive model dynamically adjusts the configuration of these trees based on usage patterns, significantly reducing the average path length required for verification and, consequently, the computational overhead associated with these processes. through a comprehensive conceptual framework, we delineate the methodology for adaptive restructuring, encompassing both binary and non-binary tree configurations. this framework is validated through a series of detailed examples, demonstrating the practical feasibility and the efficiency gains achievable with our approach. to empirically assess the effectiveness of our proposed method, we conducted rigorous experiments using real-world data from the ethereum blockchain. the results provide compelling evidence for the superiority of adaptive merkle trees, with efficiency gains of up to 30% and higher observed during the initial stages of tree restructuring. moreover, we present a comparative analysis with existing scalability solutions, highlighting the unique advantages of adaptive restructuring in terms of simplicity, security, and efficiency enhancement without introducing additional complexities or dependencies. this study's implications extend beyond theoretical advancements, offering a scalable, secure, and efficient method for blockchain data verification that could facilitate broader adoption of blockchain technology in finance, supply chain management, and beyond. as the blockchain ecosystem continues to evolve, the principles, methodologies, and empirical findings outlined herein are poised to contribute significantly to its growth and maturity. statement the findings of this article were presented at ethdenver 2024 (https://www.ethdenver.com/), a leading innovation festival that champions the web3 community's role in shaping the future of blockchain technology. a video of our presentation can be found at https://www.youtube.com/watch?v=-jjyvcaqkne. the original manuscript of this article is available on the preprint archive at https://arxiv.org/abs/2403.00406. © 2024 elsevier b.v."
"technology has brought the biggest changes to education. over the past few years, game-based learning has helped learners increase their interest in learning. but games are rarely included in higher education, especially in programming language courses. programming has long been considered a difficult subject to get started in, and although teachers around the world are aware of the importance of computational thinking in solving programming problems, little research has been done on it. based on the design-based research method and addie model, this study proposes that teachers use game learning to carry out programming activities, and analyzes its impact on computational thinking ability. this study conducted educational intervention on first-year students majoring in software in jiangxi vocational college of finance and economics. the objective is to assess whether students can try to improve their teaching effectiveness by using a game-based learning style combined with computational thinking elements when they encounter programming difficulties. © 2024, igi global. all rights reserved."
"context: in recent years, discussions have grown about ideal machine learning techniques for financial applications. reinforcement learning (rl) has been gaining prominence in decision-making systems, however it depends on subjective reward functions, generating the need for new methodologies. problem: the financial market's complexity, characterized by diverse influencing factors, poses challenges for accurate prediction. rl algorithms, relying on reward metrics, may lack objectivity in determining optimal rewards, impacting decision-making accuracy. solution: this work proposes an approach for trading financial assets, merging rl and combinatorial optimization. the methodology involves a two-pronged process: a combinatorial optimization model determines the maximum profit achievable in a given trading period, influencing the reward function of a second rl model. is theory: grounded in the theory of computational learning, this work explores the application of machine learning methodologies to enhance computational decision-making. the integration of reinforcement learning and combinatorial optimization aligns with principles of learning and adaptation in computational systems. method: the models are trained and tested on historical stock prices, particularly focusing on petrobras' stock, a brazilian petroleum company. summary of results: using financial market metrics, it was possible to observe that the proposed model managed to perform better than the baseline model. this achievement is influenced by the maximized profit obtained through the combinatorial optimization model, demonstrating the effectiveness of the proposed approach. contributions and impact in the is area: the results demonstrate that combining combinatorial optimization with reinforcement learning enhances model performance. the versatility of this approach extends beyond finance, making it applicable to various domains within information systems.  © 2024 acm."
"the bayesian statistical paradigm provides a principled and coherent approach to probabilistic forecasting. uncertainty about all unknowns that characterize any forecasting problem – model, parameters, latent states – is able to be quantified explicitly and factored into the forecast distribution via the process of integration or averaging. allied with the elegance of the method, bayesian forecasting is now underpinned by the burgeoning field of bayesian computation, which enables bayesian forecasts to be produced for virtually any problem, no matter how large or complex. the current state of play in bayesian forecasting in economics and finance is the subject of this review. the aim is to provide the reader with an overview of modern approaches to the field, set in some historical context, with sufficient computational detail given to assist the reader with implementation. © 2023 international institute of forecasters"
"time series data, spanning applications ranging from climatology to finance to healthcare, presents significant challenges in data mining due to its size and complexity. one open issue lies in time series clustering, which is crucial for processing large volumes of unlabeled time series data and unlocking valuable insights. traditional and modern analysis methods, however, often struggle with these complexities. to address these limitations, we introduce r-clustering, a novel method that utilizes convolutional architectures with randomly selected parameters. through extensive evaluations, r-clustering demonstrates superior performance over existing methods in terms of clustering accuracy, computational efficiency and scalability. empirical results obtained using the ucr archive demonstrate the effectiveness of our approach across diverse time series datasets. the findings highlight the significance of r-clustering in various domains and applications, contributing to the advancement of time series data mining. © the author(s) 2024."
"it is recommended to use two-part models for analyzing zero-inflated data that exhibit a spike at zero or have a large proportion of participants with zero values. this paper presents a variational bayesian inference procedure for the analysis of a two-part latent variable model. we take advantage of the pólya gamma stochastic representation to approximate the posterior distribution via a mean-field variational method. we propose a scheme to update the variational parameters using the coordinate ascent inference algorithm and develop a variational bayes based procedure for the variable selection and model assessment. we conduct simulation studies to assess the performance of our proposed method and compare it with the markov chains monte carlo sampling method. our results show that the proposed variational bayesian approach achieves computational efficiency without sacrificing estimation accuracy. we further illustrate the practical merits of the proposed approach by analyzing household finance survey data. © the author(s), under exclusive licence to springer-verlag gmbh germany, part of springer nature 2023."
"an optimal credit term decision in supply chain finance often needs to be made in a dynamic way considering the varying market demand among other factors. we study the dynamic credit term optimization problem (dctop), where a supplier determines the credit term in conjunction with its production and inventory decision, while anticipating a buyer’s order quantity in a leader-follower game setting. the dctop is first approached to using a continuous time optimal control model, with analytical results characterizing the structural properties of the optimal solution. to complement the structural properties, we then develop a discrete time bilevel programming model to provide computationally tractable and implementable numerical solutions. a comprehensive computational study shows significant advantage of our optimal solutions over the heuristic credit term rules in practice, and provides managerial insights regarding the impacts of key problem parameters on the optimal solutions and coordination scheme. © the author(s), under exclusive licence to springer science+business media, llc, part of springer nature 2024."
"do people's attitudes toward the (a)symmetry of an outcome distribution affect their choices? financial investors seek return distributions with frequent small returns but few large ones, consistent with leading models of choice in economics and finance that assume right-skewed preferences. in contrast, many experiments in which decision-makers learn about choice options through experience find the opposite choice tendency, in favor of left-skewed options. to reconcile these seemingly contradicting findings, the present work investigates the effect of skewness on choices in experience-based decisions. across seven studies, we show that apparent preferences for left-skewed outcome distributions are a consequence of those distributions having a higher value in most direct outcome comparisons, a “frequent-winner effect.” by manipulating which option is the frequent winner, we show that choice tendencies for frequent winners can be obtained even with identical outcome distributions. moreover, systematic choice tendencies in favor of right- or left-skewed options can be obtained by manipulating which option is experienced as the frequent winner. we also find evidence for an intrinsic preference for right-skewed outcome distributions. the frequent-winner phenomenon is robust to variations in outcome distributions and experimental paradigms. these findings are confirmed by computational analyses in which a reinforcement-learning model capturing frequent winning and intrinsic skewness preferences provides the best account of the data. our work reconciles conflicting findings of aggregated behavior in financial markets and experiments and highlights the need for theories of decision-making sensitive to joint outcome distributions of the available options. copyright © 2024 the author(s). published by pnas. this article is distributed under creative commons attribution-noncommercial-noderivatives license 4.0 (cc by-nc-nd)."
"fractional differential equations: theoretical aspects and applications presents the latest mathematical and conceptual developments in the field of fractional calculus and explores the scope of applications in research science and computational modeling. the book delves into these methods and applied computational modelling techniques, including analysis of equations involving fractional derivatives, fractional derivatives and the wave equation, analysis of fde on groups, direct and inverse problems, functional inequalities, and computational methods for fdes in physics and engineering. other modeling techniques and applications explored include general fractional derivatives involving the special functions in analysis and fractional derivatives with respect to other functions. fractional calculus, the field of mathematics dealing with operators of differentiation and integration of arbitrary real or even complex order, extends many of the modelling capabilities of conventional calculus and integer-order differential equations and finds its application in various scientific areas, such as physics, mechanics, engineering, economics, finance, biology, and chemistry, among others. © 2024 elsevier inc. all rights reserved."
"fraud detection is a crucial data-mining task in the fields of finance and social media. traditional machine-learning approaches predict risk based solely on the features of individual nodes. recent advancements in graph-based methods allow for the consideration of features across related nodes, enhancing predictive accuracy. especially, graph neural networks (gnns) have shown high performance on graph-based fraud detection tasks. however, it presents significant challenges to performance and efficiency due to the complex and heterogeneous nature of social networks. this paper introduces a novel approach for fraud detection using a relation-aware heterogeneous graph neural network (rhgnn) model, which efficiently handles the intricacies of input data represented as heterogeneous graphs. our model leverages a computation graph pre-process and hybrid propagation scheme that integrates both features and topology information for gnns, allowing for precise and scalable fraud detection. specifically, we first use a relation-aware node map-reduce to preprocess the computational graph. then we use the hybrid propagation scheme, which optimizes the collection of neighborhood nodes with reduced complexity and remains the fraud pattern on graph data. this is achieved by alternating the focus between the 1-hop neighbor and 2-hop neighbor in the input graph, thereby enhancing the model without the typical computational overhead. we employ stochastic projection reduction to manage feature dimensionality effectively, ensuring that the model remains efficient even with large-scale graph data. experimental results on various datasets, including amazon, yelpchi, and t-finance, demonstrate that our model outperforms existing methods in terms of fraud detection accuracy. © the author(s), under exclusive license to springer nature singapore pte ltd. 2024."
"this paper mainly studies the sum-of-linear-ratios problems, which have important applications in finance, economy and computational vision. in this process, we first propose a new method to re-represent the original problem as an equivalent problem (ep). secondly, by relaxing these constraints, a nonlinear relaxation subproblem is constructed for ep. in view of the special structure of the relaxation, it is reconstructed as a second-order cone programming (socp) problem, which is essentially a socp relaxation of ep. thirdly, through the structural characteristics of the objective function of ep, a region reduction technique is designed to accelerate the termination of the algorithm as much as possible. by integrating the socp relaxation and acceleration strategy into the branch and bound framework, a new global optimization algorithm is developed. further, the theoretical convergence and computational complexity of the algorithm are analyzed. numerical experiment results reveal that the algorithm is effective and feasible. © 2024 the author(s)."
"in this paper, a computational method based on parameterizing state and control variables is presented for solving stochastic optimal control (soc) problems. by using chebyshev wavelets with unknown coefficients, state and control variables are parameterized, and then a stochastic optimal control problem is converted to a stochastic optimization problem. the expected cost functional of the resulting stochastic optimization problem is approximated by sample average approximation thereby the problem can be solved by optimization methods more easily. for facilitating and guaranteeing convergence of the presented method, a new theorem is proved. finally, the proposed method is implemented based on a newly designed algorithm for solving one of the well-known problems in mathematical finance, the merton portfolio allocation problem in finite horizon. the simulation results illustrate the improvement of the constructed portfolio return. © 2024 ferdowsi university of mashhad. all rights reserved."
"in the article at hand neural networks are used to model liquidity in financial markets, under conic finance settings, in two different contexts. that is, on the one hand this paper illustrates how the use of neural networks within a two-price economy allows to obtain accurate pricing and greeks of financial derivatives, enhancing computational performances compared to classical approaches such as (conic) monte carlo. the methodology proposed for this purpose is agnostic of the underlying valuation model, and it easily adapts to all models suitable for pricing in conic financial markets. on the other hand, this article also investigates the possibility of valuing contingent claims under conic assumptions, using local stochastic volatility models, where the local volatility is approximated by means of a (combination of) neural network(s). moreover, we also show how it is possible to generate hybrid families of distortion functions to better fit the implied liquidity of the market, as well as we introduce a conic version of the sabr model, based on the wang transform, that still allows for analytical bid and ask pricing formulae. © 2024 informa uk limited, trading as taylor & francis group."
"this article presents the sorting composite quantile regression neural network (scqrnn), an advanced quantile regression model designed to prevent quantile crossing and enhance computational efficiency. integrating ad hoc sorting in training, the scqrnn ensures non-intersecting quantiles, boosting model reliability and interpretability. we demonstrate that the scqrnn not only prevents quantile crossing and reduces computational complexity but also achieves faster convergence than traditional models. this advancement meets the requirements of high-performance computing for sustainable, accurate computation. in organic computing, the scqrnn enhances self-aware systems with predictive uncertainties, enriching applications across finance, meteorology, climate science, and engineering. © the author(s), under exclusive license to springer nature switzerland ag 2024."
"in the litterature, it has been shown that multi-objective fuzzy genetics-based machine learning can be used to find a range of fuzzy classifiers with varying explainability and accuracy for use in fields such as medicine or finance. it can similarly be used to find a range of fuzzy classifiers considering both the number of features and the accuracy. to increase the classifiers' reliability, a reject option can be used to refuse the classification of uncertain patterns. to reduce the overall classification cost, we would like to use a feature-light classifier most of the time and use a more accurate feature-heavy classifier only for patterns that require a more complex classification boundary, meaning those near the classification boundary. in this paper, we consider the integration of several fuzzy classifiers to combine their individual qualities into a hierarchical fuzzy classifier using a reject option as the condition to relay the pattern between the different classifiers: this way, most input patterns will be classified by the top layer feature-light classifier whose classification cost is low while only the patterns near the classification boundary will be classified by one of the more costly bottom layers feature-heavy classifiers. through computational experiments, we discuss the performance of the newly formed classifier over several datasets and compare two different reject threshold optimization schemes. © 2024 ieee."
"purpose: this paper aims to propose a shariah-compliant multi-period fuzzy portfolio optimization model that accounts for shariah compliance through purification processes and incorporates various shariah constraints, including sustainability constraints. this model aims to ensure both ethical alignment and robust portfolio management while navigating modern financial complexities and fostering responsible and sustainable investment practices. design/methodology/approach: the methodology involves a dynamic programming method to solve the proposed program, with returns of the assets assumed to be trapezoidal fuzzy variables. this approach allows for the quantification of portfolio return and risk by the possibilistic mean and semivariance of the fuzzy returns, respectively. a numerical study based on real stock market data tests the efficiency of the proposed algorithm. findings: the research showcases the model’s effectiveness in managing shariah-compliant portfolios under financial uncertainties and supports the importance of incorporating ethical and sustainability constraints in investment decisions. it highlights the capability of the proposed model to offer a structured approach to ethical investing within the islamic finance framework. research limitations/implications: while the paper provides a solid foundation for shariah-compliant portfolio optimization, it acknowledges the complexity and computational demands of the model. future research could explore simplifying the model without compromising its ethical and shariah-compliant principles. originality/value: this work introduces a novel integration of shariah compliance with fuzzy portfolio optimization techniques, addressing the need for dynamic, ethical investment strategies in islamic finance. the incorporation of purification processes and sustainability constraints into a fuzzy portfolio optimization model represents a unique contribution to the field. © 2024, emerald publishing limited."
"solving differential equations is crucial in diverse fields, including finance, where accurate pricing of financial products is essential for risk management and market analysis. the heston model, known for its superior ability to capture market volatility, has been hampered by the computational bottleneck of its differential equation solution. this paper presents a groundbreaking machine learning-based framework that leverages gpu acceleration to significantly enhance the speed and accuracy of heston model calculations. by strate-gically partitioning the process into a neural network-based approximation and an evolutionary algorithm-based calibration, this framework achieves remarkable speedups compared to tra-ditional numerical methods. furthermore, its gpu-accelerated vectorized operations pave the way for realtime application in high-frequency trading environments. this innovative framework holds substantial promise for revolutionizing the field of financial option pricing by offering both unparalleled speed and superior accuracy.  © 2024 ieee."
"in the 1610s and 1620s, a new computational technology took hold in england: printed mathematical tables for compound interest and discounting (“present value”) problems. historians of finance and accounting have long recognized these paper tools as predecessors of essential modern techniques like “discounted cash flow.” yet the history of these tables remains hazy. what did early seventeenth-century users do with them? who used them? why did they appear when they did? this article turns to one obscure but influential text—ambrose acroyd’s tables of leasses and interest (1628)—as a guide to these questions. two key facts emerge. first, despite the prominence of similar calculations in financial applications today, these early tables were not confined to england’s nascent financial sector. rather, their foremost use related to agricultural property, specifically in assessing certain payments (“fines”) landlords charged tenants for farm leases. second, among the leading “early adopters” were institutions of the church of england. amidst the inflation of the early modern “price revolution,” bishops, cathedrals, and colleges confronted a complex of economic, political, and social pressures. mathematical tables like acroyd’s emerged out of long-running conflicts between church landlords and tenants over how to determine just and reasonable fines on church lands. discounting tables were thus not tools of instrumental rationality evincing a new capitalist mentality, but tools of social accommodation and products of the era’s “economy of obligation.” this early modern tale offers a vivid example of how and why one community turned to a mathematical algorithm to resolve conflicts about fairness. © 2024 the university of chicago. all rights reserved."
"over the past few decades, the financial industry has shown a keen interest in using computational intelligence to improve various financial processes. as a result, a range of models have been developed and published in numerous studies. however, in recent years, deep learning (dl) has gained significant attention within the field of machine learning (ml) due to its superior performance compared to traditional models. there are now several different dl implementations being used in finance, particularly in the rapidly growing field of fintech. dl is being widely utilized to develop advanced banking services and investment strategies. this chapter provides a comprehensive overview of the current state-of-the-art in dl models for financial applications. the chapter is divided into categories based on the specific sub-fields of finance, and examines the use of dl models in each area. these include algorithmic trading, price forecasting, credit assessment, and fraud detection. the chapter aims to provide a concise overview of the various dl models being used in these fields and their potential impact on the future of finance. © the author(s), under exclusive license to springer nature switzerland ag 2024."
"a predictive model refers to a mathematical or computational model that is designed to make predictions or forecasts based on input data. these models are used in various fields such as finance, healthcare, marketing, and many others to analyze historical data and make predictions about future events or trends. this study focuses on developing a predictive model for well-differentiated thyroid cancer recurrence using machine learning algorithms. leveraging a dataset with clinicopathologic features spanning a 15-year period, the study aims to contribute to the advancement of predictive modeling in thyroid cancer prognosis. the various algorithms, including logistic regression, decision tree, random forest, support vector machine, k-nearest neighbors, and xgboost, are evaluated for their performance in binary classification. the highest-performing model is then utilized to predict thyroid cancer recurrence in new data. integrating this with frontend tools has resulted in an intuitive interface that streamlines the process of uploading datasets, executing the predictive model, and presenting the results. this integration has made the prediction process more accessible and user-friendly, ultimately benefiting clinicians and researchers in the field of thyroid cancer research.  © 2024 ieee."
"this chapter aims to offer the reader a critical reflection on computational finance starting from the principles of ethical finance. with this term, we refer to those principles that arose from the 1970s onwards, are proposed to implement socio-environmental values in financial activities, from savings to employment, also in response to the process of financialization of the economy that has removed finance itself from real life of local populations. starting from a critical analysis of economic positivism that introduced the massive use of mathematics in economics, it is proposed a reflection on the concept of financial accounting and on the role of the real acquisition power of wages in order to create a financial system that determines anew a socio-environmental horizon to which the economy must strive. with these assumptions, financial tools are proposed based on the principles of ethical finance and how they can promote a process that we call economic socialization, that is to allow finance to carry forward again the social and environmental values necessary for the life of local communities. with these assumptions, the first paragraph introduces the concept and problems of economic positivism and how the process of financialization of the world economy and its impact on the financial system has been produced since 1970. in this context, some theoretical concepts are proposed such as that of the purchasing power of wages to determine a finance linked to the workforce. the second, introduces the principles of ethical finance, from birth to the present day. the third introduces some financial and socio-environmental measurement tools and models of ethical finance that could be introduced in computational finance. finally, the fourth proposes some conclusions starting from the arguments set out, including the process of economic socialization, or how finance is called to carry forward the socio-environmental values of local communities, under penalty of losing the conditions of real well-being for our societies. in this scenario, it is proposed that finance must respond to a demand for peoples’ rights. © the author(s), under exclusive license to springer nature switzerland ag 2024."
"cloud adoption in industrial sectors, such as process, manufacturing, health care, and finance, is steadily rising, but as it grows, the risk of targeted cyberattacks has increased. hence, effectively defending against such attacks necessitates skilled cybersecurity professionals. traditional human-based cyber-physical education is resource intensive and faces challenges in keeping pace with rapidly evolving technologies. this research focuses on the main advantages of incorporating large language models into cyber-physical education. the chatgpt platform serves as an online tool to educate students on fundamentals, cyberattacks, and defense concepts, fostering the development of a new generation cybersecurity experts. the proposed learning approach adheres to the chatgpt-assisted learn-apply-create model. responding to prompts provided by the learners, the learning phase engages in conceptual learning, the applying phase involves mathematical modeling of various cyberattacks, and the creating phase develops matlab program to incorporate attacks into sensor measurements for the experiment and entails developing the necessary attack detection approaches. the effectiveness of the detection method developed by chatgpt is assessed in both the simulation and real-time scenarios using a j-type thermocouple. the impact of the proposed learning platform over traditional learning methods is evaluated through an extensive comparative feedback analysis on the learner's foundational concepts, computational thinking, programming efficacy, and motivation. the study proved that integrating chatgpt into engineering education enables students to swiftly learn cyber-physical fundamentals, comprehend and model cyberattacks, create new attack signatures, and contribute to developing detection algorithms. such integration provides the learners with essential industrial skills crucial in modern industries.  © 2008-2011 ieee."
"time series data anomaly detection is to identify observations or patterns in some chronologically ordered points which are significantly inconsistent with expected patterns or normal behavior. these patterns may indicate unexpected events, unusual behavior, failures, or other unusual conditions in the system. time series anomaly detection has important applications in many fields, including industry, finance, medical care, etc. we designed a time series anomaly detection algorithm using lstm(long short-term memory) and attention. by adding an attention layer after the lstm network, the network can pay more attention to more relevant features in the input multivariate time series, thereby improving accuracy and recall. in the data preprocessing process, the decision tree algorithm is used on the original data set to remove features that have little impact on the anomaly detection results, so as to reduce the computational complexity of anomaly detection, and improve the efficiency of anomaly detection. experiments on the industrial time series data set swat data set show that the lstm-attention model proposed in this article is better to the basic lstm network in terms of precision, recall, f1score and other indicators, and achieves good time series data anomaly detection results. © 2024 spie."
"in this paper, we focus on the modeling problem of estimating data with non-sparse structures, specifically focusing on biological data that exhibit a high degree of relevant features. various fields, such as biology and finance, face the challenge of non-sparse estimation. we address the problems using the proposed method, called structured iterative division. structured iterative division effectively divides data into non-sparse and sparse structures and eliminates numerous irrelevant variables, significantly reducing the error while maintaining computational efficiency. numerical and theoretical results demonstrate the competitive advantage of the proposed method on a wide range of problems, and the proposed method exhibits excellent statistical performance in numerical comparisons with several existing methods. we apply the proposed algorithm to two biology problems, gene microarray datasets, and chimeric protein datasets, to the prognostic risk of distant metastasis in breast cancer and alzheimer’s disease, respectively. structured iterative division provides insights into gene identification and selection, and we also provide meaningful results in anticipating cancer risk and identifying key factors. © the author(s) 2024."
"artificial intelligence (ai) has flourished in all domains like medicine, finance, intelligent transportation, education, legislation, industrial automation, predictive maintenance, agriculture, energy conservation, etc. although ai has remarkable upliftment in all the domains mentioned above, its widespread acceptance or adaptability has been hindered by its lack of transparency and trustworthiness. in most cases, the computer scientists and the inventors of the ai models were also unanswerable for the decisions arrived at by their models deployed. therefore, ai models were not accepted widely. explainable artificial intelligence (exai) provides meaningful explanations for final decisions by breaking ai’s black-box nature. exai provides explanations for the internal logic unanswerable by the developers and the conclusions, thereby promoting the trustworthiness of the ai models. urthermore, all the applications developed and deployed in today’s digital environment are transdisciplinary, amalgamating concepts from different technologies to provide a sustainable, automated, global solution. therefore, exai would be a mandate for the sustainable adaptability of ai models and their applications. hence, this chapter analyses the utilization of exai for computational sustainability, the various existing exai models such as intrinsic and post-hoc methods, applications of exai in different services of smart cities, and the utilization of exai as an intelligent decision support system for computational sustainability, challenges in adoption, legal, ethical, and social issues in exai. © 2024 selection and editorial matter, lakshmi d., ravi shekhar tiwari, rajesh kumar dhanaraj and seifedine kadry; individual chapters, the contributors."
"a classical paper by [grossman, stiglitz, 1980] showed that asset prices in equilibrium necessarily contain some degree of inefficiency when information is costly. moreover, the inefficiency should decrease with decreasing information costs. however, a number of recent empirical studies cast some doubt on the postulate that real financial markets are becoming more efficient, despite the ostensible advances in technology and radical increases in the availability of information. the pricing of financial assets strongly depends on the relative wealth shares of heterogeneous groups of investors interacting in the market and the recent field of evolutionary finance has produced a number of studies showing how asset price dynamics develop under the influence of endogenously changing populations of heterogeneous traders. however, very few studies exist examining how the cost of information affects prices of assets in an evolutionary context. we therefore construct an evolutionary agent-based model of a financial market with boundedly rational traders who learn from experience. we conduct a number of computational experiments with varying information costs and show that even under zero information costs uninformed traders can survive and even dominate the market in finite time. thus, for an initially low level of information costs, a marginal decrease in them does not necessarily lead to increased price efficiency. for higher initial levels of information costs our results however agree with those of [grossman, stiglitz, 1980] in that an increasing information cost generally leads to informed traders being driven out of the market and asset prices becoming less efficient. implications of our findings for financial market regulation are discussed. © 2024 publishing house of the higher school of economics. all rights reserved."
"several large-scale datasets (e.g., wikisql, spider) for developing natural language interfaces to databases have recently been proposed. these datasets cover a wide breadth of domains but fall short on some essential domains, such as finance and accounting. given that accounting databases are used worldwide, particularly by non-technical people, there is an imminent need to develop models that could help extract information from accounting databases via natural language queries. in this resource paper, we aim to fill this gap by proposing a new large-scale text-to-sql dataset for the accounting and financial domain: booksql. the dataset consists of 100k natural language queries-sql pairs, and accounting databases of 1 million records. we experiment with and analyze existing state-of-the-art models (including gpt-4) for the text-to-sql task on booksql. we find significant performance gaps, thus pointing towards developing more focused models for this domain. ©2024 association for computational linguistics."
"the need to protect data privacy is critical in industries like healthcare, finance, and banking. federated learning (fl) has attracted widespread attention due to its decentralized, distributed training and the potential to protect privacy. however, fl presents challenges like privacy concerns, network, and data heterogeneity. this motivated us to propose a two-phase, one-pass fl algorithm, where (i) in the first phase, the differentially private dataset is generated by incorporating the laplacian mechanism, and (ii) during the second phase, we employed one-pass privacy-preserving federated extreme learning machine (op-fedelm) for generating globally shared training dataset and build the global model. elm generally tends to overfit training data at a client. therefore, we employed a modified version of the evolving clustering algorithm (ecm), an online one-pass clustering algorithm, to cluster training datasets at the clients. further, it will reduce the computational training time of elm and communication overhead. at the server, we employed a meta-clustering algorithm to cluster the updates from the clients. we also proposed another one-shot fl algorithm called privacy-preserving federated elm based on minimum least square solution (fedelm-ls). the experimental analysis concludes that op-fedelm is computationally less expensive yet achieves higher auc in three of four datasets. © the author(s), under exclusive license to springer nature switzerland ag 2024."
"this article introduces the risk balancing frontier (rbf), a new portfolio boundary in the absolute risk-total return space: the rbf arises when two risk indicators, the tracking error volatility (tev) and the value-at-risk (var), are both constrained not to exceed pre-set maximum values. by focusing on the trade-off between the joint restrictions on the two risk indicators, this frontier is the set of all portfolios characterized by the minimum var attainable for each tev level. first, the rbf is defined analytically and its mathematical properties are discussed: we show its connection with the constrained tracking error volatility frontier (jorion in financ anal j, 59(5):70–82, 2003. https://doi.org/10.2469/faj.v59.n5.2565) and the constrained value-at-risk frontier (alexander and baptista in j econ dyn control, 32(3):779–820, 2008. https://doi.org/10.1016/j.jedc.2007.03.005) frontiers. next, we explore computational issues implied with its construction, and we develop a fast and accurate algorithm to this aim. finally, we perform an empirical example and consider its relevance in the context of applied finance: we show that the rbf provides a useful tool to investigate and solve potential agency problems. © the author(s) 2024."
"computational intelligence is reshaping industries such as healthcare, finance, transportation, and manufacturing, with the increasing sophistication of artificial intelligence driving substantial market growth. the comprehensive brain function simulation (cbfs) model leverages computational intelligence to study the human brain, advancing our understanding of cognition and enabling the simulation of neurological disorders. neural networks are pivotal in machine learning, powering tasks like image recognition and decision- making. game theory models offer insights into rational decision-making, applicable in economics, politics, and social sciences. agent-based financial models simulate financial markets, analyzing dynamics and trading strategies. this study underscores computational intelligence's transformative potential, emphasizing innovation and problem-solving across diverse sectors. the burgeoning computational intelligence market and the use of models like brain function simulation, machine learning, game theory, and financial analysis herald a new era of possibilities in the field. © 2024 ieee."
"the assessment of credit risk in supply chain finance (scf) stands as a pivotal procedure in facilitating enterprises to identify appropriate financing solutions, reduce financing costs, enhance capital utilization efficiency, and mitigate the risk of debt default. multi-criteria group decision-making (mcgdm), a systematic evaluation tool, is widely used for the assessment of both qualitative and quantitative criteria. however, the conventional framework of mcgdm exhibits limitations in addressing scenarios characterized by high uncertainty in risk information, disparity in weights among decision-makers (dms) and criteria, alongside complex and non-linear risk perception. to address these limitations, this paper introduces an analytical model that integrates interval type-2 fuzzy sets (it2fss), cumulative prospect theory (cpt), and the todim (an acronym from portuguese for interactive and multicriteria decision making) method to evaluate credit risk in scf. firstly, the it2fss are utilized to represent high uncertainty in risk assessment information of dms. secondly, the dice similarity is applied to determine the weights of dms. then, we seek to improve the criterion importance through intercriteria correlation (critic) method by addressing its limitations and further integrating it with the bayesian best–worst method (bbwm), offering a robust computational framework of integrated weights for criteria. finally, the cpt-todim method based on it2fss is applied in a real case from ping an bank. through rigorous sensitivity and comparative analyses conducted within the real-world context of scf credit risk assessments, the proposed model’s theoretical robustness and practical applicability are emphatically validated. © the author(s) under exclusive licence to taiwan fuzzy systems association 2024."
"we propose a new, data-driven approach for efficient pricing of–fixed- and floating-strike–discrete arithmetic asian and lookback options when the underlying process is driven by the heston model dynamics. the method proposed in this article constitutes an extension of perotti and grzelak [fast sampling from time-integrated bridges using deep learning, j. comput. math. data sci. 5 (2022)], where the problem of sampling from time-integrated stochastic bridges was addressed. the model relies on the seven-league scheme [s. liu et al. the seven-league scheme: deep learning for large time step monte carlo simulations of stochastic differential equations, risks 10 (2022), p. 47], where artificial neural networks are employed to ‘learn’ the distribution of the random variable of interest utilizing stochastic collocation points [l.a. grzelak et al. the stochastic collocation monte carlo sampler: highly efficient sampling from expensive distributions, quant. finance 19 (2019), pp. 339–356]. the method results in a robust procedure for monte carlo pricing. furthermore, semi-analytic formulae for option pricing are provided in a simplified, yet general, framework. the model guarantees high accuracy and a reduction of the computational time up to thousands of times compared to classical monte carlo pricing schemes. © 2024 the author(s). published by informa uk limited, trading as taylor & francis group."
"multitarget sentiment analysis extracts the subjective polarity of text from multiple targets simultaneously in a given context. this approach is useful in finance, where opinions about different entities affect the target differently. examples of possible targets are other companies and society. however, typical multitarget solutions are resource-intensive due to the need to deploy multiple classification models for each target. an alternative to this is the use of multiobjective training approaches, where a single model is capable of handling multiple targets. in this work, we propose the spanish mtsacorpus 2023, a novel corpus for multitarget sentiment analysis in finance, and we evaluate its reliability with several large language models for multiobjective training. to this end, we compare three design approaches: (i) a main economic target (met) detection model based on token classification plus a multiclass classification model for sentiment analysis for each target; (ii) a met detection model based on token classification but replacing the sentiment analysis models with a multilabel classification model; and (iii) using seq2seq-type models, such as mbart and mt5, to return a response sequence containing the met and the sentiments of different targets. based on the computational resources required and the performance obtained, we consider the fine-tuned mbart to be the best approach, with a mean f1 of 80.300%. © 2024 by the authors."
"the practical problems in financial engineering are highly interdisciplinary, requiring as much facility with applied mathematics, statistics and programming as with finance. solving mathematically challenging problems and writing efficient computer programs to price complex structured products is just one part of the puzzle, however. given the number of design elements involved in creating such products, a front end that combines visualization and interactivity is as important as speed and efficiency of computations. in this note we highlight the power of the python stack for designing graphical user interfaces for engineering structured product solutions by visualizing their payoffs and prices in a web browser. object-oriented programming in python combined with the power of numpy, matplotlib and jupyter fits the bill perfectly for design and visualization in financial engineering. we find that python combined with jupyter is not only very well suited for designing and visualizing structured products and examining the impact on pricing as different design elements are tweaked, but it is also amenable to a variety of extensions and integration with other open-source computational finance libraries. © the author(s), under exclusive license to springer nature switzerland ag 2024."
"the forecasting of financial indicators stands as a significant task in computational finance. it plays a crucial role in understanding a company's future operational conditions, providing investors with references for investment decisions. existing methods predominantly focus on the independent historical financial data of listed companies without capturing the complex interactions among companies. this paper proposes an adjustable spatio-temporal forecasting network (astfn), which integrates the historical financial features with the topological features of the cross-shareholding network. focusing on predicting multivariate earnings and risk indicators in listed companies' financial data, astfn achieves optimal performance compared to all baseline models. our proposed model reveals the crucial role of leveraging network relationships in financial indicator forecasting. by adjusting a parameter that balances temporal and spatial features, astfn can enhance the overall forecasting performance. additionally, we analyze the impacts of network directionality and find that changing the direction of information passing from unidirectional to bidirectional improves the predictive accuracy of earnings indicators, but probably degrades the performance of risk indicators. it reveals the distinct propagation patterns for earnings and risk within the network. © 2024 ieee."
"in this article, gradient based descent line search scheme is proposed to solve interval optimization problems under generalized hukuhara differentiability. the innovation and importance of these concepts are presented from practical and computational point of view. the necessary condition for existence of critical point is presented in inclusion form of interval-valued gradient, without transforming it into real valued function. suitable efficient descent direction is chosen based on the monotonic property of the interval-valued function and specific interval ordering. mathematical convergence of the scheme is proved under the assumption of inexact line search for interval optimization problem. the theoretical developments are implemented with a set of interval test problems in different dimensions. a possible application in finance is provided and solved by our proposed scheme. © 2023 elsevier b.v."
"quantum physics has changed the way we understand our environment, and one of its branches, quantum mechanics, has demonstrated accurate and consistent theoretical results. quantum computing is the process of performing calculations using quantum mechanics. this field studies the quantum behavior of certain subatomic particles (photons, electrons, etc.) for subsequent use in performing calculations, as well as for large-scale information processing. these advantages are achieved through the use of quantum features, such as entanglement or superposition. these capabilities can give quantum computers an advantage in terms of computational time and cost over classical computers. nowadays, scientific challenges are impossible to perform by classical computation due to computational complexity (more bytes than atoms in the observable universe) or the time it would take (thousands of years), and quantum computation is the only known answer. however, current quantum devices do not have yet the necessary qubits and are not fault-tolerant enough to achieve these goals. nonetheless, there are other fields like machine learning, finance, or chemistry where quantum computation could be useful with current quantum devices. this manuscript aims to present a review of the literature published between 2017 and 2023 to identify, analyze, and classify the different types of algorithms used in quantum machine learning and their applications. the methodology follows the guidelines related to systematic literature review methods, such as the one proposed by kitchenham and other authors in the software engineering field. consequently, this study identified 94 articles that used quantum machine learning techniques and algorithms and shows their implementation using computational quantum circuits or ansatzs. the main types of found algorithms are quantum implementations of classical machine learning algorithms, such as support vector machines or the k-nearest neighbor model, and classical deep learning algorithms, like quantum neural networks. one of the most relevant applications in the machine learning field is image classification. many articles, especially within the classification, try to solve problems currently answered by classical machine learning but using quantum devices and algorithms. even though results are promising, quantum machine learning is far from achieving its full potential. an improvement in quantum hardware is required for this potential to be achieved since the existing quantum computers lack enough quality, speed, and scale to allow quantum computing to achieve its full potential. © 2024 the author(s)"
"in this paper, we present an efficient quantum algorithm to simulate nonlinear differential equations with polynomial vector fields of arbitrary (finite) degree on quantum platforms. ordinary differential equations (odes) and partial differential equations (pdes) arise extensively in science and engineering applications. examples of ode models include mechanics of rigid bodies, molecular dynamics, chemical kinetics, and epidemiology. nonlinear pdes arise in fluid dynamics, combustion, weather forecasting, structural mechanics, plasma dynamics, and finance to name a few. in practice, it is challenging to simulate such equations on classical computers due to high dimensionality, stiffness arising from multiple spatial/temporal scales, nonlinearities, and chaotic dynamics. typically, high performance computing is used to mitigate computational challenges and involves approximations for tractability. for sparse n-dimensional linear odes, quantum algorithms have been developed which can produce a quantum state proportional to the solution in poly(log(n))) time using the quantum linear systems algorithm (qlsa). recently, this framework was extended to systems of nonlinear odes with quadratic polynomial vector fields by applying carleman linearization that enables the embedding of the quadratic system into an approximate linear form. a detailed complexity analysis was conducted which showed significant computational advantage under certain conditions. we present an extension of this algorithm to deal with systems of nonlinear odes with k-th degree polynomial vector fields for arbitrary (finite) values of k. the steps involve: (1) mapping the k-th degree polynomial ode to a higher-dimensional quadratic polynomial ode; (2) applying carleman linearization to transform the quadratic ode to an infinite-dimensional system of linear odes; (3) truncating and discretizing the linear ode and solving using the forward euler method and qlsa. alternatively, one could apply carleman linearization directly to the k-th degree polynomial ode, resulting in a system of infinite-dimensional linear odes, and then apply step 3. this solution route can be computationally more efficient. we present detailed complexity analysis of the proposed algorithms and prove polynomial scaling of runtime on k. we demonstrate the computational framework on a numerical example. © the author(s), under exclusive licence to springer science+business media, llc, part of springer nature 2024."
"a modern computational paradigm known as fog computing provides distributed end users with very scalable and low-latency services. because local fog nodes enable the quick storage and processing of data close to data sources, it has an inherent safety benefit over cloud computing. simultaneously, the blockchain (bc) technology represents a significant advancement, prioritizing data security, anonymity, and integrity via a consensus-building process. while bc was first widely used in finance, it is now finding its way into other industries, like as healthcare. in order to forecast diseases, this research suggests a safe healthcare service that uses blockchain and fog computing. it focuses on diabetes and cardiovascular conditions. health data about patients is collected from the fog nodes and safely kept on a blockchain. after grouping patient health data using a unique rule-based clustering approach, a feature selection-based neuro-fuzzy inference system with adaptive capability (fs-anfis) is used to forecast diseases. significant real-world healthcare data trials are used to test the suggested technique. performance measures, such as purity as normalized mutual information (nmi), are used to measure the effectiveness of rule-based clustering, as prediction accuracy is used to evaluate the performance of illness prediction. comparing the testing results to other neural network methods, the suggested methodology achieved over 81% prediction accuracy, demonstrating its effectiveness. © 2024 ieee."
"an airshed concept has been widely practiced in developed countries as a tool for air quality mitigation, but its application in developing countries is still evolving. the air pollution challenges in developing countries are complex and cannot be solved merely through a city-centric approach, requiring a suitable framework for regional airshed approach to better comprehend the sources, impacts and design adequate response, rather than localised action within administrative boundaries of a city. the implementation of the airshed approach in developing countries may encounter challenges due to various constraints, including limited resources, specifically in terms of finance, and a shortage of trained researchers, such as modellers. additionally, the lack of high-performance computational facilities and institutional networking further adds to the difficulties faced by these countries. thus, the main objective of this review paper is to critically analyse the various airshed approaches that are commonly used in the developed countries. by doing so, the review identify gaps in air pollution mitigation strategies specifically in developing countries and proposes a cost-effective and practical airshed management framework that can be implemented in developing countries. airshed delineation should be based on scientific assessment of air pollution transport and accumulation through representative stations in an airshed and robust source apportionment combining meteorological factors. the domain falling in the spatial extent of airshed may be classified as nonattainment areas for maintaining the uniformity in control actions and effective implementation. building on city-specific airshed framework, an institutional framework for regional airshed management has also been suggested for planning, monitoring and implementing the participatory approach with financial autonomy and extant regulatory backup. the suggested framework can useful for the policy makers to analyse the air pollution mitigation strategies on a regional scale. © 2024 the author(s)"
"lots of challenges in the multidimensional option pricing exist since this is one of the fundamental discipline in large-scale finance problems today. in this paper, for the first time we develop some new highly accurate lattice sequences, based on component-by-component construction methods: construction of rank-1 lattice rules with prime number of points and with product weights; construction of rank-1 lattice sequences with prime number of points and with product weights; construction of polynomial rank-1 lattice sequences in base 2 and with product weights. our methods show significantly optimization compared to the results produced by the standard monte carlo algorithms and the most widely used lattice sequence. there is optimization in the relative error as well as the computational complexity and number of operation necessary to compute the arisen multidimensional integrals. the obtained results will play an extremely principal multi-sided role. © the author(s), under exclusive license to springer nature switzerland ag 2024."
"machine learning (ml) as a service has emerged as a rapidly expanding field across various industries like healthcare, finance, marketing, retail and e-commerce, industry 4.0, etc where a huge amount of data is generated. to handle this amount of data, huge computational power is required for which cloud computing used to be the first choice. however, there are several challenges in cloud computing like limitations of bandwidth, network connectivity, higher latency, etc. to address these issues, edge computing is prominent nowadays, where the data from sensor nodes is collected and processed on low-cost edge devices. as simple sensor nodes are not capable of handling complex computations of ml models, data from sensor nodes need to be transferred to some nearest edge devices for further processing. if this sensor data is related to some securitycritical application, the privacy of such sensitive data needs to be preserved both during communication from sensor node to edge device and computation in edge nodes. this increased need to perform edge-based ml on privacy-preserved data has led to a surge in interest in homomorphic encryption (he) due to its ability to perform computations on encrypted form of data. the highest form of he, fully homomorphic encryption (fhe), is capable of theoretically handling arbitrary encrypted algorithms but comes with huge computational overhead. hence, the implementation of such a complex encrypted ml model on a single edge node is not very practical in terms of latency requirements. our paper introduces a low-cost encrypted ml framework on a distributed edge cluster, where multiple low-cost edge devices (raspberry pi boards) are clustered to perform encrypted distributed k-nearest neighbours (knn) algorithm computations. our experimental result shows, knn prediction on standard wisconsin breast cancer dataset takes approximately 1.2 hours, implemented on a cluster of six pi boards, maintaining end-to-end data confidentiality of critical medical data without any requirement of costly cloud-based computation resource support. © 2024 by scitepress – science and technology publications, lda."
"learning from high dimensional data has been utilized in various applications such as computational biology, image classification, and finance. most classical machine learning algorithms fail to give accurate predictions in high dimensional settings due to the enormous feature space. in this article, we present a novel ensemble of classification trees based on weighted random subspaces that aims to adjust the distribution of selection probabilities. in the proposed algorithm base classifiers are built on random feature subspaces in which the probability that influential features will be selected for the next subspace, is updated by incorporating grouping information based on previous classifiers through a weighting function. as an interpretation tool, we show that variable importance measures computed by the new method can identify influential features efficiently. we provide theoretical reasoning for the different elements of the proposed method, and we evaluate the usefulness of the new method based on simulation studies and real data analysis. © 2023, the author(s), under exclusive licence to springer-verlag gmbh germany, part of springer nature."
"the book is designed as a reference text and explores the concepts and techniques of iot, artificial intelligence (ai), and blockchain. it also discusses the possibility of applying blockchain for providing security in various domains. the specific highlight of this book is focused on the application of integrated technologies in enhancing data models, better insights and discovery, intelligent predictions, smarter finance, smart retail, global verification, transparent governance, and innovative audit systems. the book discusses the potential of blockchain to significantly increase data while boosting accuracy and integrity in iot-generated data and ai-processed information. it elucidates definitions, concepts, theories, and assumptions involved in smart contracts and distributed ledgers related to iot systems and ai approaches. the book offers real-world uses of blockchain technologies in different iot systems and further studies its influence in supply chains and logistics, the automotive industry, smart homes, the pharmaceutical industry, agriculture, and other areas. it also presents readers with ways of employing blockchain in iot and ai, helping them to understand what they can and cannot do with blockchain. the book is aimed primarily at advanced undergraduates and graduates studying computer science, computer engineering, electrical engineering, information systems, computational sciences, artificial intelligence, and information technology. researchers and professionals will also find this book very useful. © 2024 selection and editorial matter, mohammad s. obaidat, padmalaya nayak and niranjan k. ray."
"the sparse matrix-vector multiplication (spmv) serves as a critical computational kernel within sparse linear system solvers, playing a vital role in diverse scientific, engineering, and application domains such as industrial engineering, robotics, finance, networking, transportation, medicine, and weather forecasting. this study introduces a classification-based predictive model for shared memory, aiming to optimize scheduling policies in terms of execution time. utilizing a dataset comprised of over 1000 matrices from 25 application domains, stored in block csr format, the model is trained on 67% of the data using decision tree, random forest, gradient boosting, and artificial neural network algorithms. results indicate that random forest exhibits superior accuracy with 20%, precision 4%, and recall 23% compared to other algorithms. moreover, the runtime scheduling policy consistently demonstrates the best execution time across all matrices and features, outperforming static, dynamic, and guided policies. the study provides valuable insights into optimizing spmv performance under various conditions, enhancing efficiency in solving sparse linear equation systems. © 2024 ieee."
"in recent years, the slope entropy (slopen) algorithm has been recognized as a critical tool for time series analysis and time series classification, particularly within biomedical signal processing. this paper presents a thorough literature review on the developments, applications, and advancements of the slope entropy algorithm, drawing extensively from the seminal work of dr. david cuesta frau in 2019. by meticulously examining the existing literature, we discuss the algorithm's potential for unveiling intricate dynamics inherent in time series data and its capability for enhancing signal quality recognition. we also explore the algorithm’s adaptability across various domains beyond biomedical applications, including finance and environmental monitoring. furthermore, we identify potential areas of improvement, such as computational efficiency and real-time processing capabilities, which could pave the way for novel applications and methodologies. this review culminates in providing a clear roadmap for researchers aiming to employ the slope entropy algorithm in novel settings, contributing to its continuous evolution and broader acceptance in the scientific community. © the author(s), under exclusive license to springer nature switzerland ag 2024."
"this article provides a systematic review of the theoretical and empirical academic literature on the development and extension of the log-periodic power law singularity (lppls) model, which is also known as the johansen–ledoit–sornette (jls) model or log-periodic power law (lppl) model. developed at the interface of financial economics, behavioral finance and statistical physics, the lppls model provides a flexible and quantitative framework for detecting financial bubbles and crashes by capturing two salient empirical characteristics of price trajectories in speculative bubble regimes: the faster-than-exponential growth of price leading to unsustainable growth ending with a finite crash-time and the accelerating log-periodic oscillations. we also demonstrate the lppls model by detecting the recent bubble status of the s&p 500 index between april 2020 and december 2022, during which the s&p 500 index reaches its all-time peak at the end of 2021. we find that the strong corrections of the s&p 500 index starting from january 2022 stem from the increasingly systemic instability of the stock market itself, while the well-known external shocks, such as the decades-high inflation, aggressive monetary policy tightening by the federal reserve, and the impact of the russia/ukraine war, may serve as sparks. this article is categorized under: applications of computational statistics > computational finance algorithms and computational methods > computational complexity statistical models > nonlinear models. © 2024 wiley periodicals llc."
"the financial markets are highly competitive settings that are impacted by various factors and industries. making the wrong choice can have a multiplicity of negative effects and set off a series of events that could destabilise the economy. intelligent models have been employed as instruments to support financial market decision-making in recent years.the accessibility and availability of data has created new theoretical and computational hurdles as well as unseen changes in the finance systems. financial markets have complex systems characteristics, making them hard to predict. while some recent research have attempted to predict financial markets using machine learning techniques, their performance in terms of financial returns has not been sufficient. we suggest using a 2 d- convolutional neural network (cnn) model to forecast the actions of financial markets. this strategy can eliminate biases induced by predetermined factors in technical indicators and the choosing of technical indications by constantly collecting features instead of utilising standard technical indicators that are predetermined. the results of the research show that cnn algorithm extracts more useful and flexible information than typical technical indicators and beats previous machine learning methods in regards to financial performance. © 2024 ieee."
"time series classification (tsc) is essential in fields like medicine, environmental science, and finance, enabling tasks such as disease diagnosis, anomaly detection, and stock price analysis. while machine learning models like recurrent neural networks and inceptiontime are successful in numerous applications, they can face scalability issues due to computational requirements. recently, rocket has emerged as an efficient alternative, achieving state-of-the-art performance and simplifying training by utilizing a large number of randomly generated features from the time series data. however, many of these features are redundant or non-informative, increasing computational load and compromising generalization. here we introduce sequential feature detachment (sfd) to identify and prune non-essential features in rocket-based models, such as rocket, minirocket, and multirocket. sfd estimates feature importance using model coefficients and can handle large feature sets without complex hyperparameter tuning. testing on the ucr archive shows that sfd can produce models with better test accuracy using only 10% of the original features. we named these pruned models detach-rocket. we also present an end-to-end procedure for determining an optimal balance between the number of features and model accuracy. on the largest binary ucr dataset, detach-rocket improves test accuracy by 0.6% while reducing features by 98.9%. by enabling a significant reduction in model size without sacrificing accuracy, our methodology improves computational efficiency and contributes to model interpretability. we believe that detach-rocket will be a valuable tool for researchers and practitioners working with time series data, who can find a user-friendly implementation of the model at https://github.com/gon-uri/detach_rocket. © the author(s) 2024."
"the quantum processors’ performance is predicted to surpass the traditional systems during this decade in computational performance and capabilities. this disruptive technology can significantly impact many industrial sectors in the long term. other than communication and mathematics, we expect the finance sector to be one of the first to receive the prosperity of this new cutting-edge technology. this paper provides a review of the current progress on quantum algorithms for financial applications, specifically focusing on the use cases that can be addressed through machine learning. © the author(s), under exclusive license to springer nature singapore pte ltd. 2024."
"although predictive ai models have grown to dominate computational finance, they are often limited in their applications when it comes to studying interventions and explaining behavioral outcomes. financial economics, on the other hand, has a rich history of analytical approaches to asset-pricing theory, often requiring sweeping assumptions. in this paper, we construct an agent-based model of asset markets that is able to dispense with onerous restrictions on agent behaviors and beliefs, while having analytical validity and providing insights into the functioning of asset markets. in particular, we evaluate our models with respect to several traditional financial economic theories like tobin's separation theorem and the capital asset pricing model (capm). we devise a network representing trades to show the emergence of different roles played by the agents. we study interventions, such as shocks, and explain the outcomes using our model. finally, we investigate the effects of noise trading and show that noisy agents converge to different equilibrium points due to their differences in beliefs. put together, this paper presents an agent-based model that can be used to study the effects of heterogeneous beliefs and risks of the agents and shocks to assets at a systemic level, thereby connecting localized agent and asset characteristics to global or collective outcomes. © 2024 international foundation for autonomous agents and multiagent systems."
"in recent years, the convergence of artificial intelligence (ai) and machine learning (ml) with cloud computing has sparked a revolution in the way businesses process, analyze, and utilize data. this synergy has paved the way for unprecedented advancements in various industries, from healthcare to finance, manufacturing to entertainment. this chapter explores the profound impacts of ai and ml integration in cloud computing, dissecting their implications on scalability, efficiency, security, and innovation. the integration of ai and ml algorithms within cloud computing infrastructures has led to a paradigm shift in the processing and analysis of large-scale datasets. leveraging the extensive computational power and storage capabilities of cloud platforms, ai-driven models have demonstrated remarkable proficiency in tasks ranging from image and speech recognition to natural language processing. this has empowered businesses to extract valuable insights and automate complex processes, significantly enhancing operational efficiency. © 2024, igi global. all rights reserved."
"this paper provides a bubble date-stamping mechanism using the agent-based computational finance method. the key steps of the bubble date-stamping mechanism are the construction of the simulated financial market, the computation of the characteristic indexes, and the thresholds of the price band in the simulated financial market. the present study adopts the mechanism to identify the bubbles of sample stocks in the chinese stock market from april 2003 to december 2019. the findings show that the bubbles are primarily distributed in 2006–2008, 2009–2012 and 2014–2018, respectively. furthermore, we analyse the bubble strength and the price fluctuation during the above three periods. in addition to the bubble date-stamping mechanism, the paper also studies the factors that drive the bubbles in the chinese stock market from both macro and micro perspectives. ©2023 john wiley & sons australia, ltd."
"low accuracy of manual inspections, delay in detecting defects, and shortage of skilled resources are challenges in tracing the defects in welding technology. it is very expensive and wasteful to pull apart welded automobile bodies to check the welding quality using typical quality monitoring techniques. on the other hand, machine learning (ml)-based techniques will assist in minimizing waste and supporting a more profitable and sustainable manufacturing sector. because they have robust modeling capabilities and don’t require explicit programming, ml technologies are drawing significant growing interest in contemporary industrial industries. the development of computational learning methods and pattern recognition within artificial intelligence has led to the development of ml technology. the use of ml learning techniques has grown over the past few years for a variety of industries, including smart cities, finance, banking, and education. because of numerous new cutting-edge computing approaches, today’s machine learning differs from the machine learning of the past. machine learning and data science techniques have been widely used in the manufacturing sector over decades to optimize the mechanical and microstructure properties of manufactured mechanical components. © 2024 scrivener publishing llc."
"the quick emergence in the quantity of data produced through the linked devices of internet of things (iot) models opened the novel potential to improve service qualities for budding tools considering data sharing. however, privacy problems are main issues of data providers for sharing data. the outflow of confidential data causes severe problems beyond the loss in finance of providers. a blockchain-based secured data-sharing model is devised for dealing with various kinds of parties. thus, data-sharing issue is modeled as a machine learning issue by adapting federated learning (fl). here, data privacy is controlled by sharing data in spite of exposing genuine data. at last, the fl is combined in consensus task of permissioned blockchain for accomplishing federated training. here, the data model learning is executed using a deep maxout network (dmn), which is trained using jellyfish search african vultures optimization (jsavo). moreover, the data-sharing records are generated to share data amid data providers and requestors. the proposed jsavo-based dmn outperformed with better accuracy of 93.3%, fpr of 0.054, loss function of 0.067, mean square error (mse) of 0.346, mean average precision of 94.6, rmse of 0.589, computational time of 17.47 s, and memory usage of 48.62 mb. © world scientific publishing europe ltd."
"this research paper delves into the realm of financial market forecasting, specifically focusing on predicting intraday trend reversals in index derivatives using supervised machine learning algorithms. the study encompasses a comprehensive examination of various machine learning techniques, including support vector machines, random forests, xgboost, and lstm, to develop models capable of navigating the complexities inherent in the financial markets.the primary objective of the research is to enhance the predictive accuracy of stock market movements by incorporating a range of factors such as market conditions, liquidity, and external influences. this multifaceted approach aims to capture the dynamic and often unpredictable nature of financial markets, offering a more nuanced and effective prediction model.through meticulous analysis and evaluation, the paper demonstrates the significant potential of machine learning technologies in the field of computational finance. it explores the strengths and limitations of each algorithm, providing an in-depth understanding of their applicability in real-world market scenarios.furthermore, the research identifies key areas for future exploration, emphasizing the need for a more detailed examination of macroeconomic and sociopolitical factors, as well as the utilization of high-frequency data, particularly in emerging markets. these insights pave the way for ongoing advancements in the application of machine learning for financial market analysis.overall, this paper makes a notable contribution to the field of computational finance, offering valuable perspectives and tools for academics and practitioners alike. it lays the groundwork for further research that aims to refine and expand the use of machine learning in stock market prediction, ultimately leading to more robust and versatile forecasting models. © 2024, ismail saritas. all rights reserved."
"reliable estimates of volatility and correlation are fundamental in economics and finance for understanding the impact of macroeconomics events on the market and guiding future investments and policies. dependence across financial returns is likely to be subject to sudden structural changes, especially in correspondence with major global events, such as the covid-19 pandemic. in this work we are interested in capturing abrupt changes over time in the conditional dependence across u.s. industry stock portfolios, over a time horizon that covers the covid-19 pandemic. the selected stocks give a comprehensive picture of the u.s. stock market. to this end, we develop a bayesian multivariate stochastic volatility model based on a time-varying sequence of graphs capturing the evolution of the dependence structure. the model builds on the gaussian graphical models and the random change points literature. in particular, we treat the number, the position of change points, and the graphs as object of posterior inference, allowing for sparsity in graph recovery and change point detection. the high dimension of the parameter space poses complex computational challenges. however, the model admits a hidden markov model formulation. this leads to the development of an efficient computational strategy, based on a combination of sequential monte-carlo and markov chain monte-carlo techniques. model and computational development are widely applicable, beyond the scope of the application of interest in this work.food, tobacco, textiles, apparel, leather, toys cars, tvs, furniture, household appliances machinery, trucks, planes, chemicals, off furn, paper oil, gas, and coal extraction and products computers, software, and electronic equipment telephone and television transmission wholesale, retail, and some services healthcare, medical equipment, and drugs utilities. © institute of mathematical statistics, 2024."
"ai and machine learning, powered by data and computational prowess, present a promising path for stock price prediction in the quickly changing financial scene. accurate models that comprehend the complicated, nonlinear dynamics of the stock market are required due to its increasing complexity and volatility. the stock market prediction app, powered by lstm neural networks, is now available. our goal is to accurately anticipate the nse stock's closing price the following day using a combination of nine carefully chosen predictors from market fundamentals, macroeconomics, and technical indicators. both single-layer and multilayer lstm models attentively adapted to input variables, are included in our method. a thorough analysis that considers measures like rmse, mape, and correlation coefficient highlights the single-layer lstm as the best performance. this study emphasizes the mutually beneficial interaction between finance and cutting-edge machine learning, highlighting the revolutionary impact of lstm in understanding the intricacies of the stock market. our main objective is to give traders, investors, and financial fans the tools they need to safely negotiate the complex landscape of the modern stock market. by bridging the gap between sound monetary judgement and cutting-edge technology, our stock market prediction app helps users make well-informed decisions. © the author(s), under exclusive license to springer nature singapore pte ltd. 2024."
"long-term time series forecasting is a critical task in many domains, including finance, healthcare, and weather forecasting. while transformer-based models have made significant progress in time series forecasting, their high computational complexity often leads to compromises in model design, limiting the full utilization of temporal information. to address this issue, we propose a novel hierarchical decomposition framework that disentangles latent temporal variation patterns. specifically, we decompose time series into trend and seasonal modes and further decompose seasonal temporal changes into coarse- and fine-grained states to capture different features of temporal sequences at different granularities. we use linear layers to embed local information for capturing fine-grained temporal changes and fourier-domain attention to capture multi-periodic seasonal patterns to extract coarse-grained temporal dependency information. this forms a time series forecasting modeling from fine to coarse, and from local to global. extensive experimental evaluation demonstrates that the proposed approach outperforms state-of-the-art methods on real-world benchmark datasets. © the author(s), under exclusive license to springer nature singapore pte ltd 2024."
"we consider time-inhomogeneous hawkes processes with an exponential kernel, and we analyze some properties of the model. time-inhomogeneity for the hawkes process is indispensable for short rate models or for other calibration purposes, while financial applications for the time-homogeneous case already well known. distributional properties for such a model generate computational tractability for a financial application. in this paper, moments and the laplace transform of time-inhomogeneous hawkes processes are obtained from the distributional properties of the underlying processes. as an applications to finance, we investigate the pricing formula for zero-coupon bonds when short-term interest rates are governed by the time-inhomogeneous hawkes process. numerical illustrations are also provided. as an illustrative example, we apply the derived moments and laplace transform of time-inhomogeneous hawkes processes to the pricing of zero-coupon bonds within a financial context. by considering the short-term interest rate as driven by inhomogeneous hawkes processes, we develop explicit formulae for valuing zero-coupon bonds. this application is particularly relevant for modeling interest rate dynamics in real-world scenarios, allowing for a more nuanced understanding of pricing dynamics. through numerical illustrations, we demonstrate the computational tractability of our approach, showcasing its practical utility for financial practitioners and providing insights into the intricate interplay between time-inhomogeneous hawkes processes and bond pricing in dynamic markets. © 2024 the author(s), licensee aims press."
"federated learning (fl) combined with differential privacy (dp) is widespread in healthcare, finance, and iot due to its advantages in multi-client data distribution. however, existing fl approaches overlook the differential impact levels among clients and data redundancy issues, resulting in high computational overhead and limited real-time applicability. additionally, non-independent identical distribution (non-iid) and imbalanced datasets in multi-clients pose challenges in privacy preservation and model overfitting. therefore, we propose a balanced weight strategy for multi-stage federated learning against multi-client data skewing, called bm-fl, which involves clients, intermediate trust servers (itss), and the central server (cs). first, to protect data privacy, an improved laplace epsilonϵ-differential privacy method is employed. second, a novel generative adversarial network (gan) called bc-gan is introduced. it is used to generate realistic fake samples and maintain a balanced proportion of samples across different categories. then, to make full use of each client's valuable data, we designe a balanced weight strategy. moreover, extensive experimental results clearly demonstrate the effectiveness of bm-fl in efficiently handling classification tasks involving non-iid and imbalanced datasets while maintaining privacy and security. furthermore, our method attains superior classification accuracy with fewer training epochs compared to relevant classical algorithms.  © 1989-2012 ieee."
"image encryption is a critical component of modern data security, ensuring the confidentiality, integrity, and privacy of sensitive visual content. in this paper, we present a comprehensive survey on image encryption, exploring various encryption algorithms, their strengths, weaknesses, and real-world applications. we begin by providing a background on image encryption, highlighting its importance in safeguarding image data from unauthorized access and tampering. we discuss symmetric, asymmetric, and hybrid encryption techniques, analyzing their suitability for different scenarios. evaluation metrics for assessing encryption algorithms are discussed, emphasizing the importance of selecting appropriate metrics to measure security and performance. additionally, we explore the challenges faced in image encryption, such as key management and computational complexity. the survey also delves into potential future directions in image encryption, including robustness against cryptanalysis, quantum image encryption, and multimedia encryption. furthermore, we discuss the importance of image encryption in various industries, such as military, healthcare, finance, journalism, and intellectual property protection. real-world use cases are presented, highlighting scenarios where image encryption is crucial for maintaining confidentiality, integrity, and privacy. finally, we conclude by summarizing the survey findings and identifying potential areas for further research and improvement in image encryption. this comprehensive survey serves as a valuable resource for researchers, practitioners, and decision-makers in the field of image security, facilitating the development of more secure and efficient image encryption solutions to meet the increasing demand for data protection and privacy in the digital age. © 2023"
"emerging economies, while exhibiting higher growth rates compared to developed countries, are susceptible to external shocks, leading to financial fragility. traditional analysis methods often fall short in accuracy and timeliness. this research introduces a novel approach utilizing back-propagation neural network (bpnn) to predict financial fragility in emerging markets, focusing on the brics countries. by considering twelve impactful factors and employing principal component analysis (pca), five key influencers are identified. the bpnn model is iteratively optimized to achieve superior quality. historical data validation attests to the model’s effectiveness. the study identifies five critical factors influencing financial fragility: gdp growth rate, inflation rate, monetary policy, interest rates, and bank’s capital-asset ratio. among these, gdp growth rate emerges as a significant determinant. positive growth is correlated with financial stability, while a slowdown or negative growth signals elevated risks. emerging markets are particularly vulnerable to global economic fluctuations due to their reliance on exports and foreign capital. additionally, weaker financial systems amplify their susceptibility to shocks.the research underscores the importance of building robust financial sectors, replenishing funding buffers, and proactively managing distressed assets in emerging market economies. the proposed bpnn model provides a powerful tool for risk prediction, though it requires strong indicator data support. while computational intensity and interpretability remain challenges, the benefits of bpnns outweigh these limitations. effective communication and information exchange across countries and markets are crucial for maintaining stability in emerging market finance. this study contributes valuable insights into the prediction of financial fragility in emerging markets, offering a comprehensive framework for policymakers and financial practitioners to navigate the challenges and opportunities presented by these dynamic economies. © 2024, the author(s), under exclusive licence to springer science+business media, llc, part of springer nature."
"in this paper, we presented a quadratic interpolated beetle antennae search (qibas), a variant of the beetle antennae search (bas) algorithm to solve the higher dimensional portfolio selection problem. the computational efficiency of bas and its probabilistic global convergence made it viable to solve real-world optimization-based problems. despite its numerous application, it is less accurate, not scalable, and its performance deteriorates as the dimension of the problem increases. to overcome this, qibas integrates bas with the robust approximation of quadratic interpolation. we employed qibas to a well-known finance problem known as portfolio selection as a testbed. traditionally, the portfolio problem is modeled as a convex optimization problem, which is efficient to solve but inaccurate. the cardinality constrained model with higher dimensional stock data includes stringent real-world constraints. it is more accurate but computationally challenging and not tractable, making it a perfect candidate to test qibas. the primary goal is to minimize the risk and maximize the profit while selecting the portfolio. we included up to 250 companies in simulation and compared the results with bas and two state-of-the-art swarm metaheuristic algorithms, i.e., particle swarm optimization and genetic algorithm. the results showed the promising performance of qibas in comparison with other algorithms. © 2022, the author(s), under exclusive licence to springer science+business media, llc, part of springer nature."
"blockchain technology-hype or fad? this is one of the most popular questions multitudes of technology enthusiasts, policymakers, and businesses have asked. the technology has been projected to have a promising future in spite of its current state of experts beginning to rediscover its true purpose and use case success. a number of domains of discourse such as supply chain, banking and finance, as well as land management have chalked appreciable levels of success with the technology. in this paper, we delve into the world of sports to identify the state of the art with respect to blockchain technology in major sporting activities. the article adopts a computational literature review together with bibliometric analysis methodologies to extract knowledge from scientific publications on the aforementioned goal. finally, our work contributes to knowledge and practice by providing a snapshot of blockchain technology within the sphere of sports. © 2024 by university of niš, serbia | creative commons license: cc by-nc-nd."
"stock market prediction holds significant importance in the world of finance, captivating the attention of both investors and financial researchers. the integration of artificial intelligence and advancements in computational power has led to substantial improvements in predicting stock prices, surpassing the effectiveness of traditional programmed prediction methods. in this paper, we explore three distinct and innovative methods for stock price prediction: long short-term memory (lstm), lstm combined with simple moving average (lstm-sma), and lstm combined with exponential moving average (lstm-ema). our analysis is conducted using a comprehensive historical dataset of apple’s stock prices, and the performance of each model is rigorously evaluated using critical metrics, including mean absolute percentage error (mape), mean absolute error (mae), root mean square error (rmse), and r2 score. additionally, the training time for each model is taken into account. the results show that all three models lstm, lstm-sma, and lstm-ema give good prediction results for apple’s stock price, in which the lstm model gives the best prediction results for the 21-day cluster. however, in terms of computational time, the lstm-sma model and lstm-ema model are more efficient than the lstm model. these findings highlight the potential of integrating advanced techniques to achieve more accurate and efficient stock price predictions. © icst institute for computer sciences, social informatics and telecommunications engineering 2024."
"quantum computing is gaining popularity across a wide range of scientific disciplines due to its potential to solve long-standing computational problems that are considered intractable with classical computers. one promising area where quantum computing has potential is in the speed-up of np-hard optimisation problems that are common in industrial areas such as logistics and finance. newcomers to the field of quantum computing who are interested in using this technology to solve optimisation problems do not have an easily accessible source of information on the current capabilities of quantum computers and algorithms. this paper aims to provide a comprehensive overview of the theory of quantum optimisation techniques and their practical application, focusing on their near-term potential for noisy intermediate scale quantum devices. the paper starts by drawing parallels between classical and quantum optimisation problems, highlighting their conceptual similarities and differences. two main paradigms for quantum hardware are then discussed: analogue and gate-based quantum computers. while analog devices such as quantum annealers are effective for some optimisation problems, they have limitations and cannot be used for universal quantum computation. in contrast, gate-based quantum computers offer the potential for universal quantum computation, but they face challenges with hardware limitations and accurate gate implementation. the paper provides a detailed mathematical discussion with references to key works in the field, as well as a more practical discussion with relevant examples. the most popular techniques for quantum optimisation on gate-based quantum computers, the quantum approximate optimisation algorithm and the quantum alternating operator ansatz framework, are discussed in detail. however, it is still unclear whether these techniques will yield quantum advantage, even with advancements in hardware and noise reduction. the paper concludes with a discussion of the challenges facing quantum optimisation techniques and the need for further research and development to identify new, effective methods for achieving quantum advantage. © 2023 the author(s). published by iop publishing ltd."
"as data is becoming more and more important, internet of things (iot) devices are widely used to collect information and process data from various industries, such as finance, autonomous driving, and smart factories. to address the limited computational power of iot devices in processing real-time data, both edge computing, which utilizes nearby computers with greater computation capabilities, and cloud computing with even more processing power, are widely adopted solutions. as these systems have heterogeneous software and hardware configurations, it can be challenging to understand the behavior of the application from the perspective of different resources. in this article, we propose an efficient logging and monitoring system in large-scale, heterogeneous environments for iot and edge applications. to do this, our scheme first collects system resource usage data from each compute node using the operating system's native system analysis tool. then, it consolidates the system resource usage information from multiple nodes into an integrated database which creates a comprehensive view of the system. finally, our scheme provides global system resource information in terms of specific jobs and nodes, providing a comprehensive understanding of complex heterogeneous hardware/software stacks. our evaluation, using iot and edge workloads in heterogeneous systems, demonstrates the efficiency of logging and monitoring schemes. the average network usage for windows and linux is 0.12 and 1.29 kb/s, respectively, resulting in minimal network overhead. in addition, the proposed scheme shows negligible overhead in terms of both runtime (up to 0.73%) and storage (0.0474%).  © 2014 ieee."
"economists have conducted research on several empirical phenomena regarding the behavior of individual investors, such as how their emotions and opinions influence their decisions. all those emotions and opinions are described by the word sentiment. in finance, stochastic changes might occur according to investors sentiment levels. in this study, our main goal is to apply several operational research techniques and analyze these techniques’ accurance. firstly, we represent the mutual effects between some financial process and investors sentiment with multivariate adaptive regression splines (mars) model. furthermore, we consider to extend this model by using distinct data mining techniques and compare the gain in accuracy and computational time with its strong alternatives applied in the analyses of the financial data. hence, the goal of this study is to compare the forecasting performance of sentiment index by using two-stage mars-nn (neural network), mars-rf (random forest), rf-mars, rf-nn, nn-mars, and nn-rf hybrid models. furthermore, we aim to classify the peoples’ feelings about economy according to their confidence levels. moreover, to forecast the underlying state change of the consumer confidence index (cci) and to observe the relationship with some macroeconomic data (cpi, gdp and currency rate) at a monthly interval, we apply hidden markov model (hmm). the aim is to detect the switch between these states and to define a path of these states. we also aim to use volatility models for mainly sentiment index, consumer confidence index, and other indices so that we can get better forecasting results from those datasets. © 2024, the author(s), under exclusive licence to springer-verlag gmbh germany, part of springer nature."
"this chapter discusses approaches and methods used in critical studies of the advertising and promotional industries and their interrelationship with media industries. the first part offers a brief review of approaches to the critical study of advertising industries, including the frankfurt school and various economic and cultural (para)marxist approaches from the 1960s to 1970s. then it discusses how work, especially from the 1980s, began to examine the advertising industry in greater detail, including global corporate restructuring, institutional practices and practitioners, lobbying and influence on governance and (de)regulation, as well as advertising finance and influence on media. the second part discusses how the critical analysis of governance can integrate important dimensions of the study of industrial arrangements and practices and the processes of rule-making and rule-shaping that contribute to structuring them. this advances an approach that seeks to integrate macro and micro levels of analysis, and expand beyond more formal legal-regulatory analysis to trade and general media, industry, and stakeholder discourses and behavior, using innovative research methods, including computational data analysis (cda). the chapter also discusses media-advertising industries and governance analysis from the global south, including the review of work in part one and relevant research on branded content in part two. © 2024 selection and editorial matter, joan pedro-carañana, rodrigo gómez, thomas f. corrigan and francisco sierra caballero; individual chapters, the contributors."
"this research offers an in-depth comparative analysis of various pre-trained convolutional neural network (cnn) models such as vgg16, resnet50, inceptionv3, mobilenetv2, and xception to predict stock market trends. our approach involves the conversion of time-series financial data into 2d image-like structures through the application of two distinct techniques: the gramian angular field (gaf) and the markov transition field (mtf). by applying this transformation, we leverage the power of cnns. we utilize the ideas of transfer learning and try to evaluate the performance of each model using several measures including predictive accuracy, precision, recall, f1-score, and computational efficiency. the analysis highlights the unique advantages and limitations of each model, thereby offering valuable insights into their suitability for stock market prediction tasks. this study is a significant contribution to the current body of literature on financial time series forecasting, providing a novel perspective on using pre-trained cnn models in the indian financial sector. it carries important implications for future work and practitioners in the finance and investment sectors, offering a tool for more e-market predictions. © 2024, the author(s), under exclusive license to springer nature switzerland ag."
"image encryption is a fundamental component of modern data security that guarantees the integrity, privacy, and confidentiality of sensitive visual content. this paper provides a thorough examination of image encryption, comparing and contrasting different encryption techniques and their benefits and limitations as well as real-world uses. in order to strengthen image data security against unwanted access and manipulation, we first offer some basic understanding of image encryption. the suitability of symmetric, asymmetric, and hybrid encryption techniques in various contexts is examined. we also explore the assessment criteria used to evaluate encryption algorithms, highlighting the significance of using suitable measures to precisely gauge security and effectiveness. we also discuss common issues with image encryption, like complicated key management and computational complexity. the review also explores future directions that picture encryption might take, including multi-media encryption techniques, resistance to cryptanalysis, and quantum image encryption. we also highlight how important image encryption is in a variety of fields, such as finance, healthcare, journalism, intellectual property protection, and military operations. we will conduct a thorough analysis of current cryptography schemes and multimedia encryption algorithms to provide a comprehensive overview of the current security landscape tailored to digital multimedia technology. the findings from this survey will enhance our understanding of the efficacy and dependability of secure multimedia encryption schemes, ultimately aiding in the development of more efficient and robust encryption methods for the future. this article aims to summarize and assess numerous algorithms using different methodologies based on multiple characteristics such as mse, psnr, nc, ber, and so on.  © 2024 ieee."
"ensemble methods such as bagging and random forests are ubiquitous in various fields, from finance to genomics. despite their prevalence, the question of the efficient tuning of ensemble parameters has received relatively little attention. this article introduces a cross-validation method, extrapolated cross-validation (ecv), for tuning the ensemble and subsample sizes in randomized ensembles. our method builds on two primary ingredients: initial estimators for small ensemble sizes using out-of-bag errors and a novel risk extrapolation technique that leverages the structure of prediction risk decomposition. by establishing uniform consistency of our risk extrapolation technique over ensemble and subsample sizes, we show that ecv yields δ-optimal (with respect to the oracle-tuned risk) ensembles for squared prediction risk. our theory accommodates general predictors, only requires mild moment assumptions, and allows for high-dimensional regimes where the feature dimension grows with the sample size. as a practical case study, we employ ecv to predict surface protein abundances from gene expressions in single-cell multiomics using random forests under a computational constraint on the maximum ensemble size. compared to sample-split and k-fold cross-validation, ecv achieves higher accuracy by avoiding sample splitting. meanwhile, its computational cost is considerably lower owing to the use of the risk extrapolation technique. supplementary materials for this article are available online. © 2024 american statistical association and institute of mathematical statistics."
"disruptive technologies have ignited a profound transformation in the landscape of computational finance. as the realms of finance integrate cutting-edge technologies like quantum computing, artificial intelligence, blockchain, and big data analytics, the limits of financial possibilities are continually being pushed further. as artificial intelligence, blockchain, big data analytics, and quantum computing find their place in the financial realm, the boundaries of what is possible in finance are constantly expanding. this chapter explores the impact of these disruptive technologies on computational finance, dissecting their applications, benefits, challenges, and ethical considerations. by navigating this ever-evolving intersection of technology and finance, the authors gain insights into a future where financial services are more efficient, accessible, and adaptable than ever before. the chapter analyzes the tools for better understanding of the technology in finance domain and provide the trends of research in computational finance space using bibliometric analysis. © 2024, igi global. all rights reserved."
"avitourism depends on the understanding of birds and the intention of birdwatchers to see or hear a specific species. increase of this activity strengthens the economy of communities and helps finance biodiversity conservation projects. technological products that incorporate traditional and modern tools for signal and image processing facilitate the tracking, classification and observation of birds. this article has two approaches. the first one proposes and evaluates a lightweight classification model that uses feature vector extracted from the bird's song spectrum and is based on comparison of euclidean distance between sample features and a set vector by species. the second approach adapts and evaluates convolutional neural network architectures for bird classification using the spectrogram of the bird's song. the methodology applied in both approaches consists of: pre-processing, feature extraction, classification and evaluation metrics. the main results are the feasibility of the proposed lightweight classification model with an accuracy of 0.8 and a loss of 2.32, and the feasibility of using convolutional neural networks with an accuracy above 0.9 and a loss of less than 1, in the resnet50, vgg19, and inceptionv3 architectures, this using as a minimum 30 spectrograms per species during training. it is concluded that the model that best meets the needs of fewer samples and less computational resources required for training is resnet50. additionally, it is discussed to combine the two approaches in a hierarchical and hybrid classification model that allows to introduce in the reduction and classification layers of the neural network features of another type and of other sources. © 2024 the author(s). this is an open access article distributed under the terms of the creative commons attribution license (cc by 4.0), which permits unrestricted distribution provided the original author and source are cited. all rights reserved."
"computational finance plays a pivotal role in addressing the challenges posed by the dynamic and interconnected nature of financial markets. by combining mathematical rigor with computational power, this field contributes to the development of innovative solutions for pricing, risk management, and decision-making in the financial industry. computational finance is a field that leverages mathematical techniques, statistical methods, and computational tools to analyze and solve complex financial problems. in recent years, the intersection of finance and technology has given rise to a transformative wave, reshaping the landscape of traditional financial systems. this study delves into the dynamic realm of computational finance, exploring the profound impact of disruptive technologies on established practices and processes of computational finance. © 2024, igi global. all rights reserved."
"the proliferation of decentralised finance (defi) and decentralised autonomous organisations (dao), which in current form are exposed to front-running of token transactions and proposal voting, demonstrate the need to shield user inputs and internal state from the parties executing smart contracts. in this work we present “eagle”, an efficient uc-secure protocol which efficiently realises a notion of privacy preserving smart contracts where both the amounts of tokens and the auxiliary data given as input to a contract are kept private from all parties but the one providing the input. prior proposals realizing privacy preserving smart contracts on public, permissionless blockchains generally offer a limited contract functionality or require a trusted third party to manage private inputs and state. we achieve our results through a combination of secure multi-party computation (mpc) and zero-knowledge proofs on pedersen commitments. although other approaches leverage mpc in this setting, these incur impractical computational overheads by requiring the computation of cryptographic primitives within mpc. our solution achieves security without the need of any cryptographic primitives to be computed inside the mpc instance and only require a constant amount of exponentiations per client input. © 2024, international financial cryptography association."
"large language models (llms) have exhibited an array of reasoning capabilities but face challenges like error propagation and hallucination, particularly in specialised areas like finance, where data is heterogeneous, and precision is paramount. we explore the potential of language model augmentation with external tools to mitigate these limitations and offload certain reasoning steps to external tools that are more suited for the task, instead of solely depending on the llm’s inherent abilities. more concretely, using financial domain question-answering datasets, we apply supervised fine-tuning on a llama-2 13b chat model to act both as a task router and task solver. the task router dynamically directs a question to either be answered internally by the llm or externally via the right tool from the tool set. our tool-equipped sft model, raven, demonstrates an improvement of 35.2% and 5.06% over the base model and sft-only baselines, respectively, and is highly competitive with strong gpt-3.5 results. to the best of our knowledge, our work is the first that investigates tool augmentation of language models for the finance domain. © 2024 association for computational linguistics."
"lexicon-based sentiment analysis in finance leverages specialized, manually annotated lexicons created by human experts to extract sentiment from financial texts effectively. although lexicon-based methods are simple to implement and fast to operate on textual data, they require considerable manual annotation efforts to create, maintain, and update the lexicons. these methods are also considered inferior to the deep learning-based approaches, such as transformer models, which have become dominant in various natural language processing (nlp) tasks due to their remarkable performance. however, their efficacy comes at a cost: these models require extensive data and computational resources for both training and testing. additionally, they involve significant prediction times, making them unsuitable for real-time production environments or systems with limited processing capabilities. in this paper, we introduce a novel methodology named explainable lexicons (xlex) that combines the advantages of both lexicon-based methods and transformer models. we propose an approach that utilizes transformers and shapley additive explanations (shap) for explainability to automatically learn financial lexicons. our study presents four main contributions. firstly, we demonstrate that transformer-aided explainable lexicons can enhance the vocabulary coverage of the benchmark loughran-mcdonald (lm) lexicon. this enhancement leads to a significant reduction in the need for human involvement in the process of annotating, maintaining, and updating the lexicons. secondly, we show that the resulting lexicon outperforms the standard lm lexicon in sentiment analysis of financial datasets. our experiments show that xlex outperforms lm when applied to general financial texts, resulting in enhanced word coverage and an overall increase in classification accuracy by 0.431. furthermore, by employing xlex to extend lm, we create a combined dictionary, xlex+lm, which achieves an even higher accuracy improvement of 0.450. thirdly, we illustrate that the lexicon-based approach is significantly more efficient in terms of model speed and size compared to transformers. lastly, the proposed xlex approach is inherently more interpretable than transformer models. this interpretability is advantageous as lexicon models rely on predefined rules, unlike transformers, which have complex inner workings. the interpretability of the models allows for better understanding and insights into the results of sentiment analysis, making the xlex approach a valuable tool for financial decision-making.  © 2013 ieee."
"after the great turmoil of the latest financial crisis, the criticism of the regulatory frameworks became increasingly stronger. the rules that banks needed to comply with are presumed to be procyclical and unable to prevent and mitigate the extent of strong financial and economic cycles. as a result, basel iii introduced a set of macroprudential tools to overcome these regulatory shortfalls. one tool that strives to counteract the issue of procyclicality is the countercyclical capital buffer (ccyb). this paper introduces a heterogeneous agent-based model that investigates the implication of the new regulatory measure. we develop a housing and a financial market where economic agents trade residential property that is financed by financial institutions. to examine the macroeconomic performance of the ccyb, we evaluate the dynamics of key stability indicators of the housing and the financial market under four different market conditions: in an undisturbed market and in times of three different structural shocks. computational experiments reveal that the ccyb is effective in stabilizing the housing and the financial market in all market settings. the new macroprudential tool helps to mitigate economic fluctuations and to stabilize market conditions, especially in the aftermath of a crisis. it is not able to prevent any of the crises tested. however, the extent of the stabilizing effect varies according to market conditions. in the shock scenarios, the ccyb performs better in dampening market fluctuations and increasing banking soundness than in the base scenario. © the author(s) 2024."
"with the advent of the era of big data, privacy computing analyzes and calculates data on the premise of protecting data privacy, to achieve data ‘available and invisible’. as an important branch of secure multi-party computation, the geometric problem can solve practical problems in the military, national defense, finance, life, and other fields, and has important research significance. in this paper, we study the similarity problem of geometric graphics. first, this paper proposes the adjacency matrix vector coding method of isomorphic graphics, and use the paillier variant encryption cryptography to solve the problem of isomorphic graphics confidentiality under the semi-honest model. using cryptography tools such as elliptic curve cryptosystem, zero-knowledge proof, and cut-choose method, this paper designs a graphic similarity security decision protocol that can resist malicious adversary attacks. the analysis shows that the protocol has high computational efficiency and has wide application value in terrain matching, mechanical parts, biomolecules, face recognition, and other fields. © 2023, the author(s)."
"the processing, security, and storage of real-world applications have been made possible by recent developments in big computing. this study investigates the many methods and tools that have been created and used to raise the efficacy and efficiency of large-scale computing systems. the need to manage enormous volumes of data and carry out complicated computational operations in various industries, such as education, healthcare, finance, energy, retail, and transportation, is driving this development. big computing’s potential must be fully realized, so it’s important to understand practical applications that might boost various companies’ bottom lines. this chapter examines current findings in the field of massive computing methods based on practical applications and covers the terms and procedures associated with big data in connection to a number of different factors, including performance data techniques, and the efficacy of current models used. the chapter ends with a summary of big data processing frameworks, future research objectives, and a discussion of the advantages and disadvantages of big data. © 2024 selection and editorial matter, tanvir habib sardar and bishwajeet kumar pandey; individual chapters, the contributors."
"economic credit scoring (cs), which aids in calculating the credit worth of both individuals and companies, is regarded as one of the greatest study issues in the finance field. in the banking industry, data mining techniques are believed to be helpful since they help designers and developers create appropriate goods or services for customers with the fewest possible risks. losses and loan cancellations, which are the major sources of hazards in the banking industry, are related to credit risks. a support vector machine based architecture is presented in the current study for the financial credit score prediction system. however, the existing work tends to have increased computational overhead and that requires complete data for attaining required accurate rate. the system known as the fuzzy support vector machine based outlier detection system is introduced in the suggested research study to address this (fsvm-ods). this study's first grouping of data items utilising a hybrid genetic algorithm with k-means clustering algorithm is named (hkga). the dataset must be gradually lowered in size, and calculation time must likewise be decreased. the enhanced z-score (ezs) outlier identification (od) technique was employed in the second step to identify outliers in the dataset. then, we use a customized beaver searching method to choose the database. for categorization of the datasets, a fuzzy support vector machine is utilised. the whole study project is carried out in the matlab simulation environment, and it has been shown that the suggested technique achieves a higher outlier identification rate than the current methodology. © 2023, innovative information science and technology research group. all rights reserved."
"the convergence of computational intelligence (ci) and blockchain (bc) has garnered significant interest in recent years, presenting a promising avenue for enhancing the efficiency and security of distributed applications. this research paper explores the synergistic potential of ci and bc in the context of distributed applications, examining the benefits and challenges that arise from their integration. the study begins by providing a comprehensive overview of both ci and blockchain, highlighting their individual strengths and limitations. subsequently, the paper delves into the various ways in which ci can be harnessed to optimize the performance of bc-based systems, including consensus mechanisms, smart contract execution, and data validation. moreover, it analyses the novel approaches enabled by ci, such as adaptive optimization and intelligent resource allocation, that address scalability and energy consumption issues that often plague bc networks. furthermore, the research highlights the security advantages of leveraging ci in bc applications, exploring how intelligent algorithms can enhance privacy, prevent fraud, and mitigate attacks. it also discusses the challenges encountered in integrating ci into bc ecosystems, including computational overhead, algorithmic biases, and potential ethical concerns. the chapter synthesizes empirical evidence and case studies to illustrate the real-world impact of ci-bc fusion across various domains, such as supply chain management, finance, healthcare, and the internet of things (iot). additionally, it investigates the regulatory and governance considerations that arise with the growing adoption of intelligent bc systems. through an in-depth analysis of the existing literature and emerging trends, this research paper aims to provide valuable insights into the advantages and challenges of incorporating ci into bc-based distributed applications. by understanding the potential synergies and limitations, researchers, practitioners, and policymakers can make informed decisions to harness the transformative power of ci-bc integration effectively. © 2024 selection and editorial matter, pankaj bhambri, sita rani, and muhammad fahim; individual chapters, the contributors."
"the research community’s treatise on computational economics and financial models has promising interest for the exploration and exploitation of artificial intelligence (ai)-based computing paradigm to offer enriched efficacies for business stratagems, consumer utility, and scarce resource management for enriched society evolution. in this study, ai-based neuro-stochastic bayesian networks (nsbns) are presented for mathematical models that govern the dynamics of nonlinear chaotic financial differential systems (ncfdss). the descriptive expressions for ncfds are portrayed through multi-class differential compartments for macroeconomic agents in terms of interest rate, investment demand, and price index. the reference data acquisition for the execution of the multi-layer structure of nsbns is performed with adams numerical procedure for sundry scenarios of ncfdss by varying the cost per investment, saving amount, as well as, commercial market demand elasticity. the designed nsbn outcomes consistently overlap with the reference solutions having negligible magnitude of error for each scenario of ncfds. the efficacy of proposed nsbns is presented through mean square error based convergence curves, illustrations for adaptive controlling parameters, 2d–3d visual depictions, error histogram studies, and regression indices for variants of nonlinear chaotic differential systems in mathematical finance. © the author(s), under exclusive licence to springer science+business media, llc, part of springer nature 2024."
"internet ranking algorithms play a crucial role in information technologies and numerical analysis due to their efficiency in high dimensions and wide range of possible applications, including scientometrics and systemic risk in finance (sinkrank, debtrank, etc.). the traditional approach to internet ranking goes back to the seminal work of sergey brin and larry page, who developed the initial method pagerank (pr) in order to rank websites in search engine results. recent works have studied robust reformulations of the pagerank model for the case when links in the network structure may vary; that is, some links may appear or disappear influencing the transportation matrix defined by the network structure. we make a further step forward, allowing the network to vary not only in links, but also in the number of nodes. we focus on growing network structures and propose a new robust formulation of the pagerank problem for uncertain networks with fixed growth rate. defining the robust pagerank in terms of a nonconvex optimization problem, we bound our formulation from above by a convex but nonsmooth optimization problem. driven by the approximation quality, we analyze the resulting optimality gap theoretically and demonstrate cases for its reduction. in the numerical part of the article, we propose some techniques which allow us to obtain the solution efficiently for middle-size networks avoiding all nonsmooth points. furthermore, we propose a coordinate-wise descent method with near-optimal step size and address high-dimensional cases using multinomial transition probabilities.we analyze the impact of the network growth on ranking and numerically assess the approximation quality using real-world data sets onmovie repositories and on journals on computational complexity. copyright:  © 2022 the author(s)."
"the finance-level artificial intelligence of things (aiot) is going to become a novel media in the 6g-driven digital society. inside the financial aiot environment, large-scale crowd credit assessment with the guarantee of low latency has been a general demand. facing limited computational resources, there is still a lack of effective computation offloading methods for this purpose to ensure low latency. in order to deal with such an issue, this article introduces edge computing mode and proposes a low-latency edge computation offloading scheme for trust evaluation in financial aiot. with different elements involved in the assessment process being denoted via mathematical description, a multiobjective optimization problem with constraints is formulated. then, the aforementioned optimization problem is solved by a specific search algorithm, so that optimal task offloading schemes can be found. to assess the performance of the proposal, some simulation experiments are conducted to verify the proposed task offloading method. and it can be reflected from numerical results that latency can be well reduced compared with baseline methods.  © 2023 ieee."
"the curse of dimensionality confounds the comprehensive evaluation of computational structural mechanics problems. adequately capturing complex material behavior and interacting physics phenomenon in models can lead to long run times and memory requirements resulting in the need for substantial computational resources to analyze one scenario for a single set of input parameters. the computational requirements are then compounded when considering the number and range of input parameters spanning material properties, loading, boundary conditions, and model geometry that must be evaluated to characterize behavior, identify dominant parameters, perform uncertainty quantification, and optimize performance. to reduce model dimensionality, global sensitivity analysis (gsa) enables the identification of dominant input parameters for a specific structural performance output. however, many distinct types of gsa methods are available, presenting a challenge when selecting the optimal approach for a specific problem. while substantial documentation is available in the literature providing details on the methodology and derivation of gsa methods, application-based case studies focus on fields such as finance, chemistry, and environmental science. to inform the selection and implementation of a gsa method for structural mechanics problems for a nonexpert user, this article investigates five of the most widespread gsa methods with commonly used structural mechanics methods and models of varying dimensionality and complexity. it is concluded that all methods can identify the most dominant parameters, although with significantly different computational costs and quantitative capabilities. therefore, method selection is dependent on computational resources, information required from the gsa, and available data.  © the author(s), 2023. published by cambridge university press."
"today, algorithms steer and inform more than 75% of modern trades. these mathematical constructs play an intricate role in automating processes, predicting market trends, optimizing portfolios, and fortifying decision-making in the financial domain. in an era where algorithms underpin the very foundation of financial services, it is imperative to hold a deep understanding of the intricate web of computational finance. algorithmic approaches to financial technology: forecasting, trading, and optimization takes a comprehensive approach, spotlighting the fusion of artificial intelligence(ai) and algorithms in financial operations. the chapters explore the expansive landscape of algorithmic applications, from scrutinizing market trends to managing risks. the emphasis extends to ai-driven personnel selection, implementing trusted financial services, crafting recommendation systems for financial platforms, and critical fraud detection. this book serves as a vital resource for researchers, students, and practitioners. its core strength lies in discussing ai-based algorithms as a catalyst for evolving market trends. it provides algorithmic solutions for stock markets, portfolio optimization, and robust financial fraud detection mechanisms. the text also casts a forward-looking gaze on the industry, predicting future trends while spotlighting the genetic algorithms in high-frequency trading, predictive analysis, and forecasting. including research findings, case studies, best practices, and conceptual insights makes this book an indispensable reference for undergraduate, graduate, and executive students specializing in business, finance, economics, and technology. beyond academia, the book finds resonance with industry professionals, policymakers, investors, and corporate executives, offering a nuanced understanding of the transformative impact of algorithmic approaches in shaping the future of financial technology. © 2024 by igi global. all rights reserved."
"hallucinations pose a significant challenge to the reliability and alignment of large language models (llms), limiting their widespread acceptance beyond chat-bot applications. despite ongoing efforts, hallucinations remain a prevalent challenge in llms. the detection of hallucinations itself is also a formidable task, frequently requiring manual labeling or constrained evaluations. this paper introduces an automated scalable framework that combines benchmarking llms’ hallucination tendencies with efficient hallucination detection. we leverage llms to generate challenging tasks related to hypothetical phenomena, subsequently employing them as agents for efficient hallucination detection. the framework is domain-agnostic, allowing the use of any language model for benchmark creation or evaluation in any domain. we introduce the publicly available hypotermqa benchmarking dataset, on which state-of-the-art models’ performance ranged between 3% and 11%, and evaluator agents demonstrated a 6% error rate in hallucination prediction. the proposed framework provides opportunities to test and improve llms. additionally, it has the potential to generate benchmarking datasets tailored to specific domains, such as law, health, and finance. © 2024 association for computational linguistics."
"this paper introduces a novel three-parameter invertible bimodal gumbel distribution, addressing the need for a versatile statistical tool capable of simultaneously modeling maximum and minimum extremes in various fields such as hydrology, meteorology, finance, and insurance. unlike previous bimodal gumbel distributions available in the literature, our proposed model features a simple closed-form cumulative distribution function, enhancing its computational attractiveness and applicability. this paper elucidates the behavior and advantages of the invertible bimodal gumbel distribution through detailed mathematical formulations, graphical illustrations, and exploration of distributional characteristics. we illustrate using financial data to estimate value at risk (var) from our suggested model, considering maximum and minimum blocks simultaneously. © 2023 by the authors."
"the complex interaction among economic variables, market forces, and investor psychology presents a formidable obstacle to making accurate forecasts in the realm of finance. moreover, the nonstationary, non-linear, and highly volatile nature of stock price time series data further compounds the difficulty of accurately predicting stock prices within the securities market. traditional methods have the potential to enhance the precision of forecasting, although they concurrently introduce computational complexities that may lead to an increase in prediction mistakes. this paper presents a unique model that effectively handles several challenges by integrating the moth flame optimization technique with the random forest method. the hybrid model demonstrated superior efficacy and performance compared to other models in the present investigation. the model that was suggested exhibited a high level of efficacy, with little error and optimal performance. the study evaluated the efficacy of a suggested predictive model for forecasting stock prices by analyzing data from the nasdaq index for the period spanning from january 1, 2015, to june 29, 2023. the results indicate that the proposed model is a reliable and effective approach for analyzing and forecasting the time series of stock prices. the experimental findings indicate that the proposed model exhibits superior performance in terms of predicting accuracy compared to other contemporary methodologies. © (2024), (science and information organization). all rights reserved."
"quantum machine learning is an interdisciplinary field that combines the principles of quantum physics, quantum computers, and machine learning to enhance computational performance. in recent years, there have been significant advancements in the field of quantum machine learning. researchers have developed quantum algorithms for various tasks. furthermore, quantum machine learning has been applied to various domains, such as chemistry, cryptography, and finance. previous bibliometric surveys in this area covered the period from 2014 to 2020. however, as demonstrated in this paper, the pace of publication has increased significantly from 2018 to the present. therefore, it has become essential to provide a new analysis of the current state of research in this field. in this paper, we deployed visualization tools to analyze co-authorships, co-occurrences, and keyword density in this research area. our study covers and analyzes a total of 918 publications from the web of science database and 1171 publications from the scopus database from 2006 to 2022. following the analysis, we identify research questions, opportunities, and research gaps in the field of quantum machine learning. © 2023 the author(s). published with license by taylor & francis group, llc."
"digital transactions relying on credit cards are gradually improving in recent days due to their convenience. due to the tremendous growth of e-services (e.g., mobile payments, e-commerce, and e-finance) and the promotion of credit cards, fraudulent transaction counts are rapidly increasing. machine learning (ml) is crucial in investigating customer data for detecting and preventing fraud. conversely, the advent of irrelevant and redundant features in most real-time credit card details reduces the execution of ml techniques. the feature selection (fs) approach’s purpose is to detect the most prominent attributes required for developing an effective ml approach, making sure that the classification and computational complexity are improved and decreased, respectively. therefore, this study presents an evolutionary computing with fuzzy autoencoder based data analytics for credit card fraud detection (ecfae-ccfd) technique. the purpose of the ecfae-ccfd technique is to recognize the presence of credit card fraud (ccf) in real time. to achieve this, the ecfae-ccfd technique performs data normalization in the earlier stage. for selecting features, the ecfae-ccfd technique applies the dandelion optimization-based feature selection (do-fs) technique. moreover, the fuzzy autoencoder (fae) approach can be exploited for the recognition and classification of ccf. fae is a category of artificial neural network (ann) designed for unsupervised learning that leverages fuzzy logic (fl) principles to enhance the representation and reconstruction of input data. an improved billiard optimization algorithm (iboa) could be implemented for the optimum selection of the parameters based on the fae algorithm to improve the classification performance. the simulation outcomes of the ecfae-ccfd algorithm are examined on the benchmark open-access database. the values display the excellent performance of the ecfae-ccfd method with respect to various measures. © 2024 the author(s), licensee aims press."
"sentiment analysis finds widespread applications in health, marketing, finance, stock markets, media, and politics. to analyze attitudes and emotions in textual data, handling large datasets and significant computational power is essential. traditional computing methods struggle with the growing data volume, prompting interest in quantum computing as a promising alternative with its inherent high-speed processing capacity. this study focuses on sentiment analysis applied to texts derived from bilateral conversation dialogues. the primary objective is to categorize emotions within the text as positive, neutral, or negative, while concurrently identifying the speaker. to achieve this, a novel quantum-classical hybrid approach is proposed. the quantum side of this approach includes the variational quantum circuit (vqc). on the classical side, preprocessing of the data set, feature extraction with a model containing lstm, and optimizing the parameters of vqc are performed. the proposed approach was trained and tested using a data set containing bilateral conversations. as a result of the tests, the proposed approach achieved a higher accuracy rate compared to studies using the classical approach. thus, the effectiveness of the proposed approach is confirmed.  © 2024 ieee."
"distributed mobile cloud computing (dmcc) services and blockchain technology are two developing technologies with the potential to revolutionize how we interact with mobile devices and cloud services. the usage of several mobile devices to establish a virtual cloud computing infrastructure is referred to as dmcc services. this strategy enables users to use their device’s processing power and storage capacity to undertake heavy computational activities such as data analytics and machine learning. dmcc services provide a cost-effective and scalable option for businesses and people by sharing resources across several devices. in contrast, blockchain technology is a distributed ledger system that enables safe, transparent, and verifiable transactions without the use of intermediaries. this technology is well recognized for its usage in cryptocurrency transactions, but it can also be used for supply chain management, identity verification, and digital asset tracking. dmcc and blockchain technology, when integrated, can build a powerful platform for mobile computing services. a dmcc network, for example, might be used to enable blockchain-based applications such as smart contracts or digital asset exchanges. similarly, blockchain technology might be used to secure dispersed mobile cloud computing networks, allowing users to confidently share resources and process data. overall, the convergence of dmcc and blockchain technology creates new potential for mobile computing services while also paving the road for a more secure, efficient, and collaborative mobile environment. dmcc and blockchain technology are two rapidly growing technological domains that are transforming the way we store, analyze, and manage data. dmcc enables the allocation of computing resources over a network of devices, allowing for the use of mobile devices’ processing capacity for sophisticated computations. in contrast, blockchain technology allows secure, decentralized, and immutable record-keeping that may be applied to a variety of application cases. the combination of dmcc and blockchain technology provides various advantages, including improved scalability, security, and privacy. blockchain-powered smart contracts can facilitate trustless transactions and automate complex procedures. another significant characteristic of blockchain is tokenization, which enables the creation of digital assets and the movement of value across borders without the use of intermediaries. however, difficulties like as interoperability, consensus methods, and digital identification must be solved when deploying dmcc and blockchain systems. nonetheless, dmcc services and blockchain technology have huge potential to disrupt industries such as finance, healthcare, and supply chain management. in this article, we investigate the key features and prospective uses of dmcc and blockchain technology, as well as the difficulties and opportunities they provide. © 2024 elsevier inc. all rights are reserved including those for text and data mining ai training and similar technologies."
"as the volume of data for classification tasks in machine learning grows, feature selection plays an increasingly crucial role in enhancing the efficiency and effectiveness of these tasks. existing classical feature selection algorithms often encounter challenges, such as high computational complexity and vulnerability to local optima. quantum algorithms have been proposed to overcome these limitations. here, our work proposes a new approach that combines the quantum approximate optimization algorithm (qaoa) with the classical feature selection algorithm maximum relevance-minimum redundancy (mrmr). first, we transform the feature selection problem into the quadratic unconstrained binary optimization (qubo) problem. then, we map the qubo formulation to hamiltonian with the ising model. finally, we employ qaoa with a shallow circuit to search this hamiltonian’s ground state, which matches the specific solution for feature selection. we conducted numerical experiments on eight real-world datasets for classification tasks from diverse domains, including finance, medicine, and natural hazards. experiment results suggest that qaoa-selected feature subsets size are typically less than half the complete set while achieving comparable or better classification accuracy than classical mrmr, demonstrating its preference for smaller, more efficient feature subsets. © 2023 copyright held by the owner/author(s)."
"graphs are a powerful tool for representing and analyzing complex relationships in real-world applications such as social networks, recommender systems, and computational finance. reasoning on graphs is essential for drawing inferences about the relationships between entities in a complex system, and to identify hidden patterns and trends. despite the remarkable progress in automated reasoning with natural text, reasoning on graphs with large language models (llms) remains an understudied problem. in this work, we perform the first comprehensive study of encoding graph-structured data as text for consumption by llms. we show that llm performance on graph reasoning tasks varies on three fundamental levels: (1) the graph encoding method, (2) the nature of the graph task itself, and (3) interestingly, the very structure of the graph considered. these novel results provide valuable insight on strategies for encoding graphs as text. using these insights we illustrate how the correct choice of encoders can boost performance on graph reasoning tasks inside llms by 4.8% to 61.8%, depending on the task. © 2024 12th international conference on learning representations, iclr 2024. all rights reserved."
"reinforcement learning algorithms describe how an agent can learn an optimal action policy in a sequential decision process, through repeated experience. in a given environment, the agent policy provides him some running and terminal rewards. as in online learning, the agent learns sequentially. as in multi-armed bandit problems, when an agent picks an action, he can not infer ex-post the rewards induced by other action choices. in reinforcement learning, his actions have consequences: they influence not only rewards, but also future states of the world. the goal of reinforcement learning is to find an optimal policy – a mapping from the states of the world to the set of actions, in order to maximize cumulative reward, which is a long term strategy. exploring might be sub-optimal on a short-term horizon but could lead to optimal long-term ones. many problems of optimal control, popular in economics for more than forty years, can be expressed in the reinforcement learning framework, and recent advances in computational science, provided in particular by deep learning algorithms, can be used by economists in order to solve complex behavioral problems. in this article, we propose a state-of-the-art of reinforcement learning techniques, and present applications in economics, game theory, operation research and finance. © 2021, the author(s), under exclusive licence to springer science+business media, llc, part of springer nature."
"this paper presents an effective algorithm for globally solving the sum of linear ratios problem (slrp), which has broad applications in government planning, finance and investment, cluster analysis, game theory and so on. in this paper, by using a new linearization technique, the linear relaxation problem of the equivalent problem is constructed. next, based on the linear relaxation problem and the branch-and-bound framework, an effective branch and-bound algorithm for globally solving the problem (slrp) is proposed. by analyzing the computational complexity of the proposed algorithm, the maximum number of iterations of the algorithm is derived. numerical experiments are reported to verify the effectiveness and feasibility of the proposed algorithm. finally, two practical application problems from power transportation and production planning are solved to verify the feasibility of the algorithm. © 2023,journal of industrial and management optimization. all rights reserved."
"in this work, the fourier-cosine series (cos) method has been combined with the boundary element method (bem) for a fast evaluation of barrier option prices. after a description of its use in the black and scholes (bs) model, the focus of the paper is on the application of the proposed methodology to the barrier option evaluation in the heston model, where its contribution is fundamental to improve computational efficiency and to make bem appealing among finance practitioners as a valid alternative to monte carlo (mc) or other more traditional approaches. an error analysis is provided on the number of terms used in the fourier-cosine series expansion, where the error bound estimation is based on the characteristic function of the log-asset price process. a matlab code implementing this technique is attached at the end of the paper.  © 2023 walter de gruyter gmbh, berlin/boston."
"indian stock market is unique with high volatility and complexity, so special approaches are needed for its computational understanding. this study proposes temporal relational network (trnet), a new graph neural network model designed for indian stock market prediction. trnet considers pre-defined industry-sector links and time to predict future stock prices in nifty-50 index, a significant measure for indian stock market. using a financial knowledge graph and graph convolution network, trnet effectively captures complexities and relationships in non-euclidean financial data. trnet outperforms existing models in different timeframes, showing its importance for indian stock market. we also explore how connections between stocks affect predictions and provided an explainable framework to understand the working mechanisms of the proposed approach. combining finance and intelligent computing, trnet paves the way for possibilities in the areas of computational finance using deep learning. © 2023 ieee."
"standard regression techniques model only the mean of the response variable. quantile regression (qr) is more powerful in that it depicts a comprehensive relationship between the response variable and independent covariates at different quantiles. it is particularly useful for non-normally distributed data with skewness or heterogeneity, which appear routinely in many scientific fields, such as economics, finance, public health and biology. although its theory has been well developed in the literature, its computation in big data still faces multiple challenges, especially for vertically stored big data in modern distributed environments, where communication efficiency and security are usually the primary considerations. while the popular alternating direction method of multipliers (admm) provides a general computational solution, its slow convergence becomes a bottleneck when communication cost dominates local computational consumption, such as internet of things (iot) networks. motivated by the residual projection technique, in this paper we propose an innovative iterative parallel framework, piqr, that converges faster and has a more secure data transmission plan, and establish its convergence property. this framework is further extended to composite quantile regression (cqr), a modified qr technique that improves estimation efficiency at extreme quantiles. simulation studies show that both the admm-based method and the piqr enjoy favorable estimation accuracy in distributed environments. while piqr is inferior to the admm-based method at local computation, it requires much fewer iterations to achieve convergence, and hence significantly improves the overall computational efficiency when communication cost is the dominating factor. moreover, piqr transmits only data involving the residual information between different machines, and can better prevent the leakage of important data information compared with the admm-based method. © 2023, the author(s), under exclusive licence to springer science+business media llc, part of springer nature."
"entropic risk (erisk) is an established risk measure in finance, quantifying risk by an exponential re-weighting of rewards. we study erisk for the first time in the context of turn-based stochastic games with the total reward objective. this gives rise to an objective function that demands the control of systems in a risk-averse manner. we show that the resulting games are determined and, in particular, admit optimal memoryless deterministic strategies. this contrasts risk measures that previously have been considered in the special case of markov decision processes and that require randomization and/or memory. we provide several results on the decidability and the computational complexity of the threshold problem, i.e. whether the optimal value of erisk exceeds a given threshold. in the most general case, the problem is decidable subject to shanuel’s conjecture. if all inputs are rational, the resulting threshold problem can be solved using algebraic numbers, leading to decidability via a polynomial-time reduction to the existential theory of the reals. further restrictions on the encoding of the input allow the solution of the threshold problem in np ∩ conp. finally, an approximation algorithm for the optimal value of erisk is provided. © christel baier, krishnendu chatterjee, tobias meggendorfer, and jakob piribauer;"
"featured application: maintaining production systems within industry 4.0 facilitates the application of artificial intelligence methods, techniques and tools to predict potential failures and to take maintenance actions in advance at the right time and in the right way to avoid or minimize their harmful impact, including using digital twins. maintenance of production equipment has a key role in ensuring business continuity and productivity. determining the implementation time and the appropriate selection of the scope of maintenance activities are necessary not only for the operation of industrial equipment but also for effective planning of the demand for own maintenance resources (spare parts, people, finances). a number of studies have been conducted in the last decade and many attempts have been made to use artificial intelligence (ai) techniques to model and manage maintenance. the aim of the article is to discuss the possibility of using ai methods and techniques to anticipate possible failures and respond to them in advance by carrying out maintenance activities in an appropriate and timely manner. the indirect aim of these studies is to achieve more effective management of maintenance activities. the main method applied is computational analysis and simulation based on the real industrial data set. the main results show that the effective use of preventive maintenance requires large amounts of reliable annotated sensor data and well-trained machine-learning algorithms. scientific and technical development of the above-mentioned group of solutions should be implemented in such a way that they can be used by companies of equal size and with different production profiles. even relatively simple solutions as presented in the article can be helpful here, offering high efficiency at low implementation costs. © 2023 by the authors."
"quantum computation holds the promise of highly efficient algorithms, provides exponential speedups for specific technologically significant problems, plays the most promising role in quantum technological progress, and has transformative potential in numerous industry sectors. in quantum computing, finance is recognized as one of the most beneficial areas because of the potential for many financial cases that can be solved by quantum algorithms suitable for noisy-intermedia-scale quantum(nisq) computers. the benefits of quantum computing are colossal time and computational memory reduction with which the computational tasks are performed, which leads to the accuracy of the computations. this paper presents a comprehensive summary of quantum computing for traditional and blockchain financial applications.  © 2007-2011 ieee."
"recently, with the increasing application of the internet of things (iot), various iot environments such as smart factories, smart homes, and smart grids are being generated. in the iot environment, a lot of data are generated in real time, and the generated iot data can be used as source data for various services such as artificial intelligence, remote medical care, and finance, and can also be used for purposes such as electricity bill generation. therefore, data access control is required to grant access rights to various data users in the iot environment who need such iot data. in addition, iot data contain sensitive information such as personal information, so privacy protection is also essential. ciphertext-policy attribute-based encryption (cp-abe) technology has been utilized to address these requirements. furthermore, system structures applying blockchains with cp-abe are being studied to prevent bottlenecks and single failures of cloud servers, as well as to support data auditing. however, these systems do not stipulate authentication and key agreement to ensure the security of the data transmission process and data outsourcing. accordingly, we propose a data access control and key agreement scheme using cp-abe to ensure data security in a blockchain-based system. in addition, we propose a system that can provide data nonrepudiation, data accountability, and data verification functions by utilizing blockchains. both formal and informal security verifications are performed to demonstrate the security of the proposed system. we also compare the security, functional aspects, and computational and communication costs of previous systems. furthermore, we perform cryptographic calculations to analyze the system in practical terms. as a result, our proposed protocol is safer against attacks such as guessing attacks and tracing attacks than other protocols, and can provide mutual authentication and key agreement functions. in addition, the proposed protocol is more efficient than other protocols, so it can be applied to practical iot environments. © 2023 by the authors."
"the growing utilization of machine learning techniques for forecasting has generated significant interest among individuals involved in bitcoin investing, trading, and portfolio management. this research presents a comprehensive bibliometric analysis of 123 english- language journal articles indexed in scopus, focusing on the use of machine learning techniques for forecasting bitcoin prices. the study provides valuable insights into the field's progression, patterns, and key contributors. noteworthy authors include gupta r, li j, li x, and li y, while prominent institutions include ""king abdulaziz university"", ""mansoura university"", and ""sungkyunkwan university"". china, korea, and india are the leading countries in terms of article productivity. the top publishing outlets are identified as ""ieee access"", ""computational economics"", ""entropy"", and ""finance research letters"". the study's findings cater to the interests of stakeholders in this field. this study conducts the key- word co-occurrence and co-citation analysis to identify potential clustering of topics. the findings have significant implications for academics, industry practitioners, regulators, and policymakers. © 2023 ieee."
"human-assisted computation, also called humanized or human-based computing, harnesses human intelligence to solve computational problems that are beyond the scope of existing artificial intelligence (ai) algorithms by outsourcing certain steps to humans. this approach exploits differences in abilities between humans and computer agents, creating a symbiotic human-computer interaction, and can be helpful in solving many ai-complete problems. this edited book focuses on well-known and new methodologies of optimization techniques in human-assisted computing that are used to resolve some of the very complicated and hard problems we face today. excitingly, techniques originally developed for solutions in engineering, science and technology are being applied to areas of economics, finance and, especially the social sciences. the book reviews present and developing deep model-based methods of mathematics and the less model-based, also called smart or intelligent algorithms, with their roots in engineering, computer science or informatics and how combining the strengths of humans and computers can create powerful solutions. bridging several scientific disciplines, this book will be essential reading for scientists, engineers and applied mathematicians alike. part of iop series in next generation computing. key features reviews the present state of play and future challenges in human-assisted computing. interdisciplinary coverage. focus on applications. includes case studies and real-life implemented solutions. © iop publishing ltd 2023. all rights reserved."
"value-at-risk (var) is a widely acceptable risk measure in finance and particularly, in the portfolio selection problem (psp). however, due to the non-convexity, its incorporation into an optimization model makes it challenging and difficult to solve. hence, the development of iterative-based heuristic algorithms (ibhas) utilizing the problem structure would be valuable. the existing ibhas are either inefficient in terms of solution quality and running time or are just applicable to solve simple psps without any real-world constraint (i.e., cardinality, position size, and transaction cost constraints). to overcome these shortcomings, in this paper, we present a novel ibha to efficiently solve the psp with real-world constraints under var measure. our method is based on iterative resolution of two restricted versions of the original model in which some of the binary variables utilized in the formulation of var measure are fixed at specific values. computational experiments over real-world instances indicate that not only the running time of our algorithm is short, but also its solution quality is much better than the existing ibhas in 97% of instances with an average improvement of about 10%. © 2023 the authors. international transactions in operational research © 2023 international federation of operational research societies."
"quantum computing is a game-changing technology for global academia, research centers and industries including computational science, mathematics, finance, pharmaceutical, materials science, chemistry and cryptography. although it has seen a major boost in the last decade, we are still a long way from reaching the maturity of a full-fledged quantum computer. that said, we will be in the noisy-intermediate scale quantum (nisq) era for a long time, working on dozens or even thousands of qubits quantum computing systems. an outstanding challenge, then, is to come up with an application that can reliably carry out a nontrivial task of interest on the near-term quantum devices with non-negligible quantum noise. to address this challenge, several near-term quantum computing techniques, including variational quantum algorithms, error mitigation, quantum circuit compilation and benchmarking protocols, have been proposed to characterize and mitigate errors, and to implement algorithms with a certain resistance to noise, so as to enhance the capabilities of near-term quantum devices and explore the boundaries of their ability to realize useful applications. besides, the development of near-term quantum devices is inseparable from the efficient classical simulation, which plays a vital role in quantum algorithm design and verification, error-tolerant verification and other applications. this review will provide a thorough introduction of these near-term quantum computing techniques, report on their progress, and finally discuss the future prospect of these techniques, which we hope will motivate researchers to undertake additional studies in this field. © 2023, science china press."
"high-order iterative techniques without derivatives for multiple roots have wide-ranging applications in the following: optimization tasks, where the objective function lacks explicit derivatives or is computationally expensive to evaluate; engineering; design finance; data science; and computational physics. the versatility and robustness of derivative-free fourth-order methods make them a valuable tool for tackling complex real-world optimization challenges. an optimal extension of the traub–steffensen technique for finding multiple roots is presented in this work. in contrast to past studies, the new expanded technique effectively handles functions with multiple zeros. in addition, a theorem is presented to analyze the convergence order of the proposed technique. we also examine the convergence analysis for four real-life problems, namely, planck’s law radiation, van der waals, the manning equation for isentropic supersonic flow, the blood rheology model, and two well-known academic problems. the efficiency of the approach and its convergence behavior are studied, providing valuable insights for practical and academic applications. © 2023 by the authors."
"the global financial crisis of 2008 exposed many vulnerabilities of the financial systems and how these system failures took a massive toll on governments, institutions, and societies worldwide. given the critical link between the financial and the rest of the economic sectors, financial risk management is critical to avoid similar kinds of catastrophic events in the future. playing the crucial financial intermediary role in society forces firms to evolve and manage financial risks daily. with the emergence of big data, computational frameworks, and ai algorithms, finance sector risk management is rapidly adopting them across different use cases. ai applications designed for financial risk management aim to streamline processes to improve accuracy, efficiency, and productivity while reducing costs for companies to remain competitive. these emerging ai technologies provide a fresh perspective on how risk should be viewed, assessed, and managed. this chapter has summarized and shared various examples of how ai is used for financial risk management. © 2023, igi global. all rights reserved."
"in recent times, the progress of machine learning has facilitated the development of decision support systems that exhibit predictive accuracy, surpassing human capabilities in certain scenarios. however, this improvement has come at the cost of increased model complexity, rendering them black-box models that obscure their internal logic from users. these black boxes are primarily designed to optimize predictive accuracy, limiting their applicability in critical domains such as medicine, law, and finance, where both accuracy and interpretability are crucial factors for model acceptance. despite the growing body of research on interpretability, there remains a significant dearth of evaluation methods for the proposed approaches. this survey aims to shed light on various evaluation methods employed in interpreting models. two primary procedures are prevalent in the literature: qualitative and quantitative evaluations. qualitative evaluations rely on human assessments, while quantitative evaluations utilize computational metrics. human evaluation commonly manifests as either researcher intuition or well-designed experiments. however, this approach is susceptible to human biases and fatigue and cannot adequately compare two models. consequently, there has been a recent decline in the use of human evaluation, with computational metrics gaining prominence as a more rigorous method for comparing and assessing different approaches. these metrics are designed to serve specific goals, such as fidelity, comprehensibility, or stability. the existing metrics often face challenges when scaling or being applied to different types of model outputs and alternative approaches. another important factor that needs to be addressed is that while evaluating interpretability methods, their results may not always be entirely accurate. for instance, relying on the drop in probability to assess fidelity can be problematic, particularly when facing the challenge of out-of-distribution data. furthermore, a fundamental challenge in the interpretability domain is the lack of consensus regarding its definition and requirements. this issue is compounded in the evaluation process and becomes particularly apparent when assessing comprehensibility. © 2023 by the authors."
"time series prediction has wide applications in many safety-critical scenarios, including meteorology and finance. according to previous studies, time series of recorded events, e.g., river level and stock price, usually contain a non-trivial proportion of extreme events (e.g., flood and financial crisis), which are featured with extremely large/small values, occur in time series data with a relatively low frequency, and may have huge societal consequences if overlooked by a predictive model (i.e., predictor). despite its significance in time series, we however observe the conventional square loss in time series prediction would ignore the modeling of extreme events. specifically, we prove the square loss as a learning objective of the predictor behaves equivalently as a gaussian kernel density estimator (kde) on the recorded events, which is light-tailed itself and unable to model the ground-truth event distribution, usually heavy-tailed due to the existence of extreme events. considering the benefits of forecasting extreme events, we propose a unified loss form called generalized extreme value loss (gevl), which bridges the misalignment between the tail parts of the estimation and the ground-truth via transformations on either the observed events or the estimator. following the proposed framework, we present three heavy-tailed kernels, i.e., shifted gaussian, gumbel and fréchet kernels, and derive the corresponding gevls which show different levels of trade-off between modeling effectiveness and computational resources, suitable for various downstream tasks. comprehensive experiments on a diverse set of time series predictors and real-world datasets validate that, our novel loss form substantially enhances representative time series predictors in modeling extreme events. for example, for co2 concentration rate prediction and stock price prediction, our proposed fréchet gevl respectively reduces the rmse of 6 representative dnn-based time series predictors on extreme events by over 20\%20% and 17\%17% on average, with a maximum reduction of 29.8\%29.8%.  © 1989-2012 ieee."
"gregor semieniuk (phd, economics, new school for social research) is assistant research professor of economics at the university of massachusetts amherst. he researches the political economy of rapid, policy-induced structural change required for the transition to a low-carbon economy. gregor has published widely on this topic, won grants to study it, and regularly speaks to policy and academic audiences, most recently testifying on stranded fossil-fuel assets before the us senate committee on the budget. previously, gregor was on the faculty at soas university of london and university of sussex, and he is an honorary professor at university college london. lucas chancel is an associate professor of economics at sciences po and co-director of the world inequality lab at the paris school of economics, as well as a visiting associate professor at harvard. he is the author of dozens of research articles and book chapters on global inequality and environmental issues. he coordinates and edits the world inequality reports, which present the latest data on global economic and environmental inequality. his work has attracted media and policy attention worldwide and his book, “unsustainable inequalities” (harvard university press, 2020), featured in the financial times and nature's best books of the year. eulalie saïsset is a specialist in environmental public policies and resulting inequalities. she is a graduate engineer from mines paris where she focused on the sociological and political dimensions of the environmental transition. she also holds a master's degree in public policy and development economics from the paris school of economics, where her dissertation focused on the redistributive aspects of stranding fossil fuel assets. eulalie previously worked at the world bank in washington dc as a transport consultant for the africa region. she also carried out the analysis of european industrial decarbonization policies at the think-tank la fabrique de l'industrie. philip b. holden completed his dphil in computational modeling of x-ray lasers in 1991, after which he held research positions in the university of york, université paris-sud, and the czech institute of physics. he then spent 10 years in the finance sector, designing and modeling approximately $2 billion of “big ticket” asset financings. he returned to academia via an msc in quaternary science from the university of london (rhul/ucl) and joined the open university in 2007. phil's main focus now is the development of computationally efficient earth system models, with particular focus on interdisciplinary applications and integrated assessment. dr. jean-francois mercure is senior climate economist at the world bank and associate professor in climate policy at the global systems institute, university of exeter, uk. his research focuses on developing theory, models, and methods for public policy appraisal in climate policy, and for assessing the effectiveness and socio-economic impacts of diverse types of low-carbon, energy, and other climate policies. he also develops methods to understand and assess climate-related financial risks. he co-leads two major programs at the world bank on analytical tool development and capacity development for finance ministries. neil r. edwards is professor of earth system science and university lead for sustainability research at the open university, uk. after studying mathematics at cambridge and leeds universities, neil became deeply involved with the early development of comprehensive earth system models in the 1990s, playing a key role in developing the grid-enabled integrated earth system (genie) framework and related models. through genie, he contributed to the multi-millennial climate projections of the fourth and fifth ipcc assessment reports and has published extensively on climate dynamics, integrated assessment and climate impact modeling, and uncertainty quantification. he has published over 100 refereed papers. © 2023 elsevier inc."
"clustering analysis, specifically for extensive image data, is increasingly being applied in various fields such as finance, risk management, prediction, etc., and has been a fascinating subject in many scientific discussions. deep learning, a widely used approach, and classical methods address complex classification problems stemming from real-world cases. in this study, we took various approaches to classification problems and measured their effectiveness by combining different techniques using the results of different scenarios. many approaches have been proposed to solve the clustering problem; complex clustering methods such as hierarchical, density-based, centroid-based, and graph theoretical have been submitted. however, when it comes to real-world applications, they exposed significant drawbacks when the dataset introduced immeasurable vagueness, uncertainty, or overlapping samples that made it impossible to predict and classify. several attempts have been made to improve the clustering method’s performance, including joint cnn clustering models. still, many of them carry the cons of the complicated clustering method, which limits the capability of cnn. the combined cnn clustering method is designed to address the problem with those deterministic cnn clustering models and was evaluated on a dataset we collected from the website designbyhumans.com, with enough features to represent a non-synthetic dataset. this research aims to improve upon the established model by using estimation techniques in determining model parameters and graphing plots to justify those choices and give insights into how the model performs on a non-synthetic dataset like ours. we concluded that the model significantly improved compared with a popular complex clustering method, which has been evaluated by computational time, using different metrics to represent how better separated each cluster was. based on conducted experiments and the future development of the method, we discussed and addressed some of the drawbacks of this approach. © 2023 copyright held by the owner/author(s)."
"this work is concerned with the computational solution of the time-dependent 3d parabolic heston–cox–ingersoll–ross (hcir) pde, which is of practical importance in mathematical finance. the hcir dynamic states that the model follows randomness for the underlying asset, the volatility and the rate of interest. since the pde formulation has degeneracy and non-smoothness at some area of its domain, we design a new numerical solver via semi-discretization and the radial basis function–finite difference (rbf-fd) scheme. our scheme is built on graded meshes so as to employ the lowest possible number of discretized nodes. the stability of our solver is proven analytically. computational testing is conducted to uphold the analytical findings in practice. © 2023 by the authors."
"constructing markov chains with desired statistical properties is of critical importance for many applications in economics and finance. this paper proposes a moment-matching method for generating finite-state markov chains with flexible distributions, including empirically-relevant processes with non-zero skewness and excess kurtosis. in our method, we preserve the most appealing features of the existing methods, including full analytical tractability and minimal computational cost. the new method derives the key moments of the markov chain as closed-form functions of its parameters. it thus offers a simple plug-in procedure requiring neither numerical integration nor optimization. using the method amounts to plugging the targeted values of mean, standard deviation, serial correlation, skewness, and excess kurtosis into a simple procedure. the proposed method outperforms the existing methods over a wide range of the parameter space, especially for leptokurtic processes with characteristic roots close to unity. the supplementary materials include ready-to-use computer codes of the plug-in procedures and practical guidelines. © 2022, the author(s), under exclusive licence to springer science+business media, llc, part of springer nature."
"the integral representation of the optimal exercise boundary problem for generating the continuous-time early exercise boundary for the american put option is a well-known topic in the mathematical finance community. the main focus of this paper is to provide an efficient asymptotically computational method to improve the accuracy of american put options and their optimal exercise boundary. initially, we reformulate the nonlinear singular integral model of the early exercise premium problem given in [kim et al., a simple iterative method for the valuation of american options, quant. finance. 13 (2013) 885–895] to an equivalent form which is more tractable from a numerical point of view. we then obtain the existence and uniqueness results with verifiable conditions on the functions and parameters in the resulting operator equation. the asymptotic behavior for the early exercise boundary is also analyzed which is mostly compatible with some realistic financial models. © 2023 university of guilan."
"artificial intelligence (ai), along with its subfields of machine learning (ml) and deep learning (dl), allows computational models to process and learn the data representations with various levels of abstraction. ai models are widely used in finance, healthcare, technology, business, banking, science, and agriculture. in agriculture, utilizing ample human resources at each stage of cultivation and challenging tasks involved in predicting diseases and other factors using traditional methods is ineffective and expensive. potato (solanum tuberosum) is a highly consumed and cultivated vegetable crop across the globe. the potato crop yield is highly affected by the fungal pathogens which cause leaf diseases: early blight (eb) and late blight (lb). early detection of fungal pathogens in the crop can reduce yield loss. to detect the fungal pathogens in the plants, a continuous monitoring system in the field must be established. computer vision and artificial intelligence can help farmers monitor plant diseases at every stage. their application in monitoring the diseased leaves of the crop and predicting the diseases correctly has received great attention from researchers. this chapter deals with applying a nature-inspired whale optimization algorithm (woa) in optimizing the convolutional neural network (cnn) performance in classifying the eb and lb diseases in potato leaves. © 2024 elsevier inc. all rights reserved."
"this review presents an overview of the current state-of-the-art in photonics computing, which leverages photons, photons coupled with matter, and optics-related technologies for effective and efficient computational purposes. it covers the history and development of photonics computing and modern analogue computing platforms and architectures, focusing on optimization tasks and neural network implementations. the authors examine special-purpose optimizers, mathematical descriptions of photonics optimizers, and their various interconnections. disparate applications are discussed, including direct encoding, logistics, finance, phase retrieval, machine learning, neural networks, probabilistic graphical models, and image processing, among many others. the main directions of technological advancement and associated challenges in photonics computing are explored, along with an assessment of its efficiency. finally, the paper discusses prospects and the field of optical quantum computing, providing insights into the potential applications of this technology. © 2023 the authors. advanced quantum technologies published by wiley-vch gmbh."
"predicting customer engagement in the banking industry is extremely crucial as it directly impacts customer satisfaction, retention, and profitability. this research delves into this vital aspect and utilizes state-of-the-art technologies, including explainable ensemble learning (eel), evidently ai as trustworthy open ai, and lime as an explainable ai technique. the study employs the publicly available berka dataset, which undergoes feature engineering, extraction, and deep clustering to group customers into various engagement categories. four different ensemble models are utilized, and evidently ai is employed to test and evaluate predictions thoroughly, while lime provides interpretability. the blending model is identified as the best in predicting customer engagement, with an impressive auc score of 1.000 and high values for accuracy, precision, recall, and f-1 score. the developed model is robust, accurate, and easily interpretable, providing valuable insights into customer behavior and engagement. in addition, evidently ai tests only incorrectly predicted six data points out of 2000, emphasizing the high accuracy of the model. the study's findings provide significant insights into the potential of computational finance techniques, particularly eel and trustworthy open ai, in predicting customer engagement and enhancing the accuracy and transparency of predictions in the banking industry. this research contributes to the growing body of knowledge in the area of customer engagement in the banking industry, highlighting the importance of using advanced trustworthy ai technologies to enhance customer satisfaction, retention, and profitability.  © 2023 owner/author."
"in this paper, we investigate a class of linear fractional-multiplicative programs with exponents, which have important applications in finance and economy. by introducing p variables, the problem is re-represented as an equivalent problem. immediately, two new linear relaxation strategies are proposed and embedded in the branch-and-bound framework, and the corresponding new global optimization algorithms are developed in combination with an acceleration technique. furthermore, the theoretical convergence and computational complexity of the algorithms are elucidated. numerical experiment results illustrate that both algorithms are effective and feasible. © 2023 elsevier ltd"
"electricity theft becomes a major concern for utilities in this new era of high tech, self-sufficient dwellings. finding and reducing energy losses or theft has proven challenging due to insufficient inspection methods. in terms of energy, both technical and non-technical losses (ntl) are included in distribution. energy theft is a significant factor in ntl that can strain the finances of service providers. wireless data transmission is used in modern smart metres. it follows that hi-tech dwellings can be easily hacked to steal power. many new technologies have been implemented into advance metering infrastructure (ami) to combat energy theft. it is necessary to derive the consumption pattern in order to identify illegal energy customers. using data mining methods, a computational system is designed for examining and identifying energy consumption patterns. through the use of machine learning, we are able to improve our customers' energy consumption statistics and provide them with early warning of any irregularities. multiple supervised learning techniques are examined and contrasted in relation to their predictive accuracy, recall, precision, auc as well as f1 score. these include the decision tree (dt), ann, deep ann, modified ann and adaboost. based on the results of the study, mdann is superior to alternative classifiers for supervised learning including ann adaboost as well as dt according to recall, f1 score along with auc. the upcoming research should focus on testing different supervised learning algorithms using various datasets and including appropriate pre-processing procedures to boost performance. © ismail saritas. all rights reserved."
"we propose in this work a monte carlo method for three dimensional scalar radiative transfer equations with non-integrable, space-dependent scattering kernels. such kernels typically account for long-range statistical features, and arise for instance in the context of wave propagation in turbulent atmosphere, geophysics, and medical imaging in the peaked-forward regime. in contrast to the classical case where the scattering cross section is integrable, which results in a non-zero mean free time, the latter here vanishes. this creates numerical difficulties as standard monte carlo methods based on a naive regularization exhibit large jump intensities and an increased computational cost. we propose a method inspired by the finance literature based on a small jumps - large jumps decomposition, allowing us to treat the small jumps efficiently and reduce the computational burden. we demonstrate the performance of the approach with numerical simulations and provide a complete error analysis. the multifractional terminology refers to the fact that the high frequency contribution of the scattering operator is a fractional laplace-beltrami operator on the unit sphere with space-dependent index. © 2023 elsevier inc."
"economic and social development has made financial engineering an increasingly important research area, and more and more financial problems cannot be solved directly by analytical formulas. in view of this, algorithms that apply computer technology to financial engineering have emerged. in this study, the backward stochastic differential equation (bsde) algorithm is used to investigate and analyse the problem of option pricing calculation in finance. in the research process, gbsde-theta parallel algorithm composed of bsde-theta algorithm and gpu algorithm uses the new algorithm to establish a computing model in the financial engineering field, which applies to the calculation of enterprise option pricing. the research results show that compared with the basic algorithm, the actual option values of the option pricing data obtained by using the gbsde-theta parallel algorithm are more closely matched. the computational model can achieve a speedup ratio of about 230 times of the serial version with the number of time steps n=128 and the number of simulated paths 80,000. about the relative error of the gbsde-theta algorithm, there are 80 points within 3% and only 16 points over 3.00%, which is a relatively small error. the above results show that the financial computing system obtained in this study is highly feasible and effective, and can provide a new research idea for the progress and development of other computations in the financial field.  © 2023 world scientific publishing co."
"this paper proposes a computational solver via the localized radial basis function finite difference (rbf-fd) scheme and the use of graded meshes for solving the time-dependent bates partial integro-differential equation (pide) arising in computational finance. in order to avoid facing a large system of discretization systems, we employ graded meshes along both of the spatial variables, which results in constructing a set of ordinary differential equations (odes) of lower sizes. moreover, an explicit time integrator is used because it can bypass the need to solve the large discretized linear systems in each time level. the stability of the numerical method is discussed in detail based on the eigenvalues of the system matrix. finally, numerical tests revealed the accuracy and reliability of the presented solver. © 2023 by the authors."
"quantum computing (qc) is founded on the principles of quantum entanglement and the superposition of matter. it employs advanced computation techniques rather than conventional ones. to circumvent the limitations of conventional computing, new supercomputers employ quantum mechanics knowledge, which allows for the coherence of ones and zeros. several fields like finance, healthcare, cybersecurity, transportation, climate change, and many more are taking advantage of qc. indoor environmental quality (ieq) is also one of the sectors that can benefit enormously with qc. the ieq contains several parameters. major ieq parameters are indoor air quality, thermal comfort, acoustic comfort, and visual comfort. these parameters are associated with several physical, chemical, and biological components, which need critical computational considerations for accurate results and better understanding as the data can be highly overlapped. this chapter contains possible forthcoming research opportunities available in the collaborative work between ""indoor environment"" and ""quantum computing."" © 2023, igi global. all rights reserved."
"this study builds an agent-based computational finance platform that can reproduce the basic characteristics of china's initial public offering (ipo) market and explain its anomalies. the results of our computational experiments show that along with the increasing proportion of sentiment strategy investors (i.e., those with an information advantage) entering the market, the ipo underpricing rate also rises correspondingly. sentiment investors usually suffer losses because of their irrational investment decisions, while sentiment strategy investors profit by preying on sentiment investors.  © 2023 world scientific publishing company."
"this paper proposes a xgboost-lstm-a model for credit risk assessment of enterprises on supply chain. there are two main stages. for the first stage, we use xgboost model to select out top important features from the credit risk assessment system we built. in the second stage, we use the lstm network to capture dynamic nonlinear relationship between selected indicators and credit risk value on time series, then fuse the attention mechanism to better filter out more crucial information in the hidden layer to make results more meaningful. our solution not only innovatively proposes a time series approach which captures correlations within data effectively to achieve risk warning, but also greatly reduces the computational effort by reducing the dimensionality of the data through feature selection. through empirical analysis, xgboost-lstm-a model has an accuracy rate of 81.48%, which can provide a certain reference for credit risk assessment of supply chain finance.  © 2023 acm."
"this study examines the use of a recurrent neural network for estimating the parameters of a hawkes model based on high-frequency financial data, and subsequently, for computing volatility. neural networks have shown promising results in various fields, and interest in finance is also growing. our approach demonstrates significantly faster computational performance compared to traditional maximum likelihood estimation methods while yielding comparable accuracy in both simulation and empirical studies. furthermore, we demonstrate the application of this method for real-time volatility measurement, enabling the continuous estimation of financial volatility as new price data keeps coming from the market. © 2023 elsevier inc."
"quantum machine learning has been shown to offer exponential computational boosts compared to current classical models through the exploitation of quantum mechanical properties such as entanglement and superposition. the finance sub field of fin-tech, where financial services are constructed around a customer's demands, is believed to benefit with the use of hybrid quantum-classical machine learning algorithms. classical algorithms struggle to create accurate, complex, and prompt financial analytical models resulting in over 25% of small and medium sized financial institutions losing customers each year. this study explores whether a hybrid quantum neural network (qcnn) design provides a quantum advantage in either accuracy or time in the fin tech task of credit defaulting. the hybrid quantum neural network model uses a variational quantum circuit built on a continuous variable architecture. the data flow of the network consists of a classical network, quantum data encoding (which uses quantum squeezers, interferometers, displacement, and kerr gates), qcnn, and measurement. although the quantum model took a longer time to train, it reported a higher prediction accuracy of 97.63% compared to 91.20% for the classical network. this work demonstrates the future implications of quantum integration in the financial field and the quantum advantage it offers on classification tasks. © 2023 ieee."
"the heston–hull–white three-dimensional time-dependent partial differential equation (pde) is one of the important models in mathematical finance, at which not only the volatility is modeled based on a stochastic process but also the rate of interest is assumed to follow a stochastic dynamic. hence, an efficient method is derived in this paper based on the methodology of the localized radial basis function generated finite difference (rbf-fd) scheme. the proposed solver uses the rbf-fd approximations on graded meshes along all three spatial variables and a high order time-stepping scheme. stability is also studied in detail to show under what conditions the proposed method is stable. computational simulations are given to support the theoretical discussions. © 2023 by the authors."
"nowadays, many sequences of events are generated in areas as diverse as healthcare, finance, and social network. people have been studying these data for a long time. they hope to predict the type and occurrence time of the next event by using relationships among events in the data. recently, with the successful application of recurrent neural network (rnn) in natural language processing, it has been introduced into point process. however, rnn cannot capture the long-term dependence among events well, and self-attention can partially mitigate this problem precisely. transformer hawkes process (thp) using self-attention greatly improves the performance of the hawkes process, but thp cannot ignore the effect of irrelevant events, which will affect the computational complexity and prediction accuracy of the model. in this paper, we propose an adaptively sparse transformers hawkes process (asthp). asthp considers the periodicity and nonlinearity of event time in the time encoding process. the sparsity of the asthp is achieved by substituting softmax with α-entmax: α-entmax is a differentiable generalization of softmax that allows unrelated events to gain exact zero weight. by optimizing the neural network parameters, different attention heads can adaptively select sparse modes (from softmax to sparsemax). compared with the existing models, asthp model not only ensures the prediction performance but also improves the interpretability of the model. for example, the accuracy of asthp model on mimic-ii dataset is improved by nearly 3 percentage points, and the model fitting degree and stability are also improved significantly. © 2023 world scientific publishing company."
"hydropower as a renewable source can help many countries achieve their sustainable energy and climate goals, but large projects are challenging to finance because of their costs and risks. to fully realize the climate benefits of such projects, sponsors have recently fashioned complex financing arrangements that structure and allocate risks to reduce financing costs. this paper focuses on the blended financing approach adopted for the nachtigal hydropower plant (nhp) in cameroon. the purpose of the paper is to present a detailed systems analysis of nachtigal’s financial arrangement to address the question of why the complex financing approach worked in practice. we accomplish this by creating a “financial simulator”—a computational model for evaluating risks and incentives embedded within the financing structure under different contract architectures and risk–event scenarios. our simulator is a dynamic value–risk calculator that can be easily updated to study other climate-oriented projects that involve complex financial arrangements. we evaluated three aspects of the financing/contractual arrangements that made nachtigal “bankable:” (i) guarantees that covered nonpayments, (ii) financial options on locally sourced loans; and (iii) an interest rate swap. we found: (i) the guarantees recovered project value threatened by four specific risks often associated with large hydropower investments (cost overruns, schedule delays, offtake risk, and low flow due to climate change); (ii) the mechanism significantly lowered interest rate charges; and (iii) private finance was mobilized—especially due to the options. the financial safeguards employed increased the likelihood of capturing the long-run sustainability benefits from nhp. © 2023 by the authors."
"since the introduction of the autoregressive conditional heteroscedasticity (arch) model, the literature on modeling the time-varying second-order conditional moment has become increasingly popular in the last four decades. its popularity is partly due to its success in capturing volatility in financial time series, which is useful for modeling and predicting risk for financial assets. a natural extension of this is to model time variation in higher-order conditional moments, such as the third and fourth moments, which are related to skewness and kurtosis (tail risk). this leads to an emerging literature on time-varying higher-order conditional moments in the last two decades. this paper outlines recent developments in modeling time-varying higher-order conditional moments in the economics and finance literature. using the generalized autoregressive conditional heteroscedasticity (garch) framework as a foundation, this paper provides an overview of the two most common approaches for modeling time-varying higher-order conditional moments: autoregressive conditional density (arcd) and autoregressive conditional moment (arcm). the discussion covers both the theoretical and empirical aspects of the literature. this includes the identification of the associated skewness–kurtosis domain by using the solutions to the classical moment problems, the structural and statistical properties of the models used to model the higher-order conditional moments and the computational challenges in estimating these models. we also advocate the use of a maximum entropy density (med) as an alternative method, which circumvents some of the issues prevalent in these common approaches. © 2021 john wiley & sons ltd."
"the spread of seizures across brain networks is the main impairing factor, often leading to loss-of-consciousness, in people with epilepsy. despite advances in recording and modeling brain activity, uncovering the nature of seizure spreading dynamics remains an important challenge to understanding and treating pharmacologically resistant epilepsy. to address this challenge, we introduce a new probabilistic model that captures the spreading dynamics in patient-specific complex networks. network connectivity and interaction time delays between brain areas were estimated from white-matter tractography. the model’s computational tractability allows it to play an important complementary role to more detailed models of seizure dynamics. we illustrate model fitting and predictive performance in the context of patient-specific epileptor networks. we derive the phase diagram of spread size (order parameter) as a function of brain excitability and global connectivity strength, for different patient-specific networks. phase diagrams allow the prediction of whether a seizure will spread depending on excitability and connectivity strength. in addition, model simulations predict the temporal order of seizure spread across network nodes. furthermore, we show that the order parameter can exhibit both discontinuous and continuous (critical) phase transitions as neural excitability and connectivity strength are varied. existence of a critical point, where response functions and fluctuations in spread size show power-law divergence with respect to control parameters, is supported by mean-field approximations and finite-size scaling analyses. notably, the critical point separates two distinct regimes of spreading dynamics characterized by unimodal and bimodal spread-size distributions. our study sheds new light on the nature of phase transitions and fluctuations in seizure spreading dynamics. we expect it to play an important role in the development of closed-loop stimulation approaches for preventing seizure spread in pharmacologically resistant epilepsy. our findings may also be of interest to related models of spreading dynamics in epidemiology, biology, finance, and statistical physics. copyright: © 2023 moosavi, truccolo. this is an open access article distributed under the terms of the creative commons attribution license, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited."
"there are various indicators, i.e., relative strength index (rsi) [1] [2], moving average convergence divergence (macd) [3] [4] [5] [6], stochastic oscillator [7] [8] applications, to determine market movements with buying and selling decisions in computational finance, but they have drawbacks that induced discrepancies to match against the best trading times at fixed order-triggering boundaries and delay problems. for example, rsi [1] [2]’s 70 and 30 overbuy and oversell are fixed boundaries. orders can only be triggered when rsi’s value exceeds one of these boundaries, its computation only considers past market condition prompting indicators like rsi to trigger orders with delay. in this paper, we proposed a method to reduce these problems with advanced ai technologies to generate indicators’ buy and sell signals in the best trading time. recurrent neural network (rnn) [9] has outstanding performance to learn time-series data automatic with long-time sequences but its ordinary rnn units [10] [11] such as long-short-term-memory(lstm)[12] are unable to decipher the relationships between time units called context. hence, researchers have proposed an algorithm based on rnns’ attention mechanism [13] [14] allowing rnns to learn information such as chaotic attributes [15] [16] [17] [18] and quantum properties [19] [20] [21] contained in time sequences. chaos theory [15] [16] and quantum finance theory (qft) [22] are also proposed to simulate these two features (or attributes?). quantum price level (qpl) [22] [23] is one of the well-formed qft models to simulate all possible vibration levels to locate price. the system used in this paper consists of two components 1) neural network to predict future data and solve indicators lagging problem, and 2) fuzzy logic to solve fixed order-triggering boundaries problem. its system design has two main parts 1) chaotic hlco predictor consists of lstm, lee-oscillator and attention mechanism to predict the high, low, close and open, 2) qpl-based fuzzy logic trading strategy to receive the result and trigger trading signals. this new proposed model has obtained significant results in backtesting previous data and outperformed other traditional indicators to facilitate investment decisions when market changes constantly. codes are available at https://github.com/jarvislee0423/ chaotic quantum finance ai predicted trading system. © 2022, the author(s), under exclusive licence to springer-verlag gmbh germany, part of springer nature."
"with the rise of decentralized finance and insurance technology, there has been growing interest in the financial industry for risk sharing mechanisms without a central authority or clearing house. in contrast with classic centralized risk sharing, a novel peer-to-peer risk sharing framework is proposed. the presented framework aims to devise a risk allocation mechanism that is structurally decentralized, pareto optimal, and mathematically fair. an explicit form for the pool allocation ratio matrix is derived, and convex programming techniques are applied to determine the optimal pooling mechanism in a constrained variance reduction setting. a tiered hierarchical generalization is also constructed to improve computational efficiency. as an illustration, these techniques are applied to a flood risk pooling example. flood risk is known to be difficult to cover in practice, which contributes to the stagnant development for a private insurance market. it is shown in this paper that peer-to-peer risk sharing techniques provide an economically viable alternative to traditional flood insurance policies. © 2022, the author(s), under exclusive licence to springer science+business media, llc, part of springer nature."
"smartwatches are used by millions of people for applications in health, finance, and communication. as the computational power and range of applications supported by these devices expand, it is becoming more and more important to secure access to them. while various user authentication technologies have been extensively explored in smartphone use scenarios (e.g., faceid, fingerprint, pin, or pattern) the applicability of these approaches to smartwatches is typically limited due to the small watch form factor. to improve authentication on smartwatches, we propose sonarauth, a novel user authentication system for unmodified commercial smartwatches using behavioral biometrics derived from motion, touch, and around-device motions. to capture in-air hand motions, we adapted an existing sonar system for smartwatches. we collected data from 24 participants from single touch to the watch screen with the thumb, index, and middle fingers. using a multimodal deep learning classifier, we achieved a promising mean equal error rate(eer) of 6.41% for user authentication based on a single thumb tap. we note that our system is usable and has good potential to be combined with other authentication modalities. © 2023 owner/author."
"we introduce a numerical methodology, referred to as the transport-based mesh-free method, which allows us to deal with continuous, discrete, or statistical models in the same unified framework, and leads us to a broad class of numerical algorithms recently implemented in a python library (namely, codpy). specifically, we propose a mesh-free discretization technique based on the theory of reproducing kernels and the theory of transport mappings, in a way that is reminiscent of lagrangian methods in computational fluid dynamics. we introduce kernel-based discretizations of a variety of differential and discrete operators (gradient, divergence, laplacian, leray projection, extrapolation, interpolation, polar factorization). the proposed algorithms are nonlinear in nature and enjoy quantitative error estimates based on the notion of discrepancy error, which allows one to evaluate the relevance and accuracy of, both, the given data and the numerical solutions. our strategy is relevant when a large number of degrees of freedom are present as is the case in mathematical finance and machine learning. we consider the fokker–planck–kolmogorov system (relevant for problems arising in finance and material dynamics) and a class of neural networks based on support vector machines. © 2023, the author(s), under exclusive licence to springer science+business media, llc, part of springer nature."
"optimization is essential in various fields such as finance, transportation, energy, and health care. however, solving real optimization problems, especially nondeterministic polynomial, requires considerable computational resources. metaheuristics provide fast and cost-effective solutions to these problems. in this paper, eight state-of-the-art natureinspired metaheuristic algorithms that have demonstrated excellent performance are compared in detail. in addition, a novel tournament procedure has been proposed to produce a quality ranking of selected metaheuristic algorithms, which are compared based on their optimization results, even if they were not originally tested with the same set of test functions, but only partially. the selected algorithms are evaluated using thirty-two test functions, which is a representative sample size. the evaluation also showed that while one algorithm produced the best overall results, this does not mean that this algorithm is the best for solving each function. this also highlights the need for further research in metaheuristic algorithms. © 2023 marko gulić, martina žuškin & vilim kvaternik; published by uikten. this work is licensed under the creative commons attribution-noncommercial-noderivs 4.0 license."
"dynamic linear models with discounting are state-space models that are sufficiently flexible, interpretable, and computationally efficient. as such they are increasingly applied in economics and finance. successful modelling and forecasting with such models depends on an appropriate choice of the discount factor. in this work we develop an adaptive approach to sequentially estimate this parameter, which relies on the minimisation of the one-step-ahead forecast error. simulated data and an in-depth empirical application to the problem of forecasting quarterly uk house prices show that our approach can significantly improve forecast accuracy at a computational cost that is orders of magnitude smaller than approaches based on sequential monte carlo. we also conduct an extensive evaluation of diverse forecast combination methods for the task of predicting uk house prices. our results indicate that a recent density combination method can substantially improve forecast accuracy. © 2022 the author(s)"
"portfolio management is a critical problem in both machine learning and finance communities. to predict the returns of assets, existing studies have been leveraging side information to mine price-sensitive indicators, i.e., factors. however, with the brisk expansion of factor collection, existing factor selection methods face two main issues, i.e., high-cost and low-precision. in this paper, we first formalize the task of online factor selection as an online learning problem where a learner selects a set of factors in each round and aims to minimize the long-term regret. then, we propose a randomized online factor selection framework, named refer, which not only is particularly devised to address the above two issues, but also can widely serve for any existing multifactor models. specifically, by studying the regret of existing factor-selecting policies, we propose two randomized policies along with their bandit variants that achieve sublinear regrets. the bandit variants further improve computational efficiency, and achieve a balanced trade-off between cost and precision. finally, both theoretical analysis and extensive experiments on the real-world dataset demonstrate the effectiveness of our proposed framework and policies in terms of comprehensive evaluation criteria. © 2023 elsevier ltd"
"artificial intelligence (ai) involves the development of algorithms and computational models that enable machines to process and analyze large amounts of data, identify patterns and relationships, and make predictions or decisions based on that analysis. ai has become increasingly pervasive across a wide range of industries and sectors, with healthcare, finance, transportation, manufacturing, retail, education, and agriculture are a few examples to mention. as ai technology continues to advance, it is expected to have an even greater impact on industries in the future. for instance, ai is being increasingly used in the agri-food sector to improve productivity, efficiency, and sustainability. it has the potential to revolutionize the agri-food sector in several ways, including but not limited to precision agriculture, crop monitoring, predictive analytics, supply chain optimization, food processing, quality control, personalized nutrition, and food safety. this review emphasizes how recent developments in ai technology have transformed the agri-food sector by improving efficiency, reducing waste, and enhancing food safety and quality, providing particular examples. furthermore, the challenges, limitations, and future prospects of ai in the field of food and agriculture are summarized. © 2023 by the authors."
"at a worldwide scale, artificial intelligence (ai) is now an intrinsic part of various sectors and a significant strategic element in the business agendas of several industries including health care, finance and retail. machine learning, an statistical paradigm of ai, is one of the most preferred technologies to achieve ai. machine learning-oriented approaches exploit massive sized, unstructured and complicated dataset instances to learn from previous experiences and find insightful patterns. a range of statistical, probabilistic and optimization approaches are used to achieve this task. early detection of chronic diseases is critical in the realm of biomedical research and healthcare communities, where it is pivotal to primarily diagnose the particular disease at probabilistically early stage in order to lower the death rate. pneumonia mainly affects a huge number of people, particularly children and adults, in the developing and undeveloped nations that are characterized as overcrowding, inadequate sanitation, malnutrition, lack of suitable medical services and other risk factors. it is critical to detect pneumonia at its early stage in order to properly treat the infection. this paper presents an approach, i.e. dpud (disease prediction for unstructured data). the proposed framework consists of an statistically novel and fine-tuned algorithmic procedure to address certain pain points of categorization problem such as selection of an optimal set of model hyperparameters and attain an improved statistical performance metrics with multichannel shared activation function and cost function. in experimental evaluation, our approach capitulates state-of-the-art computational achievement on chest x-ray images for the pneumonia classification dataset, while being considerably much faster at specifically test time. the methodology presented in this chapter has an empirical scope for societal improvement, modernization and progress. © 2023, the author(s), under exclusive licence to springer-verlag gmbh germany, part of springer nature."
"crude oil plays an important role in the global economy, as it contributes one-third of the energy consumption worldwide. however, despite its importance in policymaking and economic development, forecasting its price is still challenging due to its complexity and irregular price trends. although a significant amount of research has been conducted to improve forecasting using external factors as well as machine-learning and deep-learning models, only a few studies have used hybrid models to improve prediction accuracy. in this study, we propose a novel hybrid model that captures the finer details and interconnections between multivariate factors to improve the accuracy of petroleum oil price prediction. our proposed hybrid model integrates a convolutional neural network and a recurrent neural network with skip connections and is trained using petroleum oil prices and external data directly accessible from the official website of south korea’s national oil corporation and the official yahoo finance site. we compare the performance of our univariate and multivariate models in terms of the pearson correlation, mean absolute error, mean squared error, root mean squared error, and r squared ((formula presented.)) evaluation metrics. our proposed models exhibited significantly better performance than the existing models based on long short-term memory and gated recurrent units, showing correlations of 0.985 and 0.988, respectively, for 10-day price predictions and obtaining better results for longer prediction periods when compared with other deep-learning models. we validated that our proposed model with skip connections outperforms the benchmark models and showed that the convolutional neural network using gated recurrent units with skip connections is superior to the compared models. the findings suggest that, to some extent, relying on a single source of data is ineffective in predicting long-term changes in oil prices, and thus, to develop a better prediction model based on time-series based data, it is necessary to take a multivariate approach and develop an efficient computational model with skip connections. © 2023 by the authors."
"the problem of food security is global in the context of sanctions and conflicts. the study is devoted to the assessment of the level of food security of the russian federation. the paper assesses the current level of food security in russia based on the analysis of trends in the development of the agro-industrial complex, and also identifies threats and challenges to russia's food security at the present stage. the research is based on statistical data provided by the ministry of agriculture of russia, as well as analysis methods including computational and constructive, monographic, economic and statistical, comparative and system analysis, synthesis, etc. the article suggests measures to strengthen russia's food security, taking into account the unpredictable development of the russian economy. the results of the work make it possible to conclude that, despite the problems, the russian agro-industrial complex successfully provides food to the population of the country and continues to gain momentum in the export of products. russian scientists clarify the concept of innovation, the latest threats, problems of food security in individual countries and regions. the situation is aggravated by such factors as the imposition of sanctions on persons related to agricultural assets, the impact of sanctions on transportation and finance, and other restrictions, bikies. the ukrainian crisis and western restrictions against russia are damaging food stability at both regional and country-wide, global levels. © the authors, published by edp sciences. this is an open access article distributed under the terms of the creative commons attribution license 4.0 (https://creativecommons.org/licenses/by/4.0/)."
"the monotonic dependence of the outputs of a neural network on some of its inputs is a crucial inductive bias in many scenarios where domain knowledge dictates such behavior. this is especially important for interpretability and fairness considerations. in a broader context, scenarios in which monotonicity is important can be found in finance, medicine, physics, and other disciplines. it is thus desirable to build neural network architectures that implement this inductive bias provably. in this work, we propose a weight-constrained architecture with a single residual connection to achieve exact monotonic dependence in any subset of the inputs. the weight constraint scheme directly controls the lipschitz constant of the neural network and thus provides the additional benefit of robustness. compared to currently existing techniques used for monotonicity, our method is simpler in implementation and in theory foundations, has negligible computational overhead, is guaranteed to produce monotonic dependence, and is highly expressive. we show how the algorithm is used to train powerful, robust, and interpretable discriminators that achieve competitive performance compared to current state-of-the-art methods across various benchmarks, from social applications to the classification of the decays of subatomic particles produced at the cern large hadron collider. © 2023 11th international conference on learning representations, iclr 2023. all rights reserved."
"inclusive computational practices are increasingly being employed to enrich knowledge and facilitate sensemaking in stem education. embedding computational activities in computer-supported collaborative learning environments can enhance students’ experiences. this study aimed to investigate the knowledge co-construction process within tailored student-led computational lab activities designed for a computational finance module. in particular, this study focused on the analysis of the effects of different lab practices and of group composition on knowledge co-construction. the groups designed for the lab activities were internally homogenous in terms of student ability. the sample consisted of 396 answers to a weekly survey filled out by all 50 of the undergraduate students who attended the module during the ay 2020/2021. the qualitative analysis relied on an adapted version of the interaction analysis model designed by gunawardena and colleagues for collaborative knowledge construction. quantitative analyses were then conducted to study how the different lab practices and the composition of the groups affected the interaction. the findings revealed that, although the lower phases were the most prevalent, significant negotiations of meaning and discussions were activated, especially in tasks guiding towards sensemaking. furthermore, the groups composed of lower-achieving students were the most engaged in negotiating and improving understanding as a result of the group interaction. © 2023 by the authors."
"forecasting the dynamical behaviors of nonlinear systems over long time intervals represents a great challenge for scientists and has become a very active area of research. the employment of the well-known artificial recurrent neural networks (rnns)-based models requires a high computational cost, and they usually maintain adequate accuracy for complicated dynamics over short intervals only. in this work, an efficient reservoir-computing (rc) approach is presented to predict the time evolution of the complicated dynamics of a fractional order hyperchaotic finance model. compared with the well-known deep learning techniques, the suggested rc-based forecasting model is faster, more accurate for long-time prediction, and has a smaller execution time. numerical schemes for fractional order systems are generally time-consuming. the second goal of the present study is to introduce a faster, more efficient, and simpler simulator to the fractional order chaotic/hyperchaotic systems. the rc model is utilized in a proposed rc-based digital image encryption scheme. security analysis is carried out to verify the performance of the proposed encryption scheme against different types of statistical, kpa, brute-force, cca, and differential attacks. © 2023 by the authors."
"what drives nonlinearities in modern macroeconomic models? building on global methods, the present paper isolates the effects associated with an occasionally binding constraint in a widely discussed macro-finance framework. only the proper computational treatment provides a holistic understanding of the (hidden) economics in tail events, and the full reflection of large and consecutive shocks in economic forecasts. an improper treatment provides an overestimation of asset pricing dynamics and a misleading association between high levels of capital and liquidity and high levels of investment. © 2023"
"this chapter aims to draw readers' attention on the challenges of modeling in finance. the new quantitative methods offer extraordinary capabilities with the latest algorithms using ai, ml, etc., aided by high technological computational power. however, the adoption of the latest tools and techniques comes with many challenges that are limited by human resources and nuances of financial industries. unlike the recommendation-based models in the technology industry, real money is at stake in the financial industry. hence, it is not very prudent to accept the result of quantitative methods without understanding the inherited risks. despite the hype created by data scientists, the financial industry cautiously adopted the highly complicated learning tools after due diligence because investor and shareholder money is at stake and experts want to strategize financial decisions based on the data model outputs. further, the chapter brings the key highlights of financial models. © 2023, igi global. all rights reserved."
"the rapid changes in the finance industry due to the increasing amount of data have revolutionized the techniques on data processing and data analysis and brought new theoretical and computational challenges. in contrast to classical stochastic control theory and other analytical approaches for solving financial decision-making problems that heavily reply on model assumptions, new developments from reinforcement learning (rl) are able to make full use of the large amount of financial data with fewer model assumptions and to improve decisions in complex financial environments. this survey paper aims to review the recent developments and use of rl approaches in finance. we give an introduction to markov decision processes, which is the setting for many of the commonly used rl approaches. various algorithms are then introduced with a focus on value- and policy-based methods that do not require any model assumptions. connections are made with neural networks to extend the framework to encompass deep rl algorithms. we then discuss in detail the application of these rl algorithms in a variety of decision-making problems in finance, including optimal execution, portfolio optimization, option pricing and hedging, market making, smart order routing, and robo-advising. our survey concludes by pointing out a few possible future directions for research. © 2023 the authors. mathematical finance published by wiley periodicals llc."
"optimal stopping is a fundamental class of stochastic dynamic optimization problems with numerous applications in finance and operations management. we introduce a new approach for solving computationally-demanding stochastic optimal stopping problems with known probability distributions. the approach uses simulation to construct a robust optimization problem that approximates the stochastic optimal stopping problem to any arbitrary accuracy; we then solve the robust optimization problem to obtain near-optimal markovian stopping rules for the stochastic optimal stopping problem. in this paper, we focus on designing algorithms for solving the robust optimization problems that approximate the stochastic optimal stopping problems. these robust optimization problems are challenging to solve because they require optimizing over the infinite-dimensional space of all markovian stopping rules. we overcome this challenge by characterizing the structure of optimal markovian stopping rules for the robust optimization problems. in particular, we show that optimal markovian stopping rules for the robust optimization problems have a structure that is surprisingly simple and finite-dimensional. we leverage this structure to develop an exact reformulation of the robust optimization problem as a zero-one bilinear program over totally unimodular constraints. we show that the bilinear program can be solved in polynomial time in special cases, establish computational complexity results for general cases, and develop polynomial-time heuristics by relating the bilinear program to the maximal closure problem from graph theory. numerical experiments demonstrate that our algorithms for solving the robust optimization problems are practical and can outperform state-of-the-art simulation-based algorithms in the context of widely-studied stochastic optimal stopping problems from high-dimensional option pricing. copyright: © 2023 informs."
"machine learning (ml) is efficiently disrupting and modernizing cities in terms of service quality for mobility, security, robotics, healthcare, electricity, finance, etc. despite their undeniable success, ml algorithms need crucial computational efforts with high-speed computing hardware to deal with model complexity and commitments to obtain efficient, reliable, and resilient solutions. quantum computing (qc) is presented as a strong candidate to help mls reach their best performance especially for cybersecurity issues and digital defense. this paper presents quantum support vector machine (qsvm) model to detect distributed denial of service (ddos) attacks on smart micro-grid (smg). an evaluation of our approach against a real dataset of ddos attack instances shows the effectiveness of our proposed model. finally, conclusions and some open issues and challenges of the fitting of ml with qc are presented. © 2023 by the author."
"the internet of things era demands elaboration and integration of data, especially seamless access to various sectoral activities. the complexity of forms, formulas, computational languages and applications of each data seamless are available in various fields. various agencies need a forum for unification as a means and medium of open source information for the public. the development of this research is based on gis optimization in the fusion of various seamless data to form a unified cross-sectoral geoportal construction. the approach taken is a collection of literature studies accompanied by a compilation of spatial data access links that are currently open and open. this collection of links will be organized into a geoportal framework that facilitates seamless data integration between divisions and institutions. the accumulation of seamless data access identified covers geospatial fields (data.humdata.org, tanahair.indonesia.go.id, tides.big.go.id, earthexplorer.usgs.gov, landsat-catalog.lapan.go.id, srgi.big.go.id), statistics (data.go.id, bps.go.id), climate (dataonline.bmkg.go.id), finance (dataworldbank.org, ojk.go.id), local government (data.jabarprov.go.id), disasters (geohazardstep.eu, inarisk.bnpb.go.id) and natural resources (geoportal.esdm.go.id) that exist in each institution in the structure of websites and webgis that developed expressly in the 4.0 era. a compilation of seamless access links in various fields presenting data, information, attractive, dynamic, and free access. the proposed architecture was formed to harmonize data to be simple and according to the wider community's needs. data synchronization plays a very crucial role, so must implement that multi-sectoral geoportal schemes. © 2023 american institute of physics inc.. all rights reserved."
"this paper discusses a numerical model study for the simulation of flow characteristics and critical submergence for a laterally placed horizontal circular bottom intake under uniform flow. the proposed model simulates the free surface using the volume of fluid model to check the vortex formation at critical submergence. a new combined approach using phase volume fraction and swirl strength-based vortex identification mechanism is used to compute the critical submergence. the swirl strength-based vortex identification mechanism can show the vortex tube in approach flow with swirl generated at the free surface due to the axial flow withdrawal through side bottom intake at critical submergence. the computational fluid dynamics (cfd) model results were validated using experimental data, which showed a maximum error of less than ±10% in the prediction of the critical submergence. the effect of significant parameters like intake and approach flow froude number and sill height of intake on the critical submergence is discussed. the results of this study help practitioners to adopt cfd-based numerical modelling for the design of water intakes instead of entirely relying on physical model studies, which require more finance and time. © 2023, canadian science publishing. all rights reserved."
"estimation of agent-based models in economics and finance confronts researchers with a number of challenges. typically, the complex structures of such models do not allow to derive closed-form likelihood functions so that either numerical approximations to the likelihood or moment-based estimators have to be used for parameter inference. however, all these approaches suffer from extremely high computational demands as they typically work with simulations (of the agent-based model) embedded in (monte carlo) simulations conducted for the purpose of parameter identification. one approach that is very generally applicable and that has the potential of alleviating the computational burden is approximate bayesian computation (abc). while popular in other areas of agent-based modelling, it seems not to have been used so far in economics and finance. this paper provides an introduction to this methodology and demonstrates its potential with the example of a well-studied model of speculative dynamics. as it turns out, abc appears to make more efficient use of moment-based information than frequentist smm (simulated method of moments), and it can be used for sample sizes of an order far beyond the reach of numerical likelihood methods.  © 2022 walter de gruyter gmbh, berlin/boston."
"albeit natural language processing has seen major breakthroughs in the last few years, transferring such advances into real-world business cases can be challenging. one of the reasons resides in the displacement between popular benchmarks and actual data. lack of supervision, unbalanced classes, noisy data and long documents often affect real problems in vertical domains such as finance, law and health. to support industry-oriented research, we present buster, a business transaction entity recognition dataset. the dataset consists of 3779 manually annotated documents on financial transactions. we establish several baselines exploiting both general-purpose and domain-specific language models. the best performing model is also used to automatically annotate 6196 documents, which we release as an additional silver corpus to buster. © 2023 association for computational linguistics."
"deep studying technologies (dlts) were adopted with the aid of many fields, consisting of healthcare, finance, and records technology. this paper ambitions to take a look at how dlts can be improved for facts technological know-how practices, in addition to to speak about their capacity for driving advances in statistics-pushed studies. first of all, we provide a top level view of the contemporary kingdom of dlts, and the important thing algorithms that cause them to appealing for data science packages. then, we delve into a number of the ways wherein dlts may be optimized for better overall performance. we illustrate the optimization strategies via numerous examples, which include community structure search, computational useful resource management, and active mastering. moreover, we explore the use of switch studying and reinforcement getting to know to in addition improve the performance of dlts in information science. in the end, we talk the benefits of dlts for statistics technology and their ability for advancing the field. this paper highlights the significance of optimizing dlts for records technological know-how, and the numerous possibilities that leveraging these technology will offer. © 2023 ieee."
"this study comprehensively reviews the key influential and intellectual aspects of machine learning in finance. the authors employ the bibliometric approach using vosviewer software to analyse 189 academic articles from the scopus database between 1988 and december 2022. our results revealed that machine learning in the finance literature has significantly increased since 2017, indicating that the finance industry had some time to adopt newer technology. the authors find that the united states, china, and the united kingdom were the countries that most frequently investigated this topic. it was also found that the steven institute of technology (new jersey, united states) is the most active research institute in this field. we also discovered that the application of machine learning has been adopted in crowdfunding, fintech, forecasting, bankruptcy prediction, and computational finance. our research is subject to several limitations. this research only utilised the scopus database and was restricted to articles written in english. our findings assist academic scholars in exploring issues related to machine learning in finance in future studies. the outcomes of the present study may also guide market participants, particularly fintech and finance companies, on how machine learning could be used in their decision-making. © 2023 the author(s). published by informa uk limited, trading as taylor & francis group."
"hierarchical identity-based encryption can efficiently reduce the workload of private key generation and key distribution of the private key generator in the identity-based cryptography. sm9 is an identity-based cryptosystem and has become a chinese cryptographic standard and national standard. it plays a significant role in many applications, such as finance and government affairs. however, sm9 encryption algorithm does not support hierarchical encryption, which is undesirable for the large network and becomes a bottleneck for its deployments. in this paper, we proposed an efficient hierarchical identity-based encryption scheme sm9-hibe based on sm9. compared to sm9 encryption algorithm, the ciphertext in sm9-hibe only requires an additional group element and the decryption overhead increases one pairing operation only, which is independent of the length of receiver’s identity. we prove that if the dbdhi assumption holds, our scheme is proved to be ind-scpa secure in the random oracle model. finally, we theoretically analyze our scheme and make a comparison. the result shows that the sm9-hibe is comparable to the existing hibe schemes in terms of computational cost and communication overhead. © 2023 science press (china). all rights reserved."
"this article presents the computational results obtained from a real-life implementation of quantum computing application in investment analytics, which algorithm was described by a previously published paper [5]. the new results reported in this article include: 1) speed-up tests based on idealized problems; 2) speed-ups computed on data from a professional platform, accounting for imperfections on very large financial datasets in finance; and 3) speed-ups from using similar data to solve analogous problems such as implied graphs to explain factor cuts. furthermore, the article addresses the implementation challenges and computational results, in terms of both speed-ups and error rates, along with any other alternative hardware solutions (if available) such as gpus for certain users who prefer classical computer solutions. that way, users can compare and contrast the state of the art today and in the foreseeable future. overall, this article provides insights into the potential of quantum computing to revolutionize the field of finance and highlights the progress made in leveraging artificial intelligence technology for solving real-world problems. © 2023 ieee."
"the database used by blockchain innovation is run by numerous components and creates a network pattern using hash indexes. the blockchain employs numerous nodes and transmits numerous data access points, decreasing reliance on the primary internet server and preventing the danger of data destruction and server breakdown. the data files saved in the blockchain are protected using encryption techniques to maintain their authenticity and prevent illegally added or discarded data files. with technological characteristics like non-tampering, distributed ledger, and reliability, blockchain innovations and machine learning have innate benefits in supply chain finance. they also have great prospects to construct faith in order to overcome the major issues in supply chain finance, which is helpful for fostering economic advancement in the tonkin gulf region. this paper focuses on introducing the study on using blockchain innovation in supply chain finance in the tonkin gulf region and aims to offer suggestions on how supply chain finance could evolve there using blockchain innovation. this paper suggests supply chain finance game applications for pertinent investigations as well as blockchain innovation, supply chain banking threats assessment on the blockchain and machine learning, and supply chain finance implementation study methodologies in the tonkin gulf region. the empirical findings presented in this paper demonstrate that the built blockchain supply chain finance platform has an algorithm with an overall processing time of 4.10 seconds, a computational efficiency that is quicker, and the ability to more accurately analyse the pertinent concerns.  © 2023 ieee."
"this paper surveys quantum multi-agent reinforcement learning (qmarl), an emerging fusion of quantum computing and multi-agent systems. it begins by introducing the transformative potential of quantum computing in computational capabilities. examining the principles of multi-agent reinforcement learning (marl), the paper explores how quantum computing enhances learning efficiency and decision-making. focusing on the current state of qmarl, it reviews literature, methodologies, and case studies showcasing the integration of quantum algorithms with marl frameworks. the survey addresses challenges and opportunities arising from quantum technologies in multi-agent systems, such as entanglement and superposition, exploring their implications for agent coordination and learning dynamics. practical applications in domains like cybersecurity and finance underscore qmarl's transformative potential. concluding, the paper identifies research gaps and proposes future directions, emphasizing the need for scalable quantum algorithms, exploring quantum-resistant strategies in adversarial settings, and integrating quantum principles in agent communication and collaboration. this concise survey serves as a foundational guide for researchers and practitioners, offering insights into the current state and future possibilities of qmarl.  © 2023 ieee."
"natural language processing, audio-visual recognition and computer vision all make extensive use of deep learning. research is becoming increasingly interested as a result of deep learning's resounding success as a computational tool. deep learning is being used in financial and banking services as a result of fintech's recent growth. the extant literature does not, however, provide a comprehensive overview of deep learning's uses in banking and finance. the goal of this study is to give a thorough review of the model preparation, input variables, and model evaluation by reviewing and analyzing the work on the use of deep models in the major financial and banking domains. last but not least, we go through three factors that could influence how financial deep learning models perform. insight and recommendations on the state-of-the-art of deep learning models' use in banking and finance are given to academics and practitioners by this study.  © 2023 ieee."
"numbers are crucial for various real-world domains such as finance, economics, and science. thus, understanding and reasoning with numbers are essential skills for language models to solve different tasks. while different numerical benchmarks have been introduced in recent years, they are limited to specific numerical aspects mostly. in this paper, we propose a hierarchical taxonomy for numerical reasoning skills with more than ten reasoning types across four levels: representation, number sense, manipulation, and complex reasoning. we conduct a comprehensive evaluation of state-of-the-art models to identify reasoning challenges specific to them. henceforth, we develop a diverse set of numerical probes employing a semi-automated approach. we focus on the tabular natural language inference (tnli) task as a case study and measure models' performance shifts. our results show that no model consistently excels across all numerical reasoning types. among the probed models, flant5 (few-/zero-shot) and gpt-3.5 (few-shot) demonstrate strong overall numerical reasoning skills compared to other models. label-flipping probes indicate that models often exploit dataset artifacts to predict the correct labels. © 2023 association for computational linguistics."
"despite many efforts to increase access to financial services, 1,4 billion people still are unbanked. one significant barrier to decreasing this number is the lack of official personal documents (e.g., government-issued identification or utility bills) to comply with the necessary kyc/aml regulation. innovative schemes can recognize one by using inputs like the personal trail generated when one uses the phone or engages in some digital activity. this paper proposes a formal language-based approach for modeling financial inclusion services and for representing in a structured way the existing kyc/aml compliance rules from different countries. currently, those rules are written in an unstructured format using natural language and spread in regulatory documents from these jurisdictions. our proposed language is a core building block of a computational trust and risk engine model, also discussed in this paper. our approach supports the use of traditional and innovative recognition schemes, helping to overcome the barrier for those who cannot comply with conventional kyc/aml requirements. moreover, it can also be used to power the risk calculation of computational trust and risk engines. finally, the proposal is generic enough to be applied to both traditional and decentralized finance.  © 2023 ieee."
"in the last few years, space solar power has received a large increase in international attention, development funding, and was even highlighted in the five 2025 european space agency (esa) development goals. these changes are happening in parallel with multiple reports in 2021 and 2022, including from this author, that recent dramatic price reductions in the cost of launch, production of space hardware, computational capacity, and robotic assembly has made the once ludicrously expensive idea of space solar power into an economically viable opportunity. while it has been technically viable for decades, now space solar power has been found to be economically viable as well; it only awaits a first mover to demonstrate the technology and verify its theoretical viability. the recent spike of funding for space solar power development, including the esa's solaris project and donald bren's $100 million investment into research at caltech, represents only a fraction of the money and time invested since its initial suggestion in the 1940s and refinement in 1968. the next rounds of funding required to build a demonstrator and pilot plant are already standing on a large foundation of investment and research history. several other industries, notably solar photovoltaics, have also received billions in development in the last decade alone and have years of commercial experience and maturation which directly benefit the building of a technically and economically viable space solar power project. this paper focuses on the opportunity of space solar power looking through three lenses: as an economic engine; its strategic geopolitical value; and its ability for addressing environmental concerns and net zero targets. this analysis draws on present and historical findings from the international academy of astronautics (iaa) decadal assessments and a state of industry report on space solar power. based on this analysis, this paper asserts that governments and philanthropic funding sources have a powerful opportunity to help accelerate the development of space solar power with the infusion of public support, bolstered credit ratings, and source capital. this support will help catalyze and crowd-in private capital and other large investment groups including sovereign wealth funds to finance this large energy project while minimizing the cost to the public. copyright © 2023 by the international astronautical federation (iaf). all rights reserved."
"long-term forecasting is widely used in meteorology, hydrology, and finance. however, non-stationary time series make it hard to make accurate long-term predictions because of their complicated multi-period local-global temporal dynamic patterns. currently, state-of-the-art methods use transformers or temporal convolutions to obtain global and local temporal dynamic patterns. nevertheless, the former suffers from the computational complexity of self-attention mechanisms despite having a global temporal receptive field. despite being able to catch local temporal patterns, the latter requires additional layers to capture global temporal patterns. moreover, the present research disregards integrating multi-period patterns into longterm forecasting. in this paper, we propose mlgnet to tackle the mentioned challenges, which integrates local and global temporal dynamic patterns with multiple periods for longterm forecasting. in particular, we suggest using the maximal overlap discrete wavelet transform (modwt) as a multi-period decoupling method to decompose non-stationary time series and apply it for the first time to long-term forecasting. in addition, we suggest a multi-scale encoder-decoder framework to capture and fuse local-global temporal dynamic patterns in each decomposed period. inception dilated causal convolutions-based encoder and a lightweight mlp-based decoder in the framework capture local and global temporal dynamic patterns in series while avoiding the high computational complexity of self-attention mechanisms. lastly, we suggest time-separable convolutions for aggregating information on temporal dynamic patterns among multiple periods. the above method helps mlgnet better balance the representation ability of time series in 1d and 2d space. evaluation of five benchmark datasets shows that mlgnet outperforms traditional and state-of-the-art methods, with relative improvements of 13.8 % and 21.9% for multivariate and univariate long-term forecasting, respectively.  © 2023 ieee."
"in areas like businesses, finance, and science, data analysis plays a very important role. data analysis needs to be automated to improve productivity and eliminate human error as data volumes and complexity grow. python is a well-liked language for automated data analysis because of its ease of use and broad library support. for automated data analysis, we are evaluating many well-known python libraries, such as python and pandas, matplotlib, seaborn scikit-learn, and tensorflow is used in this text. we evaluate the usability, computational efficiency, and accuracy of these libraries when performing various data analysis tasks. we also go into great length on the uses of each library in various sectors, as well as its benefits and drawbacks. our study shows that the user's level of experience and the type of data analysis task both affect the library choice. we came to the conclusion that choosing the right library is crucial for automating data analysis using python and significantly affects the accuracy and efficacy of the study.  © 2023 ieee."
"in the era of big data, the analysis of large-scale time series data has gained significant importance across various applications such as finance and healthcare. however, traditional clustering approaches, often encounter accuracy and scalability issues when applied to extensive data sets. this paper introduces innovative clustering method specifically tailored for time series data to enter these challenges. these methods leverage advanced data structures for computational efficiency, optimal distance measurements to improve, improve accuracy, and parallel computing framework for enhanced scalability. rig testing involving millions of time series occurrences from real world. data sets was conducted to evaluate the effectiveness of the proposed methods. the results indicate that these novel clustering algorithms can deliver high-quality outcomes even when handling massive data sets. the exhibit verity in accommodating different time series lengths and resilience in handling noise. these characteristics contribute to effective and scalable time series clustering, entering the proposed approaches suitable for real-time analytics in data intensive environments. consequently, this research facilitates the extraction of actionable insights in real time, offering valuable contributions for both academia and industry practitioners. additionally, it broadens the spectrum of tools available for the analysis and interpretation of large-scale time series data.  © 2023 ieee."
"about sixty years ago, the anomalous magnetic response of certain magnetic alloys drew the attention of theoretical physicists. it soon became clear that understanding these systems, now called spin glasses, would give rise to a new branch of statistical physics. as physical materials, spin glasses were found to be as useless as they were exotic. they have nevertheless been recognized as paradigmatic examples of complex systems with applications to problems as diverse as neural networks, amorphous solids, biological molecules, social and economic interactions, information theory and constraint satisfaction problems. this book presents an encyclopaedic overview of the broad range of these applications. more than 30 contributions are compiled, written by many of the leading researchers who have contributed to these developments over the last few decades. some timely and cutting-edge applications are also discussed. this collection serves well as an introduction and summary of disordered and glassy systems for advanced undergraduates, graduate students and practitioners interested in the topic. sample chapter(s) preface chapter 1: simulated annealing, optimization, searching for ground states chapter 34: future perspectives contents: simulated annealing, optimization, searching for ground states (sergio caracciolo, alexander hartmann, scott kirkpatrick and martin weigel) beyond the ising spin glass i: m-vector, potts, p-spin, spherical, induced moment, random graphs (david sherrington and jairo r l de almeida) beyond the ising spin glass ii: spin glass without replicas (j michael kosterlitz) renormalization group in spin glasses (tom lubensky, tamás temesvári, imre kondor and maria chiara angelini) numerical simulations and replica symmetry breaking (víctor martín-mayor, juan j ruiz-lorenzo, beatriz seoane and a peter young) the high-dimensional landscape paradigm: spin-glasses, and beyond (valentina ros and yan v fyodorov) universal aspects of the structural glass transition from density functional theory (theodore r kirkpatrick and dave thirumalai) non-perturbative processes in glasses (peter g wolynes and tommaso rizzo) dynamical mean-field theory and the aging dynamics (andrea crisanti, silvio franz, jorge kurchan and andrea maiorano) dynamical heterogeneity in glass-forming liquids (giulio biroli, kunimasa miyazaki and david r reichman) the kauzmann transition to an ideal glass phase (chiara cammarota, misaki ozawa and gilles tarjus) the gardner glass (pierfrancesco urbani, yuliang jin and hajime yoshino) the jamming transition and the marginally stable solid (francesco arceri, eric i corwin and corey s o'hern) from polymers to the kpz equation (victor dotsenko, pierre le doussal and henri orland) emergent dynamics in glasses and disordered systems: correlations and avalanches (annette zippelius, matthias fuchs, alberto rosso, james p sethna and matthieu wyart) replica symmetry breaking in random lasers: experimental measurement of the overlap distribution (claudio conti, neda ghofraniha, luca leuzzi and giancarlo ruocco) anderson localization on the bethe lattice (saverio pascazio, antonello scardicchio and marco tarzia) quantum glasses (leticia f cugliandolo and markus müller) the cavity method: from exact solutions to algorithms (alfredo braunstein and guilhem semerjian) message passing and its applications (florent krzakala, manfred opper and david saad) information and communication (yoshiyuki kabashima and toshiyuki tanaka) the mighty force: statistical inference and high-dimensional statistics (erik aurell, jean barbier, aurélien decelle and roberto mulet) disordered systems insights on computational hardness (david gamarnik, cris moore and lenka zdeborová) neural networks: from the perceptron to deep nets (marylou gabrié, surya ganguli, carlo lucibello and riccardo zecchina) from the statistical physics of disordered systems to neuroscience (nicolas brunel, rémi monasson, haim sompolinsky and j leo van hemmen) statistical physics of biological molecules (simona cocco, andrea de martino, andrea pagnani, martin weigt and felix ritort) application of spin glass ideas in social sciences, economics and finance (jean-philippe bouchaud, matteo marsili and jean-pierre nadal) complex dynamics in ecological systems and animal behavior (m cristina marchetti, irene giardina and a altieri) optimization of random high-dimensional functions: structure and algorithms (antonio auffinger, andrea montanari and eliran subag) rigorous results in the sherrington–kirkpatrick model (wei-kuo chen, dmitry panchenko and francesco guerra) random energy models: broken replica symmetry and activated dynamics (bernard derrida, peter mottishaw and véronique gayrard) rigorous results: random constraint satisfaction problems (amin coja-oghlan, allan sly and nike sun) metastates and replica symmetry breaking (c m newman, n read and d l stein) future perspectives (giorgio parisi) readership: advanced undergraduate and graduate students, researchers and practitioners in the fields of statistical physics and its applications, with a particular focus on glassy and disordered systems, both classical and quantum, and computer science, ecological, biological and financial applications. © 2023 by world scientific publishing co. pte. ltd. all rights reserved."
"the database used by blockchain innovation is run by numerous components and creates a network pattern using hash indexes. the blockchain employs numerous nodes and transmits numerous data access points, decreasing reliance on the primary internet server and preventing the danger of data destruction and server breakdown. the data files saved in the blockchain are protected using encryption techniques to maintain their authenticity and prevent illegally added or discarded data files. with technological characteristics like non-tampering, distributed ledger, and reliability, blockchain innovations and machine learning have innate benefits in supply chain finance. they also have great prospects to construct faith to overcome the major issues in supply chain finance, which helps foster economic advancement in the tonkin gulf region. to provide recommendations on how supply chain finance could advance in the tonkin gulf region through the application of blockchain innovation, this study emphasises providing research on the topic. this article proposes blockchain innovation, supply chain banking threats assessment on the blockchain and machine learning, supply chain finance implementation research methodology in the tonkin gulf region, and supply chain finance game applications for relevant studies. the empirical findings presented in this paper demonstrate that the built blockchain supply chain finance platform has an algorithm with an overall processing time of 4.10 seconds, a computational efficiency that is quicker, and the ability to analyse the pertinent concerns more accurately.  © 2023 ieee."
"in the steady development of social economy, the technology theory with the computational intelligence method as the core is widely used in the financial business operation, based on artificial neural network and evolutionary algorithm for theoretical improvement and application innovation, can really achieve the development of financial intelligence. intelligent finance, as an important link to accelerate industrial intelligent upgrading, has a large gap between its overall development level and that of developed countries, and scientific research institutions and industrial enterprises have not formed an ecosystem and industrial chain with international influence. therefore, based on understanding the development status of financial business in china, this paper puts forward a network model based on som neural network, and the clustering simulation experiment analysis of listed companies in some region. the final results show that the research network model in this paper has stronger clustering ability, lower computational complexity, and faster convergence speed of practical work, which has a positive impact on the development of financial intelligence in the new era. © 2023 spie."
"meta-heuristic algorithms were effectively utilized in solving a wide array of problems in business, science, data mining, and finance. they are population-based approaches that develop novel optimal solutions based on the natural behaviors of creatures. a meta-heuristic method implied in reducing the number of present features and improving classification accuracy is the wrapper feature selection approach which was specifically proposed in this paper. this approach was based on the capuchin search algorithm (capsa) [1], which is mainly inspired by the functional behavior of capuchin monkeys in food-searching actions through leaping, swinging, and climbing, all implemented in solving constrained and global optimization problems. this paper aims to enhance the capsa algorithm to search in a binary search space rather than a continuous one. the proposed improvements on the algorithm can efficiently outperform other present algorithms with fewer selected features, less computational time, and high solution accuracy. the enhanced algorithm, named (ebcapsa) was tested afterwards on several biological benchmark datasets and then followed by a broad comparative study with four other algorithms, including the gray wolf optimizer (gwo) [2], particle swarm optimization (pso) [3], the firefly algorithm (fa) [4], the whale optimization algorithm (woa) [5] and the original capsa algorithm. overall results rendered a superiority of the proposed approach and clear outperformance based on the quality of the final solution in searching for the most favorable feature subsets. © 2023 ieee."
"this research study analyzes the effectiveness of two rosca modifications that have the potential to improve community resilience using computational tools and dynamical systems modeling. in order to lessen the impact of people quitting the association, first suggest an approach that sets aside a portion of each member's contributions to the rosca. then investigate the effects of this strategy on people's financial well-being and interpersonal trust in the community. this research offers a fresh perspective on how to comprehend the risk-taking behavior of bankers. expand on earlier research that indicates using artificial intelligence algorithms is a useful way to get this knowledge. the strategy makes use of a special decision-making model andbehavioral finance. © 2023 ieee."
"large language models (llms) have exploded in popularity due to their new generative capabilities that go far beyond prior state-of-the-art. these technologies are increasingly being leveraged in various domains such as law, finance, and medicine. however, these models carry significant computational challenges, especially the compute and energy costs required for inference. inference energy costs already receive less attention than the energy costs of training llms-despite how often these large models are called on to conduct inference in reality (e.g., chatgpt). as these state-of-the-art llms see increasing usage and deployment in various domains, a better understanding of their resource utilization is crucial for cost-savings, scaling performance, efficient hardware usage, and optimal inference strategies. in this paper, we describe experiments conducted to study the computational and energy utilization of inference with llms. we benchmark and conduct a preliminary analysis of the inference performance and inference energy costs of different sizes of llama-a recent state-of-the-art llm-developed by meta ai on two generations of popular gpus (nvidia v100 & a100) and two datasets (alpaca and gsm8k) to reflect the diverse set of tasks/benchmarks for llms in research and practice. we present the results of multi-node, multi-gpu inference using model sharding across up to 32 gpus. to our knowledge, our work is the one of the first to study llm inference performance from the perspective of computational and energy resources at this scale.  © 2023 ieee."
"applying existing question answering (qa) systems to specialized domains like law and finance presents challenges that necessitate domain expertise. although large language models (llms) have shown impressive language comprehension and in-context learning capabilities, their inability to handle very long inputs/contexts is well known. tasks specific to these domains need significant background knowledge, leading to contexts that can often exceed the maximum length that existing llms can process. this study explores leveraging the semi-structured nature of legal and financial data to efficiently retrieve relevant context, enabling the use of llms for domain-specialized qa. the resulting system outperforms contemporary models and also provides useful explanations for the answers, encouraging the integration of llms into legal and financial nlp systems for future research. © 2023 association for computational linguistics."
"financial distress prediction (fdp) is a vital area of research in engineering fields, finance, economy, and accounting that forecasts if the firm is failing or not according to current financial data of firms with artificial intelligence (ai), mathematical, and statistical methods. fuzzy decision-making is a kind of decision-making procedure which is utilized if uncertainty or incomplete data is existing. fuzzy decision-making is suitable in situations whereas where is required for making a decision dependent upon incomplete or uncertain data, or once there are several conflicting objectives which require that considered. it is frequently utilized in domains like management science, engineering, and economics. therefore, this study develops a quantum chameleon swarm optimization with fuzzy decision making tool (qcso-fdmt) for financial risk management. the purpose of the qcso-fdmt system is to determine if the financial firm undergoes distress or not. to accomplish this, the qcso-fdmt technique employs data pre-processing to normalize the input financial data. next, the qcso-fdmt technique derives the qcso algorithm for the optimal selection of features to improve the classification performance and reduce the computational complexity. finally, the fdmt technique receives the chosen features as input and accomplishes the classification method. to highlight the improved predictive outcome of the qcso-fdmt method, a series of simulations take place on financial datasets and the outcome stated the betterment of the qcso-fdmt methodology over other existing algorithms.  © 2023 ieee."
"this paper focuses on the application of advanced neural network frameworks, specifically long short-term memory (lstm) networks, to tackle complex time series data prediction challenges. it aims to elucidate the methodology, model architecture, parameter optimization, and potential applications of lstm-based models in the context of time series forecasting. additionally, the study incorporates supplementary techniques such as dynamic windows to enhance prediction accuracy. lstm networks are employed as a central component of this research due to their recurrent nature and memory retention capabilities, which make them well-suited to capturing temporal dependencies and patterns inherent in time series data. the model architecture is thoughtfully designed, considering factors such as the number of lstm layers, hidden units, and dropout rates, to align with the specific characteristics of the dataset. parameter tuning is performed through an extensive iterative process, encompassing over 200 training and validation iterations, to maximize model performance. to illustrate the proposed methodology's practicality, the study includes a real-world example involving 'wordle' lexical puzzles. this application serves as empirical evidence of the effectiveness and applicability of lstm-based models in solving complex time series prediction problems. looking forward, lstm-based models hold significant promise in various domains, including finance, weather forecasting, and healthcare, among others. as computational resources continue to advance, the ability to train more intricate lstm architectures offers the potential to enhance predictive accuracy further.  © 2023 ieee."
"anomalies refer to data points or events that deviate from normal and homogeneous events, which can include fraudulent activities, network infiltrations, equipment malfunctions, process changes, or other significant but infrequent events. prompt detection of such events can prevent potential losses in terms of finances, information, and human resources. with the advancement of computational capabilities and the availability of large datasets, anomaly detection has become a major area of research. among these, anomaly detection in time series has gained more attention recently due to the added complexity imposed by the time dimension. this study presents a novel framework for time series anomaly detection using a combination of bidirectional long short term memory (bi-lstm) architecture and autoencoder. the bi-lstm network, which comprises two unidirectional lstm networks, can analyze the time series data from both directions and thus effectively discover the long-term dependencies hidden in the sequential data. meanwhile, the autoencoder mechanism helps to establish the optimal threshold beyond which an event can be classified as an anomaly. to demonstrate the effectiveness of the proposed framework, it is applied to a real-world multivariate time series dataset collected from a wind farm. the bi-lstm autoencoder model achieved a classification accuracy of 96.79% and outperformed more commonly used lstm autoencoder models.  © 2023 ieee."
"due to the increasing risk of data security, distributed learning model based on real-world data analytics has attracted more attention, and it has been applied in a variety of areas ranging from medical screening to agriculture, industry, finance, and defense science. generally, participants provide their own private datasets to efficiently train the distributed models on real-world data, which inevitably leads to privacy and security concerns. without uploading raw training data, federated learning enables large-amount nodes to train a distributed model and preserves security and privacy of user sensitive information. however, federated learning is limited by expensive computational costs during collective parameter server aggregation. moreover, malicious nodes among computing nodes interfere model training to some extent and further cause the leakage of data privacy. to address the above-mentioned problems, we propose a novel decentralized federated learning by integrating blockchain and federation learning for efficient node selection and communication. based on the proposed model, a reputation-based learning nodes selection algorithm is presented to measure the probability of honest participation of distributed nodes. the simulation results demonstrate that our rblns is capable of improving the training result significantly and decreasing convergence time.  © 2023 ieee."
"development funds are essential to finance climate change adaptation and are thus an important part of international climate policy. however, the absence of a common reporting practice makes it difficult to assess the amount and distribution of such funds. this problem has attracted attention in international affairs research and is increasingly being investigated using methods of the broader field of computational social science. lately, the mentioned research field has questioned the credibility of reported figures, indicating that adaptation financing is in fact lower than published figures suggest. projects claiming a greater relevance to climate change adaptation than they target are referred to as ""overreported"". to estimate realistic rates of overreporting in large data sets over time, we propose an approach based on state-of-the-art text classification. to date, assessments of credibility have relied on small, manually evaluated samples. we use such a sample data set to train a classifier with an accuracy of 89.81% ± 0.83% (tenfold cross-validation) and extrapolate to larger data sets to identify overreporting. additionally, we propose a method that incorporates evidence of smaller, higher-quality data to correct predicted rates using bayes' theorem. this enables a comparison of different annotation schemes to estimate the degree of overreporting in climate change adaptation. our results support findings that indicate extensive overreporting of 32.03% with a credible interval of [19.81%; 48.34%]. © 2023 association for computational linguistics."
"the indeterminacy of financial markets leads investors to face different types of security returns. usually, security returns are assumed to be random variables when sufficient transaction data are available. if data are missing, they can be regarded as uncertain variables. however, uncertainty and randomness coexist. in this situation, chance theory is the main tool to deal with this complex phenomenon. this paper investigates the conditional value at risk (cvar) of uncertain random variables and its application to portfolio selection. first, we define the cvar of uncertain random variables and discuss some of its mathematical properties. then, we propose an uncertain random simulation to approximate the cvar. next, we define the inverse function of the cvar of uncertain random variables, as well as a computational procedure. as an application in finance, we establish uncertain random mean-cvar portfolio selection models. we also perform a numerical example to illustrate the applicability of the proposed models. finally, we numerically compare the mean-cvar models with the mean-variance models with respect to the optimal investment strategy.  © 2023 world scientific publishing company."
"machine learning has been widely used as part of financial markets investment strategies, whether for forecasting the financial assets exchange rate, managing market volatility, or solving different classification problems that help with decision-making. building an investment strategy using a scientific approach requires a massive amount of data, good computational power, and some expertise in the finance industry. machine learning applications to the financial field, such as price exchange rate prediction, market pattern recognition, or other trading strategy tasks, are considered optimization problems. as they require an efficient algorithm dedicated to finding a global optimum, they can be solved using metaheuristics. in this survey, we study how metaheuristic optimization techniques contribute to building a robust learning model dedicated to financial investment strategy applications.  © 2023 ieee."
"this book presents contemporary issues and challenges in finance and risk management in a time of rapid transformation due to technological advancements. it includes research articles based on financial and economic data and intends to cover the emerging role of analytics in financial management, asset management, and risk management. analytics in finance and risk management covers statistical techniques for data analysis in finance. it explores applications in finance and risk management, covering empirical properties of financial systems. it addresses data science involving the study of statistical and computational models and includes basic and advanced concepts. the chapters incorporate the latest methodologies and challenges facing financial and risk management and illustrate related issues and their implications in the real world. the primary users of this book will include researchers, academicians, postgraduate students, professionals in engineering and business analytics, managers, consultants, and advisors in it firms, financial markets, and services domains. © 2024 selection and editorial matter, nga thi hong nguyen, shivani agarwal, and ewa ziemba."
"a core research area of computational behavioral finance investigates emergent price dynamics when heterogeneous traders follow a mix of rule-based strategies and interact indirectly through a limit order book. this paper offers a detailed specification of such a model in order to raise questions about some previous findings. the questions force a comprehensive reconsideration of the price dynamics of a well-known model. this leads to a surprising clarification of the contributions of various trading strategies to market outcomes: a popular characterization of chartism proves largely irrelevant for price dynamics. we also shed new light on the volume-volatility relationship, and provide improved visualizations to expose market behavior. © 2023 informa uk limited, trading as taylor & francis group."
"we conduct an inquiry into the sociotechnical aspects of sentiment analysis (sa) by critically examining 189 peer-reviewed papers on their applications, models, and datasets. our investigation stems from the recognition that sa has become an integral component of diverse sociotechnical systems, exerting influence on both social and technical users. by delving into sociological and technological literature on sentiment, we unveil distinct conceptualizations of this term in domains such as finance, government, and medicine. our study exposes a lack of explicit definitions and frameworks for characterizing sentiment, resulting in potential challenges and biases. to tackle this issue, we propose an ethics sheet encompassing critical inquiries to guide practitioners in ensuring equitable utilization of sa. our findings underscore the significance of adopting an interdisciplinary approach to defining sentiment in sa and offer a pragmatic solution for its implementation. ©2023 association for computational linguistics."
"with the agile development process of most academic and corporate entities, designing a robust computational back-end system that can support their ever-changing data needs is a constantly evolving challenge. we propose the implementation of a data and language-agnostic system design that handles different data schemes and sources while subsequently providing researchers and developers a way to connect to it that is supported by a vast majority of programming languages. to validate the efficacy of a system with this proposed architecture, we integrate various data sources throughout the decentralized finance (defi) space, specifically from defi lending protocols, retrieving tens of millions of data points to perform analytics through this system. we then access and process the retrieved data through several different programming languages (r-lang, python, and java). finally, we analyze the performance of the proposed architecture in relation to other high-performance systems and explore how this system performs under a high computational load.  © 2023 ieee."
"artificial immune systems (ais) are a class of computationally intelligent systems inspired by the adaptive abilities of the natural immune system. however, ais has limitations in computational efficiency, scalability, and handling of high-dimensional data. this paper reviews the fusion of ais with other ai algorithms, including neural networks, evolutionary algorithms, and swarm intelligence, to enhance performance by leveraging their respective strengths. a comprehensive literature review was conducted analyzing over 12 papers on ais hybridization published in the last decade. the works were categorized based on the integrated techniques and synthesized to identify benefits and challenges. the integration of ais with the reviewed algorithms showed several benefits such as improved accuracy, faster processing, enhanced optimization and global search capabilities, and the ability to handle uncertainty and vagueness. however, challenges like increased complexity, parameter tuning, and algorithm selection were also noted. this review provides valuable insights into the fusion of ais with diverse ai techniques for developing novel systems with enhanced adaptability, problem-solving capabilities, and real-world applicability. the identified trends pave the way for future innovations in ais-based intelligent systems across security, medicine, finance, and other domains.  © 2023 ieee."
"the recent llms like gpt-4 and palm-2 have made tremendous progress in solving fundamental math problems like gsm8k by achieving over 90% accuracy. however, their capabilities to solve more challenging math problems which require domain-specific knowledge (i.e. theorem) have yet to be investigated. in this paper, we introduce theoremqa, the first theorem-driven question-answering dataset designed to evaluate ai models' capabilities to apply theorems to solve challenging science problems. theoremqa is curated by domain experts containing 800 high-quality questions covering 350 theorems1 from math, physics, ee&cs, and finance. we evaluate a wide spectrum of 16 large language and code models with different prompting strategies like chain-of-thoughts and program-of-thoughts. we found that gpt-4's capabilities to solve these problems are unparalleled, achieving an accuracy of 51% with program-of-thoughts prompting. all the existing open-sourced models are below 15%, barely surpassing the random-guess baseline. given the diversity and broad coverage of theoremqa, we believe it can be used as a better benchmark to evaluate llms' capabilities to solve challenging science problems. © 2023 association for computational linguistics."
"quantities are essential in documents to describe factual information. they are ubiquitous in application domains such as finance, business, medicine, and science in general. compared to other information extraction approaches, interestingly only a few works exist that describe methods for a proper extraction and representation of quantities in text. in this paper, we present such a comprehensive quantity extraction framework from text data. it efficiently detects combinations of values and units, the behavior of a quantity (e.g., rising or falling), and the concept a quantity is associated with. our framework makes use of dependency parsing and a dictionary of units, and it provides for a proper normalization and standardization of detected quantities. using a novel dataset for evaluation, we show that our open source framework outperforms other systems and - to the best of our knowledge - is the first to detect concepts associated with identified quantities. the code and data underlying our framework are available at https://github.com/vivkaz/cqe. ©2023 association for computational linguistics."
"for probability measures μ, ν and ρ, define the cost functionals {equation presented} where · , · denotes the scalar product and π <· , ·> is the set of couplings. we show that two probability measures μ and ν on rd with finite first moments are in convex order (i.e., μ ≤c ν) iff c (μ, ρ) ≤ c (ν, ρ) holds for all probability measures ρ on rd with bounded support. this generalizes a result by carlier. our proof relies on a quantitative bound for the infimum of {equation presented} over all 1-lipschitz functions f, which is obtained through optimal transport (ot) duality and the characterization result of ot (couplings) by rüschendorf, by rachev, and by brenier. building on this result, we derive new proofs of well known one-dimensional characterizations of convex order. we also describe new computational methods for investigating convex order and applications to model-independent arbitrage strategies in mathematical finance.  © 2023 the author(s), published by de gruyter."
"conventional approaches to text classification typically assume the existence of a fixed set of predefined labels to which a given text can be classified. however, in real-world applications, there exists an infinite label space for describing a given text. in addition, depending on the aspect (sentiment, topic, etc.) and domain of the text (finance, legal, etc.), the interpretation of the label can vary greatly. this makes the task of text classification, particularly in the zero-shot scenario, extremely challenging. in this paper, we investigate the task of zero-shot text classification with the aim of improving the ability of pre-trained language models (plms) to generalize to both seen and unseen data across varying aspects and domains. to solve this we introduce two new simple yet effective pre-training strategies, implicit and explicit pre-training. these methods inject aspect-level understanding into the model at train time with the goal of conditioning the model to build task-level understanding. to evaluate this, we construct and release utcd, a new benchmark dataset for evaluating text classification in zero-shot settings. experimental results on utcd show that our approach achieves improved zero-shot generalization on a suite of challenging datasets across an array of zero-shot formalizations. © 2023 association for computational linguistics."
"in the realm of financial markets, the precise prediction of option prices remains a cornerstone for effective portfolio management, risk mitigation, and ensuring overall market equilibrium. traditional models, notably the black-scholes, often encounter challenges in comprehensively integrating the multifaceted interplay of contemporary market variables. addressing this lacuna, this study elucidates the capabilities of a novel deep residual convolution long short-term memory (dr-clstm) network, meticulously designed to amalgamate the superior feature extraction prowess of convolutional neural networks (cnns) with the unparalleled temporal sequence discernment of long short-term memory (lstm) networks, further augmented by deep residual connections. rigorous evaluations conducted on an expansive dataset, representative of diverse market conditions, showcased the dr-clstm's consistent supremacy in prediction accuracy and computational efficacy over both its traditional and deep learning contemporaries. crucially, the integration of residual pathways accelerated training convergence rates and provided a formidable defense against the often detrimental vanishing gradient phenomenon. consequently, this research positions the dr-clstm network as a pioneering and formidable contender in the arena of option price forecasting, offering substantive implications for quantitative finance scholars and practitioners alike, and hinting at its potential versatility for broader financial instrument applications and varied market scenarios. © (2023), (science and information organization). all rights reserved."
"the rise of technology has resulted in economic emergence across the countries. in a world relying upon technology, human beings’ efficiency and emotions may go hand in hand as depicted in one of the movies “office space” which showed three professional programmers aggressively breaking a machine which disappointed them for too long. thus, despite of incrementing technological framework, one cannot be distressed with the phenomena of how technology, emotions, and managerial efficiency have mutual interdependence on each other. quite a few studies have taken place in the field of technology, emotions, and managerial efficiency, yet few have explored the interrelations between the variables in today’s challenging global scenario. thus, the research gap provides the base of the research for which the finance professionals working in it industry have been chosen where the emotional contagion level of these professionals is evaluated purposefully who are working on criticalities to save the business, maintaining their emotional stability to perform effectively. the sample size comprising of 148 working finance professionals was incorporated for research. a structured questionnaire using two settled scales: the “positive and negative affect schedule” (panas) scale and the “emotional contagion” (ec) scale (watson et al., j pers soc psychol 54:1063–1070, 1988) based on purposive sampling was applied. the relationship with both variables emotions and efficiency is investigated by applying pearson bivariate interaction. the paper presents a dynamic computational model of emotional contagion and efficiency where technology act as a mediator. the result further displayed that female finance professionals are comparatively more vulnerable to ec using it techniques than men and the notch of ec is directly connected to emotional quotient levels of manpower which helps in increasing efficiency where it act as a potential moderator. © the editor(s) (if applicable) and the author(s), under exclusive license to springer nature singapore pte ltd. 2023."
"artificial intelligence and the banking industry complement one other well. working in an industry where practically every participant adheres to a logic that tries to maximize profits, it is challenging to resist the norm of using speedy calculations, extensive data processing, and precise estimates. due to the strict mathematical limits placed on making financial assessments, the question of why financial markets still require human interaction arises frequently and the answer to it is for making financial decisions, which are governed by these mathematical limits and to overcome it the use of artificial intelligence (ai) has been necessary and also acts as remedy in both computational and other types of errors in finance sector. thus from that time ai is used in financial industries. the introduction of digital technology has its own pros and cons. financial organizations, such as insurers, are either investing billions of dollars to buy data from other industries or developing their own ai capabilities to support the growth of their businesses. business models based on ai have recently grown, and this trend is expected to continue. in some financial industries, artificial intelligence is unavoidable. it is global in scope.  © 2023 ieee."
"the approximation of an integral with numerical integration methods is a relevant problem in many fields, from finance to chemistry. monte carlo methods are a family of methods widely used for numerical integration but which are not always applicable to industrial tasks. instead, there are quantum computing methods such as quantum amplitude estimation able to offer a speed-up compared to the monte carlo methods for numerical integration which promise a significant impact, especially in finance. the quantum amplitude estimation algorithm is a fundamental quantum algorithm with the potential to achieve a quadratic speed-up. unfortunately, being competitive over classical monte carlo methods would require hardware performances currently not available. therefore, alternative methods have been developed in order to exploit fewer resources while maintaining an advantageous theoretical scaling. we present a comparison between the standard qae algorithm and two nisq-friendly versions of the qae on a numerical integration task. the monte carlo technique of metropolis-hastings instead is used as a classical benchmark. the scaling of the different algorithms are evaluated in terms of estimation error as a function of the number of samples, computational time, and length of the quantum circuits required by the quantum solutions. the conclusion highlights that an alternative approach is preferable with respect to employng the phase estimation routine. indeed, the maximum likelihood estimation guarantees the best trade-off between the length of the quantum circuits and the precision in the integral estimation. © 2023 ieee."
"it is widely believed that given the same labeling budget, active learning (al) algorithms like margin-based active learning achieve better predictive performance than passive learning (pl), albeit at a higher computational cost. recent empirical evidence suggests that this added cost might be in vain, as margin-based al can sometimes perform even worse than pl. while existing works offer different explanations in the low-dimensional regime, this paper shows that the underlying mechanism is entirely different in high dimensions: we prove for logistic regression that pl outperforms margin-based al even for noiseless data and when using the bayes optimal decision boundary for sampling. insights from our proof indicate that this high-dimensional phenomenon is exacerbated when the separation between the classes is small. we corroborate this intuition with experiments on 20 high-dimensional datasets spanning a diverse range of applications, from finance and histology to chemistry and computer vision. © 2023 proceedings of machine learning research. all rights reserved."
"large-scale language models with millions, billions, or trillions of trainable parameters are becoming increasingly popular. however, they risk becoming rapidly over-parameterized and the adaptation cost of fully fine-tuning them increases significantly. storing them becomes progressively impractical as it requires keeping a separate copy of all the fine-tuned weights for each task. by freezing all pre-trained weights during fine-tuning, parameter-efficient tuning approaches have become an appealing alternative to traditional fine-tuning. the performance of these approaches has been evaluated on common nlp tasks of the glue benchmark and shown to match full fine-tuning performance, however, their impact is less researched in domain-specific fields such as finance. this work compares the performance of a set of financial bert-like models to their fully fine-tuned counterparts by leveraging different parameter-efficient tuning methods. we see that results are comparable to traditional fine-tuning while gaining in time and resource efficiency. © 2023 association for computational linguistics."
"point process data are becoming ubiquitous in modern applications, such as social networks, health care, and finance. despite the powerful expressiveness of the popular recurrent neural network (rnn) models for point process data, they may not successfully capture sophisticated non-stationary dependencies in the data due to their recurrent structures. another popular type of deep model for point process data is based on representing the influence kernel (rather than the intensity function) by neural networks. we take the latter approach and develop a new deep non-stationary influence kernel that can model non-stationary spatio-temporal point processes. the main idea is to approximate the influence kernel with a novel and general low-rank decomposition, enabling efficient representation through deep neural networks and computational efficiency and better performance. we also take a new approach to maintain the non-negativity constraint of the conditional intensity by introducing a log-barrier penalty. we demonstrate our proposed method's good performance and computational efficiency compared with the state-of-the-art on simulated and real data. © 2023 11th international conference on learning representations, iclr 2023. all rights reserved."
"an accurate estimation of the covariance matrix is a critical component of many applications in finance, including portfolio optimization. the sample covariance suffers from the curse of dimensionality when the number of observations is in the same order or lower than the number of variables. this tends to be the case in portfolio optimization, where a portfolio manager can choose between thousands of stocks using historical daily returns to guide their investment decisions. to address this issue, past works proposed linear covariance shrinkage to regularize the estimated matrix. while effective, the proposed methods relied solely on historical price data and thus ignored company fundamental data. in this work, we propose to utilise semantic similarity derived from textual descriptions or knowledge graphs to improve the covariance estimation. rather than using the semantic similarity directly as a biased estimator to the covariance, we employ it as a shrinkage target. the resulting covariance estimators leverage both semantic similarity and recent price history, and can be readily adapted to a broad range of financial securities. the effectiveness of the approach is demonstrated for a period including diverse market conditions and compared with the covariance shrinkage prior art. © 2023 association for computational linguistics."
"ensuring data security in internet of things (iot) networks is a major challenge due to the limited computational resources and energy constraints of these devices. physical unclonable functions (pufs) are hardware-based security tools that can provide unique identification for lightweight devices, making pufs ideal for iot applications. while pufs have traditionally been used for authentication, in this paper, we propose a novel puf protocol that can provide data integrity. maintaining data integrity is crucial in many applications, including finance, healthcare, and energy. the proposed integrity based-puf protocol is especially suitable for power constraint devices as it offers low computational overhead, execution time, and efficient communication overhead. in addition, we introduce a threat model called physically active attack (paa), a security requirement called integrity preserving (ip), to provide a comprehensive range of attacks and security goals specific to the context of our protocol. we then formally prove that our protocol preserves ip under paa. we show that our proposed protocol is resistant to various attacks, including replay attacks, man-in-the-middle attacks (mitm), cloning, brute force attacks, and physical attacks. © 2023 ieee."
"machine learning (ml) has been extensively utilized in various scientific and engineering domains. but the inherent constraints and computational complexity that arise in classical machine learning are particularly evident when dealing with large-scale, high-dimensional datasets or when attempting to solve intricate optimisation problems. quantum systems exhibit unconventional patterns that are typically not effectively generated by traditional systems and can also tackle high dimensional and large size datasets efficiently. therefore, it is believed that quantum computing systems may outperform traditional computing devices in machine learning tasks. quantum machine learning (qml) has emerged as a rapidly developing field that combines the concepts of quantum mechanics and machine learning to solve complex problems across various disciplines. the domain of quantum machine learning investigates the development and execution of quantum software with the potential to facilitate machine learning at a much superior pace compared to traditional computers. however, the major challenges encountered are hardware limitations, noise, and development of quantum algorithms. the objective of this paper is to provide an insight on the foundational principles, essential algorithms, various associated platforms, potential applications and challenges being faced by qml. as data continues to grow in size and complexity, together with the emergence of nonlinear and high-dimensional real-world situations, the computing benefits offered by qml have the potential to facilitate significant advancements in the fields of healthcare, finance, robotics, logistics, communication, and cyber security. this paper presents the basis for future research work related to development of quantum algorithms in the domain of machine learning. © 2023 ieee."
"detecting outliers in data is essential in various fields, such as finance, healthcare, and many other domains with anomalies. among well-known outlier detection algorithms, local outlier factor (lof) is widely used for identifying unusual data points. however, the computational time of lof significantly increases when dealing with large datasets containing numerical and categorical features. we propose an innovative approach using block size optimisation to speed up the outlier detection process while maintaining high accuracy. by optimizing the block size, we achieve a significant improvement in lof’s performance without compromising its effectiveness. experiment results on diverse datasets containing mixed categorical and numerical features demonstrate the effectiveness of our method in accelerating outlier detection while retaining high detection accuracy. this advancement in outlier detection has the potential to improve decision-making processes. it empowers the timely identification of anomalous events, which is significant in critical applications, including cybersecurity. © the author(s), under exclusive license to springer nature switzerland ag 2023."
"in today's digital landscape, there's a growing need for realtime, precise handwritten digit classification in fields like finance and education. this study introduces a hardware neural network system, based on the perceptron model, solely for digit classification. using very-large-scale integration (vlsi) and verilog, it employs a hybrid approach that combines parallel processing and pipelining to boost computational efficiency. the perceptron, a key deep learning algorithmic model, lies at the core of this system, calculating weighted input sums and applying nonlinear sigmoid functions to produce output values. this hardware architecture enables simultaneous processing of 98 data points per clock cycle, significantly reducing multiplier and adder usage. simulations affirm the system' s efficiency, recognizing all ten digits within 115 clock cycles. this research highlights the synergy between machine learning and hardware design, unveiling specialized hardware's potential in realtime applications. although this study excels in handwritten digit recognition, future work aims to expand the test set and optimize resource usage further. this work epitomizes the fusion of machine learning and hardware design, offering innovative possibilities across domains as technology advances and refines relentlessly.  © 2023 ieee."
"in quantitative finance, modeling the volatility structure of underlying assets is vital to pricing options. rough stochastic volatility models, such as the rough bergomi model [c. bayer, p. k. friz & j. gatheral (2016) pricing under rough volatility, quantitative finance 16 (6), 887-904, doi:10.1080/14697688.2015.1099717], seek to fit observed market data based on the observation that the log-realized variance behaves like a fractional brownian motion with small hurst parameter, h < 1/2, over reasonable timescales. both time series of asset prices and option-derived price data indicate that h often takes values close to 0.1 or less, i.e. rougher than brownian motion. this change improves the fit to both option prices and time series of underlying asset prices while maintaining parsimoniousness. however, the non-markovian nature of the driving fractional brownian motion in rough volatility models poses severe challenges for theoretical and numerical analyses and for computational practice. while the explicit euler method is known to converge to the solution of the rough bergomi and similar models, its strong rate of convergence is only h. we prove rate h + 1/2 for the weak convergence of the euler method for the rough stein-stein model, which treats the volatility as a linear function of the driving fractional brownian motion, and, surprisingly, we prove rate one for the case of quadratic payoff functions. indeed, the problem of weak convergence for rough volatility models is very subtle; we provide examples demonstrating the rate of convergence for payoff functions that are well approximated by second-order polynomials, as weighted by the law of the fractional brownian motion, may be hard to distinguish from rate one empirically. our proof uses talay-tubaro expansions and an affine markovian representation of the underlying and is further supported by numerical experiments. these convergence results provide a first step toward deriving weak rates for the rough bergomi model, which treats the volatility as a nonlinear function of the driving fractional brownian motion.  © 2022 world scientific publishing company."
"currently, blockchain technology has been involved in various areas such as medicine, environment, finance, mining, etc., therefore, the rise of technology causes an increasing use among technology companies, developers and even malicious attackers. the latter, through 51% attacks, could have the ability to manipulate a blockchain network. the present proposal consists of a consensus algorithm for blockchain networks based on proof-of-work (pow) that, through the use of geolocation, aims to provide protection against 51% attacks. with enough computational power on one of these blockchain networks, an attacker could reverse transactions and identification might be impossible. it is the mining pools, which together, could have the capacity to carry out these attacks. using geolocation, it is intended to minimize the probability of generating mining pools, making their formation unfavorable. in our experiments, the algorithm reduces the probability of 51% attacks by an average of 29% compared to pow, providing a new layer of protection when generating consensus between participating blockchain nodes. copyright © 2023 by scitepress - science and technology publications, lda. under cc license (cc by-nc-nd 4.0)"
"machine learning is going to play an ever-increasing role in the landscape of computational finance. the increasing market share of the ‘algorithmic trading’ is an example of the growing use of machine learning in finance. however, ‘algorithmic trading’ is one application of machine learning. there are several applications of machine learning in quantitative finance. © 2023, the author(s), under exclusive license to springer nature singapore pte ltd."
"nowadays artificial intelligence (ai) is an integral component of the core business model of several organisations and a crucial strategic component of many industries’ strategies such as—healthcare, finance, retail education etc. on a global scale. machine learning (ml) is a technology to achieve ai. machine learning methods utilize a variety of statistical, a vast, unstructured, and complicated dataset may be analysed using probabilistic and optimization approaches to learn from the past and find relevant trends. automated oriented text categorization, network intervention detection, spam email filtering, credit card deceit detection, consumer purchase behaviour detection, and manufacturing process optimization are just a few of the many uses for these algorithms, image categorization, object recognition and disease modelling. allocating a different class oriented label to every pixel of a particular image is certainly one of the significant steps in constructing complex automated systems like driver-less vehicles/drones, human-friendly oriented robotic machines, robot-assisted medical procedure, and savvy military systems. pixel labeling’s computational purpose is to give each individual pixel in an imagery a class label. it is critical long-range (pixel) label dependencies in a model must be captured pictures to provide strong visual coherence and high class accuracy. this may be accomplished in a feed-forward architecture by considering a suitably big input to the context patch surrounding each individual pixel to be labelled. we present a strategy in this chapter that consists of an ideally finetuned recurrent convolutional neural network that mainly allows us to incorporate a broad input context without restricting the model’s capacity. our solution does not rely on any such segmentation techniques or task-specific characteristics, in contrast to the majority of the conventional methods. the system is trained on raw pixels from beginning to end, and it models complicated spatial dependencies with low inference costs. the system detects and corrects its own faults as the context dimension grows using the built-in repetition. on the stanford background dataset, our method achieves best-in-class results, while being considerably much faster at test time. the methodology presented in this chapter has an empirical scope for societal improvement, modernization and progress. © 2023, the editor(s) (if applicable) and the author(s), under exclusive license to springer nature singapore pte ltd."
"contemporary quantitative finance connects the abstract theory and the practical use of financial innovations, such as ultra-high-frequency trading and cryptocurrencies. it teaches students how to use cutting-edge computational techniques, mathematical tools, and statistical methodologies, with a focus on real-life applications. the textbook opens with chapters on financial markets, global finance, and financial crises, setting the subject in its historical and international context. it then examines key topics in modern quantitative finance, including asset pricing, exchange-traded funds, monte carlo simulations, options, alternative investments, artificial intelligence, and big data analytics in finance. complex theory is condensed to intuition, with appendices presenting advanced mathematical or statistical techniques. each chapter offers excel-based implementations, conceptual questions, quantitative problems, and a research project, giving students ample opportunity to develop their skills. clear chapter objectives, summaries, and key terms also support student learning. digital supplements, including code and powerpoint slides, are available for instructors. assuming some prior financial education, this textbook is suited to upper-level undergraduate and postgraduate courses in quantitative finance, financial engineering, and derivatives. © 2023 ahmet can inci."
"this paper explores the force of automation and its contradictions and resistances within (and beyond) the financial sector, with a specific focus on computational practices of credit-scoring and lending. it examines the operations and promotional discourses of fintech start-ups lendup.com and elevate.com that offer small loans to the sub-prime consumers in exchange for access to their online social media and mobile data, and zest ai and lenddoefl that sell automated decision-making tools to verify identity and assess risk. reviewing their disciplinary reputational demands and impacts on users and communities, especially women and people of colour, the paper argues that the automated reimagination of credit and creditability disavows the formative design of its ai and redefines moral imperatives about character to align with the interests of digital capitalism. the economic, social and cultural crises precipitated by the covid-19 pandemic have only underscored the internal contradictions of these developments, and a variety of debt resistance initiatives have emerged, aligned with broader movements for social, economic, and climate justice around the globe. cooperative lending circles such as the mission asset fund, activist groups like #notmydebt, and debt collective, a radical debt abolition movement, are examples of collective attempts to rehumanize credit and debt and resist the appropriative practices of contemporary digital finance capitalism in general. running the gamut from accommodationist to entirely radical, these experiments in mutual aid, debt refusal, and community-building provide us with roadmaps for challenging capitalism and re-thinking credit, debt, power, and personhood within and beyond the current crises. © 2022 informa uk limited, trading as taylor & francis group."
"the adjoint reversal of long evolutionary calculations (e.g. loops), where each iteration depends on the output of the previous iteration, is a common occurrence in computational engineering (e.g. computational fluid dynamics (cfd) simulation), physics (e.g. molecular dynamics) and computational finance (e.g. long monte carlo paths). for the edge case of a scalar state, the execution, as well as adjoint control flow reversal, are inherently serial operations, as there is no spatial dimension to parallelize. our proposed method exploits the run time difference between passive function evaluation and augmented forward evaluation, which is inherent to most adjoint ad techniques. for high dimensional states, additional parallelization of the primal computation can and should be exploited at the spatial level. still, for problem sizes where the parallelization of the primal has reached the barrier of scalability, the proposed method can be used to better utilize available computing resources and improve the efficiency of adjoint reversal. we expect this method to be especially useful for operator-overloading ad tools. however, the concepts are also applicable to source-to-source transformation and handwritten adjoints, or a hybrid of all approaches. for illustration, c++ reference implementations of a low dimensional evolution (lorenz attractor) and a high dimensional evolution (computational fluid dynamics problem in openfoam) are discussed. both theoretical bounds on the speedup and run time measurements are presented. © 2023, the author(s), under exclusive license to springer nature switzerland ag."
"a novel data structure that enables the storage and retrieval of linear array numeric data with logarithmic time complexity updates, range sums, and rescaling is introduced and studied. computing sums of ranges of arrays of numbers is a common computational problem encountered in data compression, coding, machine learning, computational vision, and finance, among other fields. efficient data structures enabling log n updates of the underlying data (including range updates), queries of sums over ranges, and searches for ranges with a given sum have been extensively studied ( n being the length of the array). two solutions to this problem are well-known: fenwick trees (also known as binary indexed trees) and segment trees. the new data structure extends the capabilities for the first time to further enable multiplying (rescaling) ranges of the underlying data by a scalar as well in log n. scaling by 0 can be enabled, with the effect that subsequent updates may take (log n)2 time. the new data structure introduced here consists of a pair of interacting fenwick tree-like structures, one of which holds the unscaled values and the other of which holds the scalars. experimental results demonstrating performance improvements for the multiplication operation on arrays from a few dozen to over 30 million data points are discussed. this research was done as part of ajna labs in the course of developing a decentralized finance protocol. it enables an efficient on-chain encoding and processing of an order book-like data structure used to manage lending, interest, and collateral.  © 2013 ieee."
"nowadays, there is a growing interest in including some aspects of computation and computational thinking within science subjects, in particular mathematics. moreover, the covid pandemic has increased the opportunities for remote-based work across economies, including education. this implies that workers and students should be able to collaborate remotely using collaborative technologies. in this paper, we show how tailored student-led computational practices designed for a computational finance module provide opportunities for the co-creation of knowledge in financial mathematics in a computer-supported collaborative learning environment. we analyzed students' answers to a weekly survey using gunawardena et al. (1997) interaction analysis model for collaborative knowledge construction. the results show that, in a large number of discussions, the highest levels of the collaborative learning process were achieved. © 2023 international conference on higher education advances. all rights reserved."
"research in time series analysis has been rapidly expanding during the last few decades, and massive time series with complex forms and structures appear from many fields including economics, finance, engineering, medicine and environmental sciences. this motivates the surge of new methodologies for estimation, prediction and computation, and also raises all kinds of challenges. we made a call for papers to encourage innovative theory, methodology and novel applications in complex time series analysis in both time and frequency domains. all submissions went through a regular review process. three co-guest editors handled the peer review of all invited and contributed submissions. in total we have collected twenty-four articles that include cutting-edge research on statistical and computational methods for time series data analysis and their applications. this is the first part of the special issue that consists of twelve articles. © 2023. statistics and its interface. all rights reserved."
"spike train data find a growing list of applications in computational neuroscience, streaming data and finance. statistical analysis of spike trains is based on various probabilistic and neural network models. the statistical approach relies on parametric or nonparametric specifications of the underlying model. in this paper we consider the nonparametric classification problem for a class of spike train data characterized by nonparametricaly specified intensity functions. we derive the optimal bayes rule and next formulate the plug-in non-parametric kernel classifiers. asymptotical properties of the rules are established including the limit with respect to the increasing observation time interval and the size of a training set. the obtained results are supported by a finite sample simulation studies. © 2023 ieee."
"probabilistic time series forecasting is crucial in many application domains, such as retail, ecommerce, finance, and biology. with the increasing availability of large volumes of data, a number of neural architectures have been proposed for this problem. in particular, transformer-based methods achieve state-of-the-art performance on real-world benchmarks. however, these methods require a large number of parameters to be learned, which imposes high memory requirements on the computational resources for training such models. to address this problem, we introduce a novel bidirectional temporal convolutional network that requires an order of magnitude fewer parameters than a common transformer-based approach. our model combines two temporal convolutional networks: the first network encodes future covariates of the time series, whereas the second network encodes past observations and covariates. we jointly estimate the parameters of an output distribution via these two networks. experiments on four real-world datasets show that our method performs on par with four state-of-the-art probabilistic forecasting methods, including a transformer-based approach and wavenet, on two point metrics (smape and nrmse) as well as on a set of range metrics (quantile loss percentiles) in the majority of cases. we also demonstrate that our method requires significantly fewer parameters than transformer-based methods, which means that the model can be trained faster with significantly lower memory requirements, which as a consequence reduces the infrastructure cost for deploying these models. © 2021 the author(s)"
"the discrete pareto (dp) distribution studied in this paper is a probability model with a power-law tail, which provides a convenient alternative to the well-known zipf distribution. while basic characteristics of the dp model are available explicitly, this is not an exponential family and parameter estimation connected with this model is a challenging task. with this in mind we develop a computational approach to this problem, based on the expectation-maximization (em) algorithm. in the process, we discover an interesting new probability distribution, which is a certain tilted version of the standard gamma model, and we provide a short account of its basic properties. the latter play a crucial role in our em algorithm. our computational approach to dp parameter estimation is illustrated by simulations, while a real data example from finance illustrates potential applications of the dp stochastic model. © 2021 taylor & francis group, llc."
"big data is a term used to refer to massive amounts of data, which is complex and obtained from a variety of sources. this type of data is large-scale, growing exponentially with time, and adds a substantial value if analyzed properly. it can be characterized using the 5 vs of big data. deriving meaningful insights and correlations to enable intelligent decisions is done via the process of big data analytics (bda). big data analytics can thus help an enterprise cut costs, increase efficiency, focus on local preferences and improve sales. many industries such as healthcare, finance, advertising, banking and security benefit from bda. the emergence of big data has made it disadvantageous for corporations to store such large amounts of data in silos. cloud computing provided a stable and established alternative, enabling remote access to data, faster crunching of large volumes of raw data and reliable storage of the data and the analysis. therefore it emerges as the perfect complement for big data. despite the benefits, there exist some security concerns that are not uncommon in applications of cloud computing. this chapter begins with an introduction of big data, including its advantages, followed by big data analytics, its relationship with big data and its benefits and barriers. we also analyze challenges in big data such as data storage, computational complexities and information security. this chapter discusses the relationship between big data analytics and the cloud, including cloud services used in bda. we also discuss future research areas such as high-speed processing accompanied with elevated performance and exalted throughput in bda. © 2023, the author(s), under exclusive license to springer nature switzerland ag."
"since year 1989, genetic algorithms (gas) have been increasingly and successfully used for various computational purposes (optimization, design, modelling, and control) in science, manufacturing, medicine, social sciences, technology, finances, etc. in this study, bibliometric analysis is performed to analyse the development of applications of gas. the role of gas in various domains is discussed through analysis of temporal dependencies of their percentage share in twenty nine application fields for the period from year 1989, when a significant number of applications is noted, to year 2022. for most of the studied application fields the present status corresponds to growth of the gas' percentage share. the here-observed trends in temporal variation of genetic algorithms application in various fields can be helpful in understanding the gas importance in studies employing the gas and in analyses of perspectives of application development. © 2023 taylor & francis."
"non-linear partial differential kolmogorov equations are successfully used to describe a wide range of time dependent phenomena, in natural sciences, engineering or even finance. for example, in physical systems, the allen–cahn equation describes pattern formation associated to phase transitions. in finance, instead, the black–scholes equation describes the evolution of the price of derivative investment instruments. such modern applications often require to solve these equations in high-dimensional regimes in which classical approaches are ineffective. recently, an interesting new approach based on deep learning has been introduced byby e, han and jentzen [1, 2]. the main idea is to construct a deep network which is trained from the samples of discrete stochastic differential equations underlying kolmogorov’s equation. the network is able to approximate, numerically at least, the solutions of the kolmogorov equation with polynomial complexity in whole spatial domains. in this contribution we study variants of the deep networks by using different discretizations schemes of the stochastic differential equation. we compare the performance of the associated networks, on benchmarked examples, and show that, for some discretization schemes, improvements in the accuracy are possible without affecting the observed computational complexity. © 2022, the author(s), under exclusive licence to springer science+business media, llc, part of springer nature."
"the techniques and approaches within geoinformatics and data science rely on the effective coupling of supporting infrastructures and institutions. without underlying infrastructures for data discovery, analysis, management, distribution, and preservation, new computational techniques wither on the vine for lack of input or remain isolated as niche tools that miss broader potential audiences. likewise, without supporting institutions that enable governance of policies and finances, coordination of stakeholders, and validation of new knowledge and tools, technological advances become detached from the people and organizations that operate and use them. this paper centers on a case study of work within the national center for atmospheric research (ncar) and university corporation for atmospheric research (ucar) to develop effective systems and processes for research data curation, access, discovery, and preservation. by emphasizing iterative alignment of institutional work (policies, intermediaries, governance processes, routines, and financial instruments) and infrastructural work (data storage systems, repositories, tools, and interfaces), balanced progress has been made toward developing solutions to gaps in organizational data services. © 2022 the geological society of america. all rights reserved."
"the concept of the metaverse promises a cyber-social platform, a virtual space offering a new reality, new collaboration, and communication opportunities. despite its growing popularity and anticipation that it is the next big thing, there is a research gap regarding metaverse-related perceptions and sentiments. we aim to bridge this gap by taking a computational perspective to uncover the metaverse-related sentiments and perceptions on twitter. two million tweets shared in 2021 were examined using a combination of sentiment, text, and network analysis to classify tweets and words into sentiment categories, gather frequently used phrases, and detect central words and hashtags, respectively. the findings revealed that positive sentiments and emotions (anticipation, trust, joy) are prevalent in the tweets. the prevalence of three clusters in tweets, blockchain, gaming, and virtual reality, indicates that the concept of the metaverse is perceived as interrelated and integrated with finance, entertainment, and technology. © 2023, the author(s), under exclusive licence to springer science+business media, llc, part of springer nature."
"the drug development process consumes 9–12 years and approximately one billion us dollars in costs. due to the high finances and time costs required by the traditional drug discovery paradigm, repurposing old drugs to treat cancer and rare diseases is becoming popular. computational approaches are mainly data-driven and involve a systematic analysis of different data types leading to the formulation of repurposing hypotheses. this study presents a novel scoring algorithm based on chemical and genomic data to repurpose drugs for 669 diseases from 22 groups, including various cancers, musculoskeletal, infections, cardiovascular, and skin diseases. the data types used to design the scoring algorithm are chemical structures, drug-target interactions (dti), pathways, and disease-gene associations. the repurposed scoring algorithm is strengthened by integrating the most comprehensive manually curated datasets for each data type. at drugrepo score ≥ 0.4, we repurposed 516 approved drugs across 545 diseases. moreover, hundreds of novel predicted compounds can be matched with ongoing studies at clinical trials. our analysis is supported by a web tool available at: http://drugrepo.org/. © 2022, the author(s)."
"big data is increasingly being applied in a wide variety of fields, including industry, finance, and medicine, because of its ability to handle the hurdles of processing enormous amounts of data. the map reduce model is one of the data mining strategies that are frequently and efficiently used to classify huge data. in this research, a strategy for big data categorization is created using pso, ga, aco, firefly, whale, cuckoo, and mayfly as well as a compared result of these 7 optimization classifications with 8 base classifications (rf, lr, extra tree, knn, linearsvc, gaussiannb, svc, dt) in diabetes disease prediction. the major purpose of this work is to discover the best outcomes for diabetes illness prediction in terms of accuracy and computational time. in this project, we used seven optimization algorithms pso, ga, aco, firefly, whale, cuckoo, and mayfly for the diabetes prediction model. we have also included the performance evaluation of the models with different evaluation metrics like precision, recall, and f1-score. we obtained an overall accuracy of 94% in the mayfly algorithm and 95.05% accuracy using the whale algorithm. © 2023 ieee."
"financial mathematics: from discrete to continuous time is a study of the mathematical ideas and techniques that are important to the two main arms of the area of financial mathematics: portfolio optimization and derivative valuation. the text is authored for courses taken by advanced undergraduates, mba, or other students in quantitative finance programs. the approach will be mathematically correct but informal, sometimes omitting proofs of the more difficult results and stressing practical results and interpretation. the text will not be dependent on any particular technology, but it will be laced with examples requiring the numerical and graphical power of the machine. the text illustrates simulation techniques to stand in for analytical techniques when the latter are impractical. there will be an electronic version of the text that integrates mathematica functionality into the development, making full use of the computational and simulation tools that this program provides. prerequisites are good courses in mathematical probability, acquaintance with statistical estimation, and a grounding in matrix algebra. the highlights of the text are: a thorough presentation of the problem of portfolio optimization, leading in a natural way to the capital market theory dynamic programming and the optimal portfolio selection-consumption problem through time an intuitive approach to brownian motion and stochastic integral models for continuous time problems the black-scholes equation for simple european option values, derived in several different ways a chapter on several types of exotic options material on the management of risk in several contexts © 2023 kevin j. hastings. all rights reserved."
"valuation of the american options encountered commonly in finance is quite difficult due to the possibility of early exercise alternatives. since an exact solution for the american options does not exist, effective numerical methods are needed to understand the behavior of option pricing models. therefore, in this paper, a new approach based on a high-order difference scheme is proposed to discuss the valuation of an american put option as a free boundary problem. using a front-fixing approach that transforms the unknown free boundary (optimal stopping) into a fixed one, a sixth-order finite difference scheme (fd6) in space and a third-order strong-stability preserving runge-kutta (ssprk3) in time are applied to the model converted to a nonlinear partial differential equation. the computed results revealed that the combined method is seen to attempt to pull up the capacity of the algorithm to achieve higher accuracy. it is seen that the quantitative and qualitative results produced by the method proposed with minimal computational effort are sufficiently accurate and meaningful. therefore, this article provides some new insights about the physical characteristics of financial problems and such realistic phenomena.  © 2021 walter de gruyter gmbh, berlin/boston."
"as a decentralized distributed database, blockchain, with unique consensus mechanism, allows nodes to establish point-to-point value transfer without relying on third parties. therefore, it is widely used in finance, digital rights, and other fields. however, the characteristics of open and transparent storage of ledger data in the blockchain. leads to serious risk of transaction data disclosure, and the existing privacy-preserving scheme has difficulty in achieving a balance between user privacy protection and data traceability supervision. to this end, an auditable privacy-preserving scheme for consortium blockchain is proposed based on decisional linear (dlin) encryption algorithm, pedersen commitment, dual-key stealth address protocol (dksap), and non-interactive zero-knowledge (nizk) proof. the actual transaction amount is hidden in the commitment, and the corresponding nizk proof information is generated to ensure that the amount obtained by decrypting the regulatory ciphertext is equal to the actual transaction amount, thus realizing the verification of the hidden amount. based on the hierarchical deterministic dksap, the dlin encryption algorithm is used to generate authentication and regulatory ciphertext, and the sub-public key generation algorithm is used to trace the identity of the transaction recipient. the theoretical analysis and simulation results show that this scheme can achieve lower computational and communication costs while ensuring the privacy of transaction data, achieving traceability and supervision. compared to the auditable privacy-preserving scheme traceable-monero, the regulatory ciphertext decryption time is reduced on average by 52.25%. © 2023, editorial office of computer engineering. all rights reserved."
"the goal of the paper is to analyse real-time stock market data values as they change over time. a difficult task in research is predicting and analysing future stock values. the necessity to create an automated, computational approach for forecasting stock market data values, which fluctuate depending on the degree of risk, is what spurred this research's development. currently in this an automated approach is not available the researcher used historical data from past along with stock market professionals to find a good prediction.a comparative study for stock market prediction is on in this paper using lstm and rnn the dataset we have used from yahoo finance where day to day the dataset is updated this ml method gives a better accuracy rate. © 2023 ieee."
"diversification return has been well studied in finance literature, mainly focusing on the various sources from which it may be generated. the maximization of diversification return, in its natural form, is often handed over to convex quadratic optimization for its solution. in this paper, we study the maximization problem from the perspective of rao's quadratic entropy (rqe), which is closely related to the euclidean distance matrix and hence has deep geometric implications. this new approach reveals a fundamental feature that the maximum diversification return portfolio (mdrp) admits a spherical embedding with the hypersphere having the least volume. this important characterization extends to the maximum volatility portfolio, the long-only mdrp, and the ridge-regularized mdrp. rqe serves as a unified formulation for diversification return related portfolios and generates new portfolios that are worth further investigation. as an application of this geometric characterization, we develop a computational formula for measuring the distance between a new asset and an existing portfolio that has the hyperspherical embedding. numerical experiments demonstrate the developed theory. © 2023 society for industrial and applied mathematics."
"this chapter demonstrates the crucial role that human-ai interfaces play in conveying the trustworthiness of ai solutions to their users. explainability is a central component of such interfaces, particularly in high-stake domains where human oversight is essential: justice, finance, security, and medicine. to successfully build and communicate trustworthiness, a user-centered approach to the design and development of ai solutions and their human interfaces is essential. in this chapter, we explain how proven methods for stakeholder analysis and user testing from human-computer interaction (hci) research can be adapted to human-ai interaction (haii) in support of this goal. the practical implementation of a user-centric approach is described within the context of ai applications in computational pathology. © 2023, the author(s), under exclusive license to springer nature switzerland ag."
"parallel-in-time algorithms provide an additional layer of concurrency for the numerical integration of models based on time-dependent differential equations. methods like parareal, which parallelize across multiple time steps, rely on a computationally cheap and coarse integrator to propagate information forward in time, while a parallelizable expensive fine propagator provides accuracy. typically, the coarse method is a numerical integrator using lower resolution, reduced order or a simplified model. our paper proposes to use a physics-informed neural network (pinn) instead. we demonstrate for the black-scholes equation, a partial differential equation from computational finance, that parareal with a pinn coarse propagator provides better speedup than a numerical coarse propagator. training and evaluating a neural network are both tasks whose computing patterns are well suited for gpus. by contrast, mesh-based algorithms with their low computational intensity struggle to perform well. we show that moving the coarse propagator pinn to a gpu while running the numerical fine propagator on the cpu further improves parareal’s single-node performance. this suggests that integrating machine learning techniques into parallel-in-time integration methods and exploiting their differences in computing patterns might offer a way to better utilize heterogeneous architectures. © 2023, the author(s)."
"this book is designed to equip students to navigate through ms excel and basic data computation methods, which are essential tools in research or professional settings and in classrooms. it illustrates the concepts used in research and data analysis and economic and financial decision-making in business and in daily life. the book will help students acquire knowledge and develop skills in statistical techniques and financial analysis using ms excel. with illustrations and examples, it will help the readers to: visualize, present, and analyze data through ms excel spreadsheets and tables and create personal or business spreadsheets learn how to work with spreadsheets, use formulae, and perform calculations and analysis create tables including pivot tables become familiar with basic statistical and financial measures design creative spread sheets and communicate effectively in business using spreadsheets and data analysis this revised and updated second edition will be an essential resource for students of economics, commerce, management, and other social science subjects, and will be useful to those studying econometrics, financial technology, basic computational techniques, data analysis, and applied economics. content the book is developed through three phases, with each phase standing on its own as well as providing a foundation for the next. in the first phase, excel is introduced for the students to learn entry of data, manipulation of data, carrying out operations and develop presentations. the second phase introduces basic statistical measures of data summarisation and analysis, following which these are illustrated in excel spreadsheets with the techniques introduced in the first phase. in addition, a few advanced tools of statistical analysis are introduced and illustrated in excel. the third phase introduces financial measures of common use, their general computation and working them out in excel. the book intends to illustrate the concepts used in economic and financial decision-making in business and in daily life; it helps demonstrate a deeper understanding from both theoretical and practical perspectives. an effort has been made to make the book student-friendly by using simple language and giving a number of illustrations in each chapter, solved in such a simple manner that they can be easily understood by the students. practical questions have been included at the end of each chapter so that the students can independently solve them and test their understanding of the concepts and computations introduced in the chapter. outcome at the end, students will be able to describe what a spreadsheet is and what excel's capabilities are and can work with elements that make up the structure of a worksheet. they will be able to work with spreadsheets and enter data in excel, use formulae and calculations, and create tables, charts and pivot tables. they will be familiar with basic statistical and financial measures of general use. they will be able to do basic computations in statistics and finance in excel. students will acquire the capacity to create personal and/or business spreadsheets following current professional and/or industry standards. their potential for critical thinking to design and create spreadsheets and communicate in a business setting using spreadsheet vocabulary will be enhanced. in the digital age, students necessarily need to know data, data sources and how to 'dirty' their hands with data. there can be no substitute to 'talking through numbers'. the book introduces students to a variety of indian and international data sources and teaches them how to import data-be it social, economic, financial and so on-to the excel sheet. once they master it, the data world is there for them to conquer! the educational background required for the student to understand the text is some basic english and mathematics of school-leaving level. some fl air for numbers will be an asset and for them it will be a breeze; others will have to make an effort but ample illustrations and practice questions make life simple, whether it is basic statistics or slightly intricate finance!. © 2023 d narayana, sharad ranjan, and nupur tyagi."
"the era of modern portfolio theory began with the revolutionary approach by harry markowitz in 1952. however, several drawbacks of the model have rendered it impractical to be used in reality. thus, various modifications have been done to refine the classical model, including concerns about risk measures, trading practices and computational efficiency. on the other hand, islamic finance is proven to be a viable alternative to the conventional system following its outstanding performance during the financial crisis in 2008. this emerging sector has gained a lot of attention from investors and economists due to its significantly increasing impact on today’s economy, corresponding to globalization and a demand for a sustainable investment strategy. a comprehensive literature review of the notable conventional and islamic models is done to aid future research and development of portfolio optimization, particularly for islamic investment. additionally, the study provides a concisely detailed overview of the principles of islamic finance to prepare for the future development of an islamic finance model. generally, this study outlines the comprehensive features of portfolio optimization models over the decades, with an attempt to classify and categorize the advantages and drawbacks of the existing models. the trend of portfolio optimization modelling can be captured by gathering and recording the problems and solutions of the reviewed models. © 2023 the author(s), licensee aims press."
"in this paper, we analyse the computational advantages of the spherical parametrisation for correlation matrices in the context of maximum likelihood estimation via numerical optimisation. by using the special structure of correlation matrices, it is possible to define a bijective transformation of an n× n correlation matrix r into a vector of n(n- 1) / 2 angles between 0 and π . after discussing the algebraic aspects of the problem, we provide examples of the use of the technique we propose in popular econometric models: the multivariate dcc-garch model, widely used in applied finance for large-scale problems, and the multivariate probit model, for which the computation of the likelihood is typically accomplished by simulated maximum likelihood. our analysis reveals the conditions when the spherical parametrisation is advantageous; numerical optimisation algorithms are often more robust and efficient, especially when r is large and near-singular. © 2023, the author(s)."
"in this system demonstration, we seek to streamline the process of reviewing financial statements and provide insightful information for practitioners. we develop fish, an interactive system that extracts and highlights crucial textual signals from financial statements efficiently and precisely. to achieve our goal, we integrate pre-trained bert representations and a fine-tuned bert highlighting model with a newly-proposed two-stage classify-then-highlight pipeline. we also conduct the human evaluation, showing fish can provide accurate financial signals. fish overcomes the limitations of existing research and more importantly benefits both academics and practitioners in finance as they can leverage state-of-the-art contextualized language models with their newly gained insights. the system is available online at https://fish-web-fish.de.r.appspot.com/, and a short video for introduction is at https://youtu.be/zbvzq09i6aw. © 2023 association for computational linguistics."
"this paper extends a series of deep learning models developed on us equity data to the australian market. the model architectures are retrained, without structural modification, and tested on australian data comparable with the original us data. relative to the original us-based results, the retrained models are statistically less accurate at predicting next day returns. the models were also modified in the standard train/validate manner on the australian data, and these models yielded significantly better predictive results on the holdout data. it was determined that the best-performing models were a cnn and lstm, attaining highly significant z-scores of 6.154 and 8.789, respectively. due to the relative structural similarity across all models, the improvement is ascribed to regional influences within the respective training data sets. such unique regional differences are consistent with views in the literature stating that deep learning models in computational finance that are developed and trained on a single market will always contain market-specific bias. given this finding, future research into the development of deep learning models trained on global markets is recommended. © 2022, the author(s)."
"time series forecasting is crucial in various domains, including finance, meteorology, economics, and energy management. regression trees and deep learning models are among the techniques developed to tackle these challenges. this paper presents a comparative analysis of these approaches in terms of efficacy and efficiency, using real-world datasets. our experimental results indicate that regression trees can provide comparable performance to deep neural networks with significantly lower computational demands for training and hyper-parameter selection. this finding underscores the potential of regression trees as a more sustainable and energy-efficient approach to time series forecasting, aligning with the ongoing efforts in green ai research. specifically, our study reveals that regression trees are a promising alternative for short-term forecasting in scenarios where computational efficiency and energy consumption are critical considerations, without the need for costly gpu computation. © 2023, the author(s), under exclusive license to springer nature switzerland ag."
"in this paper, we study a posteriori error estimates of the imex bdf2 scheme for time discretizations of solving parabolic partial integro-differential equations, which describe the jump-diffusion option pricing model in finance. because of the initial singularities of the solution which is due to the nonsmoothness of the payoff function, a posteriori error control and adaptivity will be crucial in solving numerically this type of equations. to derive optimal order a posteriori error estimates, quadratic reconstructions for the imex bdf2 method are introduced. by using these continuous, piecewise time reconstructions, the upper and lower error bounds depending only on the discretization parameters and the data of the problems are derived. based on these a posteriori error estimates, we further develop a time adaptive algorithm. the numerical implementations are performed with both nonuniform partitions and adaptivity in time. the adaptive algorithm reduce the computational cost substantially and provides efficient error control for the solution. © 2022, the author(s), under exclusive licence to springer science+business media, llc, part of springer nature."
"the privacy-preserving federated learning for vertically partitioned (vp) data has shown promising results as the solution of the emerging multiparty joint modeling application, in which the data holders (such as government branches, private finance, and e-business companies) collaborate throughout the learning process rather than relying on a trusted third party to hold data. however, most of the existing federated learning algorithms for vp data are limited to synchronous computation. to improve the efficiency when the unbalanced computation/communication resources are common among the parties in the federated learning system, it is essential to develop asynchronous training algorithms for vp data while keeping the data privacy. in this article, we propose an asynchronous federated stochastic gradient descent (afsgd-vp) algorithm and its two variance reduction variants, including stochastic variance reduced gradient (svrg) and saga on the vp data. moreover, we provide the convergence analyses of afsgd-vp and its svrg and saga variants under the condition of strong convexity and without any restrictions of staleness. we also discuss their model privacy, data privacy, computational complexities, and communication costs. to the best of our knowledge, afsgd-vp and its svrg and saga variants are the first asynchronous federated learning algorithms for vp data with theoretical guarantees. extensive experimental results on a variety of vp datasets not only verify the theoretical results of afsgd-vp and its svrg and saga variants but also show that our algorithms have much higher efficiency than the corresponding synchronous algorithms.  © 2012 ieee."
"crude oil price predictability has continually been considered as a fundamental argument of finance literature, given its critical propositions for risk management, investment decisions, and commercial and financial policymaking. this work presents an innovative learning framework for efficient predictive modeling of daily and weekly crude oil price (cop) information, which aims to enable sustainable management in oil markets. firstly, an optimized version of variation mode decomposition (ovmd) is proposed to adaptively decompose the original cop time series into multiple modes based on a set of optimized parameters calculated with a tree-structured parzen estimator (tpe) algorithm. secondly, an adaboost algorithm is redesigned using random forest (rf) to model the future price information in the modes with the high frequency. thirdly, a new deep network is presented to develop automatically learn spatial–temporal representations from decomposed cop data, where a novel conv-former module is designed to efficiently extract local as well as global spatial representations without incurring extra computational costs. followingly, multiple long short-term memory (lstm) networks are stacked to learn temporal representations from input modes. to further empower the representation power of our framework, a new bidirectional learning module is presented to stack the lstm layer to learn from cop data in the forward and backward directions. to validate the efficiency of the proposed framework, this work performs experimental simulations and analyses based on a case study from brent crude oil prices at both daily and weekly scales. the experimental findings show up the competent predictive modeling capabilities of the proposed framework over the cutting-edge methods rendering it as a promising solution to enable sustainable management in crude oil markets. the proposed framework can be generalized to different predictive modeling tasks and hence qualified to be used as a valuable tool for oil portfolio creation, property pricing, and risk management in crude oil markets. © 2022 elsevier ltd"
"using secure multiparty computation (mpc), organizations which own sensitive data (e.g., in healthcare, finance or law enforcement) can train machine learning models over their joint dataset without revealing their data to each other. at the same time, secure computation restricts operations on the joint dataset, which impedes computation to assess its quality. without such an assessment, deploying a jointly trained model is potentially illegal. regulations, such as the european union's general data protection regulation (gdpr), require organizations to be legally responsible for the errors, bias, or discrimination caused by their machine learning models. hence, testing data quality emerges as an indispensable step in secure collaborative learning. however, performing distribution testing is prohibitively expensive using current techniques, as shown in our experiments. we present holmes, a protocol for performing distribution testing efficiently. in our experiments, compared with three non-trivial baselines, holmes achieves a speedup of more than 10_ for classical distribution tests and up to 104_ for multidimensional tests. the core of holmes is a hybrid protocol that integrates mpc with zero-knowledge proofs and a new zk-friendly and naturally oblivious sketching algorithm for multidimensional tests, both with significantly lower computational complexity and concrete execution costs. © usenix security 2023. all rights reserved."
"nowadays, entertainment is one of the biggest industries, which continues to expand. in this study, the problem of estimating the consolation prize as a fraction of the jackpot is dealt with, which is an important issue for each casino and gambling club. solving the problem leads to the computation of multidimensional integrals. for that purpose, modifications of the most powerful stochastic quasi-monte carlo approaches are employed, in particular lattice and digital sequences, halton and sobol sequences, and latin hypercube sampling. they show significant improvements to the classical monte carlo methods. after accurate computation of the arisen integrals, it is shown how to calculate the expectation of the real consolation prize, taking into account the distribution of time, when different numbers of players are betting. moreover, a solution to the problem with higher dimensions is also proposed. all the suggestions are verified by computational experiments with real data. besides gambling, the results obtained in this study have various applications in numerous areas, including finance, ecology and many others. © 2023 by the authors."
"highlights: what are the main findings? a cortical coding method is developed inspired by the network formation in the brain, where the information entropy is maximized while dissipated energy is minimized. the execution time in the cortical coding model is far superior, seconds versus minutes or hours, compared to those of the frequently used current algorithms, while retaining comparable distortion rate. what is the implication of the main finding? the significant attribute of the methodology is its generalization performance, i.e., learning rather than memorizing data. although only vector quantization property was demonstrated herein, the cortical coding algorithm has a real potential in a wide variety of real-time machine learning implementations such as, temporal clustering, data compression, audio and video encoding, anomaly detection, and recognition, to list a few. a major archetype of artificial intelligence is developing algorithms facilitating temporal efficiency and accuracy while boosting the generalization performance. even with the latest developments in machine learning, a key limitation has been the inefficient feature extraction from the initial data, which is essential in performance optimization. here, we introduce a feature extraction method inspired by energy–entropy relations of sensory cortical networks in the brain. dubbed the brain-inspired cortex, the algorithm provides convergence to orthogonal features from streaming signals with superior computational efficiency while processing data in a compressed form. we demonstrate the performance of the new algorithm using artificially created complex data by comparing it with the commonly used traditional clustering algorithms, such as birch, gmm, and k-means. while the data processing time is significantly reduced—seconds versus hours—encoding distortions remain essentially the same in the new algorithm, providing a basis for better generalization. although we show herein the superior performance of the cortical coding model in clustering and vector quantization, it also provides potent implementation opportunities for machine learning fundamental components, such as reasoning, anomaly detection and classification in large scope applications, e.g., finance, cybersecurity, and healthcare. © 2022 by the authors."
"the rapid development of theoretical and practical innovations in corporate finance driven by supply chain finance has exacerbated the complexity of credit default risk contagion among supply chain enterprises. financial risks in the supply chain greatly hinder its sustainable development; thus, strengthening financial risk management is necessary to ensure the sustainability of the supply chain. based on the single-channel and dual-channel credit financing models of retailers in the supply chain, the purpose of this paper was to construct a model of the intensity of credit default risk contagion among supply chain enterprises under different credit financing models, and investigate the influencing factors of credit risk contagion among supply chain enterprises and its mechanism of action through a computational simulation system. the results were as follows: (1) there was a positive relationship between the production cost of suppliers and the contagion intensity of the supply chain credit default risk, and the contagion effect of the supply chain credit default risk increased significantly when both retailers defaulted on trade credit to suppliers; (2) the market retail price of the product was negatively related to the contagion intensity of the supply chain credit default risk, and the contagion intensity of the supply chain credit default risk increased significantly when both retailers defaulted on trade credit to the supplier; (3) the intensity of credit default risk contagion in the supply chain was positively correlated with both the commercial bank risk-free rate and the trade credit rate, and retailers’ repayment priority on trade credit debt was negatively correlated with suppliers’ wholesale prices and positively correlated with retailers’ order volumes, with retailers’ repayment priority positively affecting retailers’ bank credit rates and negatively affecting suppliers’ bank credit rates; and (4) retailers’ repayment priority on trade credit debt was negatively correlated with the intensity of supply chain credit default risk contagion, and the lower the retailer’s bank credit limit, the higher the trade credit limit, and the stronger the credit default contagion effect in the supply chain. © 2022 by the authors."
"this paper presents a robo-advisory prototype that synergizes a long short-term memory (lstm) based recurrent neural network (rnn) model for asset price forecasting and genetic algorithm for the portfolio optimization problem. the lstm model was found to be proficient in predicting the prices of gasoline/brent crack spread, heating oil/brent crack spread, and gas/crude oil energy spread with an rmse between 2.25 and 5.62, confirming its capacity for accurate forecasting with minimal computational latency. the genetic algorithm for portfolio allocation demonstrated dynamic adaptation to various investment objectives. under risk-minimization and return-maximization strategies, it distinctly favored gasoline/brent crack spread and heating oil/brent crack spread respectively, showcasing its ability to calibrate between risk and return profiles. the prototype leverages parallelization to achieve up to 20-fold decrease in computational latency. this paper's findings extend to the computational finance and artificial intelligence realm, presenting an efficacious machine learning-centered approach to portfolio management. the developed prototype demonstrates robustness, scalability, and efficiency under variable inputs and conditions suggesting its potential for widespread implementation in the investment advisory sector. moreover, it sets a precedent for future research in the development of ai-driven financial advisory systems, which could involve exploration of other stochastic optimization algorithms, machine learning models, and hybrid models to enhance forecasting accuracy and efficiency in financial applications. © 2023 ieee."
"supervised learning is commonly used in digital imagery, computational linguistics, and digital sound classification. deep learning's astounding achievement as an online analytical approach has piqued the curiosity of the scientific establishment. with the rise of fintech in current history, the application of machine learning (ml) in financial products and activities has become commonplace. however, there is a shortage of a systematic assessment of future research directions in finance and economics in the actual knowledge. this work evaluates the ability of the convolu-tional neural network in important financial and accounting areas to give a comprehensive study due to the advent, input variables, and parameter estimation. finally, we address three factors that may impact the results of monetary neural network architectures. this study offers scholars and operators insight and perspective on the state-of-the-art deep teaching methods in accounting and investment banking. © the institution of engineering and technology 2023. all rights reserved."
"many modern computational approaches to classical problems in quantitative finance are formulated as empirical loss minimization (erm), allowing direct applications of classical results from statistical machine learning. these methods, designed to directly construct the optimal feedback representation of hedging or investment decisions, are analyzed in this framework demonstrating their effectiveness as well as their susceptibility to generalization error. use of classical techniques shows that over-training renders trained investment decisions to become anticipative, and proves overlearning for large hypothesis spaces. on the other hand, nonasymptotic estimates based on rademacher complexity show the convergence for sufficiently large training sets. these results emphasize the importance of synthetic data generation and the appropriate calibration of complex models to market data. a numerically studied stylized example illustrates these possibilities, including the importance of problem dimension in the degree of overlearning, and the effectiveness of this approach. © 2022 wiley periodicals llc."
"electricity markets are going through a comprehensive transformation that includes the large-scale appearance of intermittent renewable generators (rgs). to handle the local effects of new rgs on the distribution grid, the more efficient utilization of distributed local flexibility (lf) resources is necessary. however, the optimal market design is not yet known for lf products. this paper investigates a novel cost allocation mechanism in the context of this market challenge. the mechanism is designed to provide several important advantages of peer-to-peer trading without creating barriers to practical application. it provides partial disintermediation. the acquisition of lf remains the responsibility of the dso, while the financial costs of the transaction are covered on power exchanges (pxs). to provide this functionality, the clearing algorithm of the px in question has to incorporate a novel feature we call the payment redistribution technique. this technique allows the buyers’ expenses to be larger than the sellers’ income, and the difference is used to finance flexibility costs. its mathematical formulation is presented and analyzed in detail, considering computational efficiency and accuracy. afterward, a realistic case study is constructed to demonstrate the operation of the algorithm and its energy market effects. © 2022 by the authors."
"many problems of computational finance, such as pricing of an asset, can be formulated as a system of linear equations. hence, solving the system of linear equations is important in computational finance. an investor would like to assess if the price of a stock is less than its expected level. if the investor sees the stock is already overpriced, then the chance that it will appreciate further will be less and the investor would like to sell the stock. other investors would also like to sell the stock as they also have the same public information. the stock will fall back to its expected level. on the contrary, if the stock is underpriced, then many investors would like to buy the stock with the assumption that the price will rise to its expected level. © 2023, the author(s), under exclusive license to springer nature singapore pte ltd."
"ethereum is one of the first blockchains executing smart contracts, i.e., financial applications directly executed on the ledger using a virtual machine. high transaction volumes caused by financial applications, including decentralized finance and non-fungible tokens, slow down the ethereum virtual machine. hence, there is a need to detail the execution characteristics of the ethereum virtual machine before its performance can be improved. this work introduces an off-line ethereum virtual machine tracer called evmtracer that produces runtime data dependence graphs from executed transactions as an alternative program representation. from the runtime dependence graphs, we can deduce valuable metrics about program execution characteristics, including the degree of parallelization and computational redundancies in smart contracts. our experiments encompass all blocks up to 12m on the ethereum mainnet. we found a geometric mean of 1.90× theoretical maximum speedup when executing the smart contracts in parallel and identified 34.97% of sload instructions as redundant.  © 2013 ieee."
"the sustainability transition requires huge investments from both the public and private sectors. covid-19 recovery packages can boost the transition. but reliable investment assurance and impact monitoring systems are also needed to guide sustainable investment and ensure that the ambitious goals are met. current decision-support tools suffer from a severe credibility deficit, mainly due to methodological and computational limitations. despite considerable progress, issues concerning lack of transparency, accuracy, comprehensiveness, and quality persist. in this opaque context, regulators are imposing new disclosure obligations. the european commission, for example, is developing a taxonomy for sustainable activities. in parallel, the international organization for standardization is developing principles to guide sustainable finance. in addition to robust standards and metrics, other enabling conditions are required to trigger the necessary behavioural changes and scale up funding to accomplish the sustainability transition. this includes institutional frameworks and governance mechanisms to ensure a fair distribution of sustainability burdens, costs, and benefits among all stakeholders. for a lasting and long-term recovery it is essential to take an integrated approach to sustainability and impact assurance, one in which no environmental impacts, social consequences, or economic externalities are ignored. natural capital assets could be managed as common asset trusts for the benefit of all current and future generations. the oceans, for instance, are custodians of a large and partly undiscovered natural capital that is essential to humanity, biodiversity, and climate regulation. transitioning to a sustainable blue economy requires such innovative governance solutions, targeted investments, and nextgen impact assurance. © 2024 selection and editorial matter, pasquale marcello falcone and edgardo sica; individual chapters, the contributors."
"the risk measure of utility-based shortfall risk (sr) proposed by föllmer and schied (finance stoch 6:429–447, 2002) has been well studied in risk management and finance. in this paper, we revisit the concept from an insurance premium perspective. under some moderate conditions, we show that the indifference equation-based insurance premium calculation can be equivalently formulated as an optimization problem similar to the definition of sr. subsequently, we call the premium functional as an insurance premium-based shortfall risk measure (ipsr) defined over non-negative random variables. we then use the latter formulation to investigate the properties of the ipsr with a focus on the case that the preference functional is a distorted expected value function based on the cumulative prospect theory (cpt). specifically, we exploit weber’s approach (weber in math finance int j math stat financ econ 16:419–441, 2006) for characterization of the shortfall risk measure to derive a relationship between properties of ipsr induced by the cpt (ipsr-cpt) and the underlying value function in terms of convexity/concavity and positive homogeneity. we also investigate the ipsr-cpt as a functional of cumulative distribution function of random loss/liability and derive local and global lipschitz continuity of the function under wasserstein metric, a property which is related to statistical robustness of the ipsr-cpt. the results cover the premium risk measures based on the von neumann-morgenstern’s expected utility and yaari’s dual theory of choice as special cases. finally, we propose a computational scheme for calculating the ipsr-cpt. © 2022, the author(s), under exclusive licence to springer-verlag gmbh germany, part of springer nature."
"so many real life problems ranging from medicine, agriculture, biology and finance are modelled by nonlinear systems. in this case, a chaotic nonlinear system is considered and, as opposed to solving linear matrix inequality (lmi), which is the usual approach but cumbersome, a completely different approach was used. in some other cases, the computation of singular value of matrix was used but the method in this study needs not such. in addition, most models, if not all, concentrate on finding a control matrix j under some sufficient conditions. the problem is that only one such matrix j is provided. in reality, the actual control quantity may have a little deviation from the theoretical j. hence, the study in this paper provides a set of infinite uncertain matrices (formula presented.) which are able to adapt to control the system under uncertain conditions. it turns out that this new method controls the system in shorter time with less computational complexities. © 2023 by the authors."
"in this paper, we analyze a method for approximating the distribution function and density of a random variable that depends in a nontrivial way on a possibly high number of independent random variables, each with support on the whole real line. starting with the integral formulations of the distribution and density, the method involves smoothing the original integrand by preintegration with respect to one suitably chosen variable, and then applying a suitable quasi-monte carlo method to compute the integral of the resulting smoother function. interpolation is then used to reconstruct the distribution or density on an interval. the preintegration technique is a special case of conditional sampling, a method that has previously been applied to a wide range of problems in statistics and computational finance. in particular, the pointwise approximation studied in this work is a specific case of the conditional density estimator previously considered by l'ecuyer, puchhammer, and ben abdellah informs j. comput., 34 (2022), pp. 1729-1748]. our theory provides a rigorous regularity analysis of the preintegrated function, which is then used to show that the errors of the pointwise and interpolated estimators can both achieve nearly first-order convergence. numerical results support the theory. © 2023 society for industrial and applied mathematics."
"statistical and computational methods are being increasingly integrated into decision support systems to aid management and help with strategic decisions. researchers need to fully understand the use of such techniques in order to make predictions when using financial data. this paper therefore presents a method based literature review focused on the predictive analytics domain. the study comprehensively covers classification, regression, clustering, association and time series models. it expands existing explanatory statistical modelling into the realm of computational modelling. the methods explored enable the prediction of the future through the analysis of financial time series and cross-sectional data that is collected, stored and processed in information systems. the output of such models allow financial managers and risk oversight professionals to achieve better outcomes. this review brings the various predictive analytic methods in finance together under one domain. © 2022 the authors"
"this chapter provides a brief overview of computational thinking ineconomics and finance. it explores the nexus between developments in machine learning, artificial intelligence and economics. it sketches shu-heng chen’s contributions to the field and gives a panoramic view of the chapters included in this volume. © 2023, the author(s), under exclusive license to springer nature switzerland ag."
"this volume addresses the implications that academic interdisciplinarity in the field of english for academic purposes (eap) and english for specific purposes (esp) has for research and pedagogy with a global reach. the editors present a coherent, research-supported analysis of the influence of interdisciplinary research and methods on the way academics collaborate on courses, develop their careers and teach students. the hitherto prevalence of disciplinary silo-like approaches to academic and scientific issues is increasingly ceding ground to an interdisciplinary synergy of different methodological and epistemological traditions. in the context of ongoing trends towards interdisciplinarity in degree programmes and the increasing popularity of such degree programmes with students (e.g., bioinformatics, computational linguistics, psycholinguistics, neuropolitics, evolutionary finance, global studies, and security studies), academics and programme administrators need awareness of the skills needed to operate in interdisciplinary contexts. studies in this edited volume examine interdisciplinary communication practices, and identify how academic writing, teaching, language proficiency assessment and degree programmes are responding to changes in the broader social, institutional and political contexts of academia. as authors in the volume demonstrate, the discursive features, literacy practices and instructional modes, and the student experience of these emerging interdisciplines deserve systematic exploration. this insightful volume sheds light on contexts across the globe and will be used by students studying eap and esp pedagogy or practice; academics in the fields of applied linguistics and higher education, as well as higher education faculty and administrators interested in interdisciplinarity in degree programmes. © 2023 selection and editorial matter, louisa buckingham, jihua dong and feng (kevin) jiang; individual chapters, the contributors."
"a satellite communication system, as a typical example of the internet of things, is a smart critical infrastructure and has become an essential component used in various services such as finances, communications, ground and air-borne navigation, utilities, power grid distribution, emergency services, agriculture, banking, and many other critical industries. in recent times, satellite communication systems have become a target for cyber-attack. in this chapter, we review satellite infrastructure and the existing cybersecurity frameworks applied in smart critical infrastructure. we identified three main cybersecurity properties for satellite smart critical infrastructure, which are real-time analysis, mitigation mechanism, and low computational overhead. these properties are mapped against existing cybersecurity frameworks applied in smart critical infrastructure. the result indicated that the existing cybersecurity frameworks are either inapplicable, incompatible, or inadequate to address the cyber-attacks in satellite smart critical infrastructure. in addition, we identify a combination of mechanisms such as runtime verification and digital twin technology to address the satellite smart critical infrastructure cybersecurity. finally, we discuss a review of the mechanisms and their applications along with our future work. © 2023, the author(s), under exclusive license to springer nature switzerland ag."
"time series forecasting has fascinated a great deal of interest from various research communities due to its wide applications in medicine, economics, finance, engineering and many other crucial fields. various studies in past have shown that intuitionistic fuzzy sets (ifss) not only handle non-stochastic non-determinism in time series forecasting but also enhance accuracy in forecasted outputs. clustering is another one of the methods that improves accuracy of time series forecasting. the contribution of this research work is a novel computational fuzzy time series (fts) forecasting method which relies on ifss and self-organized direction aware (soda) approach of clustering. the usage of soda aids in making the proposed fts forecasting method as autonomous as feasible, as it does not require human intervention or prior knowledge of the data. forecasted outputs in proposed fts forecasting method are computed using a weighted formula and weights are optimized using grey wolf optimization (gwo) method. proposed fts is applied to forecast enrolments of the university of alabama and market price of state bank of india (sbi) share at bombay stock exchange (bse), india and performance is compared in terms of root mean square error (rmse), average forecasting error (afe) and mean absolute deviation (mad). goodness of the proposed fts forecasting method in forecasting enrolments of the university of alabama and market price of sbi share is also tested using coefficient of correlation and determination, criteria of akaike and bayesian information. © 2022 elsevier ltd"
"computational techniques are widely used to solve complex optimization problems in different fields such as engineering, finance, biology, and so on. in this paper, the human conception optimizer (hco) is proposed as a novel metaheuristic algorithm to solve any optimization problems. the idea of this algorithm is based on some biological principles of the human conception process, such as the selective nature of cervical gel in the female reproductive system to allow only healthy sperm cells into the cervix, the guidance nature of mucus gel to help sperm track a genital tracking path towards the egg in the fallopian tube, the asymmetric nature of flagellar movement which allows sperm cells to move in the reproductive system, the sperm hyperactivation process to make them able to fertilize an egg. thus, the strategies pursued by the sperm in searching for the egg in the fallopian tube are modeled mathematically. the best sperm which will meet the position of the egg will be the solution of the algorithm. the performance of the proposed hco algorithm is examined with a set of basic benchmark test functions called ieee cec-2005 and ieee cec-2020. a comparative study is also performed between the hco algorithm and other available algorithms. the significance of the results is verified with statistical test methods. to validate the proposed hco algorithm, two real-world engineering optimization problems are examined. for this purpose, a complex 14 over-current relay based ieee 8 bus distribution system is considered. with the proposed algorithm, an improvement of 50% to 60% in total relay operating times is observed comparing with some existing results for the same system. another engineering problem of designing an optimal proportional integral derivative (pid) controller for a blower driven patient hose mechanical ventilator (mv) is examined. a significant improvement in terms of response time, settling time is observed in the mv system by comparing with existing results. © 2022, the author(s)."
"there are bio-inspired metaheuristics in nature rarely used in areas where there is not domain or knowledge of computational algorithms, to mention some, medicine, finance and administration. ts-mbfoa, a bacteria-based algorithm and the differential evolution algorithm (dea), are metaheuristic algorithms proposed for the optimization of complex problems mathematically modeled as linear or non-linear problems. in this paper, these algorithms are implemented to analyze their performance in the search for better solutions in constrained optimization problems. tests were conducted on four optimization problems known in the literature as benchmark problems. both algorithms were run in 30 independent executions for each problem with the same number of generations and evaluations. although the parameters of each algorithm are different, the number of evaluations was selected for a fair comparison. results are similar for both algorithms, however, dea obtains better results for the problem with the larger number of constraints. additionally, dea generates solutions in less time than ts-mbfoa. the nonparametric wilcoxon signed rank test indicates significant differences in only 3 problems. the convergence graph of both algorithms for each problem shows that after 50 generations, both algorithms are close to the best known solution in the state of the art. © 2023 instituto politecnico nacional. all rights reserved."
"mathematical reasoning is a fundamental aspect of human intelligence and is applicable in various fields, including science, engineering, finance, and everyday life. the development of artificial intelligence (ai) systems capable of solving math problems and proving theorems in language has garnered significant interest in the fields of machine learning and natural language processing. for example, mathematics serves as a testbed for aspects of reasoning that are challenging for powerful deep learning models, driving new algorithmic and modeling advances. on the other hand, recent advances in large-scale neural language models have opened up new benchmarks and opportunities to use deep learning for mathematical reasoning. in this survey paper, we review the key tasks, datasets, and methods at the intersection of mathematical reasoning and deep learning over the past decade. we also evaluate existing benchmarks and methods, and discuss future research directions in this domain. © 2023 association for computational linguistics."
"a fundamental structural transformation that must occur to break global temperature rise and advance sustainable development is the green transition to a low-carbon system. however, dismantling the carbon lock-in situation requires substantial investment in green finance. historically, investments have been concentrated in carbon-intensive technologies. nonetheless, green finance has blossomed in recent years, and efforts to organise this literature have emerged, but a deeper understanding of this growing field is needed. for this goal, this paper aims to delineate this literature’s existing groups and explore its heterogeneity. from a bibliometric coupling network, we identified the main groups in the literature; then, we described the characteristics of these articles through a novel combination of complex network analysis, topological measures, and a type of unsupervised machine learning technique called structural topic modelling (stm). the use of computational methods to explore literature trends is increasing as it is expected to be compatible with a large amount of information and complement the expert-based knowledge approach. the contribution of this article is twofold: first, identifying the most relevant articles in the network related to each group and, second, the most prestigious topics in the field and their contributions to the literature. a final sample of 3275 articles shows three main groups in the literature. the more mature is mainly related to the distribution of climate finance from the developed to the developing world. in contrast, the most recent ones are related to climate financial risks, green bonds, and the insertion of financial development in energy-emissions-economics models. researchers and policy-makers can recognise current research challenges and make better decisions with the help of the central research topics and emerging trends identified from stm. the field’s evolution shows a clear movement from an international perspective to a nationally-determined discussion on finance to the green transition. © 2023 by the authors."
"data availability and accessibility have brought in unseen changes in the finance systems and new theoretical and computational challenges. for example, in contrast to classical stochastic control theory and other analytical approaches for solving financial decision-making problems that rely heavily on model assumptions, new developments from reinforcement learning (rl) can make full use of a large amount of financial data with fewer model assumptions and improve decisions in complex economic environments. this paper reviews the developments and use of deep learning(dl), rl, and deep reinforcement learning (drl)methods in information-based decision-making in financial industries. therefore, it is necessary to understand the variety of learning methods, related terminology, and their applicability in the financial field. first, we introduce markov decision processes, followed by various algorithms focusing on value and policy-based methods that do not require any model assumptions. next, connections are made with neural networks to extend the framework to encompass deep rl algorithms. finally, the paper concludes by discussing the application of these rl and drl algorithms in various decision-making problems in finance, including optimal execution, portfolio optimization, option pricing, hedging, and market-making. the survey results indicate that rl and drl can provide better performance and higher efficiency than traditional algorithms while facing real economic problems in risk parameters and ever-increasing uncertainties. moreover, it offers academics and practitioners insight and direction on the state-of-the-art application of deep learning models in finance. © 2022 the author(s)"
"logistic distribution (logdis) is frequently used in many different applications, such as logistic regression, logit models, classification, neural networks, physical sciences, sports modeling, finance and health and disease studies. for instance, the distribution function of the logdis has the same functional form as the derivative of the fermi function that can be used to set the relative weight of various electron energies in their contributions to electron transport. the logdis has wider tails than a normal distribution (nordis), so it is more consistent with the underlying data and provides better insight into the likelihood of extreme events. for this reason the united states chess federation has switched its formula for calculating chess ratings from the nordis to the logdis. the outcomes of many real-life experiments are sequences of record-breaking data sets, where only observations that exceed (or only those that fall below) the current extreme value are recorded. the practice demonstrated that the widely used estimators of the scale and location parameters of logistic record values, such as the best linear unbiased estimators (blues), have some defects. this paper investigates the shrinkage estimators of the location and scale parameters for logistic record values using prior information about their blues. theoretical and computational justifications for the accuracy and precision of the proposed shrinkage estimators are investigated via their bias and mean square error (mse), which provide sufficient conditions for improving the proposed shrinkage estimators to get unbiased estimators with minimum mse. the performance of the proposed shrinkage estimators is compared with the performances of the blues. the results demonstrate that the resulting shrinkage estimators are shown to be remarkably efficient. © 2023, the author(s), under exclusive licence to springer-verlag gmbh germany, part of springer nature."
"given a ground set of items, the result diversification problem aims to select a subset with high “quality” and “diversity” while satisfying some constraints. it arises in various real-world artificial intelligence applications, such as web-based search, document summarization and feature selection, and also has applications in other areas, e.g., computational geometry, databases, finance and operations research. previous algorithms are mainly based on greedy or local search. in this paper, we propose to reformulate the result diversification problem as a bi-objective maximization problem, and solve it by a multi-objective evolutionary algorithm (ea), i.e., the gsemo. we theoretically prove that the gsemo can achieve the (asymptotically) optimal theoretical guarantees under both static and dynamic environments. for cardinality constraints, the gsemo can achieve the optimal polynomial-time approximation ratio, 1/2. for more general matroid constraints, the gsemo can achieve an asymptotically optimal polynomial-time approximation ratio, 1/2−ϵ/(4n), where ϵ>0 and n is the size of the ground set of items. furthermore, when the objective function (i.e., a linear combination of quality and diversity) changes dynamically, the gsemo can maintain this approximation ratio in polynomial running time, addressing the open question proposed by borodin et al. [7]. this also theoretically shows the superiority of eas over local search for solving dynamic optimization problems for the first time, and discloses the robustness of the mutation operator of eas against dynamic changes. experiments on the applications of web-based search, multi-label feature selection and document summarization show the superior performance of the gsemo over the state-of-the-art algorithms (i.e., the greedy algorithm and local search) under both static and dynamic environments. © 2022"
"traditional sentiment analysis is a sentence-level or document-level task. however, a sentence or paragraph may contain multiple target terms with different sentiments, making sentiment prediction more challenging. although pre-trained language models like bert have been successful, incorporating dynamic semantic changes into aspect-based sentiment models remains difficult, especially for domain-specific sentiment analysis. to this end, in this paper, we propose a term-based sentiment analysis (tbsa), a novel method designed to learn environmental, social, and governance (esg) contexts based on a sustainability taxonomy for esg aspect-oriented sentiment analysis. notably, we introduce a technique enhancing the esg term's attention, inspired by the success of attention-based neural networks in machine translation (bahdanau et al., 2015) and computer vision (bello et al., 2019). it enables the proposed model to focus on a small region of the sentences at each step and to reweigh the crucial terms for a better understanding of the esg aspect-aware sentiment. beyond the novelty in the model design, we propose a new dataset of 125,000+ esg analyst-annotated data points for sustainability term-based sentiment classification, which derives from historical sustainability corpus data and expertise acquired by development finance institutions. our extensive experiments combining the new method and the new dataset demonstrate the effectiveness of the sustainability tbsa model with an accuracy of 91.30% (90% f1-score). both internal and external business applications of our model show an evident potential for a significant positive impact toward furthering sustainable development goals (sdgs). © 2022 association for computational linguistics."
"in most existing linguistic distribution multiple criteria group decision-making (mcgdm) models, the sample size of group linguistic distribution assessment (lda) is often neglected, and the decision makers are viewed to be independent. the probabilistic distribution is closely related to the sample size, and there exist more or fewer interference effects among different individual opinions. thus, this article develops a quantum framework for modeling interference effects in the linguistic distribution mcgdm process. first, considering the sample size information, we redefine the ldas and provide a new computational model for them. second, to integrate a series of ldas, the linguistic distribution weighted averaging (ldwa) operator is presented. mathematic proofs reveal that the new ldwa operator can not only ensure the information integrality but also can avoid the conjunctive and disjunctive operations. third, to explore the interference effects among individual opinions, a quantum framework is constructed. in this process, individual opinions are viewed as various wave functions occurring synchronously. they interfere with each and influence the aggregation result. finally, an illustrative example of internet finance soft power evaluation is provided to verify the effectiveness. sensitivity and comparative analyses are also implemented to assess the stability and validity of our method.  © 2013 ieee."
"outlier detection is an important branch of data mining research, and has wide applications in the fields of finance, telecommunications, and biology. the traditional nearest neighbor-based outlier detection (nnod) and local outlier factor-based outlier detection (lofod) algorithms generally have high computational complexity and high false-detection rates. this paper proposes an observation-point mechanism-based outlier detection (opod) algorithm comprising four core steps: i) generating random observation points in the original data space; ii) estimating the probability density function of distance values between the given observation point and all data points; iii) calculating the probabilities of distance values for the given observation point; and iv) detecting outliers by combining the probabilities corresponding to the different observation points. extensive experiments are conducted to demonstrate the feasibility, rationality, and effectiveness of the opod algorithm. the experimental results show that the opod algorithm converges as the number of observation points increases, and can attain better detection performance with lower computation complexity than the nnod and lofod algorithms. © 2022, science press. all right reserved."
"computational experiments have emerged as a new method for quantitative analysis of complex social systems. it has been applied to many interdisciplinary research fields, such as economics, finance, and epidemiology. though the representation form of computational experiments is relatively flexible, the real system is more complex. therefore, it is important to seek a balance between the flexibility of computational modeling and the credibility of conclusion. this article proposes a customized design framework for computational experiment models, so as to meet the diverse application demands of computational experiments in different fields. finally, this article outlines some typical applications of computational experiments to provide a roadmap for its rapid development and widespread application.  © 2014 ieee."
"the sparse nonlinear programming (snp) problem has wide applications in signal and image processing, machine learning and finance, etc. however, the computational challenge posed by snp has not yet been well resolved due to the nonconvex and discontinuous ℓ-norm involved. in this paper, we resolve this numerical challenge by developing a fast newton-type algorithm. as a theoretical cornerstone, we establish a first-order optimality condition for snp based on the concept of strong β-lagrangian stationarity via the lagrangian function, and reformulate it as a system of nonlinear equations called the lagrangian equations. the nonsingularity of the corresponding jacobian is discussed, based on which the lagrange–newton algorithm (lna) is then proposed. under mild conditions, we establish the locally quadratic convergence and its iterative complexity estimation. to further demonstrate the efficiency and superiority of our proposed algorithm, we apply lna to two specific problems arising from compressed sensing and sparse high-order portfolio selection, in which significant benefits accrue from the restricted newton step. © 2021, springer-verlag gmbh germany, part of springer nature and mathematical optimization society."
"our research investigates the potential benefits of adding the behavioral finance approach to the machine learning and big data framework applied to the challenging problem of forecasting the us dollar exchange rate. more specifically, we show how to improve existing voting-based ensemble models trained to predict the next-day exchange rate trend with no need for retraining or other costly computational tasks. we assume that calendar effects would constrain investors’ actions; furthermore, their constrained individual actions would collectively induce deterministic patterns in the financial time-series movement. hence, financial time-series forecasting models could be prone to monthly repeat their performance patterns, and we could use this information to obtain better predictions and consistently achieve profit. to verify the effectiveness of our methodology, we predicted the sign of the us dollar to brazilian real rate variation. our proposed models generated a profit metric value 24% higher than the original voting-based ensemble models with 16% lower volatility, gathering two positive elements: higher return with lower risk. the experiments’ outcomes supported the hypothesis that there are considerable improvements with almost no extra computational effort by taking into account behavioral patterns in foreign exchange predictions. © 2022 elsevier ltd"
"since its inception in the last century, chinese film and tv bases have faced the problems of rapid growth in quantity and unbalanced development in quality. the existing research generally expects that the financial investment of local governments and the agglomeration of local cultural industries can improve the level of industrial development of film bases and believe that the development of film base industries is affected by regions. however, the existing theoretical research has failed to give strong support to these optimistic expectations. based on relevant finance and economics databases, this study conducts correlation analysis and multiple logistic regression to test these optimistic proposals. the results show that there is a significant correlation between the development level of film bases and the government financial support, the cultural industry agglomeration, and transportation accessibility. however, the effect of the area where the base is located is not significant. in addition, the research builds a computational model based on multiple logistic regression to predict the development level of film bases and tries to provide a reference for the development of the film industry.  © 2022 ieee."
"the dynamical behavior of chaotic processes with a noninteger-order operator is considered in this work. a lot of scientific reports have justified that modeling of physical scenarios via non-integer order derivatives is more reliable and accurate than integer-order cases. motivated by this fact, the standard time derivatives in the model equations are formulated with the novel caputo fractional-order operator. the choice of using the caputo derivative among several existing fractional derivatives has to do with the fact that it gives way for both the initial conditions and boundary conditions to be incorporated in the development of the chaotic model. numerical approximation of fractional derivatives has been the major challenge of many scholars in different areas of engineering and applied sciences. hence, we developed a numerical approximation technique, which is based on the chebyshev spectral method for solving the integer-order and non-integer-order chaotic systems which are largely found in physics, finance, biology, engineering, and other areas of applied sciences. the proposed numerical method used here is easy to implement on a digital computer, and capable of solving higher-order problems without reduction to the system of lower-order ordinary differential equations with limited computational costs. experimental results are presented for different instances of fractional-order parameters. © 2022, the author(s), under exclusive licence to springer nature india private limited."
"purpose: enterprise resource planning (erp) software which is a knowledge-based design on the interconnective communication of business units and information share, ensures that business processes such as finance, production, purchasing, sales, logistics and human resources, are integrated and gathered under one roof. this integrated system allows the company to make fast and accurate decisions and increases its competitiveness. therefore, for an enterprise, choosing the suitable erp software is extremely important. the aim of this study is to present new research on the erp software selection process by clarifying the uncertainties and find suitable software in a computational way. design/methodology/approach: erp selection problem design includes uncertainties on the expert opinions and the criteria values using intuitionistic fuzzy set theory and interval grey-numbers to macbeth multi criteria decision making method. in this paper, a new interval grey macbeth method approach is proposed, and the degree of greyness approach is used for clarifying the uncertainties. using this new approach in which grey numbers are used, it is aimed to observe the changes in the importance of the alternatives. moreover, the intuitionistic fuzzy set method is applied by considering the importance of expert opinions separately. findings: the proposed method is based on quantitative decision making derived from qualitative judgments. the results given under uncertain conditions are compared with the results obtained under crisp conditions of the same methods. with the qualitative levels of experts reflected in the decision process, it is clearly seen that erp software selection problem area has more effective alternative decision solutions to the uncertain environment, and decision makers should not undervalue the unsteadiness of criteria during erp software selection process. originality/value: this study contributes to the relevant literature by (1) utilizing the macbeth method in the selection of the erp software by optimization, and (2) validating the importance of expert opinions with uncertainties on a proper erp software selection procedure. so, the findings of this study can help the decision-makers to evaluate the erp selection in uncertain conditions. © 2021, emerald publishing limited."
"quantum annealing has gained considerable attention because it can be applied to combinatorial optimization problems, which have numerous applications in logistics, scheduling, and finance. in recent years, with the technical development of quantum annealers, research on solving practical combinatorial optimization problems using them has accelerated. however, researchers struggle to find practical combinatorial optimization problems, for which quantum annealers outperform mathematical optimization solvers. moreover, there are only a few studies that compare the performance of quantum annealers with the state-of-the-art solvers, such as gurobi and cplex. this study determines that quantum annealing demonstrates better performance than the solvers in that the solvers take longer to reach the objective function value of the solution obtained by the quantum annealers for the break minimization problem in a mirrored double round-robin tournament. we also explain the desirable performance of quantum annealing for the sparse interaction between variables and a problem without constraints. in this process, we demonstrate that this problem can be expressed as a 4-regular graph. through computational experiments, we solve this problem using our quantum annealing approach and two-integer programming approaches, which were performed using the latest quantum annealer d-wave advantage, and gurobi, respectively. further, we compare the quality of the solutions and the computational time. quantum annealing was able to determine the exact solution in 0.05 seconds for problems with 20 teams, which is a practical size. in the case of 36 teams, it took 84.8 s for the integer programming method to reach the objective function value, which was obtained by the quantum annealer in 0.05 s. these results not only present the break minimization problem in a mirrored double round-robin tournament as an example of applying quantum annealing to practical optimization problems, but also contribute to find problems that can be effectively solved by quantum annealing. copyright: © 2022 kuramata et al. this is an open access article distributed under the terms of the creative commons attribution license, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited."
"quantum computing (qc) has the potential to strongly impact various sectors like finance, healthcare, communication, and technology by driving innovation across optimization and machine learning. applications of qc in chemical, pharmaceutical, and biomolecular industries are also predicted to grow rapidly in the near future. advancements in quantum hardware and algorithms have helped accelerate the widespread adoption of qc. yet, despite the progress, several research gaps and challenges need to be addressed before leveraging qc for chemical engineering applications. quantum computers offer higher computational power due to the exploitation of their quantum mechanical properties. however, not all computationally intractable problems can benefit from qc’s computational abilities. achieving speedups over classical computing with quantum algorithms implemented on current quantum devices is possible for a few specific tasks. it is imperative to identify chemical engineering problems of practical relevance that may benefit from novel quantum techniques either with current quantum computers or of the future. here, we present an introduction to basic concepts of qc while identifying the limitations of current quantum computers. a review of quantum algorithms that may benefit optimization and machine learning in chemical engineering with current quantum computers is also provided. this work also sets expectations for quantum devices of the future by exploring similar applications that may benefit from quantum algorithms implemented on such devices. © 2022, the korean institute of chemical engineers."
"using insights from social media for making investment decisions has become mainstream. however, in the current era of information explosion, it is essential to mine high-quality social media posts. the finnlp-2022 erai task deals with assessing maximum possible profit (mpp) and maximum loss (ml) from social media posts relating to finance. in this paper, we present our team lipi's approach. we ensembled a range of sentence transformers to quantify these posts. unlike other teams with varying performances across different metrics, our system performs consistently well. our code is available here.  ©2022 association for computational linguistics."
"the feasibility problem is at the core of the modeling of many problems in various disciplines of mathematics and physical sciences, and the quasi-convex function is widely applied in many fields such as economics, finance, and management science. in this paper, we consider the stochastic quasi-convex feasibility problem (sqfp), which is to find a common point of infinitely many sublevel sets of quasi-convex functions. inspired by the idea of a stochastic index scheme, we propose a stochastic quasi-subgradient method to solve the sqfp, in which the quasi-subgradients of a random (and finite) index set of component quasi-convex functions at the current iterate are used to construct the descent direction at each iteration. moreover, we introduce a notion of hölder-type error bound property relative to the random control sequence for the sqfp, and use it to establish the global convergence theorem and convergence rate theory of the stochastic quasi-subgradient method. it is revealed in this paper that the stochastic quasi-subgradient method enjoys both advantages of low computational cost requirement and fast convergence feature. © 2022 american institute of mathematical sciences. all rights reserved."
"the feasibility problem is at the core of the modeling of many problems in various areas, and the quasi-convex function usually provides a precise representation of reality in many fields such as economics, finance and management science. in this paper, we consider the quasi-convex feasibility problem (qfp), that is to find a common point of a family of sublevel sets of quasi-convex functions, and propose a unified framework of subgradient methods for solving the qfp. this paper is contributed to establish the quantitative convergence theory, including the iteration complexity and the convergence rates, of subgradient methods with the constant/dynamic stepsize rules and several general control schemes, including the α-most violated constraints control, the s-intermittent control and the stochastic control. an interesting finding is disclosed by iteration complexity results that the stochastic control enjoys both advantages of low computational cost requirement and low iteration complexity. more importantly, we introduce a notion of hölder-type error bound property for the qfp, and use it to establish the linear (or sublinear) convergence rates for subgradient methods to a feasible solution of the qfp. preliminary numerical results to the multiple cobb-douglas productions efficiency problem indicate the powerful modeling capability of the qfp and show the high efficiency and stability of subgradient methods for solving the qfp. © 2021 elsevier b.v."
"quantitative finance is the use of mathematical models to analyse financial markets and securities. typically requiring significant amounts of computation, an important question is the role that novel architectures can play in accelerating these models. in this paper we explore the acceleration of the industry standard securities technology analysis center's (stac) derivatives risk analysis benchmark stac-a2™ by porting the heston stochastic volatility model and longstaff and schwartz path reduction onto a xilinx alveo u280 fpga with a focus on efficiency-driven computing. describing in detail the steps undertaken to optimise the algorithm for the fpga, we then leverage the flexibility provided by the reconfigurable architecture to explore choices around numerical precision and representation. insights gained are then exploited in our final performance and energy measurements, where for the efficiency improvement metric we achieve between an 8 times and 185 times improvement on the fpga compared to two 24-core intel xeon platinum cpus. the result of this work is not only a show-case for the market risk analysis workload on fpgas, but furthermore a set of efficiency driven techniques and lessons learnt that can be applied to quantitative finance and computational workloads on reconfigurable architectures more generally. © 2022 acm."
"in the field of quantitative finance, stock price trend prediction has always been one of the concerns of people. stock price time series is the most important calculation basis in stock price prediction. the usual research idea is to take the one-dimensional single variable stock price time series to construct sample matrix through phase space reconstruction (psr), and use it as the feature input of machine learning algorithm to predict the stock price. however, we often need a relatively large embedded dimension to obtain more useful information, which easily leads to the problems of high matrix dimension, over-fitting of prediction and low computational efficiency. to solve the above problems, we propose a path signature-based phase space reconstruction (psr-ps) feature engineering approach for stock trend prediction, which can effectively extract features, reduce the dimension of high-dimensional price series data information and capture the necessary and effective information. extensive experiments on several benchmark data sets from diverse real financial markets show that psr-ps outperforms psr and psr-pca (principal component analysis-based psr) in accuracy, precision and computational time, combined with different machine learning algorithms. it suggests that the proposed psr-ps is effective. © 2022, the author(s), under exclusive licence to springer nature switzerland ag."
"hidden markov models (hmm) are used in a wide range of artificial intelligence applications including speech recognition, computer vision, computational biology and finance. estimating an hmm parameters is often addressed via the baum-welch algorithm (bwa), but this algorithm tends to convergence to local optimum of the model parameters. therefore, optimizing hmm parameters remains a crucial and challenging work. in this paper, a variable neighborhood search (vns) combined with baum-welch algorithm (vns-bwa) is proposed. the idea is to use vns to escape from local minima, enable greater exploration of the search space, and enhance the learning capability of hmms models. the proposed algorithm has entire advantage of combination of the search mechanism in vns algorithm for training with no gradient information, and the bwa algorithm that utilizes this kind of knowledge. the performance of the proposed method is validated on a real dataset. the results show that the vns-bwa has better performance finding the optimal parameters of hmm models, enhancing its learning capability and classification performance. © 2022 international academic press"
"we develop adaptive replicated designs for gaussian process metamodels of stochastic experiments. adaptive batching is a natural extension of sequential design heuristics with the benefit of replication growing as response features are learned, inputs concentrate, and the metamodeling overhead rises. motivated by the problem of learning the level set of the mean simulator response, we develop five novel schemes: multi-level batching (mlb), ratchet batching (rb), adaptive batched stepwise uncertainty reduction (absur), adaptive design with stepwise allocation (adsa), and deterministic design with stepwise allocation (ddsa). our algorithms simultaneously (mlb, rb, and absur) or sequentially (adsa and ddsa) determine the sequential design inputs and the respective number of replicates. illustrations using synthetic examples and an application in quantitative finance (bermudan option pricing via regression monte carlo) show that adaptive batching brings significant computational speed-ups with minimal loss of modeling fidelity. © 2021 wiley periodicals llc."
"nowadays in the finance world, there is a global trend for responsible investing, linked with a growing need for developing automated methods for analysing environmental, social and governance (esg) related elements in financial texts. in this work we propose a solution to the finsim4-esg task, consisting in classifying sentences from financial reports as sustainable or unsustainable. we propose a novel knowledge-based latent heterogeneous representation that relies on knowledge from taxonomies, knowledge graphs and multiple contemporary document representations. we hypothesize that an approach based on a combination of knowledge and document representations can introduce significant improvement over conventional document representation approaches. we perform ensembling, both at the classifier level and at the representation level (late-fusion and early-fusion). the proposed approaches achieve competitive accuracy of 89% and are 5.85% behind the best score in the shared task.  ©2022 association for computational linguistics."
"vector autoregression (var) models are widely used to analyze the interrelationship between multiple variables over time. estimation and inference of the transition matrices of var models are crucial for practitioners to make decisions in fields such as economics and finance. however, when the number of variables is larger than the sample size, it remains a challenge to perform inference of the model parameters. the de-biased lasso and two bootstrap de-biased lasso methods are proposed to construct confidence intervals for the elements of the transition matrices of high-dimensional var models. the proposed methods are asymptotically valid under appropriate sparsity and other regularity conditions. moreover, feasible and parallelizable algorithms are developed to implement the proposed methods, which save a large amount of computational cost required by the nodewise lasso and bootstrap. simulation studies illustrate that the proposed methods perform well in finite-samples. finally, the proposed methods are applied to analyze the price data of stocks in the s&p 500 index. some stocks, such as the largest producer of gold in the world, newmont corporation, are found to have significant predictive power over most stocks. © 2021 elsevier b.v."
"semi-variance is a similar measure to variance, but it only considers values that are below the expected value. as important roles of semi-variance in finance, this paper proposes the concept of semi-variance for uncertain random variables. also, a computational approach for semi-variance is provided via inverse uncertainty distribution. as an application in finance, portfolio selection problems of uncertain random returns are solved by minimizing semi-variance in mean-semi variance models. for better illustration, mean-semi variance model is compared with mean-variance one. finally, for better understanding, some tables, figures and outputs are provided. © 2022, the author(s), under exclusive licence to springer-verlag gmbh germany, part of springer nature."
"the recent advancements in computational power and machine learning algorithms have led to vast improvements in manifold areas of research. especially in finance, the application of machine learning enables both researchers and practitioners to gain new insights into financial data and well-studied areas such as company classification. in our paper, we demonstrate that unsupervised machine learning algorithms can visualize and classify company data in an economically meaningful and effective way. in particular, we implement the data-driven dimension reduction and visualization tool t-distributed stochastic neighbor embedding (t-sne) in combination with spectral clustering. the resulting company groups can then be utilized by experts in the field for empirical analysis and optimal decision making. we are the first to demonstrate how this approach can be implemented in manifold areas of finance by developing a general decision engine. with two exemplary out-of-sample studies on portfolio optimization and company valuation with multiples, we show that the application of t-sne and spectral clustering improves competitive benchmark models. © 2022 elsevier ltd"
"matlab can run python code! python for matlab development shows you how to enhance matlab with python solutions to a vast array of computational problems in science, engineering, optimization, statistics, finance, and simulation. it is three books in one: a thorough python tutorial that leverages your existing matlab knowledge with a comprehensive collection of matlab/python equivalent expressions a reference guide to setting up and managing a python environment that integrates cleanly with matlab a collection of recipes that demonstrate python solutions invoked directly from matlab this book shows how to call python functions to enhance matlab's capabilities. specifically, you'll see how python helps matlab: run faster with numba distribute work to a compute cluster with dask find symbolic solutions to integrals, derivatives, and series summations with sympy overlay data on maps with cartopy solve mixed-integer linear programming problems with pulp interact with redis via pyredis, postgresql via psycopg2, and mongodb via pymongo read and write file formats that are not natively understood by matlab, such as sqlite, yaml, and ini who this book is for matlab developers who are new to python and other developers with some prior experience with matlab, r, idl, or mathematica. © 2022 by albert danial. all rights reserved."
"time series forecasting (tsf) is one of the most important tasks in data science, as accurate time series (ts) predictions can drive and advance a wide variety of domains including finance, transportation, health care, and power systems. however, real-world utilization of machine learning (ml) models for tsf suffers due to data drift. to address this, models must be periodically retained or redesigned, which requires significant human and computational resources. this work presents the online neuroevolution based neural architecture search (one-nas) algorithm, which to the authors' knowledge is the first neural architecture search algorithm capable of automatically designing and training new recurrent neural networks (rnns) in an online setting. without any pretraining, one-nas utilizes populations of rnns which are continuously updated with new network structures and weights in response to new multivariate input data. one-nas is tested on real-world large-scale multivariate wind turbine data and is shown to outperform traditional statistical time series forecasting, including naive, moving average, and exponential smoothing methods. © 2022 owner/author."
"the stock market is an ideal way to invest money and earn potential returns. but, even with advanced technology and computational power, this chapter still cannot predict the patterns of rise and fall in the stock market, and still, it is considered a risky business. to ease the process of investment and to provide better consciousness, this chapter proposes the prediction of stock market deviation using the arima (auto-regressive integrated moving average) algorithm and long short-term memory (lstm) algorithm. this chapter is using an algorithm which is trying to predict the future pattern for any stock based on the real-time analysis and data provided from yahoo finance data. the software provides pictorial and graphical representations. the objective is to provide short-term and long-term prediction competence to prepare for future potential investments. © 2022, igi global."
"introduced more than a half-century ago, granger causality has become a popular tool for analyzing time series data in many application domains, from economics and finance to genomics and neuroscience. despite this popularity, the validity of this framework for inferring causal relationships among time series has remained the topic of continuous debate. moreover, while the original definition was general, limitations in computational tools have constrained the applications of granger causality to primarily simple bivariate vector autoregressive processes. starting with a review of early developments and debates, this article discusses recent advances that address various shortcomings of the earlier approaches, from models for high-dimensional time series to more recent developments that account for nonlinear and non-gaussian observations and allow for subsampled and mixed-frequency time series. © 2022 annual reviews inc.. all rights reserved."
"convolutional neural networks (cnns) and deep learning technology are applied in current financial market to rapidly promote the development of finance market and internet economy. the continuous development of neural networks with more hidden layers improves the performance but increases the computational complexity. generally, channel pruning methods are useful to compact neural networks. however, typical channel pruning methods would remove layers by mistake due to the static pruning ratio of manual setting, which could destroy the whole structure of neural networks. it is difficult to improve the ratio of compressing neural networks only by pruning channels while maintaining good network structures. therefore, we propose a novel neural networks pruning by recurrent weights (nprw) that can repeatedly evaluate the significance of weights and adaptively adjust them to compress neural networks within acceptable loss of accuracy. the recurrent weights with low sensitivity are compulsorily set to zero by evaluating the magnitude of weights, and pruned network only uses a few significant weights. then, we add the regularization to the scaling factors on neural networks, in which recurrent weights with high sensitivity can be dynamically updated and weights of low sensitivity stay at zero invariably. by this way, the significance of channels can be quantitatively evaluated by recurrent weights. it has been verified with typical neural networks of lenet, vggnet, and resnet on multiple benchmark datasets involving stock index futures, digital recognition, and image classification. the pruned lenet-5 achieves the 58.9% reduction amount of parameters with 0.29% loss of total accuracy for shanghai and shenzhen 300 stock index futures. as for the cifar-10, the pruned vgg-19 reduces more than 50% flops, and the decrease of network accuracy is less than 0.5%. in addition, the pruned resnet-164 tested on the svhn reduces more than 58% flops with relative improvement on accuracy by 0.11%. © 2022 association for computing machinery."
"we study the ornstein-uhlenbeck process having a symmetric normal tempered stable stationary law and represent its transition distribution in terms of the sum of independent laws. in addition, we write the background driving lévy process as the sum of two independent lévy components. accordingly, we can design two alternate algorithms for the simulation of the skeleton of the ornstein-uhlenbeck process. the solution based on the transition law turns out to be faster since it is based on a lower number of computational steps, as confirmed by extensive numerical experiments. we also calculate the characteristic function of the transition density which is instrumental for the application of the fft-based method of carr and madan (j comput finance 2:61–73, 1999) to the pricing of a strip of call options written on markets whose price evolution is modeled by such an ornstein-uhlenbeck dynamics. this setting is indeed common for spot prices in the energy field. finally, we show how to extend the range of applications to future markets. © 2022, the author(s)."
"recently monte carlo and quasi-monte carlo approaches have become a very attractive and necessary computational tools in finance. the field of computational finance is becoming more complicated with increasing number of applications. the pricing of options is a very important in financial markets today and especially difficult when the dimension of the problem goes higher. monte carlo and quasi monte carlo methods are appropriate for solving multidimensional problems, since their computational complexity increases polynomially, but not exponentially with the dimensionality. a comprehensive experimental study based on scrambling of the sobol sequence is applied for the first time to evaluate european style options. the sobol scrambling method is not only one of the best available algorithms for high dimensional integrals but also one of the few possible methods, because in this work we show that the deterministic algorithms need a huge amount of time for the evaluation of the multidimensional integral, as it was discussed in this paper. the numerical tests show that the method is efficient for multidimensional integration and especially for computing multidimensional integrals of a very high dimension.  © 2022 author(s)."
"a novel local meshless scheme based on the radial basis function (rbf) is introduced in this article for price multi-asset options of even european and american types based on the black-scholes model. the proposed approach is obtained by using operator splitting and repeating the schemes of richardson extrapolation in the time direction and coupling the rbf technology with a finite-difference (fd) method that leads to extremely sparse matrices in the spatial direction. therefore, it is free of the ill-conditioned difficulties that are typical of the standard rbf approximation. we have used a strong iterative idea named the stabilized bi-conjugate gradient process (bicgstab) to solve highly sparse systems raised by the new approach. moreover, based on a review performed in the current study, the presented scheme is unconditionally stable in the case of independent assets when spatial discretization nodes are equispaced. as seen in numerical experiments, it has a low computational cost and generates higher accuracy. finally, the proposed local rbf scheme is very versatile so that it can be used easily for solving numerous models and obstacles not just in the finance sector, as well as in other fields of engineering and science. © 2022 university of tabriz. all right reserved."
"the rapid evolution of technology has led to the generation of high dimensional data streams in a wide range of fields, such as genomics, signal processing, and finance. the combination of the streaming scenario and high dimensionality is particularly challenging especially for the outlier detection task. this is due to the special characteristics of the data stream such as the concept drift, the limited time and space requirements, in addition to the impact of the well-known curse of dimensionality in high dimensional space. to the best of our knowledge, few studies have addressed these challenges simultaneously, and therefore detecting anomalies in this context requires a great deal of attention. the main objective of this work is to study the main approaches existing in the literature, to identify a set of comparison criteria, such as the computational cost and the interpretation of outliers, which will help us to reveal the different challenges and additional research directions associated with this problem. at the end of this study, we will draw up a summary report which summarizes the main limits identified and we will detail the different directions of research related to this issue in order to promote research for this community. © 2022 elsevier inc."
"artificial intelligence (ai) is transforming many domains, including finance, agriculture, defense, and biomedicine. in this paper, we focus on the role of ai in clinical and translational research (ctr), including preclinical research (t1), clinical research (t2), clinical implementation (t3), and public (or population) health (t4). given the rapid evolution of ai in ctr, we present three complementary perspectives: (1) scoping literature review, (2) survey, and (3) analysis of federally funded projects. for each ctr phase, we addressed challenges, successes, failures, and opportunities for ai. we surveyed clinical and translational science award (ctsa) hubs regarding ai projects at their institutions. nineteen of 63 ctsa hubs (30%) responded to the survey. the most common funding source (48.5%) was the federal government. the most common translational phase was t2 (clinical research, 40.2%). clinicians were the intended users in 44.6% of projects and researchers in 32.3% of projects. the most common computational approaches were supervised machine learning (38.6%) and deep learning (34.2%). the number of projects steadily increased from 2012 to 2020. finally, we analyzed 2604 ai projects at ctsa hubs using the national institutes of health research portfolio online reporting tools (reporter) database for 2011–2019. we mapped available abstracts to medical subject headings and found that nervous system (16.3%) and mental disorders (16.2) were the most common topics addressed. from a computational perspective, big data (32.3%) and deep learning (30.0%) were most common. this work represents a snapshot in time of the role of ai in the ctsa program. © 2021 the authors. clinical and translational science published by wiley periodicals llc on behalf of american society for clinical pharmacology and therapeutics."
"wearable devices are parts of the essential cost of goods sold (cogs) in the wheel of the internet of things (iot), contributing to a potential impact in the finance and banking sectors. there is a need for lightweight cryptography mechanisms for iot devices because these are resource constraints. this paper introduces a novel approach to an iot-based micro-payment protocol in a wearable devices environment. this payment model uses an “elliptic curve integrated encryption scheme (ecies)” to encrypt and decrypt the communicating messages between various entities. the proposed protocol allows the customer to buy the goods using a wearable device and send the mobile application’s confidential payment information. the application creates a secure session between the customer, banks and merchant. the static security analysis and informal security methods indicate that the proposed protocol is withstanding the various security vulnerabilities involved in mobile payments. for logical verification of the correctness of security properties using the formal way of “burrows-abadi-needham (ban)” logic confirms the proposed protocol’s accuracy. the practical simulation and validation using the scyther and tamarin tool ensure that the absence of security attacks of our proposed framework. finally, the performance analysis based on cryptography features and computational overhead of related approaches specify that the proposed micro-payment protocol for wearable devices is secure and efficient. © 2022, the author(s), under exclusive licence to springer science+business media, llc, part of springer nature."
"securities trading is one of the few business activities where a few seconds processing delay can cost a company a big fortune. the growing competition in the market exacerbates the situation and pushes further toward instantaneous trading even in a split second. the key lies on the performance of the underlying information system. following the computing evolution in financial services, it was a centralized process to begin with and gradually decentralized into a distribution of actual application logic across service networks. financial services have a tradition of doing most of its heavy lifting financial analysis in overnight batch cycles. in securities trading, however, it cannot satisfy the need due to its ad hoc nature and requirement of fast response. new computing paradigms, such as grid and cloud computing, aiming at scalable and virtually standardized distributed computing resources, are well suited to the challenge posed by capital market practices. both have been gaining popularity to serve as a production environment for finance services in recent years, which further leads to the recent rise of big data and the success of large-scale machine learning, impacting on broad and diverse domains, such as finance services. in this entry, the core computing competence for financial services is examined. a comparison of grid and cloud will be briefly described. how the underlying algorithm for financial analysis can take advantage of grid and cloud environment is presented. one of the most popular practiced algorithms monte carlo simulation is used in our cases study for option pricing and risk management. the various distributed computational platforms are carefully chosen to demonstrate the performance issue for financial services, which also extends to applications of big data and machine learning. © springer nature switzerland ag 2022."
"with the goal of reasoning on the financial textual data, we present in this paper, a novel approach for annotating arguments, their components and relations in the transcripts of earnings conference calls (eccs). the proposed scheme is driven from the argumentation theory at the micro-structure level of discourse. we further conduct a manual annotation study with four annotators on 136 documents. by that, we obtained inter-annotator agreement of αu = 0.70 for argument components and α = 0.81 for argument relations. the final created corpus, with the size of 804 documents, as well as the annotation guidelines are publicly available for researchers in the domains of computational argumentation, finance and finnlp.  ©2022 association for computational linguistics."
"with the continuous development of blockchain technology, blockchain gradually becomes to play an important role in the fields of finance, medicine, and new energy. in the certification of the membership of the blockchain, a third-party certificate authority (ca) is used for certification. considering the centralized structure of ca, and it is difficult for users to evaluate the credibility of ca. a decentralized blockchain membership authentication scheme and a key agreement protocol based on the elliptic curve are proposed by us. the protocol effectively addresses the credibility and single point of failure problems of centralized cas in the traditional model. through analysis, our scheme can effectively perform user registration and membership authentication instead of cas. the security and correctness of the protocol was also analyzed using the formal protocol analysis tool proverif and the ck model. the key authentication protocol we proposed can resist a variety of attacks, and the computational time consumption and communication costs are relatively low. © 2022 john wiley & sons, ltd."
"with the emergence of various online trading technologies, fraudulent cases begin to occur frequently. the problem of fraud in public trading companies is a hot topic in financial field. this paper proposes a fraud detection model for public trading companies using datasets collected from sec’s accounting and auditing enforcement releases (aaers). at the same time, this computational finance model is solved with a nonlinear activated beetle antennae search (nabas) algorithm, which is a variant of the meta-heuristic optimization algorithm named beetle antennae search (bas) algorithm. firstly, the fraud detection model is transformed into an optimization problem of minimizing loss function and using the nabas algorithm to find the optimal solution. nabas has only one search particle and explores the space under a given gradient estimation until it is less than an “activated threshold” and the algorithm is efficient in computation. then, the random under-sampling with adaboost (rusboost) algorithm is employed to comprehensively evaluate the performance of nabas. in addition, to reflect the superiority of nabas in the fraud detection problem, it is compared with some popular methods in recent years, such as the logistic regression model and support vector machine with financial kernel (svm-fk) algorithm. finally, the experimental results show that the nabas algorithm has higher accuracy and efficiency than other methods in the fraud detection of public datasets. © 2022 by the authors. licensee mdpi, basel, switzerland."
"the potential of having multiple distributed applications across multiple domains such as healthcare, finance, supply chain management and many more have made blockchain very popular among both academia and industries. blockchain provides the much-needed mechanism for decentralization of systems, where the need of trusted central authority is eliminated. proof of work (pow) is heavily adopted in both bitcoin and ethereum based blockchain, where many miners (or mining pools) compete to mine each block generated by solving a cryptographic puzzle that uses all of the previous information of the blockchain, before tying the block to the blockchain with the nonce. recently there is a shift in having two or more mining pools for the pow consensus in ethereum-based blockchain. majority of the studies conducted for multiple mining pool techniques in the literature are verified with simulation experiments. therefore, in this paper, we implement a testbed for ethereum blockchain with multiple nodes that simulate two mining pools using pow consensus in a centralized and decentralized fashion. two miner-nodes were deployed with different computational power (in term of cpu threads) and transactions executed. we evaluate the contribution of each miner node in the blockchain system and the assignment of transactions with respect to the computational resources available.  © 2022 ieee."
"this chapter examines several key models and techniques associated with prescriptive analytics. they include sentiment analysis, association rules, network analysis, recommender systems, and principal components analysis. while these methods are not designed for a particular discipline, they provide a good opportunity for finance professionals and researchers to expand their analytic toolset. an effort was made to encourage the reader to explore applications of these models in other areas, given that mathematics, statistics, and computer programming transcend many disciplines. in some cases, the models used datasets available on free data repositories. in other cases, the datasets were synthetically generated for the examples in this chapter, to highlight certain phenomena. each example includes a fully developed solution written in the r programming language. prior knowledge of r is useful but not essential. a reasonable affinity towards computational thinking and experience with other programming languages is a definite plus. © 2022, the author(s), under exclusive license to springer nature switzerland ag."
"multistage stochastic optimization problems (msop) are a commonly used paradigm to model many decision processes in energy and finance. usually, a set of scenarios (the so-called tree) describing the stochasticity of the problem are obtained and the stochastic dual dynamic programming (sddp) algorithm is often used to compute policies. quite often, the uncertainty affects only the right-hand side (rhs) of the optimization problems in consideration. after solving a msop, one naturally wants to know if the solution obtained depends on the scenarios and by how much. in this paper we show that when a msop with stage-wise independent realizations has only rhs uncertainties, solving one tree using sddp provides a valid lower bound for all trees with the same number of scenarios per stage without any additional computational effort. the only change to the traditional sddp is the way cuts are calculated. once the first tree is solved approximately, a computational assessment of the statistical significance of the current number of scenarios per stage is performed, solving for each new sampled tree, an easy lp to get a valid lower bound for the new tree. the objective of the paper is to estimate by how much the lower bound of the first tree depends on chance. the result of the computational assessment are fast estimates of the mean, variance and max variation of lower bounds across many trees. if the variance of the calculated lower bounds is small, we conclude that the cutting planes model has a small sensitivity to the trees sampled. otherwise, we increase the number of scenarios per stage and repeat. we do not make assumptions on the distributions of the random variables. the results are not asymptotic. our method has applications to the determination of the correct number of scenarios per stage. extensions for uncertainties in the objective only are possible via the dual sddp. we test our method numerically and verify the correctness of the cut-sharing technique. © 2022, the author(s), under exclusive licence to springer science+business media, llc, part of springer nature."
"time series are generated at an unprecedented rate in domains ranging from finance, medicine to education. collections composed of heterogeneous, variable-length and misaligned times series are best explored using a plethora of dynamic time warping distances. however, the computational costs of using such elastic distances result in unacceptable response times. we thus design the first practical solution for the efficient general exploration of time series leveraging multiple warped distances. genex pre-processes time series data in metric point-wise distance spaces, while providing bounds for the accuracy of corresponding analytics derived in non-metric warped distance spaces. our empirical evaluation on 66 benchmark datasets provides a comparative study of the accuracy and response times of diverse warped distances. we show that genex is a versatile yet highly efficient solution for processing expensive-to-compute warped distances over large datasets, with response times 3 to 5 orders of magnitude faster than state-of-art systems.  © 1989-2012 ieee."
"information theory is a well-established method for the study of many phenomena and more than 70 years after claude shannon first described it in a mathematical theory of communication it has been extended well beyond shannon’s initial vision. it is now an interdisciplinary tool that is used from ‘causal’ information flow to inferring complex computational processes and it is common to see it play an important role in fields as diverse as neuroscience, artificial intelligence, quantum mechanics, and astrophysics. in this article, i provide a selective review of a specific aspect of information theory that has received less attention than many of the others: as a tool for understanding, modelling, and detecting non-linear phenomena in finance and economics. although some progress has been made in this area, it is still an under-developed area that i argue has considerable scope for further development. © 2022 by the author. licensee mdpi, basel, switzerland."
"time series modelling has a wide spectrum of applications in several fields including engineering and finance. most traditional modelling techniques rely on assumptions related to the input data and manual pre-processing, based on user observations, rendering them unsuitable for analysing time-series with varying characteristics automatically, while more general modelling techniques usually require increased computational work for application and tuning. recently, a general modelling framework based on a recursive schur complement technique, that utilizes an adaptively determined set of basis functions, has been proposed. herewith, a novel modified approach based on recursive incomplete pseudoinverse matrices in conjunction with preconditioned iterative methods for large datasets, is proposed. this sparse approach greatly reduces storage requirements and the recursive nature of the procedure avoids recomputation of the preconditioner after addition of a new basis function. moreover, update of the coefficients for a different window of data and predefined basis functions can be performed utilizing the incomplete pseudoinverse matrix as preconditioner. the case of sinusoidal basis functions is presented along with a novel adaptive frequency estimation technique. the stability of the resulting model is discussed with respect to the choice of basis functions. the case of basis derived from machine learning techniques is also discussed. numerical results are given depicting the applicability, generality and effectiveness of the proposed technique. comparative results with other methods show forecasting rmse improvement between 7% to 80%, for the majority of the chosen time series. © 2022 the author(s)"
"predictive models are increasingly used to make various consequential decisions in high-stakes domains such as healthcare, finance, and policy. it becomes critical to ensure that these models make accurate predictions, are robust to shifts in the data, do not rely on spurious features, and do not unduly discriminate against minority groups. to this end, several approaches spanning various areas such as explain ability, fairness, and robustness have been proposed in recent literature. such approaches need to be human-centered as they cater to the understanding of the models to their users. however, there is little to no research on understanding the needs and challenges in monitoring deployed machine learning (ml) models from a human-centric perspective. to address this gap, we conducted semi-structured interviews with 13 practitioners who are experienced with deploying ml models and engaging with customers spanning domains such as financial services, healthcare, hiring, online retail, computational advertising, and conversational assistants. we identified various shuman-centric challenges and requirements for model monitoring in real-world applications. specifically, we found that relevant stakeholders would want model monitoring systems to provide clear, unambiguous, and easy-to-understand insights that are readily actionable. furthermore, our study also revealed that stakeholders desire customization of model monitoring systems to cater to domain-specific use cases. © 2022, association for the advancement of artificial intelligence."
"credit ratings are fundamental in assessing the credit risk of a security or debtor. the failure of the collateralized debt obligation (cdo) ratings during the financial crisis of 2007-2008 and the massive undervaluation of corporate risk leading up to the crisis resulted in a review of rating approaches. yet the fundamental metric that guides the construction of credit ratings has not changed. we study the inadequacies of the old metric in simple models of investment and in structured finance portfolio optimization tasks, and we propose a new methodology based on a buffered probability of exceedance. the new approach offers a conservative risk assessment, with substantial conceptual and computational benefits. we illustrate the new approach using several examples and report the results of a structuring step-up cdo case study, with details available in an online supplement. © 2021 elsevier b.v."
"a key component of blockchain technology is the ledger, viz., a database that, unlike standard databases, keeps in memory the complete history of past transactions as in a notarial archive for the benefit of any future test. in second-generation blockchains such as ethereum, the ledger is coupled with smart contracts, which enable the automation of transactions associated with agreements between the parties of a financial or commercial nature. the coupling of smart contracts and ledgers provides the technological background for very innovative application areas, such as decentralized autonomous organizations (daos), initial coin offerings (icos), and decentralized finance (defi), which propelled blockchains beyond cryptocurrencies that were the only focus of first generation blockchains, such as bitcoin. however, the currently used implementation of smart contracts as arbitrary programming constructs has made them susceptible to dangerous bugs that can be exploited maliciously and has moved their semantics away from that of legal contracts. we propose here to recompose the split and recover the reliability of databases by formalizing a notion of contract modeled as a finite-state automaton with well-defined computational characteristics derived from encoding in terms of allocations of resources to actors, as an alternative to the approach based on programming. to complete the work, we use temporal logic as the basis for an abstract query language that is effectively suited to the historical nature of the information kept in the ledger. © 2022 the authors"
"nowadays, in a world full of uncertainties and the threat of digital and cyber-attacks, blockchain technology is one of the major critical developments playing a vital role in the creative professional world. along with energy, finance, governance, etc., the healthcare sector is one of the most prominent areas where blockchain technology is being used. we all are aware that data constitute our wealth and our currency; vulnerability and security become even more significant and a vital point of concern for healthcare. recent cyberattacks have raised the questions of planning, requirement, and implementation to develop more cyber-secure models. this paper is based on a blockchain that classifies network participants into clusters and preserves a single copy of the blockchain for every cluster. the paper introduces a novel blockchain mechanism for secure healthcare sector data management, which reduces the communicational and computational overhead costs compared to the existing bitcoin network and the lightweight blockchain architecture. the paper also discusses how the proposed design can be utilized to address the recognized threats. the experimental results show that, as the number of nodes rises, the suggested architecture speeds up ledger updates by 63% and reduces network traffic by 10 times. © 2022 by the authors."
"from a public finance point of view, achieving sustainable development hinges on two critical factors: the subnational implementation of public policies and the efficient allocation of resources across regions through vertical intergovernmental transfers. we introduce a framework that links these two mechanisms for analyzing the impact of reallocating federal transfers in the presence of regional heterogeneity from development indicators, budget sizes, expenditure returns, and long-term structural factors. our study focuses on the case of mexico and its 32 states. using an agent-based computational model, we estimate the development gaps that will remain by the year 2030, and characterize their sensitivity to changes in the states’ budget sizes. then, we estimate the optimal distribution of federal transfers to minimize these gaps. crucially, these distributions depend on the specific development objectives set by the national government, and by various interdependencies between the heterogeneous qualities of the states. this work sheds new light on the complex problem of budgeting for the sustainable development goals at the subnational level, and it is especially relevant for the study of fiscal decentralization from the expenditure point of view. © 2021 the authors"
"the growing popularity of quantitative trading in pursuit of a systematic and algorithmic approach to investment has drawn considerable attention among traders and investment firms. consequently, an effective computational method for evaluating potential risk factors and returns is crucial for the development of algorithmic trading strategies. in traditional finance and financial engineering research, statistical approaches have been widely applied to quantitative analysis. meanwhile, investor demand for quantitative hedge funds has surged worldwide. in the current study, the multiperiod portfolio selection problem was considered in terms of the realistic transaction cost model, which is a major concern for quantitative hedge fund managers. we developed a dedicated multiagent-based deep reinforcement learning framework with a two-level nested agent structure to determine effective portfolio management methods with different objectives. in addition, we proposed a specially-designed reward function for investment performance evaluation and a novel policy network structure for trading decision-making. to efficiently identify specific asset attributes in a portfolio, each agent is equipped with a refined deep policy network and a special training method that enables the proposed reinforcement learning agent to learn risk transfer behaviors. the results revealed the effectiveness of our proposed framework, which outperformed several established or representative portfolio selection strategies. © 2022 elsevier b.v."
"this book is a detailed and step-by-step introduction to the mathematical foundations of ordinary and partial differential equations, their approximation by the finite difference method and applications to computational finance. the book is structured so that it can be read by beginners, novices and expert users. part a mathematical foundation for one-factor problems chapters 1 to 7 introduce the mathematical and numerical analysis concepts that are needed to understand the finite difference method and its application to computational finance. part b mathematical foundation for two-factor problems chapters 8 to 13 discuss a number of rigorous mathematical techniques relating to elliptic and parabolic partial differential equations in two space variables. in particular, we develop strategies to preprocess and modify a pde before we approximate it by the finite difference method, thus avoiding ad-hoc and heuristic tricks. part c the foundations of the finite difference method (fdm) chapters 14 to 17 introduce the mathematical background to the finite difference method for initial boundary value problems for parabolic pdes. it encapsulates all the background information to construct stable and accurate finite difference schemes. part d advanced finite difference schemes for two-factor problems chapters 18 to 22 introduce a number of modern finite difference methods to approximate the solution of two factor partial differential equations. this is the only book we know of that discusses these methods in any detail. part e test cases in computational finance chapters 23 to 26 are concerned with applications based on previous chapters. we discuss finite difference schemes for a wide range of one-factor and two-factor problems. this book is suitable as an entry-level introduction as well as a detailed treatment of modern methods as used by industry quants and msc/mfe students in finance. the topics have applications to numerical analysis, science and engineering. more on computational finance and the author’s online courses, see www.datasim.nl. © 2022 by john wiley & sons, ltd."
"in recent years, there has been a growing interest in the use of reference conceptual models to capture information about complex and sensitive business domains (e.g., finance, healthcare, space). these models play a fundamental role in different types of critical semantic interoperability tasks. therefore, domain experts must be able to understand and reason with their content. in other words, these models need to be cognitively tractable. this paper contributes to this goal by proposing a model clustering technique that leverages on the rich semantics of ontology-driven conceptual models (odcm). in particular, we propose a formal notion of relational context to guide the automated clusterization (or modular breakdown) of conceptual models. such relational contexts capture all the information needed for understanding entities “qua players of roles” in the scope of an objectified (reified) relationship (relator). the paper also presents computational support for automating the identification of relational contexts and this modular breakdown procedure. finally, we report the results of an empirical study assessing the cognitive effectiveness of this approach. © 2021, the author(s), under exclusive licence to springer-verlag gmbh germany, part of springer nature."
"due to the expanding world's population and the associated rise in demand for food, the global food supply chain is under a lot of pressure to keep up with demand while preserving food quality, preventing waste, avoiding deforestation, and lowering carbon footprints (fsc). due of speedy computational capabilities and the availability of data, block chain technology has emerged as a feasible strategy that might help create a safe and secure fsc. according to the large body of literature that has been written on this subject over the last several years, block chain has garnered significant interest among experts all over the globe. the authors of this paper want to illustrate the tremendous potential and usefulness of block chain in fscs by reviewing and assessing the existing studies. this study provides a review of block chain's use in fscs and discusses adoption issues such as scalability, interoperability, and high cost. additionally, it offers several possible fixes for these issues. a bibliometric analysis is also provided to help scholars and practitioners understand the framework and current direction of this field's study. the study found that the majority of the research has focused on using block chain in finance, logistics, and product authenticity. additionally, block chain is ready to take center stage as a technology for enhancing fsc traceability and transparency, reducing risk, and - most importantly - increasing trust between multiple stakeholders. © 2022 ieee."
"the quasi-monte carlo methods use specially designed deterministic sequences with improved uniformity properties compared with random numbers, in order to achieve higher rates of convergence. usually certain measures like the discrepancy are used in order to quantify these uniformity properties. the usefulness of certain families of sequences with low discrepancy, like the sobol and halton sequences, has been established in problems with high practical value as in mathematical finance. multiple studies have been done about applying these sequences also in the domains of optimisation and machine learning. currently many types of neural networks are used extensively to achieve break-through results in machine learning and artificial intelligence. the process of training these networks requires substantial computational resources, usually provided by using powerful gpus or specially designed hardware. in this work we study different approaches to employ efficiently low-discrepancy sequences at various places in the training process where their uniformity properties can speed-up or improve the training process. we demonstrate the advantage of using sobol low-discrepancy sequences in benchmark problems and we discuss various practical issues that arise in order to achieve acceptable performance in real-life problems. © 2022, springer nature switzerland ag."
"the work analyzes the possibilities of advanced monte carlo methods for multidimensional integrals related to evaluation of european style options. some basic definition for finance options are given. the theory for construction lattice rules with different generating vectors is presented. the paper presents a computational study on optimized stochastic technique based on lattice rules with an optimal generating vectors. a full monte carlo algorithm based on fast construction method for the generating vector is analyzed. © 2022, the author(s), under exclusive license to springer nature switzerland ag."
"text embedding is an essential component to build efficient natural language applications based on text similarities such as search engines and chatbots. certain industries like finance and healthcare demand strict privacy-preserving conditions that user's data should not be exposed to any potential malicious users even including service providers. from a privacy standpoint, text embeddings seem impossible to be interpreted but there is still a privacy risk that they can be recovered to original texts through inversion attacks. to satisfy such privacy requirements, in this paper, we study a homomorphic encryption (he) based text similarity inference. to validate our method, we perform extensive experiments on two vital text similarity tasks. through text embedding inversion tests, we prove that the benchmark datasets are vulnerable to inversion attacks and another privacy preserving approach, d?-privacy, a relaxed version of local differential privacy method fails to prevent them. we show that our approach preserves the performance of models compared to that the baseline has degradation up to 10% of scores for the minimum security.  ©2022 association for computational linguistics."
"artificial intelligence represents a powerful computational tool to analyze large amounts of data with and without supervision. such powerful techniques have significantly impacted the advancement of multiple sectors from social media to data security, the automotive industry, gaming, finances, and healthcare. only recently, however, artificial intelligence has been making significant strides in healthcare and biomedical research. despite such advancements and the expectation of potential breakthroughs arising from implementing artificial intelligence in these fields, there is limited knowledge transfer and training for most healthcare professionals on using these computational techniques. while there is a wide array of publications on artificial intelligence applied to scientific research, most are too technical (not aimed at the non-initiated), too broad (with an overwhelming amount of data), or too focused on a particular application. this chapter presents an overview of artificial intelligence and its derivatives, giving a historical perspective, a succinct technical explanation of the underlying basis, and some examples of its applications. it finishes with a brief discussion on the challenges for implementing ai to be fully accepted in the scientific community. © 2022, the author(s), under exclusive license to springer nature switzerland ag."
"when the internet of things (iot) is used in a typical manufacturing system, the industrial plant can be controlled remotely through the internet. this enables manufacturing and execution systems to obtain real-time work orders directly from the enterprise resource planning (erp) system. therefore, workflows for development, production, and manufacturing can be integrated with sales, market, and finance business processes. the possibility of implementing this integration, however, is dependent on the trust, security, and authentication of iot devices. many iot devices face significant security risks such as device hijacking and data leaks due to limited resources and inadequate self-protection capabilities. despite the fact that several studies have been conducted using the physical unclonable function to protect communication between iot devices from the aforementioned security threats, current solutions rely on the participation of the server to distribute the key parameters, which requires high message overhead and has a significant impact on efficiency. to fill this gap, this article proposes a consistent round hash optimized srp-6a-based end-to-end mutual authentication for secure data transfer technique with single-share trusted device collaboration can detect an unauthenticated device. in addition, our proposed technique ensures the overall system's integrity and stability during a scaling-out phenomenon, which is becoming increasingly common in complex industrial environments. furthermore, we present a formal and informal security analysis of the proposed protocol. according to the results of the performance analysis, our proposed technique has the lowest communication overhead, computational cost, and round-trip time when compared to other state-of-the-art schemes. © 2022 the author(s)"
"gambling disorder is a behavioral addiction that negatively impacts personal finances, work, relationships and mental health. in this pre-registered study (https://osf.io/5ptz9/) we investigated the impact of real-life gambling environments on two computational markers of addiction, temporal discounting and model-based reinforcement learning. gambling disorder is associated with increased temporal discounting and reduced model-based learning. regular gamblers (n = 30, dsm-5 score range 3–9) performed both tasks in a neutral (café) and a gambling-related environment (slot-machine venue) in counterbalanced order. data were modeled using drift diffusion models for temporal discounting and reinforcement learning via hierarchical bayesian estimation. replicating previous findings, gamblers discounted rewards more steeply in the gambling-related context. this effect was positively correlated with gambling related cognitive distortions (pre-registered analysis). in contrast to our pre-registered hypothesis, model-based reinforcement learning was improved in the gambling context. here we show that temporal discounting and model-based reinforcement learning are modulated in opposite ways by real-life gambling cue exposure. results challenge aspects of habit theories of addiction, and reveal that laboratory-based computational markers of psychopathology are under substantial contextual control. © 2022 the author(s)."
"with the increasing amount of available data and advances in computing capabilities, deep neural networks (dnns) have been successfully employed to solve challenging tasks in various areas, including healthcare, climate, and finance. nevertheless, state-of-the-art dnns are susceptible to quasi-imperceptible perturbed versions of the original images - adversarial examples. these perturbations of the network input can lead to disastrous implications in critical areas where wrong decisions can directly affect human lives. adversarial training is the most efficient solution to defend the network against these malicious attacks. however, adversarial trained networks generally come with lower clean accuracy and higher computational complexity. this work proposes a data selection (ds) strategy to be applied in the mini-batch training. based on the cross-entropy loss, the most relevant samples in the batch are selected to update the model parameters in the backpropagation. the simulation results show that a good compromise can be obtained regarding robustness and standard accuracy, whereas the computational complexity of the backpropagation pass is reduced. © 2022 european signal processing conference, eusipco. all rights reserved."
"artificial intelligence (ai) is broadly defined as computers programmed to simulate the cognitive functions of the human mind. in combination with the neural network (nn), big data (bd), and the internet of things (iot), artificial intelligence has transformed everyday life: self-driving cars, delivery drones, digital assistants, facial recognition devices, autonomous vacuum cleaners, and mobile navigation apps all rely on ai to perform tasks. with the rise of artificial intelligence, the job market of the near future will be radically different???many jobs will disappear, yet new jobs and opportunities will emerge. understanding artificial intelligence: fundamentals and applications covers the fundamental concepts and key technologies of ai while exploring its impact on the future of work. requiring no previous background in artificial intelligence, this easy-to-understand textbook addresses ai challenges in healthcare, finance, retail, manufacturing, agriculture, government, and smart city development. each chapter includes simple computer laboratories to teach students how to develop artificial intelligence applications and integrate software and hardware for robotic development. in addition, this text: •focuses on artificial intelligence applications in different industries and sectors •traces the history of neural networks and explains popular neural network architectures •covers ai technologies, such as machine vision (mv), natural language processing (nlp), and unmanned aerial vehicles (uav) •describes various artificial intelligence computational platforms, including google tensor processing unit (tpu) and kneron neural processing unit (npu) •highlights the development of new artificial intelligence hardware and architectures understanding artificial intelligence: fundamentals and applications is an excellent textbook for undergraduates in business, humanities, the arts, science, healthcare, engineering, and many other disciplines. it is also an invaluable guide for working professionals wanting to learn about the ways ai is changing their particular field. © 2022 by the institute of electrical and electronics engineers, inc. all rights reserved."
"with the rapid development of computer hardware and software and the rapid spread of network, the amount of data in scientific computing has exploded. big data visualization has become one of the important research contents in scientific computing. against the background of rapid development of information technology and digital technology, the scale of digital economy is increasing day by day, and china's digital economy has ushered in unprecedented progress, and it has become an important force in leading economic development. the development of digital economy promotes industrial upgrading and transformation and facilitates industrial integration, and many traditional industries have given rise to a series of new industries. scientific computational visualization is a new research discipline with a wide range of applications, including energy surveying, natural sciences, finance, and business. however, as a brand-new discipline, further research is needed in areas such as the application of scientific computational visualization techniques based on the digital economy environment. in this study, we propose a scientific computing visualization approach based on a big data framework for industry convergence analysis in the context of digital economy, which consists of a cloud computing platform for industry convergence in the context of digital economy by integrating neural networks in the platform, and scientific computing visualization by built-in gpu hardware. traditional research methods for scientific computing visualization are discussed, and algorithms and techniques are classified in terms of the types of data domains studied. finally, the further development and research directions of scientific computing visualization in the big data environment are given in conjunction with the main theoretical and application results achieved by our group in scientific computing for the field of big data visualization. scientific computing visualization will not only promote the development and application of cutting-edge information technology, like cloud computing, big data, cyber security, artificial intelligence, computational science, system theory, and so on, but also bring a unique and disruptive development and change to the industrial integration in the context of digital economy.  © 2022 shiyuan zhou et al."
"the forecasting problems in computational finance involve modelling the vagueness and imprecision inherent to the financial markets. fuzzy set theory has a unique ability to quantitatively and qualitatively model and analyze such problems. volatility forecasting plays an important role in financial risk management and in option pricing. recently, there has been a growing interest in data-driven volatility models and neurovolatility models for risk forecasting of stocks and index funds. however, even these state-of-the-art models do not take into account the fuzzy volatility in their risk forecasts.cryptocurrencies are a novel financial asset class based on the blockchain technology. cryptocurrencies have gained popularity among retail investors as a financial asset with high risks and high returns. the extremely volatile nature of cryptocurrencies (compared to traditional assets) makes forecasting their volatility more challenging. a simple algorithmic trading approach, simple moving average (sma) crossover strategy, is used to calculate the algo returns. this paper provides fuzzy forecasts of the volatility of algo returns using the data-driven exponentially weighted moving average (dd-ewma) and neuro models for six major cryptocurrencies. we also compute and compare fuzzy volatility forecasts of four major tech stocks and chicago board options exchange's (cboe) volatility index (vix) using dd-ewma and neuro models. our experimental results show that the data-driven models produce better forecasts for cryptocurrencies as compared to the neuro models, while for the regular stocks and indexes, no such definitive conclusion could be drawn.  © 2022 ieee."
"stock market value prediction is the activity of predicting future market values so as to increase gain and profit. it aids in forming important financial decisions which help make smart and informed investments. the challenges in stock market predictions come due to the high volatility of the market due to current and past performances. the slightest variation in current news, trend or performance will impact the market drastically. existing models fall short in computation cost and time, thereby making them less reliable for large datasets on a real-time basis. studies have shown that a hybrid model performs better than a stand-alone model. ensemble models tend to give improved results in terms of accuracy and computational efficiency. this study is focused on creating a better yielding model in terms of stock market value prediction using technical analysis, and it is done by creating an ensemble of long short-term memory (lstm) model. it analyzes the results of individual lstm models in predicting stock prices and creates an ensemble model in an effort to improve the overall performance of the prediction. the proposed model is evaluated on real-world data of 4 companies from yahoo finance. the study has shown that the ensemble has performed better than the stacked lstm model by the following percentages: 21.86% for the tesla dataset, 22.87% for the amazon dataset, 4.09% for nifty bank and 20.94% for the tata dataset. the model’s implementation has been justified by the above results. © 2022, the author(s), under exclusive license to springer nature singapore pte ltd."
"modelling of large scale data series is of significant importance in fields such as astrophysics and finance. the continuous increase in available data requires new computational approaches such as the use of multicore processors and accelerators. recently, a novel time series modelling and forecasting method was proposed, based on a recursively updated pseudoinverse matrix which enhances parsimony by enabling assessment of basis functions, before inclusion into the final model. herewith, a novel gpu (graphics processing unit) accelerated matrix based auto-regressive variant is presented, which utilizes lagged versions of a time series and interactions between them to form a model. the original approach is reviewed and a matrix multiplication based variant is proposed. the gpu accelerated and hybrid parallel versions are introduced, utilizing single and mixed precision arithmetic to increase gpu performance. discussions around performance improvement and high order interactions are given. a block processing approach is also introduced to reduce memory requirements for the accelerator. furthermore, the inclusion of constraints in the computation of weights, corresponding to the basis functions, with respect to the parallel implementation are discussed. the approach is assessed in a series of model problems and discussions are provided. © 2022, the author(s), under exclusive license to springer nature switzerland ag."
"this paper presents a deep learning (dl) model for natural language processing of unstructured cvs to generate a six-dimensional profile of the professional experience of the spanish companies' board of directors. we show the complete process starting with open data extraction and cleaning, the generation of a labeled dataset for supervised learning, the development, training and validation of a dl model capable of accurately analyzing the dataset, and, finally, a data analysis work based on the automated generation of the professional profiles of more than 6,000 directors of spanish listed companies between 2003 and 2020. an rnn-lstm neural network has been trained in three phases starting from a random initial state, (1) learning of basic structures of the spanish language, (2) fine tuning for scientific texts in the field of economics and finance, and (3) regression modeling to generate a six-dimensional profile based on a generalization of sentiment classification systems. the complete training has been carried out with very low computational requirements, having a total duration of 120 hours of processing in a low-end gpu. the results obtained in the validation of the dl model show great accuracy, obtaining a value for the standard deviation of the mean error between 0.015 and 0.033. as a result, we have been able to outline with a high degree of reliability the profile of the listed spanish companies' board of directors. we found that the predominant profile is that of directors with experience in executive or consultancy positions, followed by the financial profile. the results achieved show the potential of dl in social science research, particularly in finance. © 2022, universidad internacional de la rioja. all rights reserved."
"the proceedings contain 21 papers. the special focus in this conference is on computational optimization. the topics include: application of an interval-valued intuitionistic fuzzy decision-making method in outsourcing using a software program; combinatorial etudes and number theory; multicriteria optimization of an algorithm for charging energy storage elements; optimized nano grid approach for small critical loads; optimized monte carlo methods for sensitivity analysis for large-scale air pollution model; on a full monte carlo approach to computational finance; advanced monte carlo methods to neural networks; an efficient adaptive monte carlo approach for multidimensional quantum mechanics; an overview of lattice and adaptive approaches for multidimensional integrals; optimal seating assignment in the covid-19 era via quantum computing; advanced biased stochastic approach for solving fredholm integral equations; improving performance of low-cost sensors using machine learning calibration with a 2-step model; hybrid ant colony optimization algorithms—behaviour investigation based on intuitionistic fuzzy logic; scheduling algorithms for single machine problem with release and delivery times; key performance indicators to improve e-mail service quality through itil framework; contemporary bioprocesses control algorithms for educational purposes; monitoring a fleet of autonomous vehicles through a* like algorithms and reinforcement learning; rather “good in, good out” than “garbage in, garbage out”: a comparison of various discrete subsampling algorithms using covid-19 data without a response variable; using temporal dummy players in cost-sharing games."
"data-driven volatility models and neuro-volatility models have the potential to revolutionize the area of computational finance. volatility measures the variation of a time series data, and thus it is also a driving factor for the risk forecasting of returns from investment in cryptocurrencies. a cryptocurrency is a decentralized medium of exchange that relies on cryptographic primitives to facilitate the trustless transfer of value between different parties. instead of being physical money, cryptocurrency payments exist purely as digital entries on an online ledger called blockchain that describe specific transactions.many commonly used risk forecasting models do not take into account the uncertainty associated with the volatility of an underlying asset to obtain the risk forecasts. some tools from the fuzzy set theory can be incorporated into the forecasting models to account for this uncertainty. interest in the use of hybrid models for fuzzy volatility forecasts is growing. however, a major drawback is that the fuzzy coefficient hybrid models used in fuzzy volatility forecasts are not data-driven. this paper uses fuzzy set theory with data-driven volatility and data-driven neuro-volatility forecasts to study the fuzzy risk forecasts. the study focuses on long-term volatility forecasts with daily price data while briefly exploring forecasting models with high-frequency (hourly) data as an avenue for future research. simple yet effective models incorporating fuzziness to obtain fuzzy risk volatility forecasts and fuzzy var forecasts are presented. the key underlying idea, unlike the existing risk forecasting, is the use of a hybrid nonlinear adaptive fuzzy model for volatility.  © 2022 ieee."
"financial time series prediction and trading decision-making are priorities of computational intelligence for researchers in academia and the finance industry due to their broad application areas and substantial impact. however, these methods remain challenging because they retain various complex statistical properties, and the mechanism behind the processes is unknown to a large extent. a significant number of machine learning-based methods are proposed and demonstrate impressive results, especially deep learning-based models. nevertheless, due to the high complexity of massive, nonlinear, and nonindependent data and the difficulties and time consumption of complicated training models of deep learning, the performance of online trading decisions is still inadequate for practical application. this paper proposes the integrated framework of forecasting based online trading strategy (iff-bots) to satisfy better prediction performance and dynamic decisions for real-world online trading systems. our method adopts a novel isomorphic convolutional neural network (cnn)-based forecaster-classifier-executor architecture to exploit cnn-based price and trend integrated prediction and direct-reinforcement-learning-based trading decision-making. iff-bots can also achieve better real-time performance for online trading. we empirically compare the proposed approach with state-of-the-art prediction and trading methods on real-world s&p and dji datasets. the results show that the iff-bots outperforms its competitors in predicting metrics, trading profits, and real-time performance.  © 2022 - ios press. all rights reserved."
"agent-based models are computational approaches used to reproduce the interactions between economic agents. these models are widely applied in many contexts to get deeper understanding about agents' behaviors within complex systems. in this paper, we provide a bibliometric analysis about agent-based models in finance and, considering bibliographic coupling, we identify the presence of two distinct clusters of research communities, i.e., financial economics and econophysics. cluster-specific thematic analyses are conducted to understand if the two communities are characterized by different emerging and motor topics. by highlighting several differences in the clusters, we also show the two research communities specialized in different specific topics.  © 2022 juan e. trinidad segovia et al."
the proceedings contain 10 papers. the topics discussed include: computational modelling of stochastic processes for learning research; a petri net-based simulation of synchronized curriculum for it-specialists; ontology-based learning environment model of scientific studies; using a business simulator with elements of machine learning to develop personal finance management skills; modeling of the internal certification system of educational resources view or update; strategic learning towards equilibrium. exploratory analysis and models; some ways of increasing the efficiency of teaching data structures; and using augmented reality for early literacy.
"with the recent advance in large pre-trained language models, researchers have achieved record performances in nlp tasks that mostly focus on language pattern matching. the community is experiencing the shift of the challenge from how to model language to the imitation of complex reasoning abilities like human beings. in this work, we investigate the application domain of finance that involves real-world, complex numerical reasoning. we propose a new large-scale dataset, convfinqa, aiming to study the chain of numerical reasoning in conversational question answering. our dataset poses great challenge in modeling long-range, complex numerical reasoning paths in real-world conversations. we conduct comprehensive experiments and analyses with both the neural symbolic methods and the prompting-based methods, to provide insights into the reasoning mechanisms of these two divisions. we believe our new dataset should serve as a valuable resource to push forward the exploration of real-world, complex reasoning tasks as the next research focus. our dataset and code is publicly available. © 2022 association for computational linguistics."
"one of the research aims of numerical linear algebra is to look for an approximate solution to mathematical problems in a continuous version. these mathematical problems arise in engineering and natural science. numerical linear algebra as a fundamental computational science tool is frequently used in image and signal processing, data mining, computational finance, bioinformatics, telecommunication, fluid dynamics, and material science simulation. matrices, one of the fundamental concepts in numerical linear algebra, are ubiquitous in natural science and engineering fields. in this paper, we first take an investigation of essential concepts for matrices. the definition of the production of a matrix and a vector and the definition of the production of two matrices are introduced. the fundamental invariant properties such as range, null space, and rank of a matrix are discussed. we also present several results related to orthogonal properties. several results related to different norms on matrices are presented. at last, an impact on householder transformation is presented.  © copyright spie."
"sentiment analysis, also called opinion mining, is a field that studies the sentiment orientation of the evaluation subjects based on web data. due to its significance in such fields as public opinion monitoring, marketing and finance, sentiment analysis has received increasing attention in recent years. the present study focuses on the problem of latent meaning faced by sentiment analysis, builds a chinese construction corpus and studies the relations between chinese constructions and sentiment analysis by quantifying the chinese constructions within the corpus. we annotate the constructions and words expressing attitudinal and gradational meaning in the corpus. a qualitative analysis is conducted on these constructions and words. statistics of attitudinal and gradational constructions are computed according to such standards as construction type, semantic type, constant number and variable number. by comparing the statistics of the constructions and words in the corpus, we find that most of the gradational and attitudinal semantic information is loaded by words rather constructions. in spite of this, there still is a certain portion of attitudinal and gradational information expressed by constructions. the present study provides empirical data and a research method to the study of applying construction grammar to chinese sentiment analysis. chinese construction studies are also be supported by these data computed from chinese authentic texts. the present study also discusses the difficulities of applying construction grammar to chinese sentiment analysis and natural language processing and looks forward to future studies. © 2022 china national conference on computational linguistics published under creative commons attribution 4.0 international license."
"blockchain technology has become among the most influential disruptive technologies in current times. its central role in the decentralization of various real-world systems, e.g. finance/economy, energy, healthcare, operations management, computing as well as data infrastructure is indisputable. the objective of this work is to provide a concise and recent review of the implementation of optimization and deep learning techniques on blockchain-based systems. the blockchain-based application areas considered in this review are as follows: computational optimization frameworks, internet of things, smart grids, supply chain management, and healthcare data systems. the implementation of optimization and deep learning techniques on these blockchain-based real-world systems are presented and discussed. © 2022 john wiley and sons, inc."
"stock movements are influenced not only by historical prices, but also by information outside the market such as social media and news about the stock or related stock. in practice, news or prices of a stock in one day are normally impacted by different days with different weights, and they can influence each other. in terms of this issue, in this paper, we propose a fundamental analysis based neural network for stock movement prediction. first, we propose three new technical indicators based on raw prices according to the finance theory as the basic encode of the prices of each day. then, we introduce a coattention mechanism to capture the sufficient context information between text and prices across every day within a time window. based on the mutual promotion and influence of text and price at different times, we obtain more sufficient stock representation. we perform extensive experiments on the real-world stocknet dataset and the experimental results demonstrate the effectiveness of our method. © 2022 china national conference on computational linguistics published under creative commons attribution 4.0 international license."
"machine learning is an evolving branch of computational algorithms that are designed to emulate organic intelligence by learning from the surrounding environment. deep learning is a subcategory of machine learning that allows computers to learn directly from the raw data without the need for human-engineered features. these algorithms are becoming the workhorse in the new era of big data. techniques based on machine and deep learning have been applied successfully in diverse fields ranging from pattern recognition, computer vision, spacecraft engineering, finance, entertainment, and computational biology to biomedical and medical applications. the rapid increases in patient-specific information and improved computing power have motivated the deployment of machine/deep learning algorithms in a wide range of diagnostic and therapeutic radiological applications to automate laborious processes, improve workflow, and aid physicians in their pursuit to realize precision medicine. this includes but is not limited to applications in computer-aided detection, classification, and diagnosis in radiology and auto-contouring, treatment planning, response modeling (radiomics, radiogenomics), image-guidance, motion tracking, and quality assurance in radiation oncology. the ability of machine/deep learning algorithms to learn from current context and generalize to future tasks allows improvements in both the safety and efficacy of radiology and oncology practices, leading to improved efficiency and better quality of care. © springer nature switzerland ag 2022."
"pre-trained language models (plms) have dramatically improved performance for many natural language processing (nlp) tasks in domains such as finance and healthcare. however, the application of plms in the domain of commerce, especially marketing and advertising, remains less studied. in this work, we adapt pretraining methods to the domain of commerce, by proposing culg, a large-scale commercial universal language generation model which is pre-trained on a corpus drawn from 10 markets across 7 languages. we propose 4 commercial generation tasks and a two-stage training strategy for pre-training, and demonstrate that the proposed strategy yields performance improvements on three generation tasks as compared to single-stage pre-training. extensive experiments show that our model outperforms other models by a large margin on commercial generation tasks. © 2022 association for computational linguistics."
"we consider the deep learning based scheme proposed in [w. e and j. han and a. jentzen, commun. math. stat., 5 (2017), pp. 349–380] and study the effect of the number of neural networks on the gradient of the solution. we demonstrate that using one neural network improves its numerical stability for the whole path and also reduces the computational time. this is illustrated with several 100-dimensional nonlinear backward stochastic differential equations including nonlinear pricing problems in finance. © 2022, the author(s), under exclusive license to springer nature switzerland ag."
"pre-trained language models have achieved promising performance on general benchmarks, but underperform when migrated to a specific domain. recent works perform pre-training from scratch or continual pre-training on domain corpora. however, in many specific domains, the limited corpus can hardly support obtaining precise representations. to address this issue, we propose a novel transformer-based language model named varmae for domain-adaptive language understanding. under the masked autoencoding objective, we design a context uncertainty learning module to encode the token's context into a smooth latent distribution. the module can produce diverse and well-formed contextual representations. experiments on science- and finance-domain nlu tasks demonstrate that varmae can be efficiently adapted to new domains with limited resources. © 2022 association for computational linguistics."
"the paper discusses the performance of different machine learning models and a deep learning model in forecasting annual gross domestic product (gdp) per capita (ppp) data of 33 oecd countries using past year variables. it focuses on creating a universal forecasting model. for the analysis, the paper uses cross-country panel data consisting of 262 time-series variables with annual periodicity, including various growth, development, health, energy, finance, and social indicators and their lag terms for five years. the paper shows that the artificial deep neural network performed the best among the considered machine learning and deep learning models, followed by gradient boosted regressor, whereas ridge regressor performed the worst. this paper gives insight into the application of machine learning and deep learning in forecasting gdp per capita. it shows how the further improvement of these computational methods and data availability would improve the forecast accuracy and precision. © 2022 ieee."
"most commercial conversational ai products in domains spanning e-commerce, health care, finance, and education involve a hierarchy of nlp models that perform a variety of tasks such as classification, entity recognition, question-answering, sentiment detection, semantic text similarity, and so on. despite our understanding of each of the constituent models, we often do not have a clear view as to how these models affect the overall platform metrics. to bridge this gap, we define a metric known as answerability, which penalizes not only irrelevant or incorrect chatbot responses but also unhelpful responses that do not serve the chatbot's purpose despite being correct or relevant. additionally, we describe a formula-based mathematical framework to relate individual model metrics to the answerability metric. we also describe a modeling approach for predicting a chatbot's answerability to a user question and its corresponding chatbot response. © 2022 association for computational linguistics."
"in this paper, we use partial differential equations combined with artificial intelligence methods to conduct an in-depth study and analysis of the curriculum system of accounting and finance professional education and training. under the condition of high-performance advanced computers, neural network technology has been developed rapidly and started to adapt to the requirements of computational accuracy and speed in the field of pdes. neural networks have good self-learning as well as self-adaptive capabilities, and the use of deep neural networks to solve the numerical solution of partial differential equations has become a popular application that has emerged recently. the system provides a variety of interfaces for the output of calculation results, which can be directly or indirectly read into the calculation results by various common drawing software programs. in addition, the system can also directly plot the results using its plotting plug-in. this paper presents a case study to illustrate the overview of the practical work of the group before the application of accounting ai, i.e., the human manual model, reveals the problems faced by the accounting practice in this model and then discusses the specific application process and the effect it brings, emphasizing the superiority of accounting ai. intra-group heterogeneity is to fully consider the learning ability, interest, personality, gender, and other factors of individual students when grouping, to ensure the differences between the members of the group. in terms of accounting personnel and accounting credit system, which are indirectly affected by ai in accounting, this paper discusses the overview before the application of ai in accounting and the impact and challenges after the application and then comes up with countermeasures for the development of accounting. through the study of specific enterprises to elaborate the current situation of the application of accounting ai in enterprises and its impact, it exposes that the application of accounting ai has a great impact on the accounting process, accounting information quality, accounting personnel, accounting information security, and other aspects of enterprises and this paper proposes solutions in four aspects: first, machine learning for adversarial ai needs to be promoted; second, ai-related laws need to be improved; third, management accounting talent transformation needs to be realized; and fourth, the integration of artificial intelligence and accounting needs to be deepened.  © 2022 jingnan hu."
"cryptocurrencies have gained enormous momentum in finance and are nowadays commonly adopted as a medium of exchange for online payments. after recent events during which gamestop's stocks were believed to be influenced by wallstreetbets subreddit, reddit has become a very hot topic on the cryptocurrency market. the influence of public opinions on cryptocurrency price trends has inspired researchers on exploring solutions that integrate such information in crypto price change forecasting. a popular integration technique regards representing social media opinions via sentiment features. however, this research direction is still in its infancy, where a limited number of publicly available datasets with sentiment annotations exists. we propose a novel bitcoin reddit sentiment dataset, a ready-to-use dataset annotated with state-ofthe-art sentiment and emotion recognition. the dataset contains pre-processed reddit posts and comments about bitcoin from several domainrelated subreddits along with bitcoin's financial data. we evaluate several widely adopted neural architectures for crypto price change forecasting. our results show controversial benefits of sentiment and emotion features advocating for more sophisticated social media integration techniques. we make our dataset publicly available for research.  ©2022 association for computational linguistics."
"online marketing refers to the practices of promoting a company's brand to its potential customers. it helps the companies to find new venues and trade worldwide. numerous online media such as facebook, youtube, twitter, and instagram are available for marketing to promote and sell a company's product. however, in this study, we use instagram as a marketing medium to see its impact on sales. to carry out the computational process, the approach of linear regression modeling is adopted. certain statistical tests are implemented to check the significance of instagram as a marketing tool. furthermore, a new statistical model, namely a new generalized inverse weibull distribution, is introduced. this model is obtained using the inverse weibull model with the new generalized family approach. certain mathematical properties of the new generalized inverse weibull model such as moments, order statistics, and incomplete moments are derived. a complete mathematical treatment of the heavy-tailed characteristics of the new generalized inverse weibull distribution is also provided. different estimation methods are discussed to obtain the estimators of the new model. finally, the applicability of the new generalized inverse weibull model is established via analyzing instagram advertising data. the comparison of the new distribution is made with two other models. based on seven analytical tools, it is observed that the new distribution is a better model to deal with data in the business, finance, and management sectors.  © 2022 mi yantian et al."
"the novel covid-19, initially found in wuhan (china), reached quickly around the globe and turned into a worldwide pandemic situation. it has set off a significant impact on daily life, general well-being, and global finance. it is crucial to diagnose predisposed patients rapidly. there are no exact tests for covid-19 except rt-pcr which is costly and needs a huge time. recent research acquired applying radiology imaging approaches recommend that such images comprise features about the covid-19 infection. the use of machine learning techniques combined with chest imaging can be helpful in the precise recognition of this infection, and can likewise be assistive to beat the issue of an absence of specific doctors. this investigation developed a model for automatic identification of covid-19 infection utilizing chest ct images. a convolutional neural network has been applied to extract the features from the chest ct images and principle component analysis has been applied for feature selection to reduce computational effort. the proposed model (the ensemble of ml classifiers) has been developed to provide accurate diagnostics by considering the five classes (normal, mycoplasma pneumonia, bacterial pneumonia, viral pneumonia, and covid-19). the proposed model reached an accuracy of 99.3%, precision of 99.3%, and recall of 99.2%. this can help clinicians invalidate their primary checkups and can be utilized promptly to check the patients' infection rate.  © 2022 ieee."
"in this paper we review the large and growing literature on continuous-time multivariate non-gaussian models based on lévy processes applied to finance and proposed in the literature in the last years. we explain the empirical motivation and the idea behind each approach. then, we study the models focusing on the parsimony of the number of parameters, the properties of the dependence structure, and the computational tractability. for each parametric class we analyze the main features, we provide the characteristic function, the marginal moments up to order four, the covariances and the correlations. furthermore, we survey the methods proposed in literature to calibrate these models on the time-series of log-returns, with a view toward practical applications and possible numerical issues. finally, to empirically assess the differences between models, we conduct an analysis on a five-dimensional series of stock index log-returns. © 2022, the author(s), under exclusive licence to springer science+business media, llc, part of springer nature."
"in this work we study advanced stochastic methods for solving a specific multidimensional problems related to computation of european style options in computational finance. recently, stochastic methods have become a very important tool for high-performance computing of very high-dimensional problems in computational finance. here, a different kind of optimal generating vectors have been applied for the first time to a specific problem in computational finance. numerical tests show that they give superior results to the stochastic approaches used up to now. the advantages and disadvantages of various highly efficient stochastic approaches for multidimensional integrals related to evaluation of european style options have been analyzed. © 2022, springer nature switzerland ag."
"this chapter reviews the growing literature that describes machine learning applications in the field of asset pricing. in doing so, it focuses on the additional benefits that machine learning – in addition to, or in combination with, standard econometric approaches – can bring to the table. this issue is of particular importance because in recent years, improved data availability and increased computational facilities have had huge effects on finance literature. for example, machine learning techniques inform analyses of conditional factor models; they have been applied to identify the stochastic discount factor and purposefully to test and evaluate existing asset pricing models. beyond those pertinent applications, machine learning techniques also lend themselves to prediction problems in the domain of empirical asset pricing. © 2022, the author(s), under exclusive license to springer nature switzerland ag."
"over the last few years, computational power and intelligence are becoming more and more necessary in the sector of finance. more specifically, computational finance turns into a very popular topic for both academia and industry, where numerous published works from this field and especially investment and risk management, showcase the effects of these technological advancements. at the same time, the ever-increased computational demands have led to the deployment of various accelerators in order to meet both latency and power constraints for financial applications that vary from special purpose, made by economists, to general purpose digital signal processing (dsp) applied in financial time-series. one of the most widely used applications, belonging to the 2nd category, is the savitzky-golay algorithm, a filter used for smoothing time-series data. in this work, we propose a mechanism that automatically creates different accelerated savitzky-golay filters for gpus and fpgas, based on a set of pre-accelerated templates. by evaluating the provided templates with a set of real use-case parameters, a speedup of x33.5 on the nvidia t4 gpu and x21.9 on the alveo u50 fpga is achieved compared with an intel xeon gold 5218r cpu as a baseline, while achieving a decrease in power consumption of 89% and 70% respectively, disclosing a real latency-power trade-of between both accelerators.  © 2022 ieee."
"this paper presents some models of exchange rate with jumps, namely jump diffusion exchange rate models. jump diffusion models are quite common in computational and theoretical finance. it is known that exchange rates sometimes exhibit jumps during some time periods. therefore, it is important to take into account the presence of these jumps in exchange rate modeling in general. however, even the simplest jump diffusion model introduces some analytical difficulty in terms of finding a solution to the model. the models we analyze in this paper make use of approximation theory in order to come up with closed form solutions to the underlying variables. this approach leads to the branch of differential equations called functional differential equations and more specifically the so-called delay differential equations. our approach leads to a second order delay differential equation. though, in principle, these types of functional differential equations can be solved analytically in some cases, the task, in general, is quite enormous. we circumvent this technical difficulty by deriving an approximate solution using a power series expansion of the second order. therefore, we derive a complete solution to the models and also investigate the model’s predictions of the exchange rate. we introduce two jump diffusion models. the first model examines the case where there are jumps with a constant magnitude. the second model considers the case of jumps of different sizes. these are relatively simpler cases to be analyzed. we will present some computational aspects in terms of the difficulty often encountered in estimating these types of models. the difficulty increases for the type of exchange rate models being considered in this paper. taking advantage of the specification of the models we have estimated the parameters using a two-step m-estimation strategy that combines full information maximum likelihood estimation in the first step and the simulated method of moments in the second step. © 2022 the author(s). this open access article is distributed under a creative commons attribution (cc-by) 4.0 license."
"economic crises occur when the economy experiences drastic fluctuations, which in turn cause heavy losses to society and the economy. economic crises happen when there are significant losses to society and the economy as a result of the economy experiencing abrupt changes or deep recessions. the term ""economic cycle early warning mechanism""refers to a collection of theories and techniques for keeping track of, assessing, forecasting, and choosing policies regarding finances in light of the particular economic phenomenon known as economic cycle fluctuations. the traditional early warning mechanism has been unable to keep up with the demands of economic cycle early warning due to the growth of economic globalisation, and the phenomenon of incomplete early warning and poor early warning accuracy frequently occurs. a genetic algorithm simulates the natural selection and genetic mechanisms of darwin's theory of biological evolution. it is a computational model of the biological evolution process. it is a technique for looking for the best answer by simulating the course of natural evolution. genetic algorithms have advanced incredibly quickly in recent years. it has been widely applied in machine learning, neural networks, control system optimization, and the social sciences as an effective, useful, and reliable optimization technique. in order to optimize the early warning mechanism and increase the thoroughness and accuracy of early warning, this paper investigates the early warning mechanism of financial and economic cycles by combining genetic algorithms. a mathematical model of the economic cycle was built during the experiment and tested in accordance with the genetic algorithm's basic operating principle. the findings demonstrate that the genetic algorithm-based early warning system for the financial economic cycle is more complete and accurate, with a 6.4% increase in accuracy.  © 2022 lulu liu."
"the sum of lognormal variables has been a topic of interest in several fields of research such as engineering, biology and finance, among others. for example, in the field of telecommunications, the aggregate interference of radio frequency signals is modeled as a sum of lognormal variables. to date, there is no closed expression for the probability distribution function (pdf) of this sum. several authors have proposed approximations for this pdf, with which they calculate the mean and variance. however, each method has limitations in its range of parameters for mean, variance and number of random variables to be added. in other cases, long approximations as power series are used, which makes the analytical treatment impractical and reduces the computational performance of numerical operations. this paper shows an alternative method for calculating the mean and variance of the sum of lognormal random variables from a computational performance approach. our method has been evaluated extensively by monte carlo simulations. as a result, this method is computationally efficient and yields a low approximation error computation for a wide range of mean values, variances and number of random variables. © 2022. revista facultad de ingenieria. all rights reserved."
"for the past several decades, with the rapid development of internet technologies and online transaction platforms, financial institutions and investors are facing huge challenges from the global economic environment that financial markets are becoming more unpredictable and volatile than before, especially in the stock markets, commodity markets and cryptocurrency markets. interest and awareness of artificial intelligence and quantum finance are growing so fast that both academia and higher education are struggling to keep up with the accelerating demand of financial markets. quantum finance is a newly developed interdisciplinary program with the integration of quantum theory, computational finance, and even computer science, which requires students to have comprehensive knowledge reserves. meanwhile, it is extremely complicated for students to use a programming language to realize quantum finance calculations from scratch. to facilitate curricula teaching, and hands-on usage of quantum finance and ai, a quantum finance software development kit (qfsdk) is proposed based on the author's previous research on quantum finance theory and other ai research findings. the qfsdk was prepared in python programming language as the first step in introducing students to the concepts and applications of quantum finance. the qfsdk bridges the theoretical and practical chasm for learners by developing a quantum finance calculator library. it serves as an open-source template that encourages heavy contextual modification, and it supports any online platforms in the python programming language. © 2022 ieee."
"to facilitate conversational question answering (cqa) over hybrid contexts in finance, we present a new dataset, named pacific. compared with existing cqa datasets, pacific exhibits three key features: (i) proactivity, (ii) numerical reasoning, and (iii) hybrid context of tables and text. a new task is defined accordingly to study proactive conversational question answering (pcqa), which combines clarification question generation and cqa. in addition, we propose a novel method, namely unipcqa, to adapt a hybrid format of input and output content in pcqa into the seq2seq problem, including the reformulation of the numerical reasoning process as code generation. unipcqa performs multi-task learning over all sub-tasks in pcqa and incorporates a simple ensemble strategy to alleviate the error propagation issue in the multi-task learning by cross-validating top-k sampled seq2seq outputs. we benchmark the pacific dataset with extensive baselines and provide comprehensive evaluations on each sub-task of pcqa. © 2022 association for computational linguistics."
"the proceedings contain 60 papers. the special focus in this conference is on global economic revolutions. the topics include: islamic banking strategies in the world of fintech: success story of bahrain; computing financial performance of road freight transportation (trucking) industry in india using mathematical tool; computing causality between macro-economic indicators and indian financial markets; does interest rate parity hold good for inr-usd exchange rate? analysing via computational technique; the impact of fintech phenomenon on economic development: the case of bahrain; an empirical investigation of the influence of the pandemic on albanian internet banking service usage; influence of liquidity, solvency on banks’ profitability: the moderating role of net revenues; zakat banking: giving loans without interest; the need for revitalization of islamic social finance instruments in the covid-19 period in nigeria: the role of digitalization; behavioral and non-behavioral factors and the level of adapting and implementing fintech and e-banking in bahrain: suggested model; the usage of artificial intelligence in arab financial institutions; financial technology: literature review paper; the use of artificial intelligence in the field of electronic commerce; crowdsoured technology as a collabarative tool for environmental enforcement: a critical review of current applications; artificial intelligence in practice: implications for information systems research, case study uae companies; management information systems enhance corporate sustainability; residual networks for image clustering; students perceptions about teaching in smart bahraini universities; a review analysis investigating the efficacy of machine learning in intrusion detection; the role of interactivity in social commerce websites: a content analysis study."
"in this study, we consider a single objective fuzzy portfolio optimization with flexible goal and constraints, in which the sharpe ratio is chosen as the goal and the portfolio’s mean and variance are included in the constraints. although this problem has much significance in finance, it is difficult to solve because of the nonconvexity of the objective function. based on fuzzy theory and flexible optimization, the fuzzy portfolio problem is transformed to the crisp form which is proved to be a semistrictly quasiconvex programming problem for any decreasing membership functions. this property of the equivalent problem is the basis to solve the main problem efficiently by available convex programming algorithms. the computational experiments with the sp500 data set is reported to show the performance of the proposed model. © 2022, the author(s), under exclusive license to springer nature singapore pte ltd."
"at present, many companies have many problems such as high financial costs, low financial management capabilities, and redundant frameworks; at the same time, the sasac requires that the enterprise's financial strategy transfer from ""profit-driven""to ""value-driven"", finance separate from accounting to improve the operational efficiency of the company. under this background, more and more enterprise respond to the call of the sasac; in order to achieve the goals of corporate financial cost savings and financial management efficiency improved, we began to provide services through financial sharing. the research of information fusion theory involves many basic theories, which can be roughly divided into two large categories from the algorithmic point of view: probabilistic statistical method and artificial intelligence method. the main task of artificial intelligence is to realize the computer for some learning, thinking process, and wisdom formation of simulation, and an important goal of information integration is the human brain comprehensive processing ability simulation, so artificial intelligence method will have broad application prospects in the field of information fusion; the common methods have d-s evidence reasoning, fuzzy theory, neural network, genetic algorithm, rough set, and other information fusion methods. the purpose of this paper is to proceed from the internal financial situation of the enterprise, analyze data security issues in the operation of financial shared services, and find a breakthrough in solving problems. but, with constantly expanding of enterprise group financial sharing service scale, the urgent problem to be solved is how to ensure the financial sharing services provided by enterprises in the cloud computing environment. this paper combines financial sharing service theory and information security theory and provides reference for building financial sharing information security for similar enterprises. for some enterprise that have not established a financial shared service center yet, they can learn from the establishment of the financial sharing information security system in this paper and provide a reference for enterprise to avoid the same types of risks and problems. for enterprise that has established and has begun to practice a financial shared information security system, appropriate risk aversion measures combined with actual situation of the enterprise with four dimensions related to information security system optimization was formulated and described in this paper. in summary, in the background of cloud computing, financial sharing services have highly simplified operational applications, and data storage capabilities and computational analysis capabilities have been improved greatly. not only can it improve the quality of accounting information but also provide technical support for the financial sharing service center of the enterprise group, perform financial functions better, and enhance decision support and strategic driving force, with dual practical significance and theoretical significance.  © 2022 yanqing chen."
"recently there has been a growing interest in applying neural network modelling from natural language processing to financial time series prediction problems in computational finance. cryptocurrency price prediction is a challenging problem with non-stationary market price and volatility clustering. cryp-tocurrency data tends to be non-stationary, which means that predictive information extracted using deep learning techniques on observed data can not be used with future data. moreover, there is a very little signal in cryptocurrency data to indicate the future direction of the market. this paper proposes a sensible way to frame the prediction problem as a dynamic regression problem by defining the features in the feedforward neural networks and the target as an appropriate average of the historical data. the novelty of this paper is to use deep learning algorithms and statistical bootstrapping to obtain cryptocurrency price prediction and the corresponding prediction intervals. it is shown that neural networks are capable of modelling nonlinearity directly for nonlinear time series models. the proposed hybrid approach is evaluated using simulated and cryptocurrency data through numerical experiments. moreover, gaussian and boot-strap prediction intervals for the price and the volatility of the prediction errors, are also discussed in some detail. © 2022 ieee."
"the rise in the application and use of iot has made tech gadgets and gizmos smarter and interconnected than ever before. iot has expanded in a number of fields including smart cities and homes, healthcare, finance, etc. a typical iot device is likely to integrate computation, networking, and physical processes from embedded computational gadgets. to monitor and extract valuable existential patterns from the large volume of data that is generated, machine learning (ml) helps a lot by the different number of algorithms that can be developed. typically, ml is a discipline that covers the two major fields of constructing intelligent computer systems and governing these systems. ml has progressed dramatically over the past two decades and has emerged as one of the major methods for developing computer vision systems and other pedagogies. rapid advancements and enhancements in these fields are leading to extensive interactions among the devices in the heterogeneous pool of gadgets. however, with advancements, there always will be a few bottlenecks that hinder the security and safety of the device or the gadget. making use of such methodologies and techniques for user access control exposes this to endless vulnerabilities including numerous attacks and other complications. it is extremely important to protect the authenticity and privacy of the users and the data that is stored in these smart devices. this paper discusses the variety of ways in which a smart device can validate the user with proper authentication and verification. © 2022, the author(s), under exclusive license to springer nature singapore pte ltd."
"pretrained language models such as bert have achieved remarkable success in several nlp tasks. with the wide adoption of bert in real-world applications, researchers begin to investigate the implicit biases encoded in the bert. in this paper, we assess the implicit stock market preferences in bert and its finance domain-specific model finbert. we find some interesting patterns. for example, the language models are overall more positive towards the stock market, but there are significant differences in preferences between a pair of industry sectors, or even within a sector. given the prevalence of nlp models in financial decision making systems, this work raises the awareness of their potential implicit preferences in the stock markets. awareness of such problems can help practitioners improve robustness and accountability of their financial nlp pipelines. © 2022 association for computational linguistics."
"bio-inspired computing is one of the foremost subfields of artificial intelligence, which aims to tackle complex optimization problems. the main advantage of bio-inspired algorithms over traditional methods is their searching ability. portfolio selection is a popular optimization problem in economics and finance. it aims to find an optimal allocation of capital among a set of assets by maximization of return with simultaneous minimization of risk. since the portfolio optimization problem is np-hard, a large number of researchers have resorted to bio-inspired algorithms to deal with the computational complexity. this study provides an overview of the new generation bio-inspired algorithms from the recently published literature for portfolio optimization. besides, opportunities for future research within this area discussed. © 2022, the author(s), under exclusive license to springer nature singapore pte ltd."
"data-to-text generation is challenging due to the great variety of the input data in terms of domains (e.g., finance vs sports) or schemata (e.g., diverse predicates). recent end-to-end neural methods thus require substantial training examples to learn to disambiguate and describe the data. yet, real-world data-to-text problems often suffer from various data-scarce issues: one may have access to only a handful of or no training examples, and/or have to rely on examples in a different domain or schema. to fill this gap, we propose any-shot data-to-text (asdot), a new approach flexibly applicable to diverse settings by making efficient use of any given (or no) examples. asdot consists of two steps, data disambiguation and sentence fusion, both of which are amenable to be solved with off-the-shelf pretrained language models (lms) with optional finetuning. in the data disambiguation stage, we employ the prompted gpt-3 model to understand possibly ambiguous triples from the input data and convert each into a short sentence with reduced ambiguity. the sentence fusion stage then uses an lm like t5 to fuse all the resulting sentences into a coherent paragraph as the final description. we evaluate extensively on various datasets in different scenarios, including the zero-/few-/full-shot settings, and generalization to unseen predicates and out-of-domain data. experimental results show that asdot consistently achieves significant improvement over baselines, e.g., a 30.81 bleu gain on the dart dataset under the zero-shot setting. © 2022 association for computational linguistics."
"derivative information plays a crucial role in the correctness and performance of scientific computing in a wide variety of scientific domains such as computational fluid dynamics (cfd), finance engineering and so on. the reverse mode of algorithmic differentiation is particularly efficient for the computation of derivatives of multivariate vector functions $f: r^{n}\mapsto r^{m}$, where the number of inputs $n$ far exceeds the number of outputs $m$, frequently appearing as cost functions in numerical optimization kernels. in particular, reverse-mode ad is at the heart of the back propagation algorithm widely used in machine learning. the reverse mode of ad requires that the control flow of the derivative program be reversed, meaning that the results of intermediate computations (in our case, the computational graph of $f$) must be stored either in memory or secondary storage. as a result, this requirement leads to the memory wall problem, especially for large-scale numerical problems, where the results of intermediate computations cannot fit entirely in memory. in this paper, we present an algorithm called memory-efficient adjoints (me-adjoints) for solving the memory wall problem by dynamically applying a simple partitioning scheme to the computational graph of the function $f$ at runtime. our approach employs operator overloading in c++ to achieve a fully automatic adjoining process, whereby derivative programs require only trivial changes to the code as opposed to the use of checkpointing techniques, which require substantial changes to the code.  © 2022 ieee."
"in many areas such as computational biology, finance or social sciences, knowledge of an underlying graph explaining the interactions between agents is of paramount importance but still challenging. considering that these interactions may be based on nonlinear relationships adds further complexity to the topology inference problem. among the latest methods that respond to this need is a topology inference one proposed by the authors, which estimates a possibly directed adjacency matrix in an online manner. contrasting with previous approaches based on linear models, the considered model is able to explain nonlinear interactions between the agents in a network. the novelty in the considered method is the use of a derivative-reproducing property to enforce network sparsity, while reproducing kernels are used to model the nonlinear interactions. the aim of this paper is to present a thorough convergence analysis of this method. the analysis is proven to be sane both in the mean and mean square sense. in addition, stability conditions are devised to ensure the convergence of the analyzed method.  © 2015 ieee."
"the solution of large sparse linear systems is required in many scientific fields such as computational fluid dynamics, computational electromagnetism, computational finance, etc. the computation of the solution of these systems is performed with preconditioned iterative methods, which rely on effective preconditioning schemes. a new class of approximate inverses is proposed, namely incomplete inverse matrices, which are computed using a recursive schur complement-based approach. this class of approximate inverses is based on a priori knowledge of a sparsity pattern. in order to have finer control over the density of the proposed approximate inverse, especially in the case of three-dimensional problems, on-the-fly filtration is used, resulting in substantial reduction in the number of nonzero elements. implementation details and analysis for computing the proposed scheme are given. numerical results depicting the effectiveness and applicability of the proposed scheme are also provided. © 2021 john wiley & sons ltd."
"quantum computing (qc) is an emerging paradigm with the potential to offer significant computational advantage over conventional classical computing by exploiting quantum-mechanical principles such as entanglement and superposition. it is anticipated that this computational advantage of qc will help to solve many complex and computationally intractable problems in several application domains such as drug design, data science, clean energy, finance, industrial chemical development, secure communications, and quantum chemistry. in recent years, tremendous progress in both quantum hardware development and quantum software/algorithm has brought qc much closer to reality. indeed, the demonstration of quantum supremacy marks a significant milestone in the noisy intermediate scale quantum (nisq) era—the next logical step being the quantum advantage whereby quantum computers solve a real-world problem much more efficiently than classical computing. as the quantum devices are expected to steadily scale up in the next few years, quantum decoherence and qubit interconnectivity are two of the major challenges to achieve quantum advantage in the nisq era. qc is a highly topical and fast-moving field of research with significant ongoing progress in all facets. a systematic review of the existing literature on qc will be invaluable to understand the state-of-the-art of this emerging field and identify open challenges for the qc community to address in the coming years. this article presents a comprehensive review of qc literature and proposes taxonomy of qc. the proposed taxonomy is used to map various related studies to identify the research gaps. a detailed overview of quantum software tools and technologies, post-quantum cryptography, and quantum computer hardware development captures the current state-of-the-art in the respective areas. the article identifies and highlights various open challenges and promising future directions for research and innovation in qc. © 2021 john wiley & sons ltd."
"solving computational fluid dynamics problems requires using large computational resources. the computational time and memory requirements to solve realistic problems vary from a few hours to several weeks with several processors working in parallel. motivated by the need of reducing such large amount of resources (improving the industrial applications in which fluid dynamics plays a key role), this article introduces a new predictive reduced order model (rom) applied to solve fluid dynamics problems. the model is based on physical principles and combines modal decompositions with deep learning architectures. the hybrid rom, reduces the dimensionality of a database via proper orthogonal decomposition (pod), extracting the dominant features leading the flow dynamics of the problem studied. the number of degrees of freedom are reduced from hundred thousands spatial points describing the database to a few (20–100) pod modes. firstly, pod divides the spatio-temporal data into spatial modes and temporal coefficients (or temporal modes). next, the temporal coefficients are integrated in time using convolutional or recurrent neural networks. the temporal evolution of the flow is approximated after combining the spatial modes with the new temporal coefficients computed. the model is tested in two complex problems of fluid dynamics, the three-dimensional wake of a circular cylinder and a synthetic jet. the hybrid rom uses data from the initial transient stage of numerical simulations to predict the temporally converged solution of the flow with high accuracy. the speed-up factor comparing the time necessary to obtain the predicted solution using the hybrid rom and the numerical solver is ∼140–348 in the synthetic jet and ∼2897–3818 in the three dimensional cylinder wake. the robustness shown in the results presented and the data-driven nature of this rom, make it possible to extend its application to other fields (i.e. video and language processing, robotics, finances). © 2021 the author(s)"
"in friz et al. [precise asymptotics for robust stochastic volatility models. ann. appl. probab, 2021, 31(2), 896–940], we introduce a new methodology to analyze large classes of (classical and rough) stochastic volatility models, with special regard to short-time and small-noise formulae for option prices, using the framework [bayer et al., a regularity structure for rough volatility. math. finance, 2020, 30(3), 782–832]. we investigate here the fine structure of this expansion in large deviations and moderate deviations regimes, together with consequences for implied volatility. we discuss computational aspects relevant for the practical application of these formulas. we specialize such expansions to prototypical rough volatility examples and discuss numerical evidence. © 2021 the author(s). published by informa uk limited, trading as taylor & francis group."
"online active learning (oal) aims to manage unlabeled datastream by selectively querying the label of data. oal is applicable to many real-world problems, such as anomaly detection in health-care and finance. in these problems, there are two key challenges: the query budget is often limited; the ratio between classes is highly imbalanced. in practice, it is quite difficult to handle imbalanced unlabeled datastream when only a limited budget of labels can be queried for training. to solve this, previous oal studies adopt either asymmetric losses or queries (an isolated asymmetric strategy) to tackle the imbalance, and use first-order methods to optimize the cost-sensitive measure. however, the isolated strategy limits their performance in class imbalance, while first-order methods restrict their optimization performance. in this article, we propose a novel online adaptive asymmetric active learning algorithm, based on a new asymmetric strategy (merging both asymmetric losses and queries strategies), and second-order optimization. we theoretically analyze its mistake bound and cost-sensitive metric bounds. moreover, to better balance performance and efficiency, we enhance our algorithm via a sketching technique, which significantly accelerates the computational speed with quite slight performance degradation. promising results demonstrate the effectiveness and efficiency of the proposed methods.  © 1989-2012 ieee."
"the boom of financial technology and artificial intelligence (ai) has influenced financial industry. application of ai-based techniques especially deep neural networks in the stock market has drawn particular attention in recent years. moreover, applying modern technology in finance has created new fields such as computational finance and intelligent asset management. following this revolution, the present research aims to shed light on deep learning models in stocks analysis and apply theses analyses in portfolio management. to this end, we introduced a novel hybrid deep learning model for stock prediction and incorporated these predictions as investors’ views in black-litterman asset allocation model. the finding indicates that the black-litterman portfolio based on the predictions of the hybrid ceemd-cnn-lstm model constructed portfolios with high return, low extreme allocation, and low risk. furthermore, the black-litterman portfolios based on the introduced prediction model outperformed the mean-variance portfolio, equal-weighted portfolio, and the black-litterman portfolios based on the predictions of cnn-lstm and lstm. this remarkable performance could be attributed to each constituting model used in the hybrid model and portfolio formation strategy. © 2021 elsevier ltd"
"artificial intelligence (ai) provides many opportunities to improve private and public life. discovering patterns and structures in large troves of data in an automated manner is a core component of data science, and currently drives applications in diverse areas such as computational biology, law and finance. however, such a highly positive impact is coupled with a significant challenge: how do we understand the decisions suggested by these systems in order that we can trust them? in this report, we focus specifically on data-driven methods—machine learning (ml) and pattern recognition models in particular—so as to survey and distill the results and observations from the literature. the purpose of this report can be especially appreciated by noting that ml models are increasingly deployed in a wide range of businesses. however, with the increasing prevalence and complexity of methods, business stakeholders in the very least have a growing number of concerns about the drawbacks of models, data-specific biases, and so on. analogously, data science practitioners are often not aware about approaches emerging from the academic literature or may struggle to appreciate the differences between different methods, so end up using industry standards such as shap. here, we have undertaken a survey to help industry practitioners (but also data scientists more broadly) understand the field of explainable machine learning better and apply the right tools. our latter sections build a narrative around a putative data scientist, and discuss how she might go about explaining her models by asking the right questions. from an organization viewpoint, after motivating the area broadly, we discuss the main developments, including the principles that allow us to study transparent models vs. opaque models, as well as model-specific or model-agnostic post-hoc explainability approaches. we also briefly reflect on deep learning models, and conclude with a discussion about future research directions. © copyright © 2021 belle and papantonis."
"quantum computing is implicated as a next-generation solution to supplement traditional von neumann architectures in an era of post-moore's law computing. as classical computational infrastructure becomes more limited, quantum platforms offer expandability in terms of scale, energy consumption, and native 3-d problem modeling. quantum information science is a multidisciplinary field drawing from physics, mathematics, computer science, and photonics. quantum systems are expressed with the properties of superposition and entanglement, evolved indirectly with operators (ladder operators, master equations, neural operators, and quantum walks), and transmitted (via quantum teleportation) with entanglement generation, operator size manipulation, and error correction protocols. this article discusses emerging applications in quantum cryptography, quantum machine learning, quantum finance, quantum neuroscience, quantum networks, and quantum error correction. © 1997-2012 ieee."
"recent changes in the energy sector are increasing the importance of portfolio optimization for market participation. although the portfolio optimization problem is most popular in finance and economics, it is only recently being subject of study and application in electricity markets. risk modeling in this domain is, however, being addressed as in the classic portfolio optimization problem, where investment diversity is the adopted measure to mitigate risk. the increasing unpredictability of market prices as reflection of the renewable generation variability brings a new dimension to risk formulation, as market participation risk should consider the prices variation in each market. this paper thereby proposes a new portfolio optimization model, considering a new approach for risk management. the problem of electricity allocation between different markets is formulated as a classic portfolio optimization problem considering market prices forecast error as part of the risk asset. dealing with a multi-objective problem leads to a heavy computational burden, and for this reason a particle swarm optimization-based method is applied. a case study based on real data from the iberian electricity market demonstrates the advantages of the proposed approach to increase market players’ profits while minimizing the market participation risk. © 2021 elsevier ltd"
"this book introduces machine learning in finance and illustrates how we can use computational tools in numerical finance in real-world context. these computational techniques are particularly useful in financial risk management, corporate bankruptcy prediction, stock price prediction, and portfolio management. the book also offers practical and managerial implications of financial and managerial decision support systems and how these systems capture vast amount of financial data. business risk and uncertainty are two of the toughest challenges in the financial industry. this book will be a useful guide to the use of machine learning in forecasting, modeling, trading, risk management, economics, credit risk, and portfolio management. © 2021 selection and editorial matter, mohammad zoynul abedin, m. kabir hassan, petr hajek, and mohammed mohiuddin; individual chapters, the contributors."
"this book is part of a six-volume series on disaster risk reduction and resilience. the series aims to fill in gaps in theory and practice in the sendai framework, and provides additional resources, methodologies and communication strategies to enhance the plan for action and targets proposed by the sendai framework. the series will appeal to a broad range of researchers, academics, students, policy makers and practitioners in engineering, environmental science and geography, geoscience, emergency management, finance, community adaptation, atmospheric science and information technology. this volume discusses how to measure and build disaster resilience at society's capacity, drawing upon individual, institutional and collective resources to cope with and adapt to the demands and challenges of natural disaster occurrences. the book will serve as a guide, outlining the key indicators of disaster resilience in urban and rural settings, and the resources and strategies needed to build resilient communities in accordance with the targets of the sendai framework. readers will learn about multi-risk reduction approaches using computational methods, data mining techniques, and system thinking at various scales, as well as institutional and infrastructure resilience strategies based on several case studies. © springer nature switzerland ag 2021."
"sliced inverse regression (sir) is the most widely used sufficient dimension reduction method due to its simplicity, generality and computational efficiency. however, when the distribution of covariates deviates from multivariate normal distribution, the estimation efficiency of sir gets rather low, and the sir estimator may be inconsistent and misleading, especially in the high-dimensional setting. in this article, we propose a robust alternative to sir—called elliptical sliced inverse regression (esir), to analysis high-dimensional, elliptically distributed data. there are wide applications of elliptically distributed data, especially in finance and economics where the distribution of the data is often heavy-tailed. to tackle the heavy-tailed elliptically distributed covariates, we novelly use the multivariate kendall’s tau matrix in a framework of generalized eigenvalue problem in sufficient dimension reduction. methodologically, we present a practical algorithm for our method. theoretically, we investigate the asymptotic behavior of the esir estimator under the high-dimensional setting. extensive simulation results show esir significantly improves the estimation efficiency in heavy-tailed scenarios, compared with other robust sir methods. analysis of the istanbul stock exchange dataset also demonstrates the effectiveness of our proposed method. moreover, esir can be easily extended to other sufficient dimension reduction methods and applied to nonelliptical heavy-tailed distributions. © 2021 american statistical association."
"purpose: the purpose of this study is to provide a novel portfolio asset prediction by means of the modified deep learning and hybrid meta-heuristic concept. in the past few years, portfolio optimization has appeared as a demanding and fascinating multi-objective problem, in the area of computational finance. yet, it is accepting the growing attention of fund management companies, researchers and individual investors. the primary issues in portfolio selection are the choice of a subset of assets and its related optimal weights of every chosen asset. the composition of every asset is chosen in a manner such that the total profit or return of the portfolio is improved thereby reducing the risk at the same time. design/methodology/approach: this paper provides a novel portfolio asset prediction using the modified deep learning concept. for implementing this framework, a set of data involving the portfolio details of different companies for certain duration is selected. the proposed model involves two main phases. one is to predict the future state or profit of every company, and the other is to select the company which is giving maximum profit in the future. in the first phase, a deep learning model called recurrent neural network (rnn) is used for predicting the future condition of the entire companies taken in the data set and thus creates the data library. once the forecasting of the data is done, the selection of companies for the portfolio is done using a hybrid optimization algorithm by integrating jaya algorithm (ja) and spotted hyena optimization (sho) termed as jaya-based spotted hyena optimization (j-sho). this optimization model tries to get the optimal solution including which company has to be selected, and optimized rnn helps to predict the future return while using those companies. the main objective model of the j-sho-based rnn is to maximize the prediction accuracy and j-sho-based portfolio asset selection is to maximize the profit. extensive experiments on the benchmark datasets from real-world stock markets with diverse assets in various time periods shows that the developed model outperforms other state-of-the-art strategies proving its efficiency in portfolio optimization. findings: from the analysis, the profit analysis of proposed j-sho for predicting after 7 days in next month was 46.15% better than particle swarm optimization (pso), 18.75% better than grey wolf optimization (gwo), 35.71% better than whale optimization algorithm (woa), 5.56% superior to ja and 35.71% superior to sho. therefore, it can be certified that the proposed j-sho was effective in providing intelligent portfolio asset selection and prediction when compared with the conventional methods. originality/value: this paper presents a technique for providing a novel portfolio asset prediction using j-sho algorithm. this is the first work uses j-sho-based optimization for providing a novel portfolio asset prediction using the modified deep learning concept. © 2021, emerald publishing limited."
"online portfolio selection is a fundamental research problem, which has drawn extensive investigations in both machine learning and computational finance communities. the evolution of electronic trading has contributed to the growing prevalence of high-frequency trading (hft) in recent years. generally, hft requires trading strategies to be fast in execution. however, the existing online portfolio selection strategies fail to either satisfy the demand for high execution speed or make effective utilization of historical data. in response, we propose a framework named exponential gradient with momentum (egm) which integrates eg with an acknowledged optimization method in stochastic learning, i.e., momentum. specifically, momentum boosts the performance of eg by making full use of historical information. most essentially, egm can execute with only constant memory and running time in the number of assets per trading period, thus overcoming the drawback of most online strategies. the theoretical analysis reveals that egm bounds the regret sublinearly. the extensive experiments conducted on four real-world datasets demonstrate that egm outperforms relevant strategies with respect to comprehensive evaluation metrics. © 2021 elsevier ltd"
"artificial intelligence in the fourth industrial revolution is beginning to live up to its promises of delivering real value necessitated by the availability of relevant data, computational abil-ity, and algorithms. therefore, this study sought to investigate the influence of artificial intelligence on the attainment of sustainable development goals with a direct focus on poverty reduction, goal one, industry, innovation, and infrastructure development goal 9, in emerging economies. using content analysis, the result pointed to the fact that artificial intelligence has a strong influence on the attainment of sustainable development goals particularly on poverty reduction, improvement of the certainty and reliability of infrastructure like transport making economic growth and development possible in emerging economies. the results revealed that artificial intelligence is making poverty reduction possible through improving the collection of poverty-related data through poverty maps, revolutionizing agriculture education and the finance sector through financial inclusion. the study also discovered that ai is also assisting a lot in education, and the financial sector allowing the previously excluded individuals to be able to participate in the mainstream economy. therefore, it is important that governments in emerging economies need to invest more in the use of ai and increase the research related to it so that the sustainable development goals (sdgs) related to innovation, infrastructure development, poverty reduction are attained. © 2021 by the author. licensee mdpi, basel, switzerland."
"with the rapid development of science and technology, information video processing technology has become a research field that has attracted widespread attention. automatic recognition of motion gestures is an important research object in the field of information technology. it has been widely used in education, medical treatment and finance, and has promoted the further development of this technology. the application of information video processing technology in motion posture and automatic recognition of motion posture contribute to better scientific education and technical diagnosis. this article is based on the research of automatic motion detection system based on video information processing technology. based on the summary and analysis of relevant research work at home and abroad, this paper mainly studies the detection of moving posture targets, the segmentation of moving posture targets, and the automatic recognition of moving posture tracking in video sequences based on the characteristics of the motion posture. this research aims to effectively improve the effect of automatic recognition of sports high poses while appropriately reducing the computational complexity. the research focuses on finding simpler and more informative automatic recognition methods of sports poses, and a large number of targeted methods have been carried out. this research uses these algorithms to extract moving images from videos, and proposes an automatic recognition method of motion gestures based on information video processing technology. experimental results show that the average error rate of the system designed in this paper is 89.5%, which can meet the current application requirements for automatic recognition of motion gestures. © 2021 institute of physics publishing. all rights reserved."
"data marketplaces are expected to play a crucial role in tomorrow’s data economy, but such marketplaces are seldom commercially viable. currently, there is no clear understanding of the knowledge gaps in data marketplace research, especially not of neglected research topics that may advance such marketplaces toward commercialization. this study provides an overview of the state-of-the-art of data marketplace research. we employ a systematic literature review (slr) approach to examine 133 academic articles and structure our analysis using the service-technology-organization-finance (stof) model. we find that the extant data marketplace literature is primarily dominated by technical research, such as discussions about computational pricing and architecture. to move past the first stage of the platform’s lifecycle (i.e., platform design) to the second stage (i.e., platform adoption), we call for empirical research in non-technological areas, such as customer expected value and market segmentation. © 2021 by the authors. licensee mdpi, basel, switzerland."
"the ℓ1-regularized gaussian maximum likelihood method is a common approach for sparse precision matrix estimation, but one that poses a computational challenge for high-dimensional datasets. we present a novel ℓ1-regularized maximum likelihood method for performant large-scale sparse precision matrix estimation utilizing the block structures in the underlying computations. we identify the computational bottlenecks and contribute a block coordinate descent update as well as a block approximate matrix inversion routine, which is then parallelized using a shared-memory scheme. we demonstrate the effectiveness, accuracy, and performance of these algorithms. our numerical examples and comparative results with various modern open-source packages reveal that these precision matrix estimation methods can accelerate the computation of covariance matrices by two to three orders of magnitude, while keeping memory requirements modest. furthermore, we conduct large-scale case studies for applications from finance and medicine with several thousand random variables to demonstrate applicability for real-world datasets. © 2021 the author(s)"
"in this paper, we propose a cooperative strategy-based self-organization mechanism to reconstruct the network. the mechanism includes a comprehensive evaluation algorithm and structure adjustment mechanism. the self-organization mechanism can be carried out simultaneously with the parameter optimization process. by calculating the similarity and independent contribution of normative neurons, the effectiveness of fuzzy rules can be jointly evaluated, and effective structural changes can be realized. moreover, this mechanism should not set the threshold in advance in practical application. in order to optimize the parameters of sc-ir2fnn, we developed a parameter optimization mechanism based on an interaction strategy. the parameter optimization mechanism based on a joint strategy, namely multilayer optimization engine, can split sc-ir2fnn parameters into nonlinear and linear parameters for joint optimization. the nonlinear parameters are optimized by an advanced two-level algorithm, and the linear parameters are updated with the minimum biological multiplication. two parameter optimization algorithms optimize nonlinear and linear parameters, reduce the computational complexity of sc-ir2fnn, and improve the learning rate. using the principal component factor analysis method, seven representative common factors are selected to replace the original variables, which include the profitability factor of the financing enterprise, the solvency factor of the financing enterprise, the profitability factor of the core enterprise, the operation guarantee factor, and the growth ability of the financing enterprise. factors, supply chain online degree factors, financing enterprise quality, and cooperation factors, can well measure the credit risk of online supply chains. the logistic model shows that the profitability factor of the financing company, the debt repayment factor of the financing company, and the profitability of the core company are three factors that have a significant impact on the credit risk of online supply chain finance. based on the improved credit calculation model, we developed an online clue risk calculation. this method is based on site conditions and can evaluate credit risk. from the test results, the improved credit scoring system is the result of facing speculative and circular credit fraud and implies that the traders of risk commentators are in a leading position in each electronic device. the results show that risk analysis is effective in any case.  © 2022 lina wang and hui song."
"the interface between operations and finance has recently been of interest to both practitioners and academics, who recognize that coordinating decisions from both functions provides a great opportunity to improve the performance of firms. however, the link between dynamic lot-sizing and financing decisions remains largely unexplored in the literature. this paper investigates a dynamic lot-sizing problem with short-term financing and external deposits for a capital-constrained manufacturer. in each period, the manufacturer finances its operations using a combination of internal cash, short-term loans, and short-term capital subscriptions from shareholders. furthermore, the manufacturer decides on an amount to send to an external depository to take into consideration the opportunity cost of carrying cash. two formulations for this integrated problem are presented, structural properties of the optimal solution are derived, and a dynamic programming algorithm is developed to solve the problem. subsequently, the integrated problem, analysis, and solution procedure are extended to consider the case of a per-period loan limit. a numerical study is conducted to derive insights on the effect of financial and operational parameters, which are then validated using extensive computational experiments. our results show that integration leads to significant savings, smoother production, and smaller capital subscriptions and external deposits. in addition, while the discount factor affects the number of setups, inventory levels, capital subscriptions, and external deposits, the short-term interest rate determines the choice between using internal cash or short-term loans from the lender. © 2021 elsevier b.v."
"we investigate a class of fractional distributionally robust optimization problems with uncertain probabilities. they consist in the maximization of ambiguous fractional functions representing reward-risk ratios and have a semi-infinite programming epigraphic formulation. we derive a new fully parameterized closed-form to compute a new bound on the size of the wasserstein ambiguity ball. we design a data-driven reformulation and solution framework. the reformulation phase involves the derivation of the support function of the ambiguity set and the concave conjugate of the ratio function. we design modular bisection algorithms which enjoy the finite convergence property. this class of problems has wide applicability in finance, and we specify new ambiguous portfolio optimization models for the sharpe and omega ratios. the computational study shows the applicability and scalability of the framework to solve quickly large, industry-relevant-size problems, which cannot be solved in one day with state-of-the-art mixed-integer nonlinear programming (minlp) solvers. copyright: © 2020 informs"
"we present a fast, accurate estimation method for multivariate hawkes self-exciting point processes widely used in seismology, criminology, finance and other areas. there are two major ingredients. the first is an analytic derivation of exact maximum likelihood estimates of the nonparametric triggering density. we develop this for the multivariate case and add regularization to improve stability and robustness. the second is a moment-based method for the background rate and triggering matrix estimation, which is extended here for the spatiotemporal case. our method combines them together in an efficient way, and we prove the consistency of this new approach. extensive numerical experiments, with synthetic data and real-world social network data, show that our method improves the accuracy, scalability and computational efficiency of prevailing estimation approaches. moreover, it greatly boosts the performance of hawkes process-based models on social network reconstruction and helps to understand the spatiotemporal triggering dynamics over social media. © 2021, the institute of statistical mathematics, tokyo."
recently stochastic methods have become very important tool for high performance computing of very high dimensional problems in computational finance. the advantages and disadvantages of the different highly efficient stochastic methods for multidimensional integrals related to evaluation of european style options will be analyzed. multidimensional integrals up to 100 dimensions related to european options will be computed with highly efficient optimized lattice rules.  © 2021 polish information processing society.
"we discuss the objectives of any endeavor in creating artificial intelligence, ai, and provide a possible alternative. intelligence might be an unintended consequence of curiosity left to roam free, best exemplified by a frolicking infant. this suggests that our attempts at ai could have been misguided. what we actually need to strive for can be termed artificial curiosity, ac, and intelligence happens as a consequence of those efforts. for this unintentional yet welcome aftereffect to set in a foundational list of guiding principles needs to be present. we start with the intuition for this line of reasoning and formalize it with a series of definitions, assumptions, ingredients, models and iterative improvements that will be necessary to make the incubation of intelligence a reality. our discussion provides conceptual modifications to the turing test and to searle's chinese room argument. we discuss the future implications for society as ai becomes an integral part of life. we provide a road-map for creating intelligence with the technical parts relegated to the appendix so that the article is accessible to a wide audience. the central techniques in our formal approach to creating intelligence draw upon tools and concepts widely used in physics, cognitive science, psychology, evolutionary biology, statistics, linguistics, communication systems, pattern recognition, marketing, economics, finance, information science and computational theory highlighting that solutions for creating artificial intelligence have to transcend the artificial barriers between various fields and be highly multi-disciplinary. © 2020 elsevier inc."
"machine learning in finance has been on the rise in the past decade. the applications of machine learning have become a promising methodological advancement. the paper’s central goal is to use a metadata-based systematic literature review to map the current state of neural networks and machine learning in the finance field. after collecting a large dataset comprised of 5053 documents, we conducted a computational systematic review of the academic finance literature intersected with neural network methodologies, with a limited focus on the documents’ metadata. the output is a meta-analysis of the two-decade evolution and the current state of academic inquiries into financial concepts. researchers will benefit from a mapping resulting from computational-based methods such as graph theory and natural language processing. © 2021 by the authors."
"exploratory factor analysis is a dimension-reduction technique commonly used in psychology, finance, genomics, neuroscience, and economics. advances in computational power have opened the door for fully bayesian treatments of factor analysis. one open problem is enforcing rotational identifability of the latent factor loadings, as the loadings are not identified from the likelihood without further restrictions. nonidentifability of the loadings can cause posterior multimodality, which can produce misleading posterior summaries. the positive-diagonal, lower-triangular (plt) constraint is the most commonly used restriction to guarantee identifiability, in which the upper m × m submatrix of the loadings is constrained to be a lower-triangular matrix with positive-diagonal elements. the plt constraint can fail to guarantee identifiability if the constrained submatrix is singular. furthermore, though the plt constraint addresses identifiability-related multimodality, it introduces additional mixing issues. we introduce a new bayesian sampling algorithm that efficiently explores the multimodal posterior surface and addresses issues with plt-constrained approaches. supplementary materials for this article are available online. © 2020 american statistical association."
"the cumulative sum (cusum) chart is widely used and has many applications in different fields such as finance, medical, engineering, and other fields. in real applications, there are many situations in which the observations of random processes are serially correlated, such as a hospital admission in the medical field, a share price in the economic field, or a daily rainfall in the environmental field. the common characteristic of control charts that has been used to evaluate the performance of control charts is the average run length (arl). the primary goals of this paper are to derive the explicit formula and develop the numerical integral equation of the arl for the cusum chart when observations are seasonal autoregressive models with exogenous variable, sarx(p,r)l with exponential white noise. the fredholm integral equation has been used for solving the explicit formula of arl, and we used numerical methods including the midpoint rule, the trapezoidal rule, the simpson's rule, and the gaussian rule to approximate the numerical integral equation of arl. the uniqueness of solutions is guaranteed by using banach's fixed point theorem. in addition, the proposed explicit formula was compared with their numerical methods in terms of the absolute percentage difference to verify the accuracy of the arl results and the computational time (cpu). the results obtained indicate that the arl from the explicit formula is close to the numerical integral equation with an absolute percentage difference of less than 1%. we found an excellent agreement between the explicit formulas and the numerical integral equation solutions. an important conclusion of this study was that the explicit formulas outperformed the numerical integral equation methods in terms of cpu time. consequently, the proposed explicit formulas and the numerical integral equation have been the alternative methods for finding the arl of the cusum control chart and would be of use in fields like biology, engineering, physics, medical, and social sciences, among others. © 2022 by authors, all rights reserved."
"in recent years, online portfolio selection, one of the fundamental problems in computational finance, has attracted increasing interest from online learning. although existing online portfolio strategies have been shown to achieve good performance, appropriate values of their parameters need to be set in advance. however, the optimal values can only be known in hindsight. to overcome this limitation, this paper proposes a new online portfolio strategy by aggregating exponential gradient (eg (formula presented.)) expert advice using the weak aggregating algorithm (waa). first, we consider a pool of eg (formula presented.) strategies with different learning rates as experts, and compute the portfolio on the next period by aggregating all the expert advice using the waa according to their previous performance. second, we theoretically prove that the proposed strategy is universal, i.e. its average logarithmic growth rate is asymptotically the same as that of the best constant rebalanced portfolio (bcrp) in hindsight. finally, we conduct extensive experiments to examine the performance of the proposed strategy on actual stock market. numerical results show that the proposed strategy overcomes the drawbacks of existing online strategies and achieves significant performance. © operational research society 2020."
"critical decisions, such as in domains ranging from medicine to finance, are often made under threatening circumstances that elicit stress and anxiety. the negative effects of such reactions on learning and decision-making have been repeatedly underscored. in contrast, here we show that perceived threat alters the process by which evidence is accumulated in a way that may be adaptive. participants (n = 91) completed a sequential evidence sampling task in which they were incentivized to accurately judge whether they were in a desirable state, which was associated with greater rewards than losses, or an undesirable state, which was associated with greater losses than rewards. before the task participants in the “threat group” experienced a social-threat manipulation. results show that perceived threat led to a reduction in the strength of evidence required to reach an undesirable judgment. computational modeling revealed this was because of an increase in the relative rate by which negative information was accumulated. the effect of the threat manipulation was global, as the alteration to evidence accumulation was observed for information which was not directly related to the cause of the threat. requiring weaker evidence to reach undesirable conclusions in threatening environments may be adaptive as it can lead to increased precautionary action. copyright © 2021 globig et al."
"this paper proposes a mixed-integer non-linear programming (minlp) model for the integrated supplier selection and order allocation in a centralized supply chain considering the disruption risks and a risk-averse decision-maker. in order to capture a realistic scenario of considering the geographical characteristics of the suppliers, we assume that the suppliers belong to two regions: the buyer's region (domestic suppliers) and outside of the buyer's region (foreign suppliers). considering this realistic feature, the supply chain might face two types of disruption risk: first, local disruption risks which might uniquely occur inside each supplier such as equipment breakdowns, and second, regional disruption risks that might occur in the region of the suppliers located in the same geographical region such as natural hazards. we formulate the problem considering a risk-neutral decision-maker as a benchmark, and then a risk-averse model is presented. in the latter case, we apply two types of risk assessment tools introduced in the finance literature to analyze the decision maker's behavior: value-at-risk (var) and conditional value-at-risk (cvar). we show that developed models are non-convex programming, and therefore, we apply the particle swarm optimization (pso) algorithm as the solution approach. we also compare the developed pso algorithm with the genetic algorithm (ga) and the commercial gams solver to verify the efficiency of the solution method. the computational experiments indicate the impact of the decision maker's attitude on the supplier selection and the order quantity. © 2021 elsevier ltd"
"seismic performance and loss assessments are required in areas of insurance, finance and public policy. providers are structural engineers and risk management firms. there are no current procedures to evaluate the epistemic and aleatory uncertainties for such assessments. the essential issue is whether or not there is sufficient reliability in the result to use the result as the basis for risk management decisions and actions. for a single building this may be whether or not a prescribed earthquake performance level is met, life safety or if a portfolio’s vulnerability level is acceptable, whether the. loss for a given time period is less than a stated value. a method based in part on federal emergency management agency p-695, is developed for evaluating the reliability of performance and/or loss assessments for both individual and portfolios of buildings. consideration is given to how well the building investigation and corresponding evaluation process have been performed, the qualifications of the person(s) doing the assessment, the thoroughness of the building evaluation, the technical validity of the assessment procedure or model and what computational reliabilities are presented. the method characterizes the uncertainty of each component of the assessment procedure for each building by qualitative determined assignments. the resulting reliability measure is likely to be most useful for determining whether/or not a building has acceptable life safety performance, or if a portfolio has an acceptably low loss risk over a given period of time. in both cases, the reliability must either be sufficient to warrant action, or serve to indicate need for improved assessment. © 2021 by the authors. licensee mdpi, basel, switzerland."
"it is crucial and challenging for the question-answering robot (qabot) to match the customer-input questions with the priori identification questions due to highly diversified expressions, especially in the case of chinese. this article proposes a coordinated scheme to analyze the similarity between sentences in two independent domains instead of a single deep learning model. in the structure domain, the bleu and data preprocessing are applied for binary analysis to discriminate the unpredictable outliers (illegal questions) to existing library. in the semantics domain, the mc-bert model, which integrates the bert encoder and the multi-kernel convolutional top classifier, is developed to handle the non-orthogonality of class identification questions. the two-domain analyses are in parallel and the two similarity scores are coordinated for the final response. the linguistic features of chinese are also taken into account. a realistic case of qabot on energy trading service and finance is numerically studied. computational results validate the effectiveness and accuracy of the proposed algorithm: top-1 and top-3 accuracies are 90.5% and 95.5%, respectively, which are significantly superior to the latest published results. © 2021, the author(s), under exclusive licence to springer science+business media, llc part of springer nature."
"deep learning models are not yet fully applied to fluid dynamics predictions, while they are the state-of-the-art solution in many other areas i.e. video and language processing, finance, robotics. prediction problems on high-dimensional, complex dynamical systems require deep learning models devised to avoid overfitting while maintaining the required model complexity. in this work we present a deep learning prediction model based on a combination of 3d convolutional layers and a low-dimensional intermediate representation that is specifically designed to forecast the future states of this type of dynamical systems. the model predicts p future velocity-field time-slices (samples) based on k past samples from a training dataset consisting of a synthetic jet in transitional regime. the complexity of this flow is characterized by two topology patterns that are periodically changing, making this flow as a suitable example to test the performance of deep learning models to predict time states in complex flows. moreover, the wide number of applications of synthetic jets (i.e.: fluid mixing, heat transfer enhancement, flow control), points out this example as a reference for future applications, where modeling synthetic jet flows with a reduced computational effort is needed. this work additionally opens up research opportunities for other areas that also operate with complex and high-dimensional time-series data: future frame video prediction, network traffic forecasting, network intrusion detection. the proposed model is presented in detail. a comprehensive analysis of the results is provided. the results are based on a strict validation strategy to ensure its generalization. the model offers an average symmetric mean absolute error (smape) and a relative root mean square error (rrmse) of 1.068 and 0.026 respectively (one order of magnitude improvement over low-rank approximation tools), using 10 past samples and predicting 6 future samples of a two-dimensional velocity field on a 70x50 point matrix associated to a synthetic jet dataset. © 2021 elsevier ltd"
"in a holistic approach, the term ""financial engineering"" implies integrating applied econometrics and advanced computation to devise tools, models, and strategies for addressing finance-related problems. information and communication technology (technologies) or ict is the infrastructure and components that enable people and institutions such as businesses and governments to interact in the digital world. this chapter consists of five sections. an informative overview of the discipline, including the conceptualization of both financial engineering and computational finance, is discussed in the first section. the second section provides a synopsis of the evolution of financial engineering from the early 1700s to present-day fintech. an overview of algorithm concepts and software implementation and their significance in financial engineering is in the third section. the fourth section delves into ict, discussing the rise of modern computers and the digital revolution. the final subsection explores the importance of ict in finance. © the author(s), under exclusive license to springer nature switzerland ag 2021."
"we consider a system with an evolving state that can be stopped at any time by a decision maker (dm), yielding a state-dependent reward. the dm does not observe the state except for a limited number of monitoring times, which he must choose, in conjunction with a suitable stopping policy, to maximize his reward. dealing with these types of stopping problems, which arise in a variety of applications from healthcare to finance, often requires excessive amounts of data for calibration purposes and prohibitive computational resources. to overcome these challenges, we propose a robust optimization approach, whereby adaptive uncertainty sets capture the information acquired through monitoring. we consider two versions of the problem - static and dynamic - depending on how the monitoring times are chosen. we show that, under certain conditions, the same worst-case reward is achievable under either static or dynamic monitoring. this allows recovering the optimal dynamic monitoring policy by resolving static versions of the problem. we discuss cases when the static problem becomes tractable and highlight conditions when monitoring at equidistant times is optimal. lastly, we showcase our framework in the context of a healthcare problem (monitoring heart-transplant patients for cardiac allograft vasculopathy), where we design optimal monitoring policies that substantially improve over the status quo recommendations.  © 2020 informs."
"quantum computers have become reality thanks to the effort of some majors in developing innovative technologies that enable the usage of quantum effects in computation, so as to pave the way towards the design of efficient quantum algorithms to use in different applications domains, from finance and chemistry to artificial and computational intelligence. however, there are still some technological limitations that do not allow a correct design of quantum algorithms, compromising the achievement of the so-called quantum advantage. specifically, a major limitation in the design of a quantum algorithm is related to its proper mapping to a specific quantum processor so that the underlying physical constraints are satisfied. this hard problem, known as circuit mapping, is a critical task to face in quantum world, and it needs to be efficiently addressed to allow quantum computers to work correctly and productively. in order to bridge above gap, this paper introduces a very first circuit mapping approach based on deep neural networks, which opens a completely new scenario in which the correct execution of quantum algorithms is supported by classical machine learning techniques. as shown in experimental section, the proposed approach speeds up current state-of-the-art mapping algorithms when used on 5-qubits ibm q processors, maintaining suitable mapping accuracy. © 2021, the author(s)."
"comprehensive understanding of key players and actions in multiplayer sports broadcast videos is a challenging problem. unlike in news or finance videos, sports videos have limited text. while both action recognition for multiplayer sports and detection of players has seen robust research, understanding contextual text in video frames still remains one of the most impactful avenues of sports video understanding. in this work we study extremely accurate semantic text detection and recognition in sports clocks, and challenges therein. we observe unique properties of sports clocks, which makes it hard to utilize general-purpose pre-trained detectors and recognizers, so that text can be accurately understood to the degree of being used to align to external knowledge. we propose a novel distant supervision technique to automatically build sports clock datasets. along with suitable data augmentations, combined with any state-of-the-art text detection and recognition model architectures, we extract extremely accurate semantic text. finally, we share our computational architecture pipeline to scale this system in industrial setting and proposed a robust dataset for the same to validate our results. © 2021 acm."
"edge computing promises to bring computation close to the end-users to support emergent applications such as virtual reality. however, the computational capacity at the edge of the network is currently limited. to become a pervasive paradigm, edge computing needs highly dispersed decentralized deployments, that, contrary to cloud, cannot benefit from economies of scale. in this situation, crowdsourcing appears attractive - there are plenty of computing devices at the disposal of the general public, and these devices are located exactly where computing power is needed the most - at the edge of the network. crowdsourcing has been a success maker for scientific computing projects, e.g., seti@home, or distributed ledger systems empowering decentralized finance. however, as of now, there is no crowdsourced system that addresses the needs of edge computing. in this position paper, we aim to identify the causes of this shortcoming, analyze the potential ways to overcome it, and outline future directions.  © 2021 acm."
"we generalize ideas in the recent literature and develop new ones in order to propose a general class of contour integral methods for linear convection–diffusion pdes and in particular for those arising in finance. these methods aim to provide a numerical approximation of the solution by computing its inverse laplace transform. the choice of the integration contour is determined by the computation of a few suitably weighted pseudo-spectral level sets of the leading operator of the equation. parabolic and hyperbolic profiles proposed in the literature are investigated and compared to the elliptic contour originally proposed by guglielmi, lópez-fernández and nino 2020, see guglielmi et al. (math comput 89:1161–1191, 2020). in summary, the article (i)provides a comparison among three different integration profiles;(ii)proposes a new fast pseudospectral roaming method;(iii)optimizes the selection of time windows on which one may arbitrarily approximate the solution by no extra computational cost with respect to the case of a fixed time instant;(iv)focuses extensively on computational aspects and it is the reference of the matlab code [20], where all algorithms described here are implemented. © 2021, the author(s)."
"contractors are often simultaneously subject to capital shortages and resource scarcities in construction projects, forcing them to rely on loans to offset cash deficits during project execution. however, few studies have proposed exact or heuristic methods to deal with the financed-based and resource-constrained project scheduling problem (fbrcpsp). this study aims at developing flexible heuristics for the fbrcpsp. to address the fbrcpsp, a mathematical model is first constructed to maximize a project's profit considering financing costs, then modified serial schedule scheme (mssgs) and modified parallel schedule scheme (mpsgs) heuristics are designed to gain a feasible project schedule and its financial plan while complying with feasibilities of the precedence relationship, resource constraints, and credit limits concurrently. moreover, the proposed heuristics, implemented as part of a genetic algorithm (heuristics-based ga) can be applied to solve the optimization model of the fbrcpsp. in two case studies, the results showed that the heuristics-based ga adequately explored the feasible solution space so that it could obtain the best solutions near those from the integer programming (ip) technique (by optimization software cplex version 12.8) and outperformed the previous precedence-preserving ga in terms of solution quality and computational time. finally, the developed heuristics can devise or update schedules and the corresponding cash flow and resource-demand profiles, helping practitioners achieve expected profits and implement project control. © 2021 american society of civil engineers."
"this introductory chapter presents briefly the motivation (safety, bias detection, social acceptance, etc.), applications (smart energy management, predictive maintenance, bioinformatics, e-commerce, healthcare, etc.), and challenges (accuracy/transparency trade-off, adapted to different levels of users, generalized to different application domains, etc.) of explainable artificial intelligence (xai) within the context of digital transformation and cyber-physical systems. it overviews briefly the methods (transparent models, model-agnostic methods) used in order to explain how and why the decision was made by the model as well as the evaluation layout (user-expertise level, expressive power, computational complexity, accuracy, usefulness and relevance, coherence with prior belief, etc.) required in order to assess the quality (explainability, transparency, interpretability) of the provided explanations. it summarizes the gathered research contributions aiming at the development and/or the use of xai techniques in order to address the aforementioned challenges in different applications such as healthcare, finance, cybersecurity, and document summarization. © the author(s), under exclusive license to springer nature switzerland ag 2021."
"global sensitivity analysis aims at quantifying the impact of input variability onto the variation of the response of a computational model. it has been widely applied to deterministic simulators, for which a set of input parameters has a unique corresponding output value. stochastic simulators, however, have intrinsic randomness due to their use of (pseudo)random numbers, so they give different results when run twice with the same input parameters but non-common random numbers. due to this random nature, conventional sobol’ indices, used in global sensitivity analysis, can be extended to stochastic simulators in different ways. in this paper, we discuss three possible extensions and focus on those that depend only on the statistical dependence between input and output. this choice ignores the detailed data generating process involving the internal randomness, and can thus be applied to a wider class of problems. we propose to use the generalized lambda model to emulate the response distribution of stochastic simulators. such a surrogate can be constructed without the need for replications. the proposed method is applied to three examples including two case studies in finance and epidemiology. the results confirm the convergence of the approach for estimating the sensitivity indices even with the presence of strong heteroskedasticity and small signal-to-noise ratio. © 2021 the author(s)"
"optical character recognition (ocr) is a very active research area in many challenging fields like pattern recognition, natural language processing (nlp), computer vision, biomedical informatics, machine learning (ml), and artificial intelligence (ai). this computational technology extracts the text in an editable format (ms word/excel, text files, etc.) from pdf files, scanned or hand-written documents, images (photographs, advertisements, and alike), etc. for further processing and has been utilized in many real-world applications including banking, education, insurance, finance, healthcare and keyword-based search in documents, etc. many ocr toolsets are available under various categories, including open-source, proprietary, and online services. this research paper provides a comparative study of various ocr toolsets considering a variety of parameters. © 2021 university of kuwait. all rights reserved."
"stochastic differential equations (sdes), which model uncertain phenomena as the time evolution of random variables, are exploited in various fields of natural and social sciences such as finance. since sdes rarely admit analytical solutions and must usually be solved numerically with huge classical-computational resources in practical applications, there is strong motivation to use quantum computation to accelerate the calculation. here, we propose a quantum-classical hybrid algorithm that solves sdes based on variational quantum simulation. we first approximate the target sde by a trinomial tree structure with discretization and then formulate it as the time-evolution of a quantum state embedding the probability distributions of the sde variables. we embed the probability distribution directly in the amplitudes of the quantum state whereas the previous studies took the square-root of the probability distribution in the amplitudes. our embedding enables us to construct simple quantum circuits that simulate the time-evolution of the state for general sdes. we also develop a scheme to compute the expectation values of the sde variables and discuss whether our scheme can achieve quantum speedup for the expectation-value evaluations of the sde variables. finally, we numerically validate our algorithm by simulating several types of stochastic processes. our proposal provides a new direction for simulating sdes on quantum computers. © 2021 authors. published by the american physical society."
"lie detection is considered a concern for everyone in their day-to-day life, given its impact on human interactions. thus, people normally pay attention to both what their interlocutors are saying and to their visual appearance, including the face, to find any signs that indicate whether or not the person is telling the truth. while automatic lie detection may help us to understand these lying characteristics, current systems are still fairly limited, partly due to lack of adequate datasets to evaluate their performance in realistic scenarios. in this work, we collect an annotated dataset of facial images, comprising both 2d and 3d information of several participants during a card game that encourages players to lie. using our collected dataset, we evaluate several types of machine learning-based lie detectors in terms of their generalization, in person-specific and cross-application experiments. we first extract both handcrafted and deep learning-based features as relevant visual inputs, then pass them into multiple types of classifier to predict respective lie/non-lie labels. subsequently, we use several metrics to judge the models’ accuracy based on the models predictions and ground truth. in our experiment, we show that models based on deep learning achieve the highest accuracy, reaching up to 57% for the generalization task and 63% when applied to detect the lie to a single participant. we further highlight the limitation of the deep learning-based lie detector when dealing with cross-application lie detection tasks. finally, this analysis along the proposed datasets would potentially be useful not only from the perspective of computational systems perspective (e.g., improving current automatic lie prediction accuracy), but also for other relevant application fields, such as health practitioners in general medical counselings, education in academic settings or finance in the banking sector, where close inspections and understandings of the actual intentions of individuals can be very important. © 2021 by the authors. licensee mdpi, basel, switzerland."
"the proceedings contain 78 papers. the topics discussed include: retailers, all omni-shoppers are not the same; the impact of tacit knowledge acquisition on firm performance: the moderating effect of redundant resources; opinion-aware retrieval models based on sentiment and intensity of lexical features; financial flexibility during the pre- and post-global financial crisis periods; application of deep reinforcement learning algorithm in smart finance; tax risk prediction of real estate based on convolutional neural network; krylov subspace methods for big data analysis of large computational electromagnetics applications; industrial economic cooperation between china and nordic countries under the double circulation pattern: basis and prospect; and the support system for anomaly detection with application in mainframe management process."
"in this work we investigate advanced stochastic methods for solving a specific multidimensional problems related to computation of european style options in computational finance. recently stochastic methods have become very important tool for high performance computing of very high dimensional problems in computational finance. the advantages and disadvantages of several highly efficient stochastic methods connected to european options evaluation will be analyzed. for the first time multidimensional integrals up to 100 dimensions related to european options will be computed with highly efficient lattice rules. © 2022, the author(s), under exclusive license to springer nature singapore pte ltd."
"quadratic programming is an important optimization problem that has applications in many areas such as finance, control, and management. quadratic programs arisen in practice are often large but sparse, and they usually cannot be solved efficiently without exploiting their structures. since existing methods for quadratic programming deal with dense cases and do not take advantage of any specific sparsity patterns in the problems, we propose a method to efficiently compute the search directions for the primal-dual path-following interior-point method for the large-scale quadratic programs whose hessian matrices have block diagonal structures and whose constraint matrices are dense by exploiting the special sparsity pattern in the problems to avoid unnecessary computations involving blocks of zeros. examples of quadratic programs with such structure, to which our method can be applied, are linear model predictive control in automatic control and portfolio optimization where securities from different sectors are weakly correlated. the time complexity of our method is significantly smaller than that of using a sparse linear solver. additionally, the computational results show that our method is faster. © 2021, prince of songkla university. all rights reserved."
"factor models are central to understanding risk-return trade-offs in finance. since fama and french (1993), hundreds of factors have been found to have explanatory power for asset pricing. to construct a factor model, two tasks have to be performed: feature selection, selecting a small subset given a large number of factors to overcome overfitting in regression, and feature engineering, determining the interactions between the factors. in this work, the process of constructing factor models (not the factors themselves) is examined. a unified, two-step process of dimensionality reduction and nonlinear transformation that produces parsimonious, general factor models is proposed. comparisons between frameworks implementing linear feature selection models as well as non-linear feature reduction techniques are conducted. a second stage generalizes the models by learning nonlinear interactions. the framework attempts to strike a balance between accuracy and interpretability. results of computational experiments on historical financial data, on three models of varying degrees of non-linearity and interpretability suggest that mixed-integer-programming-based formulations are suitable for the task of linear financial factor selection and that the second-stage nonlinearity due to neural networks improves accuracy. © 2021 elsevier b.v."
"in recent years, deep learning has attracted wide attention in many academic fields and financial industries. computational intelligence has become a very hot topic. at the same time, with the widespread popularity of machine learning, deep learning has gradually become more outstanding performance arouses people's interest. nowadays, deep learning models have been able to achieve many tasks that human experts can only complete, and continue to create a lot of benefits. as a very special application scenario, finance has been crossed and integrated with deep learning. at the same time, there are also a lot of research opportunities in this field. in this article, we try to introduce the application of deep learning in finance from the financial field and deep learning model.  © 2021 acm."
"we introduce a new family of numerical algorithms for approximating solutions of general high-dimensional semilinear parabolic partial differential equations at single space-time points. the algorithm is obtained through a delicate combination of the feynman–kac and the bismut–elworthy–li formulas, and an approximate decomposition of the picard fixed-point iteration with multilevel accuracy. the algorithm has been tested on a variety of semilinear partial differential equations that arise in physics and finance, with satisfactory results. analytical tools needed for the analysis of such algorithms, including a semilinear feynman–kac formula, a new class of seminorms and their recursive inequalities, are also introduced. they allow us to prove for semilinear heat equations with gradient-independent nonlinearities that the computational complexity of the proposed algorithm is bounded by o(dε-(4+δ)) for any δ∈ (0 , ∞) under suitable assumptions, where d∈ n is the dimensionality of the problem and ε∈ (0 , ∞) is the prescribed accuracy. moreover, the introduced class of numerical algorithms is also powerful for proving high-dimensional approximation capacities for deep neural networks. © 2021, the author(s)."
"in order to study the impact of stop-loss trading on traders' behavior and asset prices in the market, this paper constructs a multi-agent market model with stop-loss strategy based on the method of agent-based computational finance. the model simulation results show that when the stop-loss threshold is touched in the market, it is easy to trigger continuous stop-loss trading, resulting in a behavioral cascade between traders. this kind of transaction cascading leads to an increase in the convergence of trader behavior, an imbalance between sell orders and buy orders in the market, an abnormal collapse in market prices and a liquidity black hole. © 2021, the editorial department of complex systems and complexity science. all right reserved."
"the project title cited in the first sentence of the acknowledgements is incorrect. the correct title is: “dynamics of economy and finance from the economic network point of view” and not “macro-economy under covid-19 influence: data-intensive analysis and the road to recovery.” the correct acknowledgements should read: this study is conducted as a part of the project “dynamics of economy and finance from the economic network point of view” undertaken at the research institute of economy, trade and industry (rieti). the authors are grateful for helpful comments and suggestions by discussion paper seminar participants at rieti. the authors appreciate the support by intage inc. to make the data available. the authors would also like to thank participants of the conferences of computational social science japan, japan institute of marketing science, and japan society for evolutionary economics for their helpful discussions and comments. we would like to thank editage (www.editage.com) for english language editing. copyright: © 2021 mizuno et al. this is an open access article distributed under the terms of the creative commons attribution license, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited."
"financial literacy is an important part of personal finance practices that can provide human well-being. recent studies covered topics on financial decision-making, yet few papers showed the problems people discuss in online communities related to finances. the goal of our pilot study is to find out which problems people face in financial management and what is the context of those problems. we did this using computational text analysis techniques in an attempt to reveal the essential problems people ask for personal finance advice on social media. the work contributes to a discussion of studies of personal finance practices by exploring the problems and assessing their prevalence and sentiment in user communication. © 2021 copyright for this paper by its authors. use permitted under creative commons license attribution 4.0 international (cc by 4.0)."
"a domain specific question answering (qa) dataset dramatically improves the machine comprehension performance. this paper presents a new global banking standards qa dataset (gbs-qa) in the banking regulation domain. the gbs-qa has three values. first, it contains actual questions from market players and answers from global rule setter, the basel committee on banking supervision (bcbs) in the middle of creating and revising banking regulations. second, financial regulation experts analyze and verify pairs of questions and answers in the annotation process. lastly, the gbs-qa is a totally different dataset with existing datasets in finance and is applicable to stimulate transfer learning research in the banking regulation domain. © 2021 association for computational linguistics."
"this chapter presents a survey of some recent methods used in economics and finance to account for cyclical dependence and account for their multifaced dynamics: nonlinearities, extreme events, asymmetries, non-stationarity, time-varying moments. to circumvent the caveats of the standard spectral analysis, new tools are now used based on copula spectrum, quantile spectrum and laplace periodogram in both non-parametric and parametric contexts. the chapter presents a comprehensive overview of both theoretical and empirical issues as well as a computational approach to explain how the methods can be implemented using the r package. © 2021, springer nature switzerland ag."
"in many areas of science, including business disciplines, statistical decisions are often made almost exclusively at a conventional level of significance. serious concerns have been raised that this contributes to a range of poor practices such as p-hacking and data-mining that undermine research credibility. in this paper, we present a decision-theoretic approach to choosing the optimal level of significance, with a consideration of the key factors of hypothesis testing, including sample size, prior belief, and losses from type i and ii errors. we present the method in the context of testing for linear restrictions in the linear regression model. from the empirical applications in accounting, economics, and finance, we find that the decisions made at the optimal significance levels are more sensible and unambiguous than those at a conventional level, providing inferential outcomes consistent with estimation results, descriptive analysis, and economic reasoning. computational resources are provided with two r packages. © 2019 accounting foundation, the university of sydney"
"in this paper, we develop sindhi subjective lexicon using a merger of existing english resources: nrc lexicon, list of opinion words, sentiwordnet, sindhi-english bilingual dictionary, and collection of sindhi modifiers. the positive or negative sentiment score is assigned to each sindhi opinion word. afterwards, we determine the coverage of the proposed lexicon with subjectivity analysis. moreover, we crawl multi-domain tweet corpus of news, sports, and finance. the crawled corpus is annotated by experienced annotators using the doccano text annotation tool. the sentiment annotated corpus is evaluated by employing support vector machine (svm), recurrent neural network (rnn) variants, and convolutional neural network (cnn). © 2021 association for computational linguistics."
"amid much recent interest we discuss a variance gamma model for rugby union matches (applications to other sports are possible). our model emerges as a special case of the recently introduced gamma difference distribution though there is a rich history of applied work using the variance gamma distribution - particularly in finance. restricting to this special case adds analytical tractability and computational ease. our three-dimensional model extends classical two-dimensional poisson models for soccer. analytical results are obtained for match outcomes, total score and the awarding of bonus points. model calibration is demonstrated using historical results, bookmakers' data and tournament simulations.  © 2020 walter de gruyter gmbh, berlin/boston 2020."
"the use of conditional probabilities has gained in popularity in various fields such as medicine, finance, and imaging processing. this has occurred especially with the availability of large datasets that allow us to extract the full potential of the available estimation algorithms. nevertheless, such a large volume of data is often accompanied by a significant need for computational capacity as well as a consequent compilation time. in this article, we propose a low-cost estimation method: we first demonstrate analytically the convergence of our method to the desired probability and then we perform a simulation to support our point. © 2021 ali labriji et al."
"quantum computers are expected to surpass the computational capabilities of classical computers during this decade, and achieve disruptive impact on numerous industry sectors, particularly finance. in fact, finance is estimated to be the first industry sector to benefit from quantum computing not only in the medium and long terms, but even in the short term. this review paper presents the state of the art of quantum algorithms for financial applications, with particular focus to those use cases that can be solved via machine learning. © 2021 ieee"
"asset or security returns are an example of phenomena whose distributions still cannot be convincingly modeled in a parametric framework. james r. (jim) thompson (1938–2017) used a variety of nonparametric approaches to develop workable investing solutions in such an environment. we review his ground breaking exploration of the veracity of the capital asset pricing model (capm), and several nonparametric approaches to portfolio formulation including the simugram™, variants of his max-median rule, and tukey weightings. this article is categorized under: applications of computational statistics > computational finance statistical and graphical methods of data analysis > transformations statistical and graphical methods of data analysis > modeling methods and algorithms. © 2020 wiley periodicals llc."
"value at risk is a device to measure the maximum possible loss when the right tail distribution is ignored. since uncertain random variables provide a tool to deal with phenomena in which uncertainty and randomness simultaneously exist, this paper proposes a computational approach for value at risk of uncertain random variables. in fact, a chance distribution can be expressed based on expectation with respect to probability distribution functions. thus, chance distributions are computed via monte carlo simulation. and consequently, value at risk is obtained via statistical quintile index. as an application in finance, portfolio selection problems of uncertain random returns are optimized via mean–value at risk models. © 2021, the author(s), under exclusive licence to springer-verlag gmbh, de part of springer nature."
"continuous-time separated linear programming problems have a wide range of real-world applications such as in business, economics, finances, communications, manufacturing, and so on. in this paper, we extend the technique that is presented by wen et al. [1] for two classes of these problems. we introduce both primal and dual models for separated problems. in addition, by using discrete problems we obtain approximate solutions with error bounds. moreover, we establish a computational procedure, to solve any separated continuous-time model and any state-constrained separated model. furthermore, after we put the separated problem in the form that is presented by wen et al. [1], we conclude that approximate solutions converge to an optimal solution for continuous-time separated problems. © published under licence by iop publishing ltd."
"structured meshes are widely used for scientific computations such as computational fluid dynamics (cfd) applications or finance. modern applications often have grid points in the millions. to perform such computations parallelisation is crucial. however it is unfeasible to port each application every time a new architecture arrives, hence in recent years the demand for automatic parallelisation and optimisation for the used hardware is increasing. the ops (oxford parallel library for structured mesh solvers) has shown good performance and scaling on a wide range of hpc architectures. this research aims to extend the ops framework with a sycl backend to extend the range of architectures that ops can support and further increase performance portability of ops applications. the performance of the intel oneapi is struggling with reductions due to high synchronisation cost, but shows promising performance gain on builtin reduction constructs on an intel® xeon® gold 6226r. we compare the performance of hipsycl on nvidia v100 gpu to the cuda implementations.  ©2021 ieee."
"machine learning (ml) algorithms are used in many fields such as finance, education, industry, medicine and e-commerce. ml algorithms show performance differences depending on the dataset and processing steps. choosing the right algorithm, preprocessing and postprocessing methods has great importance to achieve good results. in this paper, random forest classifier, logistic regression classifier, decision tree classifier, linear discriminant analysis, k-nearest neighbor classifier and svc methods are compared to predict mobile phone price class. the ""mobile price classification""dataset which is taken from kaggle.com is used to evaluate methods. firstly, all values at the dataset are checked to verify that there are no missing values. after that, scaling is applied to dataset in order to obtain more relevant data for ml algorithms. then, feature selection methods which reduce the computational cost by reducing the number of inputs are performed to get meaningful features. finally, the parameters of classification algorithms are tuned to improve the system accuracy. according to obtained results, it is seen that anova f-test feature selection method is more convenient for this dataset. it gives satisfying accuracy with a minimum number of features. it is also seen that the svc classifier has the highest test accuracy compared to other models.  © 2021 ieee."
"purpose: the purpose of this study is to thoroughly review studies that have used blockchain technology in financial services. this study will help provide a holistic framework that would highlight the current state and challenges of the blockchain in the financial services sector. design/methodology/approach: the objective of this study is to systematically examine and organize the current body of research literature that either quantitatively or qualitatively explored the use of blockchain technology in financial services. the study uses prisma-guided systematic review along with bibliometric analysis to achieve the purpose. findings: this study contributes to the existing literature by exploring and analyzing systematic studies available on blockchain with special reference to financial services sector. with blockchain based on five principles, namely, computational logic, peer-to-peer transmission, irreversibility of records, distributed database and transparency with pseudonym has immense potential to unleash and transform the financial service industry. with increasing blockchain-based operations of decentralized banking, insurance, trade finance, financial markets and cryptocurrency market, the subject is rapidly growing and seeking considerable contribution from scholars from around the world. research limitations/implications: this study uses systematic literature review approach, which has its own demerits. like other studies based on systematic literature review, this study also suffers from a certain bias such as sample selection bias, publication bias, data interpretation and the combination of quantitative and qualitative studies in the population. further, the adoption and resultant benefits of blockchain have not been empirically tested. practical implications: this study can help policymakers and institutions in determining their future course of action, as it highlights the state of research in the area of blockchain technology and financial services. originality/value: very few studies have done a comprehensive review of literature on blockchain in financial services. © 2020, emerald publishing limited."
"we propose a novel monte carlo simulation method for two-dimensional stochastic differential equation (sde) systems based on approximation through continuous-time markov chains (ctmcs). specifically, we propose an efficient simulation framework for asset prices under general stochastic local volatility (slv) models arising in finance, which includes the heston and the stochastic alpha beta rho (sabr) models as special cases. our simulation algorithm is constructed based on approximating the latent stochastic variance process by a ctmc. compared with time-discretization schemes, our method exhibits several advantages, including flexible boundary condition treatment, weak continuity conditions imposed on coefficients, and a second order convergence rate in the spatial grids of the approximating ctmc under suitable regularity conditions. replacing the stochastic variance process with a discrete-state approximation greatly simplifies the direct sampling of the integrated variance, thus enabling a highly efficient simulation scheme. extensive numerical examples illustrate the accuracy and efficiency of our estimator, which outperforms both biased and unbiased simulation estimators in the literature in terms of root mean squared error (rmse) and computational time. this paper is focused primarily on the simulation of sdes which arise in finance, but this new simulation approach has potential for applications in other contextual areas in operations research, such as queuing theory. © 2020 elsevier b.v."
"computation outsourcing is an integral part of cloud computing. it enables end-users to outsource their computational tasks to the cloud and utilize the shared cloud resources in a pay-per-use manner. however, once the tasks are outsourced, the end-users will lose control of their data, which may result in severe security issues especially when the data is sensitive. to address this problem, secure outsourcing mechanisms have been proposed to ensure security of the end-users' outsourced data. in this paper, we investigate outsourcing of general computational problems which constitute the mathematical basics for problems emerged from various fields such as engineering and finance. to be specific, we propose affine mapping based schemes for the problem transformation and outsourcing so that the cloud is unable to learn any key information from the transformed problem. meanwhile, the overhead for the transformation is limited to an acceptable level compared to the computational savings introduced by the outsourcing itself. furthermore, we develop cost-aware schemes to balance the trade-offs between end-users' various security demands and computational overhead. we also propose a verification scheme to ensure that the end-users will always receive a valid solution from the cloud. our extensive complexity and security analysis show that our proposed cost-aware secure outsourcing (caso) scheme is both practical and effective. © 2008-2012 ieee."
"we consider integration with respect to a d-dimensional spherical gaussian measure arising from computational finance. importance sampling (is) is one of the most important variance reduction techniques in monte carlo (mc) methods. in this paper, two kinds of is are studied in randomized quasi-mc (rqmc) setting, namely, the optimal drift is (odis) and the laplace is (lapis). traditionally, the lapis is obtained by mimicking the behavior of the optimal is density with odis as its special case. we prove that the lapis can also be obtained by an approximate optimization procedure based on the laplace approximation. we study the promises and limitations of is in rqmc methods and develop efficient rqmc-based is procedures. we focus on how to properly combine is with conditional mc (cmc) and dimension reduction methods in rqmc. in our procedures, the integrands are first smoothed by using cmc. then the lapis or the odis is performed, where several orthogonal matrices are required to be chosen to reduce the effective dimension. intuitively, designing methods to determine all these optimal matrices seems infeasible. fortunately, we prove that as long as the last orthogonal matrix is chosen elaborately, the choices of the other matrices can be arbitrary. this helps to significantly simplify the rqmc-based is procedure. due to the robustness and the superiority in efficiency of the gradient principal component analysis (gpca) method, we use the gpca method as an effective dimension reduction method in our rqmc-based is procedures. moreover, we prove the integrands obtained by the gpca method are statistically equivalent. numerical experiments illustrate the superiority of our proposed rqmc-based is procedures.  © 2021 society for industrial and applied mathematics."
"to know the future is to know the past. the ability to properly estimate the future of a system is an elusive problem. researchers have developed many tools to do just that, but a unified approach does not exist. intertemporal causalities are main signages for predictions in computational finance. here, since past value of a variable is highly correlated with the present and future of that variable, time series data analytics is much sought after modality for predictions. for a large temporal data set, time period bias is a very common sampling error, resulting in circumstance-specific unique observations only. experts cannot extend such observations to a larger industry with wider problem spaces. in this paper, we propose a solution to fit any time series data, with an aim to eliminate the time period bias. in this work, we have created a system that meshes previously created systems such as arima, arma, and ar. this helps to create a dynamic system that conforms to the specified time series data and modulates to create a specialized architecture for future prediction. we have taken test cases with varying hyperparameters and found a median accuracy of 94.95 % with a minimum delay in the training of 7 days and a median delay in training the model of 60 days.  © 2021 ieee."
"sovereign debt and currencies play an increasingly influential role in the development of any country, given the need to obtain financing and establish international relations. a recurring theme in the literature on financial crises has been the prediction of sovereign debt and currency crises due to their extreme importance in international economic activity. nevertheless, the limitations of the existing models are related to accuracy and the literature calls for more investigation on the subject and lacks geographic diversity in the samples used. this article presents new models for the prediction of sovereign debt and currency crises, using various computational techniques, which increase their precision. also, these models present experiences with a wide global sample of the main geographical world zones, such as africa and the middle east, latin america, asia, europe, and globally. our models demonstrate the superiority of computational techniques concerning statistics in terms of the level of precision, which are the best methods for the sovereign debt crisis: fuzzy decision trees, adaboost, extreme gradient boosting, and deep learning neural decision trees, and for forecasting the currency crisis: deep learning neural decision trees, extreme gradient boosting, random forests, and deep belief network. our research has a large and potentially significant impact on the macroeconomic policy adequacy of the countries against the risks arising from financial crises and provides instruments that make it possible to improve the balance in the finance of the countries. © 2021 by the authors. licensee mdpi, basel, switzerland."
"the present work aims to adapt the artificial organic networks (aon), a nature-inspired, supervised, metaheuristic, machine learning class, for computational finance purposes, applied as an efficient stock market index forecasting model. thus, the proposed model aims to forecast a stock market index, with the aid of other economic indicators, employing a historic dataset of at least eleven years for all the variables. to accomplish this, a target function is proposed: a multiple non-linear regressive model. the relevance of computational finance is discussed, pointing out that is an area that has developed significantly in the last decades with different applications, some of these are: rich portfolio optimization, index-tracking, credit risk, stock investment, among others. specifically, the index tracking problem (itp) concerns the prediction of stock market prices, being this a complex problem of the kind np-hard. in this work, is discussed the undertaken innovative approach to implement the aon method, its main properties, as well as its implementation using the topology defined as artificial hydrocarbon network (ahn), to tackle the itp. finally, we present the results of using a hybrid method based on k-means and the ahn configuration; within the result, the relative error obtained with this hybrid method was 0.0057. © 2021, springer nature switzerland ag."
"due to intense competition and lack of real estate on the front page of large e-commerce platforms, sellers are sometimes motivated to garner non-genuine signals (clicks, add-to-carts, purchases) on their products, to make them appear more appealing to customers. this hurts customers' trust on the platform, and also hurts genuine sellers who sell their items without looking to game the system. while it is important to find the sellers and the buyers who are colluding to garner these non-genuine signals, doing so is highly nontrivial. firstly, the set of bad actors in the system is a very small fraction of all the buyers/sellers on the platform. secondly, bad actors “hide” with the good ones, making them hard to detect. in this paper, we develop congcn, a context aware heterogeneous graph convolutional network to detect bad actors on a large heterogeneous graph. while our method is motivated by abuse detection in e-commerce, the method is applicable to other areas such as computational biology and finance, where large heterogeneous graphs are pervasive, and the amount of labeled data is very limited. we train congcn via novel sampling methods, and context aware message passing in a semi-supervised fashion to predict dishonest buyers and sellers in e-commerce. extensive experiments show that our method is effective, beating several baselines; generalizable to an inductive setting and highly scalable. copyright © 2021, association for the advancement of artificial intelligence (www.aaai.org). all rights reserved"
"real life problems are inside a complex environment. they are typically ill-defined problems; that is, the goals are not definite, we don’t know what counts as an alternative and how many alternatives there are, it’s unclear what the consequences might be and how to estimate their probabilities and utilities. jim savage dubbed this environment characterized by uncertainty as large world. small worlds instead are in principle predictable and without surprises and they are characterized by the knowledge of all relevant variables, their consequences and probabilities. the conditions of small world are the requirements of neoclassical rationality, as herbert simon stressed in his nobel lecture. in these worlds the problems may be well-defined but they can also be computationally intractable. as is well known, an example of a computational tractable problem is the dice game. instead the well-defined problem of the chess game is computationally intractable. in any case the real world is most of the time large and these conditions of knowledge are rarely met. since they are rarely met, the normative rational requirements of neoclassical economics are unjustified and the application of their theories can easily lead to a disaster. is finance an example of large world? or are there financial phenomena that may be considered examples of small worlds? and in this case may they be dealt with by rational tools such as probabilistic reasoning and utility maximization? and if this is the case, what is the role of financial literacy and education? is financial literacy sufficient to empower the financial decision making of savers and investors or should it be strengthened by training them also in behavioural finance and “debiasing” techniques? and can financial literacy avoid including risk literacy, which is the technique used to reason easily about probability calculus and statistics?. © riccardo viale, umberto filotto, barbara alemanni and shabnam mousavi 2021."
"recent years have brought a revolution in the field of artificial intelligence on an unprecedented scale. advances in hardware, availability of large data sets, as well as innovation in architectural and algorithmic design, enabled successful application of machine learning models based on multi-layered artificial neural networks to a variety of problems of practical interest. unsupervised problems, as well as applications outside of mainstream computer science, such as computational social science, psychometrics, econometrics, people analytics, stock market prediction, social engineering, biology, and even art became the new frontiers for deep neural networks. we believe that advancements in neural information processing systems will likely revolutionize the field of finance in the coming years. this chapter provides an introduction to the basic ideas, methods, and architectures on which most modern neural ai systems are based. after reading this chapter, the reader should gain appreciation and understanding of neural ai systems, and anticipate future developments in research and applications of ai, and deep learning in particular. the appendix grounds the main concepts presented here by combining them in a case study involving the design of a real-world neural ai system. applications of introduced concepts to alternative finance are stressed throughout. the reader should anticipate a high impact of deep learning systems within alternative finance in the coming years. this chapter, together with the appendix, form a good basis for understanding the core principles behind these future applications of ai in alternative finance, and will enable the reader to grasp the main themes that are likely to persist in the near future. © the editor(s) (if applicable) and the author(s) 2021."
"this paper examines direction-of-change predictability in commodity futures markets using a variety of binary probabilistic techniques. as well as traditional techniques, we apply variable length markov chain (vlmc) analysis, an innovative technique popularised in computational biology when predicting dna sequences (bühlmann & wyner, 1999). to the best of our knowledge, this is the first application of vlmc in finance. our results show that both vlmc and technical analysis methods provide strong predictability of the direction-of-change of commodity returns, with annualised mean returns of approximately 8%, substantially higher than the passive long strategy. our results suggest that a short-term learning effect is present in commodities market which can be exploited using innovative direction-of-change forecasting techniques. © 2021 elsevier inc."
"machine learning and data science in the power generation industry explores current best practices and quantifies the value-add in developing data-oriented computational programs in the power industry, with a particular focus on thoughtfully chosen real-world case studies. it provides a set of realistic pathways for organizations seeking to develop machine learning methods, with a discussion on data selection and curation as well as organizational implementation in terms of staffing and continuing operationalization. it articulates a body of case study-driven best practices, including renewable energy sources, the smart grid, and the finances around spot markets, and forecasting. © 2021 elsevier inc. all rights reserved."
"one goal of financial research is to determine fair prices on the financial market. as financial models and the data sets on which they are based are becoming ever larger and thus more complex, financial instruments must be further developed to adapt to the new complexity, with short runtimes and efficient use of memory space. here we show the effects of combining known strategies and incorporating new ideas to further improve numerical techniques in computational finance. in this paper we combine an adi (alternating direction implicit) scheme for the temporal discretization with a sparse grid approach and the combination technique. the later approach considerably reduces the number of ""spatial""grid points. the presented standard financial problem for the valuation of american options using the heston model is chosen to illustrate the advantages of our approach, since it can easily be adapted to other more complex models.  © 2021 global science press."
"time series modelling and forecasting techniques have a wide spectrum of applications in several fields including economics, finance, engineering and computer science. most available modelling and forecasting techniques are applicable to a specific underlying phenomenon and its properties and lack generality of application, while more general forecasting techniques require substantial computational time for training and application. herewith, we present a general modelling framework based on a recursive schur - complement technique, that utilizes a set of basis functions, either linear or non-linear, to form a model for a general time series. the basis functions need not be orthogonal and their number is determined adaptively based on fitting accuracy. moreover, no assumptions are required for the input data. the coefficients for the basis functions are computed using a recursive pseudoinverse matrix, thus they can be recomputed for different input data. the case of sinusoidal basis functions is presented. discussions around stability of the resulting model and choice of basis functions is also provided. numerical results depicting the applicability and effectiveness of the proposed technique are given. © 2021, springer nature switzerland ag."
"researchers working on maximum likelihood estimation (mle) of dynamic term structure models report facing numerous computational challenges when attempting optimization. this has initiated our research on the usage of machine learning methods specific to the sub-field of deep learning for developing optimization models that can overcome the computational issues. in this work, we use these advances to design an algorithm framework reliant upon an unsupervised neural network to conduct mle of a macro-finance model to assess the impact of changes in key variables influencing monetary policy on a single interest rate associated with a policy rule that governs all nineteen states comprising the euro area. then, we investigate the dynamic nature of these effects by training the variables as predictors of yield convergence to a single interest rate (i.e., signal response) and use the resultant trained machine learning model to generate interest rate predictions. finally, we group the predictions into clusters using density-based spatial clustering of applications with noise algorithm. the results we get suggest that proper training of key policy indicators (i.e., predictors) can yield convergence to a single interest rate (i.e., signal). however, our findings also indicate a strong disparity in variation in the coefficients driving the reaction of the signal to predictors across the member states, which suggests that achieving such a convergence in practice may be a challenge. © 2021 ieee."
"in this paper, we introduce a numerical method for nonlinear parabolic partial differential equations (pdes) that combines operator splitting with deep learning. it divides the pde approximation problem into a sequence of separate learning problems. since the computational graph for each of the subproblems is comparatively small, the approach can handle extremely high dimensional pdes. we test the method on different examples from physics, stochastic control, and mathematical finance. in all cases, it yields very good results in up to 10,000 dimensions with short run times. © 2021 society for industrial and applied mathematics"
"this paper describes the performance of the team cs60075 team2 at semeval 2021 task 1 - lexical complexity prediction. the main contribution of this paper is to fine-tune transformer-based language models pre-trained on several text corpora, some being general (e.g., wikipedia, bookscorpus), some being the corpora from which the complex dataset was extracted, and others being from other specific domains such as finance, law, etc. we perform ablation studies on selecting the transformer models and how their individual complexity scores are aggregated to get the resulting complexity scores. our method1 achieves a best pearson correlation of 0.784 in sub-task 1 (single word) and 0.836 in sub-task 2 (multiple word expressions). © 2021 association for computational linguistics."
"many applications in the areas of finance, environments, service systems, and statistical learning require the computation for a general function of the expected performances associated with one or many random objects. when complicated stochastic systems are involved, such computation needs to be done by stochastic simulation and the computational cost can be expensive. we design simulation algorithms that exploit the common random structure shared by all random objects, referred to as stratified splitting. we discuss the optimal simulation budget allocation problem for the proposed algorithms. a brief numerical experiment is conducted to illustrate the performance of the proposed algorithm with various budget allocation rules.  © 2021 ieee."
"fuzzy numbers play an important part in the advanced theory of computational finance, which is undergoing a revolution aided by the powerful simulation and data-driven models. the volatility parameter is the most important parameter for pricing short-term options and arguably one of the important parameters for all financial time series. traditional option pricing models determine the option's expected return without taking into account the uncertainty associated with the underlying asset price volatility. fuzzy set theory can be used to explicitly account for such uncertainty. there has been a growing interest in studying fuzzy option pricing for european options and binary options using hybrid models. however, those fuzzy coefficient hybrid models are not data-driven. this paper uses data-driven volatility forecasts to study fuzzy european option pricing. a simple yet effective fuzzy option pricing model incorporating data-driven exponentially weighted moving average (ewma) and neuro volatility models is presented to obtain fuzzy volatility forecasts and fuzzy option prices. the dynamics of the underlying assets is complex and hence it is preferable to use the monte carlo (mc) simulation, to get rid of the assumptions in the commonly used black scholes (bs) models. using the data-driven fuzzy a-cuts of the annualized volatility, a-cuts of the call and put option prices based on novel mc simulation are obtained using $t$ or laplace distributed asset log returns. the driving idea, unlike the existing mc option pricing with normality assumption, is that the proposed novel mc method uses the data-driven $t$ or laplace distribution of asset log returns and nonlinear adaptive fuzzy volatility. © 2021 ieee."
"the exponential rise in software computing and internet technologies have broadened the horizon of cloud computing applications serving numerous purposes like business processes, healthcare, finance, socialization, etc. in the last few years the increase is security breaches and unauthorized data access has forced industry to achieve computationally efficient and robust security system. the increase in multimedia data communication over different cloud applications too demands an efficient security model, which is expected to have low computational complexity, negligible quality-compromise and higher security robustness. major conventional security-systems like cryptography and steganography undergo high computational overhead, thus limiting their potential towards cloud-communication where each data input used to be of large size and a gigantic amount of multimedia data is shared across the network. to alleviate above stated problems and enable a potential solution, in this paper a highly robust lightweight feistel structure based substitution permutation crypto model is developed for multimedia data security over uncertain cloud environment. our proposed model applies substitution permutation crypto concept with feistel structure which performs substitution-permutation over five rounds to achieve higher confusion and diffusion. to retain higher security with low computation, we applied merely 64-bit block cipher and equal key-size. matlab based simulation revealed that the proposed lightweight security model achieves better attack-resilience even maintaining low entropy, high-correlation, and satisfactory computation time for multimedia data encryption. such robustness enables our proposed security model to be applied for real-world cloud data security. © 2020, springer science+business media, llc, part of springer nature."
"in this paper, we describe our design of acm30070 ""computational finance"", a core module in the bsc in financial mathematics in the school of mathematics and statistics. the over-arching purpose of this module is to help students to develop mathematical, statistical and coding skills, along with significant knowledge and critical thinking, that allows them to effectively construct, manipulate and visualize financial datasets and to build financial mathematical models. the use of computation and a fintech software (fincad analytics) are pointed out as essential to facilitate sensemaking in computational finance. more broadly, we discuss the education-research based rationale behind the ""learning by doing"" and ""flipped classroom"" institutional models that we have chosen for acm30070, and we show how the modern ""inclusive"" definition of computation has been embedded into the learning activities. an accurate description of the design principles and implementation is also presented. at the end of the paper, we briefly introduce a discipline-based education research that will follow from this module design. © 2021. international conference on higher education advances. all rights reserved."
"the davenport spectrum is a modification of the classical kolmogorov spectrum for the inertial range of turbulence that accounts for non-scaling low frequency behavior. like the classical fractional brownian motion vis-à-vis the kolmogorov spectrum, tempered fractional brownian motion (tfbm) is a new model that displays the davenport spectrum. the autocorrelation of the increments of tfbm displays semi-long range dependence (hyperbolic and quasi-exponential decays over moderate and large scales, respectively), a phenomenon that has been observed in a wide range of applications from wind speeds to geophysics to finance. in this paper, we use wavelets to construct the first estimation method for tfbm and a simple and computationally efficient test for fbm vs tfbm alternatives. the properties of the wavelet estimator and test are mathematically and computationally established. an application of the methodology shows that tfbm is a better model than fbm for a geophysical flow data set. © 2019 elsevier inc."
"rural territories of ukraine are locked in a “circle of decline” by two mutually reinforcing trends: firstly, the lack of jobs and sustainable business activity, and secondly, the lack of positive changes in the service sector for the rural population. this is often conditioned upon the low development of infrastructural facilities. this causes the need to finance modern, efficient and environmentally friendly rural infrastructure, which will increase the comfort and quality of living, stop the outflow of the working-age population to cities,improve the demographic situation in rural areas, and contribute to the development of the rural economy. therefore, the research is aimed at diagnosing the current state of infrastructure support for rural areas and finding effective tools to eliminate the causes of their socio-economic decline. the research methodology is based on the use of such general scientific research methods as economic analysis and synthesis in the interpretation of statistical data sets, comparison in determining dynamic changes in the socio-demographic characteristics of rural areas, logical method in making generalisations and conclusions, graphical and computational-constructive for constructing a lorentz curve, etc. the essence, definition and types of infrastructure are established. the current level of socio-economic development of rural areas of ukraine is clarified and its interdependence with infrastructure support is discovered. the level of transport, housing and communal services, medical, cultural and educational, trade,and business infrastructure development is described.the paper highlights the experience of stimulating rural development in the eu countries, in particular, outlines financial instruments and the scope of support and comprehensive assistance to rural businesses, environmental protection, competitiveness, and social integration. european investment funds and their role in the development of eu rural infrastructure are described copyright © the author(s). this is an open access article distributed under the terms of the creative commons attribution license 4.0 (https://creativecommons.org/licenses/by/4.0/)"
"fraud detection in finance such as finding fraudulent credit card or debit card transactions is still an open research problem and the solutions are evolving. in a financial transactions database, the number of fraudulent transactions will be very less which creates the class imbalance. imbalanced data always causes serious challenges to a fraud detection model. existing solutions such as undersampling and oversampling alleviate class imbalance problem but still they have lot of limitations. for example, oversampling demands significant computational time and undersampling suffer from loss of samples containing critical information related to majority class, in turn leads to poor generalization of the fraud detection model. we developed an entropy-based undersampling with dynamic stacked ensemble model for fraud detection, which we named as eustack. to achieve undersampling, it evaluates the information content from each sample using shannon entropy and selects the most informative subset of samples from the majority class. a two-level stacked ensemble is combined with this new undersampling method to improve the generalization performance of the fraud detection model. the credit card transactions dataset by 'worldline and the machine learning group of ulb, hosted at kaggle' was used to verify the robustness of eustack. the dataset is highly imbalanced with 492 fraudulent transactions (0.172%) out of 284,807 transactions. eustack was evaluated using f1 score and matthews correlation coefficient (mcc). experimental results demonstrate that it achieved high f1 (0.88) and mcc (0.88) scores when compared to the conventional undersampling based fraud detection methods.  © 2021 ieee."
"the sheer volume of financial statements makes it difficult for humans to access and analyze a business's financials. robust numerical reasoning likewise faces unique challenges in this domain. in this work, we focus on answering deep questions over financial data, aiming to automate the analysis of a large corpus of financial documents. in contrast to existing tasks on general domain, the finance domain includes complex numerical reasoning and understanding of heterogeneous representations. to facilitate analytical progress, we propose a new large-scale dataset, finqa, with question-answering pairs over financial reports, written by financial experts. we also annotate the gold reasoning programs to ensure full explainability. we further introduce baselines and conduct comprehensive experiments in our dataset. the results demonstrate that popular, large, pre-trained models fall far short of expert humans in acquiring finance knowledge and in complex multi-step numerical reasoning on that knowledge. our dataset - the first of its kind - should therefore enable significant, new community research into complex application domains. the dataset and code are publicly available. © 2021 association for computational linguistics"
"data marketplaces are expected to play a crucial role in tomorrow’s data economy but hardly achieve commercial exploitation. currently, there is no clear understanding of the knowledge gaps in data marketplace research, especially neglected research topics that may contribute to advancing data marketplaces towards commercialization. this study provides an overview of the state of the art of data marketplace research. we employ a systematic literature review (slr) approach and structure our analysis using the service-technology-organization-finance (stof) model. we find that the extant data marketplace literature is primarily dominated by technical research, such as discussions about computational pricing and architecture. to move past the first stage of the platform’s lifecycle (i.e., platform design) to the second stage (i.e., platform adoption), we call for empirical research in non-technological areas, such as customer expected value and market segmentation. © 2021 42nd international conference on information systems, icis 2021 treos: ""building sustainability and resilience with is: a call for action"". all rights reserved."
"artificial intelligence (ai) has been widely used in manufacturing, healthcare, sports, finance, and other areas to model nonlinearities and make reliable predictions. in manufacturing, ai has been applied to improve processes, reduce costs, and increase reliability. a novel manufacturing process that has been augmented with ai is incremental sheet forming (isf), a technology that applies a step-by-step incremental feed to a sheet metal or polymer blank using a cnc machine. the quality of the produced part is affected by parameters related to four process elements: the blank, the blank holder, the forming tool, and the cnc machine (applied force). the isf process is greatly affected by forming process parameters, material property parameters, and geometric parameters. numerous research efforts have correlated the relationship between the isf parameters to final product attributes using analytical, experimental, and numerical techniques. however, these techniques are not efficient due to the nonlinearities and complexities of the relationships, the time-consuming nature of the process, and extensive computational time needed to simulate the process. to compensate for these shortcomings, researchers have started to apply ai techniques in analysis of isf. the aim of this paper is to review the application of ai in the isf process in forming of metal sheets in order to summarize the contributions of prior research efforts and identify potential opportunities for future research. © 2021 the authors. published by elsevier b.v."
"data marketplaces are expected to play a crucial role in tomorrow’s data economy but hardly achieve commercial exploitation. currently, there is no clear understanding of the knowledge gaps in data marketplace research, especially neglected research topics that may contribute to advancing data marketplaces towards commercialization. this study provides an overview of the state of the art of data marketplace research. we employ a systematic literature review (slr) approach and structure our analysis using the service-technology-organization-finance (stof) model. we find that the extant data marketplace literature is primarily dominated by technical research, such as discussions about computational pricing and architecture. to move past the first stage of the platform’s lifecycle (i.e., platform design) to the second stage (i.e., platform adoption), we call for empirical research in non-technological areas, such as customer expected value and market segmentation. © 2021 34th bled econference: digital support from crisis to progressive change, bled 2021 - proceedings. all rights reserved."
"pricing interest-rate financial derivatives is a major problem in finance, in which it is crucial to accurately reproduce the time evolution of interest rates. several stochastic dynamics have been proposed in the literature to model either the instantaneous interest rate or the instantaneous forward rate. a successful approach to model the latter is the celebrated heath-jarrow-morton framework, in which its dynamics is entirely specified by volatility factors. in its multifactor version, this model considers several noisy components to capture at best the dynamics of several time-maturing forward rates. however, as no general analytical solution is available, there is a trade-off between the number of noisy factors considered and the computational time to perform a numerical simulation. here, we employ the quantum principal component analysis to reduce the number of noisy factors required to accurately simulate the time evolution of several time-maturing forward rates. the principal components are experimentally estimated with the five-qubit ibmqx2 quantum computer for 2×2 and 3×3 cross-correlation matrices, which are based on historical data for two and three time-maturing forward rates. this paper is a step towards the design of a general quantum algorithm to fully simulate on quantum computers the heath-jarrow-morton model for pricing interest-rate financial derivatives. it shows indeed that practical applications of quantum computers in finance will be achievable in the near future.  © 2021 authors. published by the american physical society. published by the american physical society under the terms of the creative commons attribution 4.0 international license. further distribution of this work must maintain attribution to the author(s) and the published article's title, journal citation, and doi."
"this article offers a novel perspective on the implications of increasingly autonomous and “black box” algorithms, within the ramification of algorithmic trading, for the integrity of capital markets. artificial intelligence (ai) and particularly its subfield of machine learning (ml) methods have gained immense popularity among the great public and achieved tremendous success in many real-life applications by leading to vast efficiency gains. in the financial trading domain, ml can augment human capabilities in price prediction, dynamic portfolio optimization, and other financial decision-making tasks. however, thanks to constant progress in the ml technology, the prospect of increasingly capable and autonomous agents to delegate operational tasks and even decision-making is now beyond mere imagination, thus opening up the possibility for approximating (truly) autonomous trading agents anytime soon. given these spectacular developments, this article argues that such autonomous algorithmic traders may involve significant risks to market integrity, independent from their human experts, thanks to self-learning capabilities offered by state-of-the-art and innovative ml methods. using the proprietary trading industry as a case study, we explore emerging threats to the application of established market abuse laws in the event of algorithmic market abuse, by taking an interdisciplinary stance between financial regulation, law and economics, and computational finance. specifically, our analysis focuses on two emerging market abuse risks by autonomous algorithms: market manipulation and “tacit” collusion. we explore their likelihood to arise in global capital markets and evaluate related social harm as forms of market failures. with these new risks in mind, this article questions the adequacy of existing regulatory frameworks and enforcement mechanisms, as well as current legal rules on the governance of algorithmic trading, to cope with increasingly autonomous and ubiquitous algorithmic trading systems. we demonstrate how the “black box” nature of specific ml-powered algorithmic trading strategies can subvert existing market abuse laws, which are based upon traditional liability concepts and tests (such as “intent” and “causation”). we conclude by addressing the shortcomings of the present legal framework and develop a number of guiding principles to assist legal and policy reform in the spirit of promoting and safeguarding market integrity and safety. © 2021 university of pennsylvania law school. all rights reserved."
"deep learning provides useful insights by analyzing information especially in the field of finance with advanced computing technology. although, rnn–lstm network with the advantage of sequential learning has achieved great success in the past for time series prediction. conversely, developing and selecting the best computational optimized rnn–lstm network for intra-day stock market forecasting is a real challenging task as a researcher. since it analyses the most volatile data, requires to cope with two big factors such as time lag and the large number of architectural hyperparameters that affect the learning of the model. furthermore, in addition to the design of this network, several former studies use trial and error based heuristic to estimate these factors which may not guarantee the most optimal network. this paper defines the solution to solve the above-mentioned challenging problems using the hybrid mechanism of the rnn–lstm network integrating with a metaheuristic optimization technique. for this, a two-hybrid approach namely rnn–lstm with flower pollination algorithm and rnn–lstm with particle swarm optimization has been introduced to develop an optimal deep learning model to enhance the intra-day stock market prediction. this model suggests a systematic method which helps us with an automatic generation of optimized network. as the obtained network with tuned hyper parametric values-led towards a more precise learning process with the minimized error rate and accuracy enhancement. in addition, the comparative results evaluated over six different stock exchange datasets reflect the efficacy of an optimized rnn–lstm network by attaining maximum forecasting accuracy approximately increment of 4–6% using the metaheuristic approach. © 2020, ohmsha, ltd. and springer japan kk, part of springer nature."
"interpretability methods like integrated gradient and lime are popular choices for explaining natural language model predictions with relative word importance scores. these interpretations need to be robust for trustworthy nlp applications in high-stake areas like medicine or finance. our paper demonstrates how interpretations can be manipulated by making simple word perturbations on an input text. via a small portion of word-level swaps, these adversarial perturbations aim to make the resulting text semantically and spatially similar to its seed input (therefore sharing similar interpretations). simultaneously, the generated examples achieve the same prediction label as the seed yet are given a substantially different explanation by the interpretation methods. our experiments generate fragile interpretations to attack two sota interpretation methods, across three popular transformer models and on three different nlp datasets. we observe that the rank order correlation drops by over 20% when less than 10% of words are perturbed on average. further, rank-order correlation keeps decreasing as more words get perturbed. furthermore, we demonstrate that candidates generated from our method have good quality metrics. our code is available at: github.com/qdata/textattack-fragile-interpretations. © 2021 association for computational linguistics."
"in recent years, machine learning (ml) algorithms have been applied in many areas such as healthcare, finance and autonomous vehicles. at the same time, there is an increasing need for making ml systems accountable, which would help deal with situations when these systems made wrong decisions or predictions. currently there exist three major frameworks for accountable ml: model card toolkit, datasheets, and factsheets. however, the greatest limitation of these frameworks is that they are mostly focusing on qualitative information about the machine learning models. in this research, we discuss in detail these three frameworks and future development directions of accountable ml frameworks; we recommend the implementation of causality, decision provenance, and computational tests for achieving better ml accountability. copyright © 2021 for this paper by its authors."
"the ever-growing financial, healthcare, manufacturing, and other sectors are generating volumes of big data. with recent progress in the area of cloud computing, many small as well as medium enterprises are moving to the cloud for serving the needs of organizations to store and compute their big data. cloud computing also provides these organizations with useful insights to data stored. different computational models are present for big data analytics but the four v’s (volume, velocity, variety, and variability) are still making the computation analytics a challenge. along with this, the introduction of cloud computing technology raised protection and security issues associated with the data as no other person should be able to gain access to the private data of the customers, especially in the field of finance, health, etc. many techniques for securing and protecting data on cloud have been introduced in the due course of time. in this chapter, introduction of cloud computing and big data as upcoming technologies is covered and also the relationship between the two technologies is discussed in detail. in this chapter, the need for retrieving information from widespread big data on the web along with the challenges for information retrieval in big data and cloud is discussed. the chapter also discusses the details about the security concerns of big data on the cloud platform followed by the different algorithms that can be used for the encryption and decryption of data. the chapter concludes with the case studies including the comparison of the results of various encryption techniques used for data storage on cloud. © 2022 selection and editorial matter dharmender saini, gopal chaudhary, and vedika gupta; individual chapters, the contributors."
"this study aims to introduce the application of artificial intelligence in the area of islamic finance by examining the dynamic changes of gross domestic product (gdp) under fully markup-based financing. the application of artificial intelligence using agent-based computational model (abm) is employed to conduct the simulation. the simulation result shows that the movement of gdp under markup-based financing which represents islamic financial system has better performance compared to interest-based lending. in this regard, profit shared to depositors has positive impact on gdp which also proofs that islamic banking system may promote sustainable economic growth and may create wealth for the whole society. this study proofs that islamic bank essentially more stable than conventional bank and hence may fights against crisis. this study is potentially the initial work to examine the dynamic changes of economic growth under fully islamic financial system by applying artificial intelligence concept as its methodology. thus, this study is expected to contribute to the development of islamic economics and finance research. © islamic monetary economics and finance. all rights reserved."
"this chapter discusses hybrid computational intelligence systems in various domains. the major techniques which are considered for the hybridization are the neural network, fuzzy logic, and genetic systems. various neuro-fuzzy systems for course selection for students, web page classifications, finding a suitable profile for matrimonial applications and job, student aptitude testing, diagnosis, and emotional detection are presented in this chapter. the fuzzy convolutional neural network for covid-19 analysis from various ct scanned lung images is also discussed in this chapter with necessary details such as neural network architecture, fuzzy membership functions, and training data sets. the chapter also discuss genetic fuzzy systems for fashion design and evolving neural network topologies with the required details. the genetic encoding, operations, neural network architecture, and fuzzy membership functions for these applications are discussed in detail. the other examples and applications demonstrated in the chapter include fuzzy collaborative filtering of movie recommendations, software quality evaluation using neuro-fuzzy-genetic hybridization, and introduction to the application of type-2 neuro-fuzzy system for the reliability of software products. for most of the applications included in this chapter, generic process flows and architectures of the system are illustrated graphically along with other details. these architecture are often having multiple layers and they are domain independent. the examples and applications discussed use these architectures to show the utilities of the architecture. at the end, the chapter enlists approximately 40 core and applied research and project possibilities in various areas. these applications include domain independent fuzzy activation function, fuzzy ontology as knowledge representation structure, idea generation framework, automatic knowledge discovery, elearning system, automatic evolution of decision trees, healthcare advisory systems, product recommendation, and trust based network, consumer modeling, game playing, finance, marketing, etc. © 2021, the author(s), under exclusive license to springer nature singapore pte ltd."
"artificial intelligence (ai) has dramatically changed the landscape of science, industry, defence, and medicine in the last several years. supported by considerably enhanced computational power and cloud storage, the field of ai has shifted from mostly theoretical studies in the discipline of computer science to diverse real-life applications such as drug design, material discovery, speech recognition, self-driving cars, advertising, finance, medical imaging, and astronomical observation, where ai-produced outcomes have been proven to be comparable or even superior to the performance of human experts. in these applications, what is essentially important for the development of ai is the data needed for machine learning. despite its prominent importance, the very first process of the ai development, namely data collection and data preparation, is typically the most laborious task and is often a limiting factor of constructing functional ai algorithms. lab-on-a-chip technology, in particular microfluidics, is a powerful platform for both the construction and implementation of ai in a large-scale, cost-effective, high-throughput, automated, and multiplexed manner, thereby overcoming the above bottleneck. on this platform, high-throughput imaging is a critical tool as it can generate high-content information (e.g., size, shape, structure, composition, interaction) of objects on a large scale. high-throughput imaging can also be paired with sorting and dna/rna sequencing to conduct a massive survey of phenotype-genotype relations whose data is too complex to analyze with traditional computational tools, but is analyzable with the power of ai. in addition to its function as a data provider, lab-on-a-chip technology can also be employed to implement the developed ai for accurate identification, characterization, classification, and prediction of objects in mixed, heterogeneous, or unknown samples. in this review article, motivated by the excellent synergy between ai and lab-on-a-chip technology, we outline fundamental elements, recent advances, future challenges, and emerging opportunities of ai with lab-on-a-chip technology or ""ai on a chip""for short. this journal is  © the royal society of chemistry."
"the field of artificial intelligence is currently experiencing relentless growth, with innumerable models emerging in the research and development phases across various fields, including science, finance, and engineering. in this work, the authors review a large number of learning techniques aimed at project management. the analysis is largely focused on hybrid systems, which present computational models of blended learning techniques. at present, these models are at a very early stage and major efforts in terms of development is required within the scientific community. in addition, we provide a classification of all the areas within project management and the learning techniques that are used in each, presenting a brief study of the different artificial intelligence techniques used today and the areas of project management in which agents are being applied. this work should serve as a starting point for researchers who wish to work in the exciting world of artificial intelligence in relation to project leadership and management. © 2021, universidad internacional de la rioja. all rights reserved."
"high-dimensional changepoint analysis is a growing area of research and has applications in a wide range of fields. the aim is to accurately and efficiently detect changepoints in time series data when both the number of time points and dimensions grow large. existing methods typically aggregate or project the data to a smaller number of dimensions, usually one. we present a high-dimensional changepoint detection method that takes inspiration from geometry to map a high-dimensional time series to two dimensions. we show theoretically and through simulation that if the input series is gaussian, then the mappings preserve the gaussianity of the data. applying univariate changepoint detection methods to both mapped series allows the detection of changepoints that correspond to changes in the mean and variance of the original time series. we demonstrate that this approach outperforms the current state-of-the-art multivariate changepoint methods in terms of accuracy of detected changepoints and computational efficiency. we conclude with applications from genetics and finance. © 2020, the author(s)."
"the analysis of dynamic economic models routinely leads to the mathematical problem of determining an unknown function for which no closed-form solution exists. economists must then resort to methods of numerical approximation when analyzing such models. among the computational methods that have been successfully applied in economics and finance, one set of techniques stands out due to its flexibility and robustness: projection methods. in this article, we describe the basic steps of these methods for several different applications, surveying many successful applications of projection methods to dynamic economic models. importantly, we emphasize that the ever-increasing complexity and dimensionality of dynamic models have made the previously used simpler methods obsolete and the applications of projection methods all but mandatory. we closely examine the most recent endeavors in the literature on solving economic models with projection methods. © 2020 annual reviews inc.. all rights reserved."
"although machine learning is frequently associated with neural networks, it also comprises econometric regression approaches and other statistical techniques whose accuracy enhances with increasing observation. what constitutes high quality machine learning is yet unclear though. proponents of deep learning (i.e. neural networks) value computational efficiency over human interpretability and tolerate the ‘black box’ appeal of their algorithms, whereas proponents of explainable artificial intelligence (xai) employ traceable ‘white box’ methods (e.g. regressions) to enhance explainability to human decision makers. we extend brooks et al.’s [2019. ‘financial data science: the birth of a new financial research paradigm complementing econometrics?’ european journal of finance 25 (17): 1627–36.] work on significance and relevance as assessment critieria in econometrics and financial data science to contribute to this debate. specifically, we identify explainability as the achilles heel of classic machine learning approaches such as neural networks, which are not fully replicable, lack transparency and traceability and therefore do not permit any attempts to establish causal inference. we conclude by suggesting routes for future research to advance the design and efficiency of ‘white box’ algorithms. © 2020 the author(s). published by informa uk limited, trading as taylor & francis group."
"in the present computer era, the vulnerabilities inherent in the internet architecture enable various kinds of attacks. distributed denial of service (ddos) is one of such prominent attack that is a lethal threat to internet domain that harnesses its computing and communication resources. the increase in network traffic rates of legitimate traffic and its flow similarity with attack traffic has made the ddos detection very difficult despite deployment of diversified defense solutions. the isps are bound to invest heavily to counter such problems which has a significant impact on company finances. to provide uninterrupted quality services to the end users, isps needs to deploy a distributed solution for timely detection and discrimination of attack and behaviorally similar flash events (fe) traffic. such distributed defense systems can be deployed at source-end, intermediate network-end or at the victim-end location. since the volume of traffic to be analyzed is very large, the detection accuracy and low computational complexity of the proposed defense solution is always a challenging problem. this paper proposes an isp level distributed, collaborative and automated (d-cad) defense system for detecting ddos attacks and fes, and has the capability to effectively distinguishing the two. additionally, d-cad defense system is also capable of categorizing fe traffic and has low computational complexity. the proposed system is validated in novel software defined networks (sdn) using mininet emulator. the results show that d-cad defense system outperformed its existing counterparts on various detection system evaluation metrics. © 2020, springer-verlag gmbh germany, part of springer nature."
"the internet of health things (ioht) is an extended version of the internet of things that is acting a starring role in data sharing remotely. these remote data sources consist of physiological processes, such as treatment progress, patient monitoring, and consultation. the main purpose of ioht platform is to intervene independently from geographically remote areas by providing low-cost preventive or active healthcare services. several low-power biomedical sensors with limited computing capabilities provide ioht's communication, integration, computation, and interoperability. however, ioht transfers iot data via ip-centric internet, which has implications for security and privacy. to address this issue, in this paper, we suggest using named data networking (ndn), a future internet model that is well suited for mobile patients and caregivers. as the ioht contains a lot of personal information about a user's physical condition, which can be detrimental to users' finances and health if leaked, therefore, data protection is important in the ioht. experts and scholars have researched this area, but the reconstruction of existing schemes could be further improved. also, doing computing-intensive tasks leads to slower response times, which further worsens the performance of ioht. we are trying to resolve such an error, so a new ndn-based certificateless signcryption scheme is proposed for ioht using the security hardness of the hyperelliptic curve cryptosystem. security analysis and comparisons with existing schemes show the viability of the designed scheme. the final results confirm that the designed scheme provides better security with minimal computational and communicational resources. finally, we validate the security of the designed scheme against man-in-the-middle attacks and replay attacks using the avispa tool.  © 2021 aroosa et al."
"branching processes form an important and wide subclass of stochastic processes. they have numerous applications in different scientific and practical areas, from biology and cell proliferationthrough epidemiology, physics and neutral chain reactions to finance and insurance. many of them involve multitype modeling. statistical estimation of the process' characteristics is an important issue in their study. the nature of the models require a big amount of data, which is often impossible to be observed, considering only the total population sizes. in order to obtain the maximum likelihood estimators when the information is not sufficient, we use approximation methods. in our studythe em algorithm serves as an effective instrument for calculation of the estimates when a part of data is hidden. we propose examples of two-type branching processes with multinomial offspring distributions and a software implementation of the em algorithm, illustrated via simulations and computational results.  © 2020 author(s)."
"purpose: the situations of covid-19 will certainly have an adverse effect over and above health care on factors of the internet of things (iot) market. to overcome all the above issues, iot devices and sensors can be used to track and monitor the movement of the people, so that necessary actions can be taken to prevent the spread of coronavirus disease (covid-19). mobile devices can be used for contact tracing of the affected person by analyzing the geomap of the travel history. this will prevent the spread and reset the economy to the normal condition. design/methodology/approach: to respond to the global covid-19 outbreak, the social-economic implications of covid-19 on specific dimensions of the global economy are analyzed in this study. the situations of covid-19 will certainly have an adverse effect over and above health care on factors of the iot market. to overcome these issues iot devices and sensors can be used to track and monitor the movement of the people so that necessary actions can be taken to prevent the spread of covid-19. mobile devices can be used for contact tracing of the affected person by analyzing the geomap of the travel history. this will prevent the spread and reset the economy to the normal condition. a few reviews, approaches, and guidelines are provided in this article along these lines. moreover, insights about the effects of the pandemic on various sectors such as agriculture, medical industry, finance, information technology, manufacturing and many others are provided. these insights may support strategic decision making and policy framing activities for the top level management in private and government sectors. findings: with insecurities of a new recession and economic crisis, key moments such as these call for strong and powerful governance in health, business, government, and large society. instant support measures have to be initiated and adapted for those who can drop through the cracks. mid- and long-term strategies are required to stabilize and motivate the economy during this recession. originality/value: a comprehensive social-economic development strategy that consists of sector by sector schemes and infrastructure that supports business to ensure the success of those with reliable and sustainable business models is necessary. from the literature analysis and real world observations it is concluded that the iot, sensors, wearable devices and computational technologies plays major role in preserving the economy of the country by preventing the spread of covid-19. © 2020, emerald publishing limited."
"an approach to calculating the measure of uncertainty in computational problems of management, economics and finance is proposed. the approach consists of direct and inverse problems. in the direct problem, the values of the uncertainty measure of the desired quantities are calculated from the known values of the uncertainty measure of known quantities. in the inverse problem, the values of the vagueness measure of the source data are searched for, at which the specified values of the vagueness measure of the desired values are provided. the inverse problem is hadamard ill-posed problem, and an additional condition is involved for its solution in the form of the principle of equal influences. a definition of ""soft"" computing is given within the framework of the paradigms of econometrics and the theory of errors [1]. the approach proposed in this paper to calculating the uncertainty measure in a direct problem is compared with the approach of fuzzy set theory and soft computing by l. zadeh [2]. two examples of calculating the measure of vagueness of the desired values are discussed: in the dealer problem (training example) and in the problem of evaluating the attractiveness of a real investment project. it is concluded that when conducting financial, economic and managerial calculations in a situation of vagueness, the discussed probabilistic approach should be used, which delivers elegant and easy-to-interpret results. the paper concludes with the application of a probabilistic approach to estimating the measure of vagueness of endogenous variables in descriptive models. © 2020 institute of physics publishing. all rights reserved."
"we survey the works applying text analytics to the study of news media in financial markets beyond intraday horizons, and expand into the fundamental economic theory and concepts relevant to the field. we compare and contrast the news sources, textual analysis methods and empirical modelling approaches adopted within the literature. we distil and categorise the key empirical insights, and summarise the bibliographic history of the literature so far. while this rapidly growing field has yielded many exciting discoveries, there are a number of promising avenues for future research which will only benefit from continued advances in computational technology. © 2019 accounting and finance association of australia and new zealand"
"solving optimization problems with parallel algorithms has a long tradition in or. its future relevance for solving hard optimization problems in many fields, including finance, logistics, production and design, is leveraged through the increasing availability of powerful computing capabilities. acknowledging the existence of several literature reviews on parallel optimization, we did not find reviews that cover the most recent literature on the parallelization of both exact and (meta)heuristic methods. however, in the past decade substantial advancements in parallel computing capabilities have been achieved and used by or scholars so that an overview of modern parallel optimization in or that accounts for these advancements is beneficial. another issue from previous reviews results from their adoption of different foci so that concepts used to describe and structure prior literature differ. this heterogeneity is accompanied by a lack of unifying frameworks for parallel optimization across methodologies, application fields and problems, and it has finally led to an overall fragmented picture of what has been achieved and still needs to be done in parallel optimization in or. this review addresses the aforementioned issues with three contributions: first, we suggest a new integrative framework of parallel computational optimization across optimization problems, algorithms and application domains. the framework integrates the perspectives of algorithmic design and computational implementation of parallel optimization. second, we apply the framework to synthesize prior research on parallel optimization in or, focusing on computational studies published in the period 2008–2017. finally, we suggest research directions for parallel optimization in or. © 2019 elsevier b.v."
"this paper explores the effects of fiscal policy in an economy with search and matching frictions. to this end, a dynamic general-equilibrium model with government sector is calibrated to bulgarian data (1999–2018). two regimes are compared and contrasted–the exogenous (observed) vs. optimal policy (ramsey) case. the focus of the paper is on the relative importance of consumption vs. income taxation, as well as on the provision of utility-enhancing public services. the main findings from the computational experiments performed in the paper are: (i) the optimal steady-state income tax rate is zero; (ii) the benevolent ramsey planner provides the optimal amount of the utility-enhancing public services, which are now three times lower; (iii) the optimal steady-state consumption tax needed to finance the optimal level of government spending is (formula presented.), slightly lower than the rate in the exogenous policy case. © 2020 informa uk limited, trading as taylor & francis group."
"computational intelligence in finance has been a very popular topic for both academia and financial industry in the last few decades. numerous studies have been published resulting in various models. meanwhile, within the machine learning (ml) field, deep learning (dl) started getting a lot of attention recently, mostly due to its outperformance over the classical models. lots of different implementations of dl exist today, and the broad interest is continuing. finance is one particular area where dl models started getting traction, however, the playfield is wide open, a lot of research opportunities still exist. in this paper, we tried to provide a state-of-the-art snapshot of the developed dl models for financial applications. we not only categorized the works according to their intended subfield in finance but also analyzed them based on their dl models. in addition, we also aimed at identifying possible future implementations and highlighted the pathway for the ongoing research within the field. © 2020 elsevier b.v."
"as a typical stylized fact in stock market, fat-tail phenomenon has been widely concerned and applied to risk management practice, while the mechanism of its production has been a mystery for a long time. based on the discussion of the modeling paradigms of the emerging agent-based computational finance, classical finance and behavioral finance, this paper deems that the later two models cannot study fat-tail phenomenon. the agent-based computational finance model has great potential, but its ability to solve financial problems is impeded by the confused modeling solutions. in this paper, an agent-based modeling approach for computational finance is presented. according to the new approach, a simple and analysable model is established. the evidence that market microstructure can produce fat-tail phenomenon has been found, and its mechanism has been analyzed. results and conclusions of this study will influence ideas of applying fat-tail phenomenon to risk management. © 2020 ieee."
"this paper evaluates an editorial and seven invaluable and interesting review papers for the journal of risk and financial management (jrfm). the topics covered include the rising complexity of bank regulatory capital requirements from global guidelines to their united states (us) implementation, connections among big data, computational science, economics, finance, marketing, management and psychology, factors, outcome, and the solutions of supply chain finance, with a review and future directions, time-varying price-volume relationship, adaptive market efficiency, and a survey of the empirical literature, improved covariance matrix estimation for portfolio risk measurement, stock investment and excess returns, with a critical review in the light of the efficient market hypothesis, and a cross section analysis of country equity returns, and a review of the empirical literature. © 2020 by the author."
"communication networks are expanding rapidly and becoming increasingly complex. as a consequence, the conventional rule-based algorithms or protocols may no longer perform at their best efficiencies in these networks. machine learning (ml) has recently been applied to solve complex problems in many fields, including finance, health care, and business. ml algorithms can offer computational models that can solve complex communication network problems and consequently improve performance. this paper reviews the recent trends in the application of ml models in communication networks for prediction, intrusion detection, route and path assignment, quality of service improvement, and resource management. a review of the recent literature reveals extensive opportunities for researchers to exploit the advantages of ml in solving complex performance issues in a network, especially with the advancement of software-defined networks and 5g.  © 2013 ieee."
"artificial neural networks (anns) are of great interest because of their ability to solve problems connected to interpretation of results obtained by various analytical methods. these results sometimes differ from the ordinary form in term of vast number of results for one measurement. examples of those results include near infrared spectroscopy (nirs) spectra or results that have to be in a specific interval. anns are composed of group of nonlinear regression and discrimination statistical methods and are often used for their ability of visualization and prediction which is based on their learned and trained knowledge. use of anns has been widely studied since they correspond to computational systems that aim to imitate some properties of biological neurons. basically, the ann is a system which corresponds to human brain in term of neurons that are linked by synaptic connections. the neurons are divided into i) incoming; which are stimulated by external environment, ii) internal or hidden neurons and iii) output neurons; which provide communication to the outside system. there are a lot of advantages of anns such as: use for nonlinear and non-parametric modeling, stability (with enough data) and high noise tolerance. due to their characteristics, anns have found wide areas of application, from finance and medicine, over geology and physics to food engineering. in this chapter, the application of anns in food engineering will be presented. according to available novel literature, anns have been used in food engineering for control, monitoring and modeling of industrial food processes. furthermore, anns are used for recognition, detection, classification as well as for the search of patterns, prediction of on-line parameters, image processing and optimization. why and how anns are applied is explained in this chapter using examples from food/beverage matrices. © 2020 nova science publishers, inc."
"we describe an implementation of the de-biased estimator using mixed sequences; these are sequences obtained from pseudorandom and low-discrepancy sequences. we use this implementation to numerically solve some stochastic differential equations from computational finance. the mixed sequences, when combined with brownian bridge or principal component analysis constructions, offer convergence rates significantly better than the monte carlo implementation.  © 2020 walter de gruyter gmbh, berlin/boston 2020."
"business management is concerned with organizing and efficiently utilizing resources of a business, including people, in order to achieve required goals. one of the main aspects in this process is planning, which involves deciding operations of the future and consequently generating plans for action. computational models, both theoretical and empirical, help in understanding and providing a framework for such a scenario. statistics and probability can play an important role in empirical research as quantitative data is amenable for analysis. in business management, analysis of risk is crucial as there is uncertainty, vagueness, irregularity, and inconsistency. an alternative and improved approach to deterministic models is stochastic models like monte carlo simulations. there has been a considerable increase in application of this technique to business problems as it provides a stochastic approach and simulation process. in stochastic approach, we use random sampling to solve a problem statistically and in simulation, there is a representation of a problem using probability and random numbers. monte carlo simulation is used by professionals in fields like finance, portfolio management, project management, project appraisal, manufacturing, insurance and so on. it equips the decision-maker by providing a wide range of likely outcomes and their respective probabilities. this technique can be used to model projects which entail substantial amounts of funds and have financial implications in the future. the proposed chapter will deal with concepts of monte carlo simulation as applied to business management scenario. a few specific case studies will demonstrate its application and interpretation. © 2021, the author(s), under exclusive license to springer nature switzerland ag."
"the field of computational finance is evolving ever faster. this book collects a number of novel contributions on the use of computational methods and techniques for modelling financial asset prices, returns, and volatility, and on the use of numerical methods for pricing, hedging, and risk management of financial instruments. © 2020 by the author."
"reconstruction of neuronal populations from ultra-scale optical microscopy (om) images is essential to investigate neuronal circuits and brain mechanisms. the noises, low contrast, huge memory requirement, and high computational cost pose significant challenges in the neuronal population reconstruction. recently, many studies have been conducted to extract neuron signals using deep neural networks (dnns). however, training such dnns usually relies on a huge amount of voxel-wise annotations in om images, which are expensive in terms of both finance and labor. in this paper, we propose a novel framework for dense neuronal population reconstruction from ultra-scale images. to solve the problem of high cost in obtaining manual annotations for training dnns, we propose a progressive learning scheme for neuronal population reconstruction (plnpr) which does not require any manual annotations. our plnpr scheme consists of a traditional neuron tracing module and a deep segmentation network that mutually complement and progressively promote each other. to reconstruct dense neuronal populations from a terabyte-sized ultra-scale image, we introduce an automatic framework which adaptively traces neurons block by block and fuses fragmented neurites in overlapped regions continuously and smoothly. we build a dataset 'visor-40' which consists of 40 large-scale om image blocks from cortical regions of a mouse. extensive experimental results on our visor-40 dataset and the public bigneuron dataset demonstrate the effectiveness and superiority of our method on neuronal population reconstruction and single neuron reconstruction. furthermore, we successfully apply our method to reconstruct dense neuronal populations from an ultra-scale mouse brain slice. the proposed adaptive block propagation and fusion strategies greatly improve the completeness of neurites in dense neuronal population reconstruction.  © 1982-2012 ieee."
"over the past few months, the covid-19 pandemic has postponed many renewable energy projects because of disruptions in the technology and finance supply. additionally, the existing power plants are inefficient because of a record drop in demand for goods and services caused by lockdowns in cities. this situation poses huge challenges to the resilience of renewable energy supply networks in the face of deeply hazardous events, such as the covid-19 pandemic. therefore, the purpose of this study was to design a resilient renewable energy supply network considering supply, demand, and payment risks caused by covid-19. the objective of the proposed model was to determine the optimal amount of electric power generated and stored to meet the demands and the risk-sharing effort index to maximize the total resilient profit of the power plant and determine the optimal price adjustment index to minimize the cost to consumers. a government subsidy-based risk-sharing model was developed to enhance the resilience of the concerned renewable energy supply network under the pandemic. to overcome uncertainties in both random and risk events, a robust fuzzy-stochastic programming model was proposed to solve these research problems. computational experiments were conducted on the test supply network in vietnam. the results showed that the resilient energy supply network with the risk-sharing model tended to stabilize the total profit with the different impact levels of covid-19 compared to the network without risk-sharing. the proposed model efficiently tackled both uncertainties in random and hazardous events and had a higher profit and shorter cpu time compared to the robust optimization mode. © 2020 institution of chemical engineers"
"for research domains such as life sciences, which pursue fundamental scientific understanding and applications intended for immediate use, academic entrepreneurship has played a pivotal role in commercialization. this paper presents an evaluation method of researchers related to user-inspired fundamental research, using global databases of startup finances and academic research papers of “startup readiness.” case studies of startups related to biopharmaceutical research topics suggest that the biopharmaceutical field has rich opportunities stemming from scientific research, commercialization, and entrepreneurship. this evaluation method sorts specific industry segments by which financing activities are active, and by which related growing research topics attract increased academic attention. we constructed networks of author citation and co-authorship from paper citation networks related to research topics in industry segments in the biopharmaceutical domain. results obtained across all research topics we surveyed demonstrated that authors in the top 10% of degree centrality ranking in both networks are far more likely to be startup participants than other authors. our computational approach might provide convenient, dynamic, global, and real-time understanding of the “startup readiness” of researchers working with research topics for which academic attention is emerging in actively financed biopharmaceutical fields. © 2020 the author(s); business; computer science; pharmaceutical science; biotechnology; startup readiness; research-based startups; paper citation networks; co-authorship networks; academic entrepreneurship; startup finances; venture capital; user-inspired fundamental research; technological innovation; biopharmaceuticals. © 2020 the author(s)"
"this paper presents a new computational finance approach. it combines a grid pattern recognition technique allied to an evolutionary computation optimization kernel based on genetic algorithms, creating a dynamic way to attribute a score to the signal that takes volatility into consideration and normalizing the pattern detection by fixing the grid size with the ultimate goal of reduce risk and increase profits. for pattern matching, a template based approach using a fixed size grid of weights is adopted to describe the desired trading patterns, taking not only the closing price into consideration, but also the variation of price in each considered time interval of the time series. the scores assigned to the grid of weights will be optimized by the genetic algorithm and, at the same time, the genetic diversity of possible solutions will be preserved using a speciation technique, giving time for individuals to be optimized within their own niche. the adoption of this approach has the goal of reducing the investment risk and check if it outperforms similar approaches. this system was tested against state-of-the-art solutions, namely the existing adaptable grid of weights and a non speciated approach, considering real data from the stock market. the developed approach using the grid of weights had 21.3% of average return over the testing period against 10.9% of the existing approach and the use of speciation improved some of the training results as genetic diversity was taken into consideration. © 2020 elsevier ltd"
"elliptic curve cryptosystem has been widely applied in a lot of fields, such as finance, e-commerce and e-government. in this paper, we propose an optimized fpga implementation of elliptic curve cryptosystem over 256-bit prime fields, which has high computational performance and low resource consumption. specifically, we design a novel modular multiplier supporting four-level pipelining, which only needs 7 clock cycles to complete a single modular multiplication. it can process 4 modular multiplication operations (4 mmpo) simultaneously. we further design, on the basis of 4mmpo, a parallel architecture to efficiently implement point doubling and point addition operation. finally, we testify the validity of our ecc processor on xilinx's virtex-7 fpga platform. the result shows that it takes only 0.15ms for an elliptic curve point multiplication, the maximum frequency of the processor is about 123.27mhz and the resource only needs 22938 look-up tables (luts). © 2020 ieee."
"the method of model averaging has become an important tool to deal with model uncertainty, for example in situations where a large amount of different theories exist, as are common in economics. model averaging is a natural and formal response to model uncertainty in a bayesian framework, and most of the paper deals with bayesian model averaging. the important role of the prior assumptions in these bayesian procedures is highlighted. in addition, frequentist model averaging methods are also discussed. numerical techniques to implement these methods are explained, and i point the reader to some freely available computational resources. the main focus is on uncertainty regarding the choice of covariates in normal linear regression models, but the paper also covers other, more challenging, settings, with particular emphasis on sampling models commonly used in economics. applications of model averaging in economics are reviewed and discussed in a wide range of areas including growth economics, production modeling, finance and forecasting macroeconomic quantities. ( jel c11, c15, c20, c52, o47). © 2020 american economic association. all rights reserved."
"unsupervised anomaly detection-which aims to identify outliers in data sets without the use of labeled training data-is critically important across a variety of domains including medicine, security, defense, finance, and imaging. in particular, detection of anomalous pixels within hyperspectral images is used for purposes ranging from the detection of military targets to the location of invasive plant species. kernel methods have frequently been employed for this unsupervised learning task but are limited by their sensitivity to parameter choices and the absence of a validation step. here, we use reconstruction error in the kernel principal component analysis (kpca) feature space as a metric for anomaly detection and propose, via batch gradient descent minimization of a novel loss function, to automate the selection of the gaussian rbf kernel parameter, σ. in addition, we leverage an ensemble of learned models to reduce computational cost and improve detection performance. we describe how to select the model ensemble and show that our method yields better detection accuracy relative to competing algorithms on a pair of data sets. © 2020 ieee."
"in mathematical finance, a process of calibrating stochastic volatility (sv) option pricing models to real market data involves a numerical calculation of integrals that depend on several model parameters. this optimization task consists of large number of integral evaluations with high precision and low computational time requirements. however, for some model parameters, many numerical quadrature algorithms fail to meet these requirements. we can observe an enormous increase in function evaluations, serious precision problems and a significant increase of computational time. in this paper, we numerically analyse these problems and show that they are especially caused by inaccurately evaluated integrands. we propose a fast regime switching algorithm that tells if it is sufficient to evaluate the integrand in standard double arithmetic or if a higher precision arithmetic has to be used. we compare and recommend numerical quadratures for typical sv models and different parameter values, especially for problematic cases. © 2019, © 2019 informa uk limited, trading as taylor & francis group."
"increasingly, analytical methods such as mathematical modeling, statistical analysis and mathematical optimization are used in the management of real objects to establish optimal solutions to complex decision-making problems [1]. among the mathematical modeling approaches, the technique of linear programming boasts with widest application. in it, the criterion for the effectiveness of the model is a linear objective function, which must be maximized or minimized in compliance with certain linear constraints. this technique allows to make a quantitative assessment of decisions on real problems in various areas such as finance, production, sales and distribution, transport, personnel management, marketing and others. linear programming tasks aim to achieve optimal allocation of limited resources under certain constraints. resources can be raw materials, labor, machinery, time, money, while constraints can be the company's possible costs or available raw materials. for many organizations, it is crucial to ensure good planning and distribution of the workforce, thus reducing costs and achieving more efficient use of resources, as well as fairer workload. transport companies make no exception in that respect. transport service operators need to plan the number of drivers and the number of vehicles correctly in order to comply with their timetables. these two problems are well described in the literature and are known as crew scheduling problem (csp) and vehicle scheduling problem (vsp), respectively. the aim of resolving them is to minimize the operating costs regarding the use of vehicles and drivers' salaries in case of operational restrictions for vehicles and labor regulations for drivers, and to increase the quality of the offered service [2], [3], [4]. concentrating on these problems allows reaching a relatively easy and effective solution. however, it should be mentioned that the results obtained may not fully satisfy the requirements of transport service operators. other disadvantages in solving such a class of tasks is that the applied methods are susceptible to interference by dispatchers and they can compromise the resulting solutions. the complexity and size of the tasks are time consuming in reaching an optimal solution and require a large amount of hardware resources. the problem is common and is solved by transport companies operating in different countries. it also concerns bulgarian public transport operators, according to publications in the media [5], [6]. this reaffirms the need to approach the problem with the help of mathematical tools and offer a management option for this socially significant issue. the paper presents two consecutive tasks for optimal allocation of resources in the transport system. both tasks are examples of demand modeling. the first task aims to calculate the number of tram drivers in each work shift needed to complete the daily schedule of the tram lines. the second task aims to calculate the number of employees for whom the working week starts from the respective day, so that their total number is minimal. the branch and bound approach is used. it is consisted of a series of subproblems for solving mixed and integer problems. they are solved systematically until the best solution to the main problem is found. problems are solved with the matlab program, using the intlinprog function. then, a modification of the resource allocation tasks is presented. it examines several time ranges for many tram lines (for the first task - different work shifts; for the second task - days of the week). the remainder of the paper is organized as follows. the specific problem is described in section 1. the mathematical modeling is given in section 2. in section 3 an algorithm and a script for optimal distribution of tram drivers are described. the computational results are presented in section 4 and the paper is finalized with conclusions in section 5. © 2020 acm."
"credit value adjustment is the charge applied by financial institutions to the counter-party to cover the risk of losses on a counterpart default event. in this paper we estimate such a premium under the bates stochastic model (bates in the review of financial studies 9(1): 69–107, 1996), which considers an underlying affected by both stochastic volatility and random jumps. we propose an efficient method which improves the finite-difference monte carlo (fdmc) approach introduced by de graaf et al. (journal of computational finance 21, 2017) in particular, the method we propose consists in replacing the monte carlo step of the fdmc approach with a finite difference step and the whole method relies on the efficient solution of two coupled partial integro-differential equations which is done by employing the hybrid tree-finite difference method developed by briani et al. (arxiv:1603.07225 2016;ima journal of management mathematics 28(4): 467–500, 2017;the journal of computational finance 21(3): 1–45, 2017). moreover, the direct application of the hybrid techniques in the original fdmc approach is also considered for comparison purposes. several numerical tests prove the effectiveness and the reliability of the proposed approach when both european and american options are considered. subject classification numbers as needed. © 2020, springer-verlag gmbh germany, part of springer nature."
"finance and especially computational finance is one of the areas in which artificial intelligence and machine learning has found a deep impact in the way financial problems are handled. as compared to traditional approaches the machine learning techniques have greater ease of use, higher degree of accuracy and the evaluation time to get the end result has been considerably reduced. in this paper, a general way of assessing risks and prediction of returns of different types of stocks using various machine learning techniques is reviewed and discussed and compared with the traditional methods. earlier, more statistical and numerical methods were used for the same purpose. however, the nature of data in financial time series is nonlinear and chaotic. they do not follow linear characteristics, and are often a combination of white noise. to interpret this data meaningfully requires removal of noise and learning only the data that is actually suitable for analysis. hence machine learning methods are found to be more suitable for applying in the financial domain. in the paper, the machine learning methods are reviewed which are used in the area of stock selection for portfolio construction and portfolio management. © 2020 ieee."
"the rising of the computational oracles in our society justifies the effort to study the oracle theory and its application. we will introduce a finance based model of differential equations, which is able to model the introduction of an oracle in a game. analyzing this model we will find surprising results connected with economic crysis and oracle supremacy. © 2020 acm."
"the described r package allows to estimate dynamic model averaging (dma), dynamic model selection (dms) and median probability model. the original methods, and additionally, some selected modifications of these methods are implemented. for example the user can choose between recursive moment estimation and exponentially moving average for variance updating in the base dma. moreover, inclusion probabilities can be computed in a way using “google trends” data. the code is written with respect to minimise the computational burden, which is quite an obstacle for dma algorithm if numerous variables are used. for example, this package allows for parallel computations and implementation of the occam’s window approach. however, clarity and readability of the code, and possibility for an r-familiar user to make his or her own small modifications in reasonably small time and with low effort are also taken under consideration. except that, some alternative (benchmark) forecasts can also be quickly performed within this package. indeed, this package is designed in a way that is hoped to be especially useful for practitioners and researchers in economics and finance. © 2020 by the author."
"the computational revolution in simulation techniques has shown to become a key ingredient in the field of bayesian econometrics and opened new possibilities to study complex economic and financial phenomena. applications include risk measurement, forecasting, assessment of policy effectiveness in macro, finance, marketing and monetary economics. © 2020 by the authors."
"bankruptcy prediction has been broadly investigated using financial ratios methodologies. one involved factor is the quality of the portfolio of loans which is given. hence, having a model to classify/predict position of each loan candidate based on several features is important. in this work, an application of machine learning approach in mathematical finance and banking is discussed. it is shown how we can classify some lending portfolios of banks under several features such as rating categories and various maturities. dynamic updates of the portfolio are also given along with the top probabilities showing how the financial data of this type can be classified. the discussions and results reveal that a good algorithm for doing such a classification on large economic data of such type is the k-nearest neighbors (knn) with k = 1 along with parallelization even over the support vector machine, random forest, and artificial neural network techniques to save as much as possible on computational time. © 2020 by the authors. licensee mdpi, basel, switzerland."
"developing effective machine learning models to maximize the generalization capability of the learned data patterns while minimizing the risk of overfitting and also the ultimate computational costs of the trained models are very challenging yet significant tasks in many real-world applications. due to the high complexity and huge amount of data involved in financial applications, the critical task of constructing appropriate machine learning models will surely become more challenging. typically, data scientists or researchers may employ various optimizers to enhance the feature selection and/or tuning of the involved hyper-parameters in order to improve the overall performance of the obtained learning models. in this paper, a very flexible optimizer, namely the adaptive multi-population optimization algorithm (ampoa), utilizing multiple populations with different search strategies is considered in which two of its variants are carefully evaluated with the other state-of-theart optimizers to enhance the feature selection process of the commonly used k-nearest-neighbor classifier approach on a set of 12 well-known benchmark problems. the first variant named the bampoa-r approach employs a rounding technique to determine on the binary decision of choosing an input feature or not for the underlying classifier while the other bampoa-t approach utilizes a specific transform function to decide on the feature selection. to demonstrate the feasibility of the proposed optimizer for financial applications, the bampoa-r approach with the best performance in the previous test is integrated with the support vector regression (svr) as the bampoa-r-svr algorithm to compare against other regressors on 10 major financial market indexes. the evaluation results clearly demonstrate the remarkable performance of the proposed ampoa approach to enhance the prediction performances of the underlying machine learning models through more vigorous feature selection for numerous real-world applications including those challenging problems in computational finance. © 2020 ieee."
"outperforming the markets through active investment strategies is one of the main challenges in finance. the random movements of assets and the unpredictability of catalysts make it hard to perform better than the average market, therefore, in such a competitive environment, methods designed to keep low transaction costs have a significant impact on the obtained wealth. this paper focuses on investing techniques to beat market returns through online portfolio optimization while controlling transaction costs. such a framework differs from classical approaches as it assumes that the market has an adversarial behavior, which requires frequent portfolio rebalancing. this paper analyses critically the known online learning literature dealing with transaction costs and proposes a novel algorithm, namely online gradient descent with momentum (ogdm), to control (theoretically and empirically) the costs. the existing algorithms designed for this setting are either (i) not providing theoretical guarantees, (ii) providing a bound to the total regret, conditionally on unrealistic assumptions or (iii) computationally not efficient. in this paper, we prove that ogdm has nice theoretical, empirical, and computational performances. we show that it has regret, considering costs, of the order [equation], t being the investment horizon, and has θ(m) per-step computational complexity, m being the number of assets. furthermore, we show that this algorithm provides competitive gains when compared empirically with state-of-the-art online learning algorithms on a real-world dataset.  © 2020 acm."
"based on the parallel cyclic reduction technique, a promising new parallel algorithm is designed for pentadiagonal systems. subject to fulfilling stability conditions, this highly parallelizable algorithm works very well for systems of any size. the solver is implemented on a graphics processing unit using the cuda programming platform where it is empirically studied for its performance in comparison with some of the present-day prominent parallel solvers. the construction of the new algorithm is originally motivated by a real-world application in computational finance. accordingly, it is employed successfully to numerically solve the convection-dominated heston partial differential equation for pricing a financial option, and implementation of the full solver is discussed in detail. © 2021 society for industrial and applied mathematics publications. all rights reserved."
"this paper provides theoretical evidence about financial distress in the business sector in relation to firms' targeted free cash flow (fcf). an agent-based model of a pure market economy is designed so that a population of firms interact with one another and with a bank. the model determines the interfirm payment network arising from supplier–customer relationships on the basis of a random graph with uniform attachment mechanisms. the interfirm payment network shapes both the liquidity available to each firm and the debts firms incur to finance these payments. eventually, firms might not have sufficient liquidity to meet their debt requirements, hence their financial distress. firms that target higher fcfs must reduce their payments to suppliers for the same amount of payments they expect to receive from customers. this influences the interfirm payment network and, therefore, firms' financial distress. on this basis, computational experiments introduce variations in fcf targets. the lowest fcf targets lead to the lowest levels of financial distress in the business sector. our simplified case of interactions opens the way for further research that employs more complex agent-based models. © 2019 john wiley & sons, ltd."
"the rough bergomi (rbergomi) model, introduced recently in bayer et al. [pricing under rough volatility. quant. finance, 2016, 16(6), 887–904], is a promising rough volatility model in quantitative finance. it is a parsimonious model depending on only three parameters, and yet remarkably fits empirical implied volatility surfaces. in the absence of analytical european option pricing methods for the model, and due to the non-markovian nature of the fractional driver, the prevalent option is to use the monte carlo (mc) simulation for pricing. despite recent advances in the mc method in this context, pricing under the rbergomi model is still a time-consuming task. to overcome this issue, we have designed a novel, hierarchical approach, based on: (i) adaptive sparse grids quadrature (asgq), and (ii) quasi-monte carlo (qmc). both techniques are coupled with a brownian bridge construction and a richardson extrapolation on the weak error. by uncovering the available regularity, our hierarchical methods demonstrate substantial computational gains with respect to the standard mc method. they reach a sufficiently small relative error tolerance in the price estimates across different parameter constellations, even for very small values of the hurst parameter. our work opens a new research direction in this field, i.e. to investigate the performance of methods other than monte carlo for pricing and calibrating under the rbergomi model. © 2020 informa uk limited, trading as taylor & francis group."
"in this modern era, the financial market, more specifically, the stock markets all over the world, deal with an enormous amount of real-time data that facilitates the data analytics and prediction in the field of finance. the main objective of this paper is to propose a novel model of neural network based on long-short term memory (lstm) and utilizing one of the most powerful evolutionary algorithms, namely the differential evolution (de), to forecast the next day's stock price of a company. this study focuses on optimizing the ten network hyperparameters related to the detection of temporal patterns of a given dataset, namely, the size of the time window, batch size, the number of lstm units in hidden layers, the number of hidden layers (lstm and dense), dropout coefficient for each layer, and the network training optimization algorithm. to the best of our knowledge, this is the first time that all this set of parameters have been optimized simultaneously. then, the lstm has been optimized by de to gain the lower root mean squared error (rmse) for prediction. the proposed model achieved 8.092 rmse as its objective value, which is better in comparison with the best statistical forecasting models such as naive, ets, and sarima, which are the-state-of-the-art methods in this filed. moreover, for shortening the training time as the main source of computational expensiveness, the proposed method works with a lower number of epochs. by this way, de tries to find a shallower and faster network even with higher accuracy, which is a remarkable approach. © 2020 ieee."
"purpose: this paper aims to provide a reflective discussion on the different avenues of blockchain application in islamic finance in promoting trust and transparency for increased accountability between parties involved in the delivery of sharīʿah-compliant products and services. design/methodology/approach: this paper discusses on blockchain benefits in islamic finance while providing an illustration with smart sukuk. having identified the advantages of the development of islamic financial technology (i-fintech), this study ends by debating a couple of challenges (computational codification of sharīʿah principles and environmental impact) that have to be addressed to promote the development of a real sustainable islamic fintech. findings: this paper also identifies two challenges in using blockchain in i-fintech. the first challenge refers to the extent to which sharīʿah principles can be computationally encoded. blockchain makes public all transactions that ease sharīʿah compliance checks and determine if these transactions are islamic in nature but this check can be done only after their operation. the second challenge is related to the algorithmic protocol used to validate smart contracts (including smart sukuk). this situation calls into question the principles of maqasid al-sharīʿah according to which transactions should not harm society. originality/value: in the current debates related to the development of islamic fintech, this paper also identifies two challenges in using blockchain in i-fintech. © 2021, emerald publishing limited."
"multiagent incentive contracts are advanced techniques for solving decentralized decision-making problems with asymmetric information. the principal designs contracts aiming to incentivize non-cooperating agents to act in his or her interest. due to the asymmetric information, the principal must balance the efficiency loss and the security for keeping the agents. we prove both the existence conditions for optimality and the uniqueness conditions for computational tractability. the coupled principal-agent problems are converted to solving a hamilton–jacobi–bellman equation with equilibrium constraints. extending the incentive contract to a multiagent setting with history-dependent terminal conditions opens the door to new applications in corporate finance, institutional design, and operations research. © 2020 by the authors. li-censee mdpi, basel, switzerland."
"industry 4.0 is a new approach to organizing the manufacturing process. this new industry asserts itself as the convergence of the virtual world, digital design, management (operations, finance and marketing) with the products and objects of the real world. the use of real-time information in production scheduling decisions and energy-constrained maintenance is becoming possible thanks to new developments in information technology and industrial computing, such as industry 4.0. the gradual implementation of industry 4.0 in industries has led to an increase in the amount of energy consumed in industrial systems to operate various machines. in addition, the computational efficiency of these real-time embedded systems (rtes) depends solely on the timely completion of tasks. performing production and maintenance tasks with reduced energy consumption within critical timeframes is a challenge for designers. however, energy efficiency and on-time delivery are two conflicting goals, since the former is only achieved with a significant trade-off of the latter. in this paper, we propose a new flowchart of the real-time energy-efficient scheduling (rtees) of production and maintenance system in industry 4.0 based on optimization. © 2020 ieee."
"the theory of markowitz portfolio has had enormous value and extensive applications in finance since it came into being. with the advent of the big-data era and the increasingly complicated financial market, the resource consumption of computing portfolio investments is significantly increasing. cloud computing offers a good platform to efficiently compute large-scale portfolio investments, in particular, for resource-limited investors. in this paper, a markowitz model (mm) is taken into consideration for outsourcing to a public cloud in a privacy-conscious way. as in general computation outsourcing, outsourcing mm inevitably faces four issues, namely, input/output privacy, correctness, verification, and substantial computation gain for investors; it has consistent complexity with the original methods when the cloud solves the encrypted version. however, the proposed cloud-assisted privacy-conscious mm employs location-scrambling and value-alteration encryption operations, which can protect the mm's input/output privacy well. moreover, the correctness of solving mm over an encrypted domain in the cloud side can be demonstrated and the results returned by the cloud can be verified. furthermore, both theoretical and experimental analyses validate that the investor can obtain a huge amount of computational gain, and the cloud complexity consistent with that of the original case when solving the encrypted version. © 2019 elsevier inc."
"our high expectations for the internet of things (iot) and how it will positively influence our lifestyles depend on a secure and trusted implementation of it, especially in sensitive sectors such as health and finance. iot platforms and solutions must provide confidentiality, integrity, and availability (cia) in a secure and transparent way. due to the extremely large scales of iot, traditional centralized solutions for security provisioning cannot be employed in their original form. this article discusses the authentication problem in iot, which is fundamental to providing cia. we propose a hierarchical security architecture for this problem and focus on computationally lightweight authentication protocols that can intelligently distribute the computational load across multiple levels and effectively push the load toward upper layers. © 2018 ieee."
"successful forecasting requires integrating financial theory, market behavior, exploding sources of data, and computational innovation. building accurate computational models can be achieved by assembling the most comprehensive toolbox. both financial econometrics and machine learning approaches help to achieve this objective. machine learning tools provide the ability to make more accurate predictions by accommodating nonlinearities in data, understanding complex interaction among variables, and allowing the use of large, unstructured datasets. the tools of financial econometrics remain critical in answering questions related to inference among the variables describing economic relationships in finance; when properly applied, their role has not diminished with the introduction of machine learning. © 2020 portfolio management research. all rights reserved."
"this paper extends the singular fourier–padé (sfp) method proposed by chan [singular fourier–padé series expansion of european option prices. quant. finance, 2018, 18, 1149–1171] for pricing/hedging early-exercise options–bermudan, american and discrete-monitored barrier options–under a lévy process. the current sfp method is incorporated with the filon–clenshaw–curtis (fcc) rules invented by domínguez et al. [stability and error estimates for filon–clenshaw–curtis rules for highly oscillatory integrals. ima j. numer. anal., 2011, 31, 1253–1280], and we call the new method sfp–fcc. the main purpose of using the sfp–fcc method is to require a small number of terms to yield fast error convergence and to formulate option pricing and option greek curves rather than individual prices/greek values. we also numerically show that the sfp–fcc method can retain a global spectral convergence rate in option pricing and hedging when the risk-free probability density function is piecewise smooth. moreover, the computational complexity of the method is (formula presented.) with n, a (small) number of complex fourier series terms, (formula presented.), a number of chebyshev series terms and l, the number of early-exercise/monitoring dates. finally, we compare the accuracy and computational time of our method with those of existing techniques in numerical experiments. © 2020 informa uk limited, trading as taylor & francis group."
"outlier detection is a key data analysis technique that aims to find unusual data objects in a data set. it has been widely used in varied areas, including communication networks, finance, medicine, environmental studies, etc. many applications in these areas involve categorical data. for example, the data set used in the application of intrusion detection normally includes a group of captured packets, which tend to have categorical attributes such as 'protocol'. although there are many outlier detection algorithms for applications involving numerical data, only a few existing schemes can handle categorical data. and the schemes designed for categorical data seriously suffer from two problems: low detection precision and high time complexity. in this paper, we present two novel outlier detection algorithms for categorical data sets. first of all, we describe a simple scheme based on entropy, outlier detection tree (odt). with odt, a classification tree is constructed to classify the data set into two classes: a normal class and an abnormal class. thereafter, each data object is identified as an outlier or a normal one using the if-then rules in the tree. furthermore, we propose an advanced outlier detection algorithm, fast-odt, which achieves both high detection accuracy and low time complexity. our experimental results indicate that fast-odt outperforms the existing algorithms in terms of outlier detection precision and computational complexity.  © 2013 ieee."
"in many modern data analysis problems, the available data is not static but, instead, comes in a streaming fashion. performing bayesian inference on a data stream is challenging for several reasons. first, it requires continuous model updating and the ability to handle a posterior distribution conditioned on an unbounded data set. secondly, the underlying data distribution may drift from one time step to another, and the classic i.i.d. (independent and identically distributed), or data exchangeability assumption does not hold anymore. in this paper, we present an approximate bayesian inference approach using variational methods that addresses these issues for conjugate exponential family models with latent variables. our proposal makes use of a novel scheme based on hierarchical priors to explicitly model temporal changes of the model parameters. we show how this approach induces an exponential forgetting mechanism with adaptive forgetting rates. the method is able to capture the smoothness of the concept drift, ranging from no drift to abrupt drift. the proposed variational inference scheme maintains the computational efficiency of variational methods over conjugate models, which is critical in streaming settings. the approach is validated on four different domains (energy, finance, geolocation, and text) using four real-world data sets. © 2020 by the authors. licensee mdpi, basel, switzerland."
"we assess the effects of regulatory caps in the loan-to-value (ltv) ratio for housing mortgages using an agent-based model. sellers, buyers and banks interact within a computational framework that enables the application of ltv caps to a one-step housing market. we first conduct a simulation exercise; later, we calibrate the probability distributions based on actual european data from the household finance and consumption survey. in both cases, the application of an ltv cap results in a modified distribution of buyers in terms of property values, bidding prices and properties sold, depending on the probability distributions of the ltv ratio, wealth and debt-to-income ratios considered. the results are of similar magnitude to other studies in the literature embodying other analytical approaches, and they suggest that our methodology can potentially be used to gauge the impact of common macroprudential measures. © european central bank 2020. all rights reserved."
"internet of things (iot) has revolutionized the world with its innovative and precise solutions catering a broad spectrum of applications. smart cities, smart financial services, smart logistics, smart agriculture systems, smart healthcare management system are few applications worth mentioning. device authentication and data security is a major concern in iot networks as the devices used in these networks have inadequate computational processing and memory storage resources. privacy is of paramount importance in applications such as healthcare and finance. as complex algorithms cannot be implemented on the resource-constrained iot devices, research in the aforementioned areas is still an active subject. in this paper, we propose a mutual device and server authentication protocol using physical unclonable functions (pufs) for iot devices. pufs can be used to uniquely identify the devices and can also be used to generate random keys. as the name suggests, the keys generated by the pufs cannot be reproduced. using this unclonability feature, an authentication protocol has been developed. an iot based temperature and humidity sensing module is developed to integrate the protocol and validate its effectiveness in providing the device authentication and data security. © 2020 ieee."
"in the last few years, there was a growth regarding the use of computational methods in the field of finance, especially to negotiations in the stock market. in this paper, we aim to bring new ideas and approaches to the development of automated trading or bots based on historical data of financial series. our model, named pattern searcher, was inspired in unsupervised learning methods and evolutionary optimization. given a trading agent with its predefined parameters, the method uses the power of genetic algorithm (ga) to search, within a set of financial indicators, for the region that provides a higher positive financial return. this implementation exhibited desirable properties compared to some machine learning methods, such as the simplification of the system flow and the generation of rules that humans can clearly understand. besides, we have generated strategy portfolios, composed by the strategies derived from the pattern searcher method, which were also optimized via ga. the system was able to generate very profitable trading agents and portfolios on the brazilian stock market, surpassing important benchmarks. © 2020 ieee."
"this paper applies an algorithm for the convolution of compactly supported legendre series (the conleg method) (cf. hale and townsend, an algorithm for the convolution of legendre series. siam j. sci. comput., 2014, 36, a1207–a1220), to pricing european-type, early-exercise and discrete-monitored barrier options under a lévy process. the paper employs chebfun (cf. trefethen et al., chebfun guide, 2014 (pafnuty publications: oxford), available online at: http://www.chebfun.org/) in computational finance and provides a quadrature-free approach by applying the chebyshev series in financial modelling. a significant advantage of using the conleg method is to formulate option pricing and option greek curves rather than individual prices/values. moreover, the conleg method can yield high accuracy in option pricing when the risk-free smooth probability density function (pdf) is smooth/non-smooth. finally, we show that our method can accurately price options deep in/out of the money and with very long/short maturities. compared with existing techniques, the conleg method performs either favourably or comparably in numerical experiments. © 2020 informa uk limited, trading as taylor & francis group."
"in this article, we consider the effects of filibuster change on judicial appointments, judicial voting, and opinion drafting. the filibuster effectively empowers a minority of 41 senators by requiring 60 votes to break off debate on a nomination. we develop a game-theoretic model that explains that the elimination of the filibuster changed the relevant “pivotal senator,” whose support was necessary to secure a nomination. freed of the power of the minority of senators, presidents ought to exercise freer rein in naming judicial nominees closer to their preferred ideology. moreover, sitting judges who seek elevation to a higher court ought to alter their “signal” that they would be good candidates to match the preferences of the newly relevant pivotal senator. to test our hypotheses empirically, we use the 2013 elimination of the filibuster in the u.s. senate for lower federal court judicial nominations as an exogenous shock. we explore how the change in the filibuster rule affected the characteristics of judges president obama nominated to the federal courts. we find statistically significant shifts in the background characteristics of judges confirmed to the federal courts of appeals after the elimination of the filibuster. compared to the earlier obama appointees, these judges were more likely to be female, slightly younger, and to have previously clerked for a liberal judges, but less likely to be nonwhite. in addition, we find that there was a statistically significant increase in the confirmation of judges with liberal ideologies, as measured by their common space campaign finance scores. these liberal ideologies mapped onto actual votes in politically charged cases. compared to obama judges confirmed before the rule change, these judges were more likely to cast pro-choice votes in abortion cases and anti-death penalty votes in death penalty cases. we also find evidence that the elimination of the filibuster had a polarizing effect on sitting federal district judges, especially those with a greater chance of promotion to the courts of appeals. using computational content analysis, we find that after the change in the filibuster rule, democratic judges were more likely to use politically charged words signaling their very liberal ideological positions in abortion opinions and republican judges were more likely to use words signaling their conservative views. these findings are useful in assessing the desirability of restoring the judicial filibuster, as well in assessing the debate over the retention of the legislative filibuster. © 2020 cornell law school and wiley periodicals llc."
"anomaly detection problems (also called change-point detection problems) have been studied in data mining, statistics and computer science over the last several decades (mostly in non-network context) in applications such as medical condition monitoring, weather change detection and speech recognition. in recent days, however, anomaly detection problems have become increasing more relevant in the context of network science since useful insights for many complex systems in biology, finance and social science are often obtained by representing them via networks. notions of local and non-local curvatures of higher-dimensional geometric shapes and topological spaces play a fundamental role in physics and mathematics in characterizing anomalous behaviours of these higher dimensional entities. however, using curvature measures to detect anomalies in networks is not yet very common. to this end, a main goal in this paper to formulate and analyze curvature analysis methods to provide the foundations of systematic approaches to find critical components and detect anomalies in networks. for this purpose, we use two measures of network curvatures which depend on non-trivial global properties, such as distributions of geodesics and higher-order correlations among nodes, of the given network. based on these measures, we precisely formulate several computational problems related to anomaly detection in static or dynamic networks, and provide non-trivial computational complexity results for these problems. this paper must not be viewed as delivering the final word on appropriateness and suitability of specific curvature measures. instead, it is our hope that this paper will stimulate and motivate further theoretical or empirical research concerning the exciting interplay between notions of curvatures from network and non-network domains, a much desired goal in our opinion. © 2020, springer science+business media, llc, part of springer nature."
"the history of research in finance and economics has been widely impacted by the field of agent-based computational economics (ace). while at the same time being popular among natural science researchers for its proximity to the successful methods of physics and chemistry for example, the field of ace has also received critics by a part of the social science community for its lack of empiricism. yet recent trends have shifted the weights of these general arguments and potentially given ace a whole new range of realism. at the base of these trends are found two present-day major scientific breakthroughs: the steady shift of psychology towards a hard science due to the advances of neuropsychology, and the progress of reinforcement learning due to increasing computational power and big data. we outline here the main lines of a computational research study where each agent would trade by reinforcement learning. © springer nature switzerland ag 2021."
"this article describes the implementation of a methodology and the application of data mining focused on a set of data about profiles of people who declare themselves financially insolvent. the relevant parts of the methodology that led to obtaining a support product to detect the insolvencies of individuals or companies are discussed, such as an economic risk calculator, based on supervised algorithms of data analysis with the information obtained as a use case of a company dedicated to detecting insolvencies in order to demonstrate the effectiveness of the application or use of computational intelligence and data mining to determine the level of risk of your finances and will prospect people who may be clients of the company based on product marketing. © 2020, associacao iberica de sistemas e tecnologias de informacao. all rights reserved."
"thailand has a long history in welcoming the private sector to participate in the development of large-scale infrastructure projects through a program called public private partnership or ppp. build-operate-transfer (bot) and build-transfer-operate (bto) are two of commonly used ppp contracts for transportation infrastructure, in which the private (or the concessionaire) is responsible for the risks relating to the design, finance, construction, and operation and maintenance (o&m) of the project. as some of ppp transportation infrastructure projects in thailand are approaching its final year of a concession period, decision as to extend or to end the contract must be made by a contracting agency, that is: (1) to extend the concession contract to the existing concessionaire for a certain period of years or (2) to let the concession expire and to have a responsible public agency assume the operation. this paper is to propose a decision-making model based on a level of risk exposure as a computational tool for comparing between a renewal package offered by the concessionaire and expected benefits generated under the operation by the public agency. a case study project, a bot elevated road located in bangkok, will be used to illustrate the application of the proposed model. one of the implications of this study is that the estimated figure computed using the proposed method may be treated as a public sector comparator (psc), which can be used as an initial benchmark for the negotiation with the existing or a new concessionaire. © 2020 american society of civil engineers."
"the graphics processing unit (gpu) is extensively used in diverse domains, such as finance, machine learning, and image processing. the gpu can be underutilized as multiple applications may not share the same gpu concurrently owing to a memory oversubscription issue. for example, when applications that require fewer computational resources but a larger gpu memory are running instantaneously, the gpu memory may be insufficient; consequently, the number of gpu applications running simultaneously is restricted, decreasing gpu utilization. further, it can even stop the execution of applications that are running on the gpu. to this end, we propose flexgpu, which schedules the kernels of the gpu applications that run on the same gpu according to their features. this framework 1) schedules the kernel at the launching time according to its features to improve gpu utilization and 2) temporarily checkpoints and restores non-dependent content in the gpu memory to/from the host memory, which avoids oversubscription of the gpu when out-of-memory failure occurs and allows more kernels to run concurrently on the gpu. the experimental results show that compared to existing methods, our approach demonstrates a 7 times improvement in performance in terms of execution time and enables a 2.5 times increase in the concurrent execution of applications. © 2020 ieee."
"global sensitivity analysis allows the modeler to assess the importance of a model parameter in terms of its impact on the variance of the model output. parameters that are not important can be frozen, and the important ones can be treated with care. this information, however, could be sensitive to data used in estimation of the parameters. to address this, we develop a notion of robustness of a model using randomized sobol’ sensitivity indices. we use the robustness definition to compare some models from computational finance. © 2020, springer nature switzerland ag."
"in this paper, we present splitting approaches for stochastic/deterministic coupled differential equations, which play an important role in many applications for modelling stochastic phenomena, e.g., finance, dynamics in physical applications, population dynamics, biology and mechanics. we are motivated to deal with non-lipschitz stochastic differential equations, which have functions of growth at infinity and satisfy the one-sided lipschitz condition. such problems studied for example in stochastic lubrication equations, while we deal with rational or polynomial functions. numerically, we propose an approximation, which is based on picard iterations and applies the doleans-dade exponential formula. such a method allows us to approximate the non-lipschitzian sdes with iterative exponential methods. further, we could apply symmetries with respect to decomposition of the related matrix-operators to reduce the computational time. we discuss the different operator splitting approaches for a nonlinear sde with multiplicative noise and compare this to standard numerical methods. © 2020 by the authors. licensee mdpi, basel, switzerland."
"we present a deep recurrent neural network architecture to solve a class of stochastic optimal control problems described by fully nonlinear hamilton jacobi bellman partial differential equations. such pdes arise when considering stochastic dynamics characterized by uncertainties that are additive, state dependent, and control multiplicative. stochastic models with these characteristics are important in computational neuroscience, biology, finance, and aerospace systems and provide a more accurate representation of actuation than models with only additive uncertainty. previous literature has established the inadequacy of the linear hjb theory for such problems, so instead, methods relying on the generalized version of the feynman-kac lemma have been proposed resulting in a system of second-order forward-backward sdes. however, so far, these methods suffer from compounding errors resulting in lack of scalability. in this paper, we propose a deep learning based algorithm that leverages the second-order fbsde representation and lstm-based recurrent neural networks to not only solve such stochastic optimal control problems but also overcome the problems faced by traditional approaches, including scalability. the resulting control algorithm is tested on a high-dimensional linear system and three nonlinear systems from robotics and biomechanics in simulation to demonstrate feasibility and out-performance against previous methods. © 2020 m.a. pereira, z. wang, t. chen, e.a. reed & e.a. theodorou."
"treating high dimensionality is one of the main challenges in the development of computational methods for solving problems arising in finance, where tasks such as pricing, calibration, and risk assessment need to be performed accurately and in real-time. among the growing literature addressing this problem, gass et al. [finance stoch., 22 (2018), pp. 701-731] propose a complexity reduction technique for parametric option pricing based on chebyshev interpolation. as the number of parameters increases, however, this method is affected by the curse of dimensionality. in this article, we extend this approach to treat high-dimensional problems: additionally, exploiting low-rank structures allows us to consider parameter spaces of high dimensions. the core of our method is to express the tensorized interpolation in the tensor train format and to develop an efficient way, based on tensor completion, to approximate the interpolation coefficients. we apply the new method to two model problems: american option pricing in the heston model and european basket option pricing in the multidimensional black-scholes model. in these examples, we treat parameter spaces of dimensions up to 25. the numerical results confirm the low-rank structure of these problems and the effectiveness of our method compared to advanced techniques. © 2020 society for industrial and applied mathematics publications. all rights reserved."
"the purpose of the book is to provide a broad-based accessible introduction to three of the presently most important areas of computational finance, namely, option pricing, algorithmic trading and blockchain. this will provide a basic understanding required for a career in the finance industry and for doing more specialised courses in finance. © 2020 emerald publishing limited."
"this article examines regulation’s understanding of technology in american financial markets as means for rethinking the contours and institutional limits of governance in the age of financialization. the article identifies how the securities and exchange commission perceived markets and their conceptual relation to technology throughout much of the long twentieth century by distilling the “ontologies” expressed by the agency’s leadership. despite the fact that sec’s commissioners recognized technologies as playing a central role in the market’s current and future operations, these were never effectively brought under regulatory scrutiny even when such action fell under the commission's jurisdictional remit. rather than regulating technologies as constitutive of markets, the governance of the material devices of finance was discursively kept at arm's-length by presenting them as intractable objects that were external to financial innovation. triangulating across several techniques of computational text analysis, this article shows how this interpretation of technology mirrored distinct shifts in how regulators understood markets, from being physical trading sites populated by agents that required vetting and certification to a distributed, multi-sited system surveilled through transparency and disclosure. throughout these ontological transitions, technology’s inscrutability remained, limiting the capacity of the state and its regulatory agencies to shape the evolution of finance and its underlying infrastructures. © 2020, springer nature b.v."
"this book discusses the interplay of stochastics (applied probability theory) and numerical analysis in the field of quantitative finance. the stochastic models, numerical valuation techniques, computational aspects, financial products, and risk management applications presented will enable readers to progress in the challenging field of computational finance. when the behavior of financial market participants changes, the corresponding stochastic mathematical models describing the prices may also change. financial regulation may play a role in such changes too. the book thus presents several models for stock prices, interest rates as well as foreign-exchange rates, with increasing complexity across the chapters. as is said in the industry, ""do not fall in love with your favorite model."" the book covers equity models before moving to short-rate and other interest rate models. we cast these models for interest rate into the heath-jarrow-morton framework, show relations between the different models, and explain a few interest rate products and their pricing. the chapters are accompanied by exercises. students can access solutions to selected exercises, while complete solutions are made available to instructors. the matlab and python computer codes used for most tables and figures in the book are made available for both print and e-book users. this book will be useful for people working in the financial industry, for those aiming to work there one day, and for anyone interested in quantitative finance. the topics that are discussed are relevant for msc and phd students, academic researchers, and for quants in the financial industry. © 2020 by cornelis w. oosterlee and lech a. grzelak. all rights reserved."
"log-normal distribution is used widely in application fields such as economics and finance. this paper considers confidence interval estimates for common signal-to-noise ratio of log-normal distributions based on generalized confidence interval (gci), adjusted method of variance estimates recovery, and computational approaches. a simulation study is conducted to compare the performance of these confidence intervals. a monte carlo simulation is applied to report coverage probability and average length of the confidence intervals. based on the simulation study, for k= 3, the gci can be used. for k= 6, the results of gci approach perform similarly to the results of computational approach. for k= 10, the computational approach can be considered as an alternative to estimate the confidence interval. a numerical example based on real data is presented to illustrate the proposed approaches. © 2019, shiraz university."
"particularly in the last decade, internet usage has been growing rapidly. however, as the internet becomes a part of the day to day activities, cybercrime is also on the rise. cybercrime will cost nearly $6 trillion per annum by 2021 as per the cybersecurity ventures report in 2020. for illegal activities, cybercriminals utilize any network computing devices as a primary means of communication with a victims' devices, so attackers get profit in terms of finance, publicity and others by exploiting the vulnerabilities over the system. cybercrimes are steadily increasing daily. evaluating cybercrime attacks and providing protective measures by manual methods using existing technical approaches and also investigations has often failed to control cybercrime attacks. existing literature in the area of cybercrime offenses suffers from a lack of a computation methods to predict cybercrime, especially on unstructured data. therefore, this study proposes a flexible computational tool using machine learning techniques to analyze cybercrimes rate at a state wise in a country that helps to classify cybercrimes. security analytics with the association of data analytic approaches help us for analyzing and classifying offenses from india-based integrated data that may be either structured or unstructured. the main strength of this work is testing analysis reports, which classify the offenses accurately with 99 percent accuracy. © 2020 by the authors."
"in practice, classification problems have appeared in many scientific fields, including finance, medicine and industry. it is critically important to develop an effective and accurate classification model. although numerous useful classifiers have been proposed, they are unstable, sensitive to noise and slow in computation. to overcome these drawbacks, the combination of feature selection techniques with traditional machine learning models is of great help. in this paper, a novel feature selection method called the opposition-based seagull optimization algorithm (osoa) is proposed and studied. the osoa is constructed based on an soa whose population is determined by the opposition-based learning (obl) algorithm. to evaluate its overall classification performance, some measures, including classification accuracy, number of selected features, receiver operating characteristic curve (roc), and computation time, are adopted. the empirical results indicate that the suggested method exhibits higher or similar accuracy and computational efficiency in comparison with genetic algorithm (ga)-, simulated annealing (sa)-, and fisher score (fs)-based classification models. the experimental results show that the osoa is a computationally efficient feature selection technique that has the ability to select relevant variables. furthermore, it performs well with high-dimensional data whose number of variables exceeds the number of samples. thus, the osoa is an effective approach for the enhancement of classification performance. © 2013 ieee."
"the collection and analysis of data play an important role in many fields of science and technology, such as computational biology, quantitative finance, information engineering, machine learning, neuroscience, medicine, and the social sciences. especially in the era of big data, researchers can easily collect data characterised by massive dimensions and complexity. in celebration of professor kai-tai fang’s 80th birthday, we present this book, which furthers new and exciting developments in modern statistical theories, methods and applications. the book features four review papers on professor fang’s numerous contributions to the fields of experimental design, multivariate analysis, data mining and education. it also contains twenty research articles contributed by prominent and active figures in their fields. the articles cover a wide range of important topics such as experimental design, multivariate analysis, data mining, hypothesis testing and statistical models. © springer nature switzerland ag 2020."
"data mining is a critical technology for extracting valuable knowledge from databases. it has been used in many fields, like retail, finance, biology, etc. in computational intelligence, fuzzy logic has been applied in many intelligent systems widely because it is simple and similar to human inference. fuzzy utility mining combines utility mining and fuzzy logic for getting linguistic utility knowledge. in this paper, we study a more challenging, complicated, but practical topic called temporal fuzzy utility data mining, which considers the temporal periods in transactions, purchased amounts, item profits, and understandable linguistic terms as important factors. although an apriori-based algorithm was proposed previously, its execution was not efficient. we thus use a modified tree structure based on the classical frequent-pattern tree to improve its performance. a tree-based mining algorithm is also proposed to mine temporal fuzzy utility itemsets from quantitative transactional databases. the tree structure is built to keep all temporal fuzzy utility 1-itemsets in a database. all the high temporal fuzzy utility itemsets in a database can be obtained by traversing the tree-based structure. the proposed algorithm gets the final results through two phases. in the first phase, a procedure like fp-growth is used to find the candidate itemsets. in the second phase, the temporal fuzzy utility database is scanned to decide whether the candidate itemsets are desired. experimental results show that the proposed algorithm is superior to the existing algorithm for temporal fuzzy utility mining in terms of processing time and used memory.  © 2013 ieee."
"the possibility that a discrete process can be fruitfully approx-imated by a continuous one, with the latter involving a differential system, is fascinating. important theoretical insights, as well as significant computa-tional efficiency gains may lie in store. a great success story in this regard are the navier-stokes equations, which model many phenomena in fluid flow rather well. recent years saw many attempts to formulate more such contin-uous limits, and thus harvest theoretical and practical advantages, in diverse areas including mathematical biology, economics, finance, computational opti-mization, image processing, game theory, and machine learning. caution must be applied as well, however. in fact, it is often the case that the given discrete process is richer in possibilities than its continuous differential system limit, and that a further study of the discrete process is practically rewarding. furthermore, there are situations where the continuous limit process may provide important qualitative, but not quantitative, infor-mation about the actual discrete process. this paper considers several case studies of such continuous limits and demonstrates success as well as cause for caution. consequences are discussed. © 2020, american institute of mathematical sciences."
"network approaches have become pervasive in many research fields. they allow for a more comprehensive understanding of complex relationships between entities as well as their group-level properties and dynamics. many networks change over time, be it within seconds or millions of years, depending on the nature of the network. our focus will be on comparative network analyses in life sciences, where deciphering temporal network changes is a core interest of molecular, ecological, neuropsychological and evolutionary biologists. further, we will take a journey through different disciplines, such as social sciences, finance and computational gastronomy, to present commonalities and differences in how networks change and can be analysed. finally, we envision how borrowing ideas from these disciplines could enrich the future of life science research. © 2020 the author(s) published by the royal society. all rights reserved."
"the pricing of financial options is usually based on statistical sampling of the evolution of the underlying under a chosen model, using a suitable numerical scheme. it is widely accepted that using low-discrepancy sequences instead of pseudorandom numbers in most cases increases the accuracy. it is important to understand and quantify the reasons for this effect. in this work, we use global sensitivity analysis in order to study one widely used model for pricing of options, namely the heston model. the heston model is an important member of the family of the stochastic volatility models, which have been found to better describe the observed behaviour of option prices in the financial markets. by using a suitable numerical scheme, like those of euler, milstein, kahl-jäckel, andersen, one has the flexibility needed to compute european, asian or exotic options. in any case the problem of evaluating an option price can be considered as a numerical integration problem. for the purposes of modelling and complexity reduction, one should make the distinction between the model nominal dimension and its effective dimension. another notion of “average dimension” has been found to be more practical from the computational point of view. the definitions and methods of evaluation of effective dimensions are based on computing sobol’ sensitivity indices. a classification of functions based on their effective dimensions is also known. in the context of quantitative finance, global sensitivity analysis (gsa) can be used to assess the efficiency of a particular numerical scheme. in this work we apply gsa based on sobol sensitivity indices in order to assess the interactions of the various dimensions in using the above mentioned schemes. we observe that the gsa offers useful insight on how to maximize the advantages of using qmc in these schemes. © springer nature switzerland ag 2020."
"a major bottleneck of the current machine learning (ml) workflow is the time consuming, error prone engineering required to get data from a datastore or a database (db) to the point an ml algorithm can be applied to it. this is further exacerbated since ml algorithms are now trained on large volumes of data, yet we need predictions in real-time, especially in a variety of time-series applications such as finance and real-time control systems. hence, we explore the feasibility of directly integrating prediction functionality on top of a data store or db. such a system ideally: (i) provides an intuitive prediction query interface which alleviates the unwieldy data engineering; (ii) provides state-of-the-art statistical accuracy while ensuring incremental model update, low model training time and low latency for making predictions. as the main contribution we explicitly instantiate a proof-of-concept, tspdb∗ which directly integrates with postgresql. we rigorously test tspdb's statistical and computational performance against the state-of-the-art time series algorithms, including a long-short-term-memory (lstm) neural network and deepar (industry standard deep learning library by amazon). statistically, on standard time series benchmarks, tspdb outperforms lstm and deepar with 1.1-1.3x higher relative accuracy. computationally, tspdb is 59-62x and 94-95x faster compared to lstm and deepar in terms of median ml model training time and prediction query latency, respectively. further, compared to postgresql's bulk insert time and its select query latency, tspdb is slower only by 1.3x and 2.6x respectively. that is, tspdb is a real-time prediction system in that its model training/prediction query time is similar to just inserting/reading data from a db. as an algorithmic contribution, we introduce an incremental multivariate matrix factorization based time series method, which tspdb is built off. we show this method also allows one to produce reliable prediction intervals by accurately estimating the time-varying variance of a time series, thereby addressing an important problem in time series analysis. © 2021 a. agarwal, a. alomar & d. shah."
"the concept of local volatility as well as the local volatility model are one of the classical topics of mathematical finance. although the existing literature is wide, there still exist various problems that have not drawn sufficient attention so far, for example: a) construction of analytical solutions of the dupire equation for an arbitrary shape of the local volatility function; b) construction of parametric or non-parametric regression of the local volatility surface suitable for fast calibration; c) no-arbitrage interpolation and extrapolation of the local and implied volatility surfaces; d) extension of the local volatility concept beyond the black-scholes model, etc. also, recent progresses in deep learning and artificial neural networks as applied to financial engineering have made it reasonable to look again at various classical problems of mathematical finance including that of building a no-arbitrage local/implied volatility surface and calibrating it to the option market data. this book was written with the purpose of presenting new results previously developed in a series of papers and explaining them consistently, starting from the general concept of dupire, derman and kani and then concentrating on various extensions proposed by the author and his co-authors. this volume collects all the results in one place, and provides some typical examples of the problems that can be efficiently solved using the proposed methods. this also results in a faster calibration of the local and implied volatility surfaces as compared to standard approaches. the methods and solutions presented in this volume are new and recently published, and are accompanied by various additional comments and considerations. since from the mathematical point of view, the level of details is closer to the applied rather than to the abstract or pure theoretical mathematics, the book could also be recommended to graduate students with majors in computational or quantitative finance, financial engineering or even applied mathematics. in particular, the author used to teach some topics of this book as a part of his special course on computational finance at the tandon school of engineering, new york university. © 2020 by world scientific publishing co. pte. ltd. all rights reserved."
"introduction this book introduces machine learning methods in finance. it presents a unified treatment of machine learning and various statistical and computational disciplines in quantitative finance, such as financial econometrics and discrete time stochastic control, with an emphasis on how theory and hypothesis tests inform the choice of algorithm for financial data modeling and decision making. with the trend towards increasing computational resources and larger datasets, machine learning has grown into an important skillset for the finance industry. this book is written for advanced graduate students and academics in financial econometrics, mathematical finance and applied statistics, in addition to quants and data scientists in the field of quantitative finance. machine learning in finance: from theory to practice is divided into three parts, each part covering theory and applications. the first presents supervised learning for cross-sectional data from both a bayesian and frequentist perspective. the more advanced material places a firm emphasis on neural networks, including deep learning, as well as gaussian processes, with examples in investment management and derivative modeling. the second part presents supervised learning for time series data, arguably the most common data type used in finance with examples in trading, stochastic volatility and fixed income modeling. finally, the third part presents reinforcement learning and its applications in trading, investment and wealth management. python code examples are provided to support the readers’ understanding of the methodologies and applications. the book also includes more than 80 mathematical and programming exercises, with worked solutions available to instructors. as a bridge to research in this emergent field, the final chapter presents the frontiers of machine learning in finance from a researcher’s perspective, highlighting how many well-known concepts in statistical physics are likely to emerge as important methodologies for machine learning in finance. © springer nature switzerland ag 2020."
"blockchain is widely used in encrypted currency, internet of things (iot), supply chain finance, data sharing, and other fields. however, there are security problems in blockchains to varying degrees. as an important component of blockchain, hash function has relatively low computational efficiency. therefore, this paper proposes a new scheme to optimize the blockchain hashing algorithm based on prca (proactive reconfigurable computing architecture). in order to improve the calculation performance of hashing function, the paper realizes the pipeline hashing algorithm and optimizes the efficiency of communication facilities and network data transmission by combining blockchains with mimic computers. meanwhile, to ensure the security of data information, this paper chooses lightweight hashing algorithm to do multiple hashing and transforms the hash algorithm structure as well. the experimental results show that the scheme given in the paper not only improves the security of blockchains but also improves the efficiency of data processing.  © 2020 jinhua fu et al."
"the study is directed towards development of an adaptive decision support system for modeling and forecasting nonlinear nonstationary processes in economy, finances and other areas of human activities. the structure and parameter adaptation procedures for the regression and probabilistic models are proposed as well as the respective information system architecture and functional layout are developed. the system development is based on the system analysis principles such as adaptive model structure estimation, optimization of model parameter estimation procedures, identification and taking into consideration of possible uncertainties met in the process of data processing and mathematical model development. the uncertainties are inherent to data collecting, model constructing and forecasting procedures and play a role of negative influence factors to the information system computational procedures. reduction of their influence is favourable for enhancing the quality of intermediate and final results of computations. the illustrative examples of practical application of the system developed proving the system functionality are provided. © v. danilov, o. gozhyj, i. kalinina, a. belas, p. bidyuk, o. jirov, 2020."
"modelling and simulation (m&s) have gradually become irreplaceable tools in the field of nuclear science and technology (ns&t), including nuclear energy systems. this is partly due to growing computational resources and advances in computational science and partly to the difficulties to finance, build and license new experimental facilities. however, the utilization of m&s for research and development (r&d) and education and training (e&t) applications is somewhat hampered by limited accessibility to controlled and sensitive nuclear m&s tools as well as by the desires of the developers of these tools to retain their intellectual property (ip). open-source software and open-access data are growingly perceived as means to accelerate innovation by promoting synergistic collaborative developments while lowering the barriers associated to code distribution, modification, and sharing. open-source software development is ideal for r&d and e&t purposes because it permits the enhancement of understanding, the use of advanced computational methods and it promotes the cooperation among researchers and scientists, without rigorous constraints on quality assurance or reliance on proprietary data for technology-specific validation. as a fundamental research tool, this helps to mitigate constraints related to dual use of such technology. it is in this context that an initiative is being launched under the aegis of the international atomic energy agency (iaea) to promote the development and application of open-source multi-physics simulation in support of r&d and e&t in ns&t. this paper presents scope and objectives of this initiative. © the authors, published by edp sciences. this is an open access article distributed under the terms of the creative commons attribution license 4.0 (http://creativecommons.org/licenses/by/4.0/)."
"nonlinear stochastic modeling plays a significant role in disciplines such as psychology, finance, physical sciences, engineering, econometrics, and biological sciences. dynamical consistency, positivity, and boundedness are fundamental properties of stochastic modeling. a stochastic coronavirus model is studied with techniques of transition probabilities and parametric perturbation. well-known explicit methods such as euler maruyama, stochastic euler, and stochastic runge-kutta are investigated for the stochastic model. regrettably, the above essential properties are not restored by existing methods. hence, there is a need to construct essential properties preserving the computational method. the non-standard approach of finite difference is examined to maintain the above basic features of the stochastic model. the comparison of the results of deterministic and stochastic models is also presented. our proposed efficient computational method well preserves the essential properties of the model. comparison and convergence analyses of the method are presented. © 2021 tech science press. all rights reserved."
"this book presents the principles and methods for the practical analysis and prediction of economic and financial time series. it covers decomposition methods, autocorrelation methods for univariate time series, volatility and duration modeling for financial time series, and multivariate time series methods, such as cointegration and recursive state space modeling. it also includes numerous practical examples to demonstrate the theory using real-world data, as well as exercises at the end of each chapter to aid understanding. this book serves as a reference text for researchers, students and practitioners interested in time series, and can also be used for university courses on econometrics or computational finance. © springer nature switzerland ag 2020. all rights are reserved."
"finding the largest subset of sequences (i.e., time series) that are correlated above a certain threshold, within large datasets, is of significant interest for computer vision and pattern recognition problems across domains, including behavior analysis, computational biology, neuroscience, and finance. maximal clique algorithms can be used to solve this problem, but they are not scalable. we present an approximate, but highly efficient and scalable, method that represents the search space as a union of sets called ε- expanded clusters, one of which is theoretically guaranteed to contain the largest subset of synchronized sequences. the method finds synchronized sets by fitting a euclidean ball on ε-expanded clusters, using jung's theorem. we validate the method on data from the three distinct domains of facial behavior analysis, finance, and neuroscience, where we respectively discover the synchrony among pixels of face videos, stock market item prices, and dynamic brain connectivity data. experiments show that our method produces results comparable to, but up to 300 times faster than, maximal clique algorithms, with speed gains increasing exponentially with the number of input sequences. © 2020 ieee"
"in this paper, we propose a method for bounding the probability that a stochastic differential equation (sde) system violates a safety specification over the infinite time horizon. sdes are mathematical models of stochastic processes that capture how states evolve continuously in time. they are widely used in numerous applications such as engineered systems (e.g., modeling how pedestrians move in an intersection), computational finance (e.g., modeling stock option prices), and ecological processes (e.g., population change over time). previously the safety verification problem has been tackled over finite and infinite time horizons using a diverse set of approaches. the approach in this paper attempts to connect the two views by first identifying a finite time bound, beyond which the probability of a safety violation can be bounded by a negligibly small number. this is achieved by discovering an exponential barrier certificate that proves exponentially converging bounds on the probability of safety violations over time. once the finite time interval is found, a finite-time verification approach is used to bound the probability of violation over this interval. we demonstrate our approach over a collection of interesting examples from the literature, wherein our approach can be used to find tight bounds on the violation probability of safety properties over the infinite time horizon. © 2020, the author(s)."
"graphs arise naturally in many real-world applications including social networks, recommender systems, ontologies, biology, and computational finance. traditionally, machine learning models for graphs have been mostly designed for static graphs. however, many applications involve evolving graphs. this introduces important challenges for learning and inference since nodes, attributes, and edges change over time. in this survey, we review the recent advances in representation learning for dynamic graphs, including dynamic knowledge graphs. we describe existing models from an encoder-decoder perspective, categorize these encoders and decoders based on the techniques they employ, and analyze the approaches in each category. we also review several prominent applications and widely used datasets and highlight directions for future research. ©2020 seyed mehran kazemi, rishab goel, kshitij jain, ivan kobyzev, akshay sethi, peter forsyth, pascal poupart."
"online portfolio selection is regarded as a fundamental problem in computational finance. pattern-matching methods, and the corn-k algorithm in particular, have provided promising results. despite making notable progress, there exists a gap in the current state of the art – systematic risk is not considered. the lack of attention to systematic risk could lead to poor investment returns, especially in volatile markets. in response to this, we extend the corn-k algorithm to present dricorn-k – a dynamic risk correlation-driven non-parametric algorithm. dricorn-k continuously adjusts a portfolio’s market sensitivity based on the current market conditions. we measure market sensitivity using the β measure. dricorn-k aims to take advantage of upward market trends and protect portfolios against downward market trends. to this end, we implement a number of market classification methods. we find that an exponentially weighted moving linear regression method provides the best classification of current market conditions. we further conducted an empirical analysis on five real world stock indices: the jse top 40, bovespa, dax, djia and nikkei 225 against twelve state of the art algorithms. the results show that dricorn-k can deliver improved performance over the current state of the art, as measured by cumulative return, sharpe ratio and maximum drawdown. the experimental results lead us to conclude that the addition of dynamic systematic risk adjustments to corn-k can result in improved portfolio performance. © 2020, springer nature switzerland ag."
"we provide an overview on the recent advances in textual analysis for social sciences. count-based economic model, structured statistical tool, and plain-vanilla machine learning apparatus each have their own merits and limitations. to take a data-driven approach to capture complex linguistic structures while ensuring computational scalability and economic interpretability, a general framework for analyzing large-scale text-based data is needed. we discuss the recent attempts combining the strengths of neural network language models, such as word embedding, and generative statistical modeling, such as topic modeling. we also describe typical sources of texts and the applications of these methodologies to issues in finance and economics and discuss promising future directions. © 2021 by world scientific publishing co. pte. ltd."
"the rapidly evolving coronavirus pandemic brings a devastating effect on the entire world and its economy as a whole. further instability related tocovid-19 will negatively affect not only on companies and financial markets, but also on traders and investors that have been interested in saving their investment, minimizing risks, and making decisions such as how to manage their resources, how much to consume and save, when to buy or sell stocks, etc., and these decisions depend on the expectation of when to expect next critical change. trying to help people in their subsequent decisions, we demonstrate the possibility of constructing indicators of critical and crash phenomena on the example of bitcoin market crashes for further demonstration of their efficiency on the crash that is related to the coronavirus pandemic. for this purpose, the methods of the theory of complex systems have been used. since the theory of complex systems has quite an extensive toolkit for exploring the nonlinear complex system, we take a look at the application of the concept of entropy in finance and use this concept to construct 6 effective entropy measures: shannon entropy, approximate entropy, permutation entropy, and 3 recurrence based entropies. we provide computational results that prove that these indicators could have been used to identify the beginning of the crash and predict the future course of events associated with the current pandemic. © 2020 ceur-ws. all rights reserved."
"the main aim of this study is to build a robust novel approach that is able to detect outliers in the datasets accurately. to serve this purpose, a novel approach is introduced to determine the likelihood of an object to be extremely different from the general behavior of the entire dataset. this paper proposes a novel two-level approach based on the integration of bagging and voting techniques for anomaly detection problems. the proposed approach, named bagged and voted local outlier detection (bv-lof), benefits from the local outlier factor (lof) as the base algorithm and improves its detection rate by using ensemble methods. several experiments have been performed on ten benchmark outlier detection datasets to demonstrate the effectiveness of the bv-lof method. according to the results, the bv-lof approach significantly outperformed lof on 9 datasets of 10 ones on average. in the bv-lof approach, the base algorithm is applied to each subset data multiple times with different neighborhood sizes (k) in each case and with different ensemble sizes (t). in our study, we have chosen k and t value ranges as [1-100]; however, these ranges can be changed according to the dataset handled and to the problem addressed. the proposed method can be applied to the datasets from different domains (i.e. health, finance, manufacturing, etc.) without requiring any prior information. since the bv-lof method includes two-level ensemble operations, it may lead to more computational time than single-level ensemble methods; however, this drawback can be overcome by parallelization and by using a proper data structure such as r∗-tree or kd-tree. the proposed approach (bv-lof) investigates multiple neighborhood sizes (k), which provides findings of instances with different local densities, and in this way, it provides more likelihood of outlier detection that lof may neglect. it also brings many benefits such as easy implementation, improved capability, higher applicability, and interpretability. © 2020 2020 alican dogan et al., published by sciendo."
"the abstract the dynamic programming approach is a popular and efficient tool to solve markov decision problems (mdps) with finite state and action spaces. the discounted cost criterion is considered over infinite planning horizon and the goal is to minimize the utility function. this optimization sometimes requires considerable computational efforts in term of complexity. in this regard, researchers contribute by providing tests in order to identify suboptimal decisions at each iteration step on the planning horizon. in this manuscript, we give a new test of sub-optimality after proposing lower and upper bounds on the performance system in the literature of action elimination procedures. next we compare this result to semmouri and jourhmane test. markov chains are used in a wide way to model stochastic optimization situations that appear in various areas such that forest management, management of energy consumption, finance and artificial intelligence. © 2020, institute of advanced scientific research, inc. all rights reserved."
"recent progress of graphics processing unit (gpu) computing with applications in science and technology has demonstrated tremendous impact over the last decade. however, financial applications by gpu computing are less discussed and may cause an obstacle toward the development of financial technology, an emerging and disruptive field focusing on the efficiency improvement of our current financial system. this chapter aims to raise the attention of gpu computing in finance by first empirically investigating the performance of three basic computational methods including solving a linear system, fast fourier transform, and monte carlo simulation. then a fast calibration of the wing model to implied volatilities is explored with a set of traded futures and option data in high frequency. at least 60% executing time reduction on this calibration is obtained under the matlab computational environment. this finding enables the disclosure of an instant market change so that a real-time surveillance for financial markets can be established for either trading or risk management purposes. © 2021 by world scientific publishing co. pte. ltd."
"prediction tasks are often carried out efficiently by soft computing methods. this paper presents how techniques in machine learning and soft computing areas can be easily applied to problems in computational finance. one such prevalent problems is that of portfolio allocation. the typical case is that of predicting stocks with high returns and allocating them to the basket of portfolios. this process is known as the stock selection in portfolio construction. it is about this problem, the paper is designed to address with the help of machine learning task especially one of the supervised learning methods, artificial neural networks. once this task is accomplished, to find out an efficient portfolio, among a basket of financial portfolios, applied various approaches. one such approach is to compute the minimum variance portfolio subject to the target return. this is the basis of mean variance theory put forward by markowitz. based on this approach the neural network is trained to attain an efficient portfolio. a specific market (india-bse, nse) and a particular asset (such as a stock market) are focused. © 2020 ieee."
"online portfolio selection is an important fundamental problem in computational finance, which has been further developed in recent years. as the financial market changes rapidly, investors need to dynamically adjust asset positions according to various financial market information. however, existing online portfolio strategies are always designed without considering this information, which limits their practicability to some extent. to overcome this limitation, this paper exploits the available side information and presents a novel online portfolio strategy named “waacs”. specifically, all the constant rebalanced portfolio strategies are considered as experts and the weak aggregating algorithm is applied to aggregate all the expert advice according to their previous cumulative returns under the same side information state as the current period. furthermore, waacs is theoretically proved to be a universal portfolio, i.e., its growth rate is asymptotically the same as that of the best state constant rebalanced portfolio, which is a benchmark strategy considering side information. numerical experiments show that waacs achieves significant performance and demonstrate that considering side information improves the performance of the proposed strategy. © 2019, springer-verlag gmbh germany, part of springer nature."
"this chapter aims to present the current state of the art in text analysis and its potential applications in finance. recent advances in computational linguistics and natural language processing are welcome by the financial profession as the volume of text recorded, published or posted outgrows the capacity of humans to analyze it. texts available to financial institutions contain a wealth of information that can be used to target marketing, create early-warning systems in client management, signal potential cases of fraud or abuse and strengthen risk analysis. experience shows, however, that text analysis is challenging. human languages are diverse, they offer a freedom of structuring information and allow ambiguity. the chapter presents some key innovations in areas such as sentiment analysis, topic identification, disambiguation and multilingualism. it covers the underlying theory, methods and current applications. the chapter concludes with a review of challenges and new research areas. this work was supported by the national science centre in poland under grant number 2014/13/b/hs4/01577. © 2021 selection and editorial matter, lech gąsiorkiewicz and jan monkiewicz."
"this paper develops a computational framework for inverting gompertz–makeham mortality hazard rates, consistent with compensation laws of mortality for heterogeneous populations, to define a longevity-risk-adjusted global (l-rag) age. to illustrate its salience and possible applications, the paper calibrates and presents l-rag values using country data from the human mortality database (hmd). among other things, the author demonstrates that when properly benchmarked, the longevity-risk-adjusted global age of a 55-year-old swedish male is 48, whereas a 55-year-old russian male is closer in age to 67. the paper also discusses the connection between the proposed l-rag age and the related concept of biological age, from the medical and gerontology literature. practically speaking, in a world of growing mortality heterogeneity, the l-rag age could be used for pension and retirement policy. in the language of behavioral finance and economics, a salient metric that adjusts chronological age for longevity risk might help capture the public's attention, educate them about lifetime uncertainty and induce many of them to take action — such as working longer and/or retiring later. © 2020 the author(s)"
"nowadays, critical sectors in government, finance, and military are facing increasingly high security challenges. however, traditional public-key crypto-systems based on computational complexity are likely to suffer from upgrade computational power. quantum key distribution (qkd) is a promising technology to effectively address the challenge by providing secret keys due to the laws of quantum physics. limited by the transmission distance of quantum communications, remote parties have to share secret keys by exchanging keys through the trusted relay nodes hop by hop. however, if relaying hop by hop is still used in metro quantum-optical networks (mqon), a large amount of key resources will be wasted since the distance between any two nodes is short. therefore, the problem of how to distribute quantum keys with lower waste of key resources over mqon is urgent. in order to solve this problem, we design a novel quantum node structure that is able to bypass itself. also, by extending the connectivity graph, auxiliary graphs are constructed to describe the adjacency of quantum nodes in different levels influenced by the physical distance. based on the novel node, two routing, wavelength and time-slot assignment algorithms are proposed, in which some middle nodes can be bypassed to reduce the resource consumption as long as the distance between the two parties meets the requirement of quantum key distribution. simulations have been conducted to verify the performance of the proposed algorithms in terms of blocking probability, resource utilization, number of bypassed nodes, and security rate per service. numerical results illustrate that our algorithms perform better on resource utilization than a traditional scheme without bypass. furthermore, a tradeoff between the keys saved and blocking probability is analyzed and discussed in our paper. © 2020 optical society of america under the terms of the osa open access publishing agreement."
"optimizing over the cone of nonnegative polynomials, and its dual counterpart, optimizing over the space of moments that admit a representing measure, are fundamental problems that appear in many different applications from engineering and computational mathematics to business. in this paper, we review a number of these applications. these include, but are not limited to, problems in control (e.g., formal safety verification), finance (e.g., option pricing), statistics and machine learning (e.g., shape-constrained regression and optimal design), and game theory (e.g., nash equilibria computation in polynomial games). we then show how sum of squares techniques can be used to tackle these problems, which are hard to solve in general. we conclude by highlighting some directions that could be pursued to further disseminate sum of squares techniques within more applied fields. among other things, we briefly address the current challenge that scalability represents for optimization problems that involve sum of squares polynomials and discuss recent trends in software development. © 2018 american mathematical society."
"the recurrent neural network is generally utilized in an assortment of areas, such as pattern recognition, natural language processing and computational learning. time series prediction is one of the most challenging topics for many years due to its application in finance and decision making. this article centers chiefly on the methods and techniques of forecasting future prices of two stocks: iam. pa and ora. pa. an experimental investigation is grounded on two years of historical information. furthermore, the statistics of the stocks and the predictions are made for 22 days in advance. the prediction performance compares three approaches of the recurrent neural network: elman recurrent neural network in the first stage, long short-term memory recurrent neural network for the next phase, and gated recurrent unit in the third phase. the article aims to accommodate a comparative analysis among these three models based on mean square error, time per step, memory and the number of hidden nodes required for excellent accuracy. © international association of engineers."
"in this paper, we present an agent-based model (abm) of multi-dimensional transportation choices for individuals and firms given anticipated aggregate traveler demand patterns. conventional finance, economic and policy evaluation techniques have already been widely adopted to more evidenced based decision-making process with the aim to understand the financial, economic and social impacts on transportation choices. prior scholars have examined common practices used to measure profitability for investment appraisal including internal rate of return (irr), net present value (npv) and risk analysis approaches, incorporating the concepts of time value of money and uncertainty to assess potential financial gains with different transportation projects. however, using conventional capital budget planning or static scenario analysis alone cannot capture significant, interactive and nonlinear project, demand and market uncertainties. here we build an agent-based model on the current california high-speed rail (hsr) to provide insights into firm investment decisions from a computational finance perspective, given the coupling of individual choices, aggregate social demand, and government policy and tax incentives. given individual level choice and behavioral aspects, we combine financial accounting and economic theory to identify more precise marginal revenue streams and project profitability over time to help mitigate both project and potential, system market risk. © 2020, springer nature switzerland ag."
"information networks are pivotal to the operational utility of key industries like medical, finance, governments, etc. however, applications in this area are not adequate in representing relationships between nodes[34]. trending graph learning methodologies[9, 16] like graph convolutional networks (gcns)[6] lack both representational power and accuracy to perform abstract computational tasks like prediction, classification, recommendation, etc. on real-time social networks. furthermore, most such approaches known to date rely on learning temporal adjacency matrices to describe shallow attributes[9, 16] like word co-occurance pmi[3] changes[6] and are unable to capture complex evolving entity relationships in real life for applications like event prediction, link prediction, topic tracking, etc.[34]. importantly, such models ignore knowledge information geometry[1, 24, 32] completely, and sacrifices fidelity to speed of convergence. to address these challenges, a novel relational flux turbulence (rft) model was developed in this study - to identify relational turbulence in online social networks (osns). very good correlations between relational turbulence and sentiments exchanged within social transactions show promise in achieving these objectives. © springer nature switzerland ag 2020."
"the abandonment option of an operating oil project refers to the right to shut down or transfer the project. as a kind of american real option, it minimizes the impact of bad operating conditions, thus increases the initial project value. meanwhile, as a put option, it maximizes the management flexibility in unfavorable environment, especially in the current low oil prices. this article uses the trinomial tree, rather than the binomial tree widely practiced in finance, to value the option. its lattice structure shows flexibility and intelligibility, and improves computational efficiency and accuracy. in this article, the abandonment option value incorporates uncertainties of oil price, exchange rate, political environment and taxation policy. the risk-neutral based decisions are relatively objective for oil companies. the case study indicates that the relative relationship between the abandonment option value and the project scrap value or selling price is the key to the decision-making results. a novel conclusion from the risk-neutral prospective is that, the project is more likely to be sold at higher risk scenario or with higher profit requirement. moreover, export duty and mineral extraction tax have a greater impact on the abandonment timing than corporate income tax. this decision-making model can be introduced with modifications to other investments with increasing risk of falling asset price. © 2018, springer science+business media, llc, part of springer nature."
"nowadays, the lack of data is a major challenge in many it fields such as medical, social sciences and finance. invalid approaches to handling missing data can lead to biased results,overly confident intervals and imprecise inferences. to characterize the missing, the missing patterns and the missing mechanism are used. the missing model visually shows the absence of values in the dataset where the missing mechanism addresses the definition of the missing with probability values. missing mechanism the classifications depend on the missing models such as missing at random (mar), missing completely at random (mcar) and missing not at random (mnar). imputation of miss forestis widely used to manage missing data. but the computational complexity will increase if we use this method for high dimensional multidimensional data(where p > n ).thus, the main objective of this article is to improve the accuracy of the imputation technique and to develop a distributed imputation technique for management. lack in high dimensional multidimensional data. © 2020, institute of advanced scientific research, inc.. all rights reserved."
"the rough heston model has recently attracted the attention of many finance practitioners and researchers as it maintains the basic structure of the classical heston model while having descriptive capabilities in terms of microstructural foundations of the market. using the fact that the characteristic function of log-price in this model could be expressed in terms of the solution of a nonlinear parametric fractional riccati differential equation not admitting a closed-form solution, devising efficient numerical schemes for pricing and calibration under this model has become a crucial need in the computational finance community. although the fractional adams method has been used in most of the recent studies on the rough heston model, this method suffers from some stability and convergence issues in treating the problem. in this paper, we present a numerical method based on newton-kantorovich quasi-linearization to solve the nonlinearity issue followed by spectral collocation based on """"polyfractonomials"""" to approximate the fractional derivative in an accurate and efficient manner. we provide sufficient conditions under which our method is convergent and the order of convergence is also obtained. in order to guarantee the specified convergence rate, we first prove some regularity results on the linearized problem and then employ the proposed scheme to solve a practical calibration problem from the spx options market. the efficiency of the proposed method is illustrated by comparing the obtained results with those of the fractional adams method as well as a fast hybrid scheme based on fractional power series expansion. © 2020 society for industrial and applied mathematics publications. all rights reserved."
"a robust estimator is proposed for the parameters that characterize the linear regression problem. it is based on the notion of shrinkages, often used in finance and previously studied for outlier detection in multivariate data. a thorough simulation study is conducted to investigate: the efficiency with normal and heavy-tailed errors, the robustness under contamination, the computational time, the affine equivariance and breakdown value of the regression estimator. two classical data-sets often used in the literature and a real socioeconomic data-set about the living environment deprivation of areas in liverpool (uk), are studied. the results from the simulations and the real data examples show the advantages of the proposed robust estimator in regression. © 2020, springer-verlag gmbh germany, part of springer nature."
"volatility can be defined as the conditional expectation of the squared return of a financial asset. many of the classical nonparametric regression estimators can be applied in volatility prediction. examples of nonparametric estimators include moving averages and kernel estimators. however, it has been difficult to beat some parametric estimators from the generalized autoregressive conditionally heteroscedastic family using nonparametric estimators. we review some promising suggestions for nonparametric volatility prediction. this article is categorized under: applications of computational statistics > computational finance statistical and graphical methods of data analysis > nonparametric methods statistical models > time series models. © 2020 wiley periodicals, inc."
"financial time series forecasting is undoubtedly the top choice of computational intelligence for finance researchers in both academia and the finance industry due to its broad implementation areas and substantial impact. machine learning (ml) researchers have created various models, and a vast number of studies have been published accordingly. as such, a significant number of surveys exist covering ml studies on financial time series forecasting. lately, deep learning (dl) models have appeared within the field, with results that significantly outperform their traditional ml counterparts. even though there is a growing interest in developing models for financial time series forecasting, there is a lack of review papers that solely focus on dl for finance. hence, the motivation of this paper is to provide a comprehensive literature review of dl studies on financial time series forecasting implementation. we not only categorized the studies according to their intended forecasting implementation areas, such as index, forex, and commodity forecasting, but we also grouped them based on their dl model choices, such as convolutional neural networks (cnns), deep belief networks (dbns), and long-short term memory (lstm). we also tried to envision the future of the field by highlighting its possible setbacks and opportunities for the benefit of interested researchers. © 2020"
"recent studies show that deep neural networks (dnn) are vulnerable to adversarial samples that are generated by perturbing correctly classified inputs to cause the misclassification of dnn models. this can potentially lead to disastrous consequences, especially in security-sensitive applications such as unmanned vehicles, finance and healthcare. existing adversarial defense methods require a variety of computing units to effectively detect the adversarial samples. however, deploying adversary sample defense methods in existing dnn accelerators leads to many key issues in terms of cost, computational efficiency and information security. moreover, existing dnn accelerators cannot provide effective support for special computation required in the defense methods. to address these new challenges, this paper proposes dnnguard, an elastic heterogeneous dnn accelerator architecture that can efficiently orchestrate the simultaneous execution of original (target) dnn networks and the detect algorithm or network that detects adversary sample attacks. the architecture tightly couples the dnn accelerator with the cpu core into one chip for efficient data transfer and information protection. an elastic dnn accelerator is designed to run the target network and detection network simultaneously. besides the capability to execute two networks at the same time, dnnguard also supports the non-dnn computing and allows the special layer of the neural network to be effectively supported by the cpu core. to reduce off-chip traffic and improve resources utilization, we propose a dynamical resource scheduling mechanism. to build a general implementation framework, we propose an extended ai instruction set for neural networks synchronization, task scheduling and efficient data interaction. we implement dnnguard based on risc-v and nvdla, and evaluate its performance impacts with six target networks and three typical detection networks. experiment results show that dnnguard can effectively validate the legitimacy of the input samples in parallel with the target dnn model, achieving an average 1.42× speedup compared with the state-of-the-art accelerators. © 2020 association for computing machinery."
"portfolio optimization is one of the most important problems in the finance field. the traditional markowitz mean-variance model is often unrealistic since it relies on the perfect market information. in this work, we propose a two-stage stochastic portfolio optimization model with a comprehensive set of real-world trading constraints to address this issue. our model incorporates the market uncertainty in terms of future asset price scenarios based on asset return distributions stemming from the real market data. compared with existing models, our model is more reliable since it encompasses real-world trading constraints and it adopts cvar as the risk measure. furthermore, our model is more practical because it could help investors to design their future investment strategies based on their future asset price expectations. in order to solve the proposed stochastic model, we develop a hybrid combinatorial approach, which integrates a hybrid algorithm and a linear programming (lp) solver for the problem with a large number of scenarios. the comparison of the computational results obtained with three different metaheuristic algorithms and with our hybrid approach shows the effectiveness of the latter. the superiority of our model is mainly embedded in solution quality. the results demonstrate that our model is capable of solving complex portfolio optimization problems with tremendous scenarios while maintaining high solution quality in a reasonable amount of time and it has outstanding practical investment implications, such as effective portfolio constructions. © 2019, springer-verlag gmbh germany, part of springer nature."
"vulnerable nature of price forecasts, such as an unpredictability of future and numbers of socio-economic factors that affect market stability, often makes investment risky. earlier studies in finance suggested that constructing a portfolio can promise risk-spread gains. while fund standardization improved the traditional theories by reducing the computational complexity and by associating every interaction in the portfolio, such a method still cannot become a winning strategy because it does not measure the current value or the relative price of each asset. inspired by the works of finding returns per risk, we attempt to design an optimal portfolio by searching products that have potential to grow further. more specifically, we first analyze risk-adjusted returns in the previous periods and use their inertia as a momentum. however, because historic movements alone do not fully elucidate future changes nor guarantee positive returns, we scored the relative values of each stock to make more informed estimations. using the capital asset pricing model, we measured the values of each stock and determined those undervalued. in this study, we applied a genetic algorithm to optimize portfolios while incorporating the momentum strategy and the asset valuations. the proposed ga model was tested in two separate markets, sp500 and kospi200, and projected greater profits than that from both the previous method with momentum method and the market indexes. from the experimental results, the proposed capm+ method was found to be very effective in financial data analysis and to lay a groundwork for a sustainable investment execution. © 2013 ieee."
"recently, zou et al. (2017) proposed a novel covariance regression model to study the relationship between the covariance matrix of responses and their associated similarity matrices induced by auxiliary information. to estimate the covariance regression model, they introduced five estimators: the maximum likelihood, ordinary least squares, constrained ordinary least squares, feasible generalized least squares and constrained feasible generalized least squares estimators. among these five, they recommended the constrained feasible generalized least squares estimator due to its estimation efficiency and computational convenience. under the normality assumption, they further demonstrated the theoretical properties of these estimators. however, the data in the area of finance and accounting may exhibit heavy tails. hence, to broaden the usefulness of the covariance regression model, we relax the normality assumption and employ lee’s (2004) approach to obtain inferences for covariance regression parameters based on the five estimators proposed by zou et al. (2017). two empirical examples are presented to illustrate the practical applications of the covariance regression model in analyzing stock return comovement and herding behavior of mutual funds. © 2021 by world scientific publishing co. pte. ltd."
"currently, machine learning (ml) is becoming ubiquitous in everyday life. deep learning (dl) is already present in many applications ranging from computer vision for medicine to autonomous driving of modern cars as well as other sectors in security, healthcare, and finance. however, to achieve impressive performance, these algorithms employ very deep networks, requiring a significant computational power, both during the training and inference time. a single inference of a dl model may require billions of multiply-and-accumulated operations, making the dl extremely compute- and energy-hungry. in a scenario where several sophisticated algorithms need to be executed with limited energy and low latency, the need for cost-effective hardware platforms capable of implementing energy-efficient dl execution arises. this paper first introduces the key properties of two brain-inspired models like deep neural network (dnn), and spiking neural network (snn), and then analyzes techniques to produce efficient and high-performance designs. this work summarizes and compares the works for four leading platforms for the execution of algorithms such as cpu, gpu, fpga and asic describing the main solutions of the state-of-the-art, giving much prominence to the last two solutions since they offer greater design flexibility and bear the potential of high energy-efficiency, especially for the inference process. in addition to hardware solutions, this paper discusses some of the important security issues that these dnn and snn models may have during their execution, and offers a comprehensive section on benchmarking, explaining how to assess the quality of different networks and hardware systems designed for them. © 2013 ieee."
"algorithmic differentiation (ad) shown to be an essential tool to get sensitivity information for va in multiple areas of science such as computational fluid dynamics (cfd) applications or finance. yet there is no sufficient tool to ease the cost of providing performance portable ad codes, especially for modern hardware like gpu clusters. this paper sketches our plans and progress so far to extend the ops framework with an adjoint tape (storage for descriptors of intermediate steps and intermediate states of variables) and shows preliminary performance results on cpu nodes. the ops (oxford parallel library for structured mesh solvers) has shown good performance and scaling on a wide range of hpc architectures. our work aims to exploit the benefits of ops to provide performance portable adjoint implementations for future structured mesh stencil applications using ops with minimal modifications. © 2020 ieee."
"the arthaśāstra of kauṭilya (ca. first century bce-third century ce) is the most important source on state administration from classical india. its far-ranging instructions on state activities depict an ideal-typical kingdom heavily reliant on computation, particularly with respect to state finances. nevertheless, computational practices themselves are little discussed, and no general study of them in the arthaśāstra yet exists. this chapter is a primarily philological effort to frame an initial inquiry into such practices in the text through a study of the terms through which computation is expressed or implied. after introducing the arthaśāstra, i examine: 1. various means of assigning value as laid out in the text (including an overview of mensuration in the arthaśāstra); 2. some of the most prevalent numerical operations and procedures; 3. the use of these in a few examples of state activities; and 4. how computation was conceived among other evaluative activities. © the editor(s) (if applicable) and the author(s), under exclusive license to springer nature switzerland ag 2020."
"the aim of this article is to introduce a hybrid method, based on the combination of monte carlo simulation and neural networks, which ensures for a general model an optimal compromise between accuracy and computing time. the major contribution of this work is that the aforesaid improvements are made whatever the hypotheses adopted to simplify the reality of the studied problem. in this article, this methodology is applied to the option pricing as a problematic in the fields of finance. based on a database of 6 contracts of european calls on the cac 40 index, the obtained results are in accordance with our expectations. indeed, they highlight the precision of this methodology and the decrease of the computational time, in comparison with that of the monte carlo simulation. it, thus, implied that the proposed method can be used as an alternative solution to any derivative pricing problem. © 2020 taylor & francis group, llc."
"brief biography: qian li was born in shandong, china, in 1989. she received the bachelors degree in mathemat- ics and applied mathematics and minor bachelors degree in finance from the tianjin university and nankai university (tianjin, china) in 2012, and the master and ph.d. degrees in operations research from suny stony brook university, stony brook, ny, usa in 2014 and 2019, respectively. in 2012, she joined the department of applied mathematics and statistics (ams), stony brook university, and in 2014 she became a ph.d. candidate focus on computational ge- ometry. since august 2012, she has been teaching assistant, summer instructor and research assistant in department of ams. her main research is sensor networks and coverage under uncertainty. © 2020 copyright is held by the owner/author(s)."
"smart cities are envisioned to efficiently use two most critical resources: water and energy. advanced techniques are being developed to conserve water and to minimize the use of conventional energy. the integration of battery energy storage (bes) technology and renewable energy (re) sources in the system is very much required to enable the efficient coupling and modularization of electricity and water infrastructure. this chapter presents a conceptual framework to demonstrate an internet of things (iot)-based intelligent hardware-software platform to efficiently manage the most critical infrastructure in sustainable smart cities: water and energy technologies. the integrated framework focuses on the challenges of water-energy nexus. it includes advanced technologies for metering and communication, and computational intelligence using machine learning techniques in purview of the infrastructural constraints for optimal integration of bes and re source. towards developing the conceptual framework, the systematic approach of developing an intelligent hardware-software platform is presented in this chapter. the proposed framework will be deployed in the smart city test bed at gujarat international finance tec-city (gift), india. test bed for water-energy nexus includes water treatment plant (wtp), sewage treatment plant (stp), and street lightings connected with bes and solar photovoltaic (pv) generation. we also discuss the use cases towards water-energy nexus that will be implemented in this framework. the intellectual merit of the framework is the development of hardware-software technology platform—comprising a set of common knowledge, theoretical approaches, mathematical models, optimization, and analytical tools—that will contribute in making possible the promising environmentally sustainable smart city. © 2020, springer science and business media deutschland gmbh. all rights reserved."
"given a large volume of multi-dimensional data streams, such as that produced by iot applications, finance and online web-click logs, how can we discover typical patterns and compress them into compact models? in addition, how can we incrementally distinguish multiple patterns while considering the information obtained from a pattern found in a streaming setting? in this paper, we propose a streaming algorithm, namely streamscope, that is designed to find intuitive patterns efficiently from event streams evolving over time. our proposed method has the following properties: (a) it is effective: it operates on semi-infinite collections of co-evolving streams and summarizes all the streams into a set of multiple discrete segments grouped by their similarities. (b) it is automatic: it automatically and incrementally recognizes such patterns and generates models for each of them if necessary; (c) it is scalable: the complexity of our method does not depend on the length of the data streams. our extensive experiments on real data streams demonstrate that streamscope can find meaningful patterns and achieve great improvements in terms of computational time and memory space over its full batch method competitors. © 2019 association for computing machinery."
"forecasting time series data is an important subject in economics, business, and finance. traditionally, there are several techniques such as univariate autoregressive (ar), univariate moving average (ma), simple exponential smoothing (ses), and more notably autoregressive integrated moving average (arima) with their many variations that can effectively forecast. however, with the recent advancement in the computational capacity of computers and more importantly developing more advanced machine learning algorithms and approaches such as deep learning, new algorithms have been developed to forecast time series data. this article compares different methodologies such as arima, random forest (rf), support vector machine (svm), long short-term memory (lstm) and wavenets for estimating the future price of bitcoin. © 2019 ieee."
"in this research, we employed distributed systems to explore the similarities in parking ticket records using unsupervised machine learning algorithms on a large dataset. using 37 million ticket records (9 gb) collected by the new york city department of finance, we applied an algorithm to cluster existing tickets and dive deeper to find the distribution of precincts within different clusters. amazon web services including s3, ec2 and emr, and tools like mongodb and apache spark were used in this endeavor. in this study, computational time and cost for different emr settings were evaluated. we conclude that there are significant computational advantages to using distributed systems when implementing unsupervised learning on a large dataset as well as storing and managing data. we also observed that it is time efficient for a cluster with more workers instead of fewer workers with large memory space for the utilized data set. however we observed a trade-off between the execution time and the total cost for the cluster configuration. © 2019 ieee."
"the paper presents an agent-based model of a credit economy which includes a securitisation process and a bailout mechanism for bank bankruptcies. within this framework, banks are able to sell mortgages to a financial vehicle corporation, which finances its activity by creating mortgage-backed securities and selling them to a mutual fund. in turn, the mutual fund collects liquidity by selling shares to households and remunerates them with a monthly interest. the impact of this mechanism is analysed by means of computational experiments for different levels of banks’ securitisation propensity. furthermore, we study a set of systemic risk indicators which have the aim of assessing the imbalances in the financial system. two of them are the mortgage-to-gdp ratio and the capital adequacy ratio, which are constructed to detect only the on-balance sheet changes in banks’ credit exposure. we consider two additional indicators, similar to the previous ones with the only difference that they are also able to account for the off-balance sheet items. moreover, we adopt an indicator, the so-called “virtuous–unvirtuous cycle” indicator, which, besides off-balance assets, targets also the gdp. the results show that higher securitisation propensity weakens the financial stability of banks with relevant effects on different sectors of the economy. most importantly, the analysis of systemic risk reveals the important issue of designing suitable systemic risk indicators for predicting incoming financial crises, finding that an essential feature of these indicators should be to integrate banks’ off-balance sheet assets. © 2019, springer-verlag gmbh germany, part of springer nature."
"blockchains-with their inherent properties of transaction transparency, distributed consensus, immutability and cryptographic verifiability-are increasingly seen as a means to underpin innovative products and services in a range of sectors from finance through to energy and healthcare. discussions, too often, make assertions that the trustless nature of blockchain technologies enables and actively promotes their suitability-there being no need to trust third parties or centralised control. yet humans need to be able to trust systems, and others with whom the system enables transactions. in this paper, we highlight that understanding this need for trust is critical for the development of blockchain-based systems. through an online study with 125 users of the most well-known of blockchain based systems-the cryptocurrency bitcoin-we uncover that human and institutional aspects of trust are pervasive. our analysis highlights that, when designing future blockchain-based technologies, we ought to not only consider computational trust but also the wider eco-system, how trust plays a part in users engaging/disengaging with such eco-systems and where design choices impact upon trust. from this, we distill a set of guidelines for software engineers developing blockchain-based systems for societal applications. © 2019 ieee."
"i provide a general overview of recent developments on topics concerning advanced analyses for computational economics and finance. in particular, i introduce a special issue that presents a selection of papers presented at the fourth international symposium in computational economics and finance (iscef), organized in paris in april 2016 (www.iscef.com). these papers deal with various topics in computational economics and finance. they apply different computational tools and econometrics tests (linear, nonlinear, parametric and nonparametric) and use recent data. accordingly, they offer interesting challenges and results that can help to improve and contribute to the related literature, highlighting the interest of computational economics and finance analyses to answer key questions in macroeconomics and finance. © 2018, springer science+business media, llc, part of springer nature."
"machine learning (ml) has been widely used in data analytics and processing applications. computational finance, as known as a data-intensive domain, is also a major battlefield of ml. financial streaming data is highly time-varying, thus adaptive ml algorithms are required to capture the dynamic data patterns while maintaining high accuracy through application runtime. therefore, system designers make a conservative choice which leverages oversized models. however, the high demand of computing capacity makes ml algorithm hard to meet realtime constraints occasionally in terms of inference and training (re-training) process. to better exploit the tradeoff between latency and accuracy in streaming financial applications, we propose data complexity-driven ml adaptation (mla). mla identifies model configurable parameters contributing in ml tradeoff between execution latency and accuracy as tuning adaptable parameters (taps) that are highly related with data complexity. under realtime constraint, taps reference value could be provided before online execution. experimental results show that mla could significantly improve the ml computational efficacy in streaming applications by dynamically adjusting taps in current sliding time window. the satisfaction to time constraint is statistically beyond 95% with tolerable accuracy loss (less than 2.47% on average). © 2019 ieee."
"core is a new cyber-infrastructure which will facilitate computational operations research exchange. or models arise in many engineering domains, such as design, manufacturing, and services (e.g., banking/finance, health systems), as well as specific infrastructure-centric applications such as logistics/supply chains, power system operations, telecommunications, traffic/ transportation, and many more. in addition, modern or tools have also been adopted in many foundational disciplines, such as computer science, machine learning, and others. given the broad footprint of or, the development of a robust cyber-infrastructure has the potential to not only promote greater exchange of data, models, software, and experiments but also enhance reproducibility and re-usability, both within or, and across multiple disciplines mentioned above. core also has the potential to drastically reduce the computational burden on research communities which study resource allocation using analytics. this paper presents an overview of the functionality, design, and computations using core. © 2019 ieee."
"with the rapid development of internet finance, credit scoring has played a significant role in peer to peer lending platforms. however, the massive and high-dimensional characteristics of the credit data make it difficult to directly build the credit scoring model. therefore, the feature selection is attracting more and more attention, which can be used for processing the complicated credit data. in this paper, we proposed a novel feature selection approach, which is specifically designed for analyzing the customer data in credit scoring. firstly, we proposed a strategy that combining multiple filters to select the different candidate feature subsets from customer data. then, a new separable degree index is proposed, which can select the optimal feature subset for credit scoring. experimental results indicated that the performance of the approach we proposed is superior to other single filters, and the computational cost is greatly reduced compared to the traditional wrappers. © 2019 association for computing machinery."
"credit scoring plays a critical role in many areas such as business, finance, engineering and health. the kolmogorov–smirnov statistic is one of the most important performance evaluation criteria for scoring methods and has been widely used in practice. however, none of the existing scoring methods deals with the kolmogorov–smirnov statistic directly at the modeling stage. to fill the gap, a new credit scoring method that directly maximizes the kolmogorov-smirnov statistic (dmks) is proposed. theoretically, the consistency of the proposed dmks estimator is proved. computationally, an iterative marginal optimization algorithm and a smoothed pool-adjacent-violators algorithm are proposed to overcome the computational difficulties caused by the neither smooth nor continuous objective function. empirically, results of simulation studies and two real business examples are presented. the proposed method compares favorably with the popular existing scoring methods considering the tradeoff among predictive ability in terms of ks, computational complexity and practical interpretability. © 2018 elsevier b.v."
"matrix-vector operations play pivotal role in engineering and scientific applications ranging from machine learning to computational finance. matrix-vector operations have time complexity of o(n2) and they are challenging to accelerate since these operations are memory bound operations where ratio of the arithmetic operations to the data movement is o(1). in this paper, we present a systematic methodology of algorithm-architecture co-design to accelerate matrix-vector operations where we emphasize on the matrix-vector multiplication (gemv) and the vector transpose-matrix multiplication (vtm). in our methodology, we perform a detailed analysis of directed acyclic graphs of the routines and identify macro operations that can be realized on a reconfigurable data-path that is tightly coupled to the pipeline of a processing element. it is shown that the pe clearly outperforms state-of-the-art realizations of gemv and vtm attaining 135% performance improvement over multicore and 200% over general purpose graphics processing units. in the parallel realization on redefine coarse-grained reconfigurable architecture, it is shown that the solution is scalable. © 2019 ieee."
"advances in big data make it possible to make short-term forecasts for market trends from previously unexplored sources. trading strategies were recently developed by exploiting a link between the online search activity of certain terms semantically related to finance and market movements. here we build on these earlier results by exploring a data-driven strategy which adaptively leverages the google correlate service and automatically chooses a new set of search terms for every trading decision. in a backtesting experiment run from 2008 to 2017 we obtained a 499% cumulative return which compares favourably with benchmark strategies. a crowdsourcing exercise reveals that the term selection process preferentially selects highly specific terms semantically related to finance (e.g. wells fargo bank), which may capture the transient interests of investors, but at the cost of a shorter span of validity. the adaptive strategy quickly updates the set of search terms when a better combination is found, leading to more consistent predictability. we anticipate that this adaptive decision framework can be of value not only for financial applications, but also in other areas of computational social science, where linkages between facets of collective human behavior and online searches can be inferred from digital footprint data. © 2019, the author(s)."
"partial differential equations with distributional sources—in particular, involving (derivatives of) delta distributions—have become increasingly ubiquitous in numerous areas of physics and applied mathematics. it is often of considerable interest to obtain numerical solutions for such equations, but any singular (“particle”-like) source modeling invariably introduces nontrivial computational obstacles. a common method to circumvent these is through some form of delta function approximation procedure on the computational grid; however, this often carries significant limitations on the efficiency of the numerical convergence rates, or sometimes even the resolvability of the problem at all. in this paper, we present an alternative technique for tackling such equations which avoids the singular behavior entirely: the “particle-without-particle” method. previously introduced in the context of the self-force problem in gravitational physics, the idea is to discretize the computational domain into two (or more) disjoint pseudospectral (chebyshev–lobatto) grids such that the “particle” is always at the interface between them; thus, one only needs to solve homogeneous equations in each domain, with the source effectively replaced by jump (boundary) conditions thereon. we prove here that this method yields solutions to any linear pde the source of which is any linear combination of delta distributions and derivatives thereof supported on a one-dimensional subspace of the problem domain. we then implement it to numerically solve a variety of relevant pdes: hyperbolic (with applications to neuroscience and acoustics), parabolic (with applications to finance), and elliptic. we generically obtain improved convergence rates relative to typical past implementations relying on delta function approximations. © 2018, springer science+business media, llc, part of springer nature."
"online portfolio selection is one of the fundamental problems in the field of computational finance. although existing online portfolio strategies have been shown to achieve good performance, we always have to set the values for different parameters of online portfolio strategies, where the optimal values can only be known in hindsight. to tackle the limits of existing strategies, we present a new online portfolio strategy based on the online learning character of weak aggregating algorithm (waa). firstly, we consider a number of exponential gradient (eg(η)) strategies of different values of parameter η as experts, and then determine the next portfolio by using the waa to aggregate the experts’ advice. furthermore, we theoretically prove that our strategy asymptotically achieves the same increasing rate as the best eg(η) expert. we prove our strategy, as eg(η) strategies, is universal. we present numerical analysis by using actual stock data from the american and chinese markets, and the results show that it has good performance. © 2019, springer science+business media, llc, part of springer nature."
"the main objective of this study is to investigate the behaviour of default prediction models based on credit scoring methods and computational techniques with machine learning algorithms. the predictive capabilities of the models were compared to identify default-prediction mechanisms in the “my home, my life” program (programa “minha casa, minha vida” — pmcmv). the pmcmv is one of the largest government initiatives in the world to finance home ownership in the low-income population. implemented by the brazilian government, the programme has provided financing in excess of usd 84 billion and by 2016 had already contracted for the construction of over 4.5 million housing units, with 3.3 million units already delivered. the models developed in this study involve different time intervals for default prediction as well as analysis without the use of traditional discriminatory variables (gender, age, and marital status). three measurements were used to evaluate the quality of the prediction models: area under the roc curve, the kolmogorov–smirnov index, and the brier score. the results indicated that (1) the accuracy of the models improves as the number of days overdue used to define the default variable increases; (2) the best prediction results were obtained with traditional ensemble techniques — in this case bagging (bg), random forest (rf), and boosting; and (3) there was a negative impact on all criteria when a smaller number of observations was used, especially on the type ii error. it was also found that the discriminatory power of the credit risk rating system is preserved when removing discriminatory variables from the models. applying the bg algorithm, which is the best prediction method, a default rate of 11.80% could be reduced to 2.95%, which leads to a selection that would result in 197,905 fewer delinquent contracts in the pmcmv, thus representing a savings of approximately usd 3.0 billion in credit losses. © 2019 elsevier b.v."
"monitoring the multivariate coefficient of variation over time is a natural choice when the focus is on stabilising the relative variability of a multivariate process, as is the case in a significant number of real situations in engineering, health sciences, and finance, to name but a few areas. however, not many tools are available to practitioners with this aim. this paper introduces a new control chart to monitor the multivariate coefficient of variation through an exponentially weighted moving average (ewma) scheme. concrete methodologies to calculate the limits and evaluate the performance of the chart proposed and determine the optimal values of the chart's parameters are derived based on a theoretical study of the statistic being monitored. computational experiments reveal that our proposal clearly outperforms existing alternatives, in terms of the average run length to detect an out-of-control state. a numerical example is included to show the efficiency of our chart when operating in practice. © 2019 john wiley & sons, ltd."
"purpose: “paying for performance” has been the corporate mantra for ages, but finding the right performance benchmarks continues to be an enigma. equally significant is the ongoing debate on the superiority of economic value added (eva) aligned executive incentive plans over traditional financial performance benchmarks to ensure optimal goal congruence between the corporate and the executive performances. consequently, this paper aims to explore a plausible linkage between executive compensation and eva for indian corporates from a social constructivist perspective. design/methodology/approach: the study uses a mixed method approach where the quantitative analysis of responses from the survey of senior personnel/finance executives of indian firms is complemented by the qualitative analysis of personal interviews to provide contextual depth to the quantitative data. findings: based on the study, the researchers construct an understanding that eva is a superior concept but has restricted utility primarily owing to its computational complexity and unaudited characteristics. the researchers’ interpretive inference finds mandatory disclosure of an audited eva figure in the corporate financial statements as a prime requirement for eva to emerge as an objective and visible performance measure. practical implications: attention of policymakers is sought towards standardising its computation and ensuring its disclosure to bring it at par with the conventional executive financial performance benchmarks. originality/value: the narrative on benefits and the challenges of adopting eva aligned performance management system is provided directly by the top-level executives responsible for designing the “paying for performance” policies. © 2018, emerald publishing limited."
"both bonds and stocks are two major forms of investment under financial instruments, but they represent fundamentally two different forms of securities. in computational finance, a significant amount of work has been done for prediction of future prices of stock market. however, bond price prediction has not been exploited to a great extent. keeping this in mind, a support vector regression (svr) based approach has been proposed to predict the future bond price of indian market. svr takes the past bond price as input and predicts the future price of bond. to exploit the usefulness of the suggested scheme, it has been compared with the traditional neural network based approach in terms of mean squared error (mse) and squared correlation coefficient. from the simulation result analysis, it has been observed that svr shows improved overall performance as compared to the neural network based approach. © 2019 ieee."
"the block maxima method divides sample data into equal blocks. predictions are based on the maximum values of the observations. choosing an efficient and proper block size for the block maxima method is an important issue and varies across fields (e.g., flood, rainfall, finance). however, the main problem is deciding which block size is suitable or optimal for the prediction. in the literature, it is a known fact that the selection of a small block size leads to bias, while the selection of a large block size leads to a variance problem. in one respect, this issue is any trade off problem between the bias and the variance. this paper proposes simple and easy computational method to specify the optimal block size selection process for the block maxima method. © 2019, strojarski facultet. all rights reserved."
"we consider the problem of multiple-radio-access technology (multi-rat) connectivity in heterogeneous networks. recently, multi-rat connectivity has received significant attention both from industry and academia because of its potential as a solution to increase throughput, enhance communication reliability, and to minimize communication latency. in this paper, we propose a new risk-averse-traffic-allocation scheme that allows trading off expected throughput for risk measured in throughput semivariance. semivariance is a mathematical quantity used originally in finance and economics to measure the dispersion of a portfolio return below a risk-aversion benchmark. we adopt semivariance as a measure of throughput dispersion below a risk-aversion-throughput benchmark. we formulate the multi-rat connectivity problem as a semivariance-optimization problem. there are computational issues associated with semivariance optimization. however, there are approximations to the semivariance that overcome these issues. we study these approximations and develop our algorithm based on one of them. we evaluate the performance of our algorithm through simulation of a system with three radio-access technologies, namely, 4g lte, 5g nr, and wifi. simulation results show the potential gains of using our algorithm. © 2020 ieee."
"in various subjects, there exist concise and consistent relationships between input and output parameters. discovering the relationships, or more precisely compact laws, in a data set is of great interest in many fields, such as physics, chemistry, and finance. in recent years, the field of data discovery has made great progress towards discovering these laws in practice thanks to the success of machine learning. however, machine learning methods relate the input and output data by considering them separately instead of equally. in addition, the analytical approaches to finding the underlying theories governing the data are relatively slow. in this paper, we develop an innovative approach on discovering compact laws. a novel algebraic equation formulation is proposed such that constant determination and candidate equation verification can be explicitly solved with low computational time. this algebraic equation formulation does not distinguish between input and output variables, and converts the problem of deriving meaning from data into solving a linear algebra equation and searching for linear equations that fit the data. we also derive a more efficient search algorithm using finite fields. rigorous proofs and computational results are presented in validating these methods. the algebraic formulation allows for the search of equation candidates in an explicit mathematical manner. for a certain type of compact theory, our approach assures convergence, with the discovery being computationally efficient and mathematically precise. © 2019 elsevier b.v."
"pringle, j. and stretch, d.d., 2019. a new approach for the stochastic simulation of regional wave climates conditioned on synoptic scale meteorology. journal of coastal research, 35(6), 1331-1342. coconut creek (florida), issn 0749-0208. statistical modelling of wave climates is an important tool in coastal and ocean engineering design and vulnerability assessments. modern techniques of multivariate modelling that exploit copulas are now being developed and used for risk assessment applications in diverse fields ranging from finance to hydrology and coastal engineering. many such statistical models do not directly exploit the physical links between events of interest, such as floods or extreme storm waves, and their fundamental drivers. on the other hand, process-based models that attempt to include those links are subject to modelling errors because of limited understanding of the processes or limitations, or both, in the available computational resources to adequately resolve those processes. this study introduces a new mixed approach to the stochastic simulation of wave climates that is conditioned on synoptic-scale meteorological circulation patterns (cps) as the key drivers of waves. copulas are used for the multivariate dependence structure in the model, and the cp occurrences are treated as a markov chain. simulated wave time series are shown to reproduce observed wave statistics from a case study site, including extremum statistics. the new techniques presented here should improve statistical modelling while retaining their simplicity and parsimony relative to full process-based models. © coastal education and research foundation, inc. 2019."
"the analysis of time-series is a productive field, which is applied in different areas such as finance, bio-medicine, neurology, among others. however, one of the main challenges is the identification of non-linear patterns. thus, the apparent chaotic behavior of a time-series can mean the manifestation of a dynamic system. often, these phenomena are recurrent, meaning that certain regions of their available state space are frequently visited along of time. for this reason, the use of recurrence plots (rps) and recurrent quantification analysis (rqa) are used to extract features of time series that allow their better understanding and facilitate prediction tasks (classification, regression and novelty detection). however, to successfully apply this transformation in the aforementioned tasks, it is necessary to obtain the best combination of three parameters: time lag (τ), embedding dimension (m) and recurrence rate (rr). in other studies to find these parameters it is necessary to apply the prediction process for each possible combination, which represents a high computational cost. we propose to use a measure that seeks to maximize the entropy with the lowest possible randomness to calculate rp[τ,m,rr] before the application of the prediction. in this way, reduce the computational complexity, where we initially validate these claims using bitcoin’s multidimensional time-series, with results that surpass the accuracy of previous studies. © springer nature switzerland ag 2020."
"portfolio optimization is the process of determining the best combination of securities and proportions with the aim of having less risk and obtaining more profit in an investment. utilizing covariance as a risk measure, mean-variance portfolio optimization model has brought a revolutionary approach to quantitative finance. since then, along with the advancements in computational power and algorithmic enhancements, a lot of efforts have been made on improving this model by considering real-life conditions and solving model variants with various methodologies tested on various data and performance measures. a comprehensive literature review of recent and novel papers is crucial to establish a pattern of the past, and to pave the way on future directions. in this paper, a total of 175 papers published in the last two decades are selected within the scope of operations research community and reviewed in detail. thus, a comprehensive survey on the deterministic models and applications suggested for mean-variance portfolio optimization in which several variants of this model as well as additional real-life constraints are studied. the review classifies the approaches according to exact and approximate attempts and analyzes the proposed algorithms based on various data and performance indicators in depth. areas of future research are outlined. © 2019 elsevier ltd"
"research involving blockchain technology has been growing due to its decentralization and immutability characteristics. several applications from the most varied areas can benefit from the blockchain, such as finance, logistics, and others. internet of things (iot) is a new paradigm that has emerged to provide services to users through intelligent objects arranged in the day-to-day and provided various research, for example, smart buildings, smart cities, precision agriculture, and e-health. e-health applications can be considered as a set of computational solutions focused on the health area. this paper aims to study the introduction of blockchain technology in an e-health approach. a blockchain tool was used in conjunction with an e-health database and evaluated its performance. the results indicated that the transaction validation times are favorable for the characteristics of an e-health application. © 2020, ifip international federation for information processing."
"the objective of this study is to determine the models or standards and computational tools used in the processes of government initiatives of information technologies in the education, finance and health sectors. methods of mapping and systematic review of literature are used to evaluate and interpret the information available in two databases of scientific nature such as scopus y web of science published in the years between 2012 and 2017. the results were recorded in an instrument information extraction built with two approaches, the first was recorded information on the current context on it governance and the second was located information related to research questions. the search found 619 documents that met the inclusion and exclusion criteria. in these works, the mapping method was applied to obtain the summary of data in each of the research questions. it is concluded that the present work identified the information technology government processes that have had greater relevance in the education, finance and health sectors during the years in which the documents were reviewed, allowing the orientation of future works considering the investigated and tested models. © 2019, universidad pablo de olavide. all rights reserved."
"inverse problems are widely encountered in fields as diverse as physics, geophysics, engineering and finance. in the present paper, a covariance based framework for the estimation of their uncertainty is presented and applied to the problem of inverse force identification. a key step in its application involves the propagation of frequency response function (frf) uncertainty through a matrix inversion, for example, between mobility and impedance. to this end a linearised inverse propagation relation is derived. this relation may be considered a generalisation of work presented in the particle physics literature, where we consider both complex valued and non-square matrices through a bivariate description of their uncertainty. results are illustrated, first, through a numerical simulation where force-moment pairs are applied to a free-free beam model. an experimental study then illustrates the in-situ determination of blocked forces and their subsequent use in the prediction of an operational response. the uncertainties predicted by the proposed framework are in agreement with those acquired through monte-carlo (mc) methods for small input variance but are obtained at much lower computational cost, and with improved insight. in the process of illustrating the propagation framework, matrix condition number, often taken as an indicator of uncertainty, is shown to relate poorly to a more rigorous uncertainty estimate, leaving open the question as to whether condition number is an appropriate indicator of experimental uncertainty. © 2018"
"kalman filter (kf) is a key operation in many engineering and scientific applications ranging from computational finance to aircraft navigation. recently, there have been proposals in the literature for acceleration of kf using modified faddeeva algorithm (mfa) where the classical householder transform (ht) is used in implementation of mfa on a custimizable platform called redefine. redefine is a coarse-grained reconfigurable architecture that has capabilities of recomposing data-paths at run-time and on-demand. in this paper, we present realization of kf using mfa where we implement mfa using modified householder transform (mht) presented in the literature. we call this as m2fa. it is shown that the implementation of kf using m2fa clearly outperforms the implementation of kf using mfa on redefine and also the realization of kf on redefine is scalable. performance improvements over state-of-the-art implementations are also discussed. © 2019 ieee."
"blockchain technology is a decentralized distributed system meant for secure computation and information sharing platform free from any central authority that enables multiple authoritative domain which do not trust each other, to cooperate, coordinate and collaborate in rational decision-making process. bitcoin is pioneer crypto currency platform that uses blockchain technology but over the time it has moved beyond bitcoin and has successfully bypassed to other application domain. beside bitcoin, ethereum, hyperledger and corda, quorum have emerged as a successful distributive computing platform. major consortium like enterprise ethereum alliance, hyperledger and r3 are formed with aim to come up with business solution. today we have different blockchain application in public sector, finance, supply-chain, healthcare and iot. in many industries people think blockchain will be transformative. financial sector is most enthusiastic in adopting blockchain followed by supply chain management. blockchain is closely looked at in india too. in 2018 reserve bank of india (rbi) has issued a white paper in order to identify the potential application areas of blockchain in indian banking. niti aayog is working on national strategy for blockchain which will identify the area where country can implement blockchain. blockchain is still new and has issues like scalability, security and privacy breaches, limited transaction loads, and high (computational) costs, besides its technical problem, adopting blockchain into indian market system has its own specific challenges. this paper discusses the prospect of blockchain technology in india, its current position and likely problem in its adoptation to indian market place. this paper also give overview about blockchain and its fundamental technologies and prominent blockchain platform, it also gives account of application area's in which blockchain is implemented successfully around the world. © 2019 ieee."
"event studies in finance have focused on traditional news headlines to assess the impact an event has on a traded company. the increased proliferation of news and information produced by social media content has disrupted this trend. although researchers have begun to identify trading opportunities from social media platforms, such as twitter, almost all techniques use a general sentiment from large collections of tweets. though useful, general sentiment does not provide an opportunity to indicate specific events worthy of affecting stock prices. this work presents an event clustering algorithm, utilizing natural language processing techniques to generate newsworthy events from twitter, which have the potential to influence stock prices in the same manner as traditional news headlines. the event clustering method addresses the effects of pre-news and lagged news, two peculiarities that appear when connecting trading and news, regardless of the medium. pre-news signifies a finding where stock prices move in advance of a news release. lagged news refers to follow-up or late-arriving news, adding redundancy in making trading decisions. for events generated by the proposed clustering algorithm, we incorporate event studies and machine learning to produce an actionable system that can guide trading decisions. the recommended prediction algorithms provide investing strategies with profitable risk-adjusted returns. the suggested language models present annualized sharpe ratios (risk-adjusted returns) in the 5-11 range, while time-series models produce in the 2-3 range (without transaction costs). the distribution of returns confirms the encouraging sharpe ratios by identifying most outliers as positive gains. additionally, machine learning metrics of precision, recall, and accuracy are discussed alongside financial metrics in hopes of bridging the gap between academia and industry in the field of computational finance. © 2020 walter de gruyter gmbh, berlin/boston."
"we propose a numerical method to solve 3-dimensional partial differential equations with variable coefficients, which appear in applications focused on studying random walks in the presence of an absorbing barrier. we restrict ourselves to the case of the kolmogorov backward equation, which most commonly arises in mathematical finance. a probabilistic interpretation of the problem we use allows to apply carr's randomization for dimensionality reduction and employ the wiener-hopf factorization to solve the arising 1-dimensional problems. the key idea of the approach proposed is to realize the expected present value operators, which appear in the factorization identity, as convolutions with certain exponential distributions. we generalize our method for the case of exponentially distributed jumps presence using the russian splitting method. the wiener-hopf factors related to the jump component of the operator again involve a convolution with exponential kernels. the numerical iterative scheme suggested to evaluate wiener-hopf operators is much less computationally expensive than a standard fast fourier transform-based approach. © published under licence by iop publishing ltd."
"the financial crisis of 2008 brought with it a renewed interest in the study of capitalism across disciplines. while historians have since led the way with writings on commodities, labor, finance, and institutions, these have been largely conveyed from the euro-american perspective without necessarily probing other values, parameters, and conditions beyond western political economy that have shaped business over time. this article suggests that to assess the benefits and limits of the global capitalist experience, we must prioritize unconventional subjects, sources, and styles in how we frame research questions, analyze evidence, and cast narratives. such an endeavor is especially timely given the growing influence of regional markets around the world and the increasing prominence of computational tools to generate and analyze novel datasets. © 2019 the president and fellows of harvard college."
"parabolic partial differential equations (pdes) and backward stochastic differential equations (bsdes) are key ingredients in a number of models in physics and financial engineering. in particular, parabolic pdes and bsdes are fundamental tools in pricing and hedging models for financial derivatives. the pdes and bsdes appearing in such applications are often high-dimensional and nonlinear. since explicit solutions of such pdes and bsdes are typically not available, it is a very active topic of research to solve such pdes and bsdes approximately. in the recent article (e et al., multilevel picard iterations for solving smooth semilinear parabolic heat equations, arxiv:1607.03295) we proposed a family of approximation methods based on picard approximations and multilevel monte carlo methods and showed under suitable regularity assumptions on the exact solution of a semilinear heat equation that the computational complexity is bounded by o(dε-(4+δ)) for any δ∈ (0 , ∞) where d is the dimensionality of the problem and ε∈ (0 , ∞) is the prescribed accuracy. in this paper, we test the applicability of this algorithm on a variety of 100-dimensional nonlinear pdes that arise in physics and finance by means of numerical simulations presenting approximation accuracy against runtime. the simulation results for many of these 100-dimensional example pdes are very satisfactory in terms of both accuracy and speed. moreover, we also provide a review of other approximation methods for nonlinear pdes and bsdes from the scientific literature. © 2019, springer science+business media, llc, part of springer nature."
"in data mining field, though an intensive investigation is done,the problem of frequent item set mining is still an interesting and challenging problem in terms of its computational complexity. the concept of frequent item set forms the basis of association rule mining and correlation mining and is widely used in market basket analysis, finance, and health care systems. though,the research community has contributed a lot in this field,the interest in the problem persists due to the underlying complexity involved in computation resulting in combinatorial explosion of itemsets. in this paper, an effort is made to understand the main techniques used to solve these problems and to provide a comprehensive survey of the most influential algorithms in this field. © 2020, institute of advanced scientific research, inc. all rights reserved."
"the extent of epistemic uncertainty in modeling and analysis of complex systems is ever growing, mainly due to increasing levels of the openness, heterogeneity and versatility in cloud-based applications that are being adopted in critical sectors, like banking and finance. state-of-the-art approaches for model-based performance assessment do not embed such uncertainty in analytic models, hence the predicted results do not account for the parametric uncertainty. in this paper, we develop a method for incorporating epistemic uncertainty of the input parameters (i.e., the arrival rate λ and the service rate μ) to the m/m/1 queueing models, that are commonly used to analyze system performance. we consider two steady state and average output measures: the number of entities in the system and the response time. we start with closed-form solutions for these measures that enable us to study the propagation of epistemic uncertainty in input parameters to these output measures. we demonstrate the suitability of our method for the performance analysis of a cloud-based system, where the epistemic uncertainty comes from continuous re-deployment of applications across servers of different computational capabilities. system simulation results validate the ability of our models to produce satisfactorily accurate predictions of system performance indices under epistemic uncertainty. © 2019 elsevier b.v."
"an advance-cash-credit (acc) payment scheme is commonly used in business transactions. for instance, a contractor usually demands that a customer prepay 10% of the total cost (i.e., an advance payment) when a letter of agreement is signed to install a new driveway, kitchen, garage, etc. upon delivery of the materials, a cash-on-delivery (i.e., a cash payment) to cover the contractor's materials cost is then required. finally, the customer will pay the remainder of the total cost after the work is completed and accepted as satisfactory (i.e., a credit payment). in fact, an acc payment type is a generalized payment scheme which includes advance, cash, credit, advance-cash, advance-credit, and cash-credit payments as special cases. in this paper, we develop an inventory model interfaced with marketing, operations, and finance in a supplier-retailer chain in which: (1) the demand curve is downward sloping, (2) the seller demands the buyer use an acc payment for the total cost, and (3) for generality, shortages are allowed with a fixed market tolerance period. as a result, the retailer must determine optimal selling price, replenishment cycle, and shortage interval simultaneously to maximize the total profit for various situations. we demonstrate that an increase in the fraction of advance payment raises selling price, while an increase in the fraction of credit payment reduces selling price. the computational results reveal that if the supplier grants a credit payment, then the retailer has the highest profit amongst all of the payment types. this also results in the lowest selling price. © 2019 elsevier b.v."
"options with extendable features have many applications in finance and these provide the motivation for this study. the pricing of extendable options when the underlying asset follows a geometric brownian motion with constant volatility has appeared in the literature. in this paper, we consider holder-extendable call options when the underlying asset follows a mean-reverting stochastic volatility. the option price is expressed in integral forms which have known closed-form characteristic functions. we price these options using a fast fourier transform, a finite difference method and monte carlo simulation, and we determine the efficiency and accuracy of the fourier method in pricing holder-extendable call options for heston parameters calibrated from the subprime crisis. we show that the fast fourier transform reduces the computational time required to produce a range of holder-extendable call option prices by at least an order of magnitude. numerical results also demonstrate that when the heston correlation is negative, the black-scholes model under-prices in-the-money and over-prices out-of-the-money holder-extendable call options compared with the heston model, which is analogous to the behaviour for vanilla calls. © 2019 australian mathematical society."
"many domains like deep learning, machine learning, and computational finance uses graphical processing units (gpus) for decreasing the execution time. gpus are widely used in data centers for high performance computing where virtualization techniques are intended for optimizing the resource utilization. however, these data centers have some downsides like energy consumption and acquisition cost. we propose to take this resource virtualization a step forward by making use of 'sparse' computers over the internet that have powerful processing capabilities. the approach is inspired from the smart grid model. in this way, we want to evaluate the feasibility of introducing a service that will provide processing capabilities through renting processing resources from domestic users. this model can prove usefully for solving subdivision of the main algorithm. © 2019 ieee."
"the proceedings contain 27 papers. the special focus in this conference is on jonathan borwein commemorative. the topics include: mathematics education in the computational age: challenges and opportunities; mathematics education for indigenous students in preparation for engineering and information technologies; origami as a teaching tool for indigenous mathematics education; dynamic visual models: ancient ideas and new technologies; a random walk through experimental mathematics; introduction; a holistic approach to empirical analysis: the insignificance of p, hypothesis testing and statistical significance; do financial gurus produce reliable forecasts?; entropy maximization in finance; introduction; symmetry and the monotonicity of certain riemann sums; binary constant-length substitutions and mahler measures of borwein polynomials; the borwein brothers, pi and the agm; the road to quantum computational supremacy; nonlinear identities for bernoulli and euler polynomials; metrical theory for small linear forms and applications to interference alignment; improved bounds on brun’s constant; extending the pslq algorithm to algebraic integer relations; short walk adventures; risk and utility in the duality framework of convex analysis; characterizations of robust and stable duality for linearly perturbed uncertain optimization problems; comparing averaged relaxed cutters and projection methods: theory and examples; introduction; on the educational legacies of jonathan m. borwein; how mathematicians learned to stop worrying and love the computer."
"nowadays, the pricing of financial instruments under continuous-time markov switching models have received a widespread attention from researchers and practitioners in the finance industry. lattice-based approaches are amongst the most widely used approaches to solve the pricing problem in this context. recently, yuen and yang (2010) have proposed a simple and fast recombining trinomial tree method to handle the case of regime-switching geometric brownian motion processes. in this paper, we generalize their approach to the regime-switching (exponential) mean-reverting case with state-dependent switching rates and derive the necessary conditions for the positivity of conditional branching probabilities. we employ the hull and white's tree-building procedure to limit tree growth away from the long-run mean of the process. we use the proposed lattice framework to price contingent claims of european, american and barrier type, and demonstrate its applicability in pricing default-free zero-coupon bonds. in the proposed tree structure, the number of nodes is substantially decreased and the computational cost is effectively reduced compared to the usual approaches. extensive numerical experiments illustrate the efficiency and flexibility of the proposed scheme. © 2019 elsevier b.v."
"bonus pay policy for teachers in the u.s. is analyzed in this paper. we quantitatively argue that, because of the decentralized education finance system in the u.s., this policy may lead to higher teacher and household sorting across school districts. this then may lead to higher variance of achievement and lower mean achievement. formally, we use an equilibrium political economy model of education at which households, heterogeneous in exogenously set income, and teachers, heterogeneous in exogenously set quality, are endogenously allocated across two school districts. public education expenditures, which includes teachers’ wage payment and non-teacher related education spending, are financed through local income taxation. income tax rate in each district is determined via majority voting. achievement depends on the efforts chosen by teachers and non-teacher related education spending. teacher efficiency wage per unit of quality is determined at the national teacher labor market. we first calibrate our benchmark model by matching certain statistics from the u.s. data. then in a computational experiment, we introduce bonus pay for teachers which rises with average achievement. we find that for the recently observed level of average bonus pay (6.59% of average base salary), variance of achievement is 2.46% higher and mean achievement is 1.79% lower than the benchmark. variance of achievement reaches its peak when average bonus pay is 14.06% and then it starts falling. also, mean achievement always falls as average bonus pay rises. © 2019 elsevier b.v."
"qun ren, feifei xu, xiaoyi ji, the use of the pathfinder network scaling to measure online customer reviews: a theme park study, strategic change: briefings in entrepreneurial finance, volume 28 pp. 333–344. the name of the third author of this has been mispelled in the online and print version. the name of the third author is xiaoyi ji. qun rena, feifei xub* and xiaoyi jic aschool of business law and communications, solent university, southampton, hampshire, so14 0yn, uk; binternational business school, university of lincoln, lincoln, ln5 7at, uk; cdepartment of mathematics, uppsala university, sweden. *corresponding author author bios. qun ren (first author), phd, is a senior lecture at solent university, uk. her research interests focus on marketing communications, b2b marketing and business/innovation ecosystem. school of business law and communications. solent university. east park terrace. southampton, so14 0yn, uk. email: jessie.ren@solent.ac.uk xu feifei (corresponding author and second author), phd, is a reader in university of lincoln, uk. her study interests mainly include tourism in protected areas, cross-cultural issues in tourism and digital tourism. international business school university of lincoln lincoln, uk, ln5 7at email: fxu@lincoln.ac.uk xiaoyi ji (third author), is a postgraduate in uppsala university, sweden. her study interests mainly include statistics and computational mathematics. department of mathematics. uppsala university. ångströmlaboratoriet, lägerhyddsvägen 1, uppsala, sweden, 752 61. email: xiaoyi.ji.3874@student.uu.se in addition, penet/pnet should be replaced with pfnet throughout the text in section 4.3, delete the content of: (in my opinion) the author apologies for any confusion. © 2019 john wiley & sons, ltd."
"computational intelligence applications in banking and finance have been developed and deployed in the recent past and have been offering business solutions in both front end and back end processes in order to create efficiency and exceptional customer experience. in recent times, ai and machine learning are perceived to be the most valuable enabler to achieve competitive advantage by enhancing the decision making capabilities and transforming the banking industry. this paper will highlight the applications of ai and evaluate its utility in different functional areas of financial industry focusing primarily on automation of banking operations and customer engagement. it concludes with an analysis of how banking and financial organizations frame their environment and effectively use computational intelligence to improve their business. © 2019 ieee."
"big data analytics has gained wide attention from both academia and industry as the demand for understanding trends in massive datasets increases. recent developments in sensor networks, cyber-physical systems, and the ubiquity of the internet of things (iot) have increased the collection of data (including health care, social media, smart cities, agriculture, finance, education, and more) to an enormous scale. however, the data collected from sensors, social media, financial records, etc. is inherently uncertain due to noise, incompleteness, and inconsistency. the analysis of such massive amounts of data requires advanced analytical techniques for efficiently reviewing and/or predicting future courses of action with high precision and advanced decision-making strategies. as the amount, variety, and speed of data increases, so too does the uncertainty inherent within, leading to a lack of confidence in the resulting analytics process and decisions made thereof. in comparison to traditional data techniques and platforms, artificial intelligence techniques (including machine learning, natural language processing, and computational intelligence) provide more accurate, faster, and scalable results in big data analytics. previous research and surveys conducted on big data analytics tend to focus on one or two techniques or specific application domains. however, little work has been done in the field of uncertainty when applied to big data analytics as well as in the artificial intelligence techniques applied to the datasets. this article reviews previous work in big data analytics and presents a discussion of open challenges and future directions for recognizing and mitigating uncertainty in this domain. © 2019, the author(s)."
"in this work, we introduce the moldavian and romanian dialectal corpus (moroco), which is freely available for download at https://github.com/butnaruandrei/moroco. the corpus contains 33564 samples of text (with over 10 million tokens) collected from the news domain. the samples belong to one of the following six topics: culture, finance, politics, science, sports and tech. the data set is divided into 21719 samples for training, 5921 samples for validation and another 5924 samples for testing. for each sample, we provide corresponding dialectal and category labels. this allows us to perform empirical studies on several classification tasks such as (i) binary discrimination of moldavian versus romanian text samples, (ii) intra-dialect multi-class categorization by topic and (iii) cross-dialect multi-class categorization by topic. we perform experiments using a shallow approach based on string kernels, as well as a novel deep approach based on character-level convolutional neural networks containing squeeze-and-excitation blocks. we also present and analyze the most discriminative features of our best performing model, before and after named entity removal. © 2019 association for computational linguistics"
"this paper presents a method using logistic regression to predict and detect 'startup readiness' of researchers in the biopharmaceutical domain, and to suggest determinants to improve their 'startup readiness,' using databases of start-up finances, research papers, patents, academic organizations, and national socioeconomics. this method sorts specific industry segments by which financing activities are active, and by which related growing research topics attract increased academic attention. in research domains such as the biopharmaceutical field, which include pursuit of fundamental scientific understanding and applications intended for immediate use, abundant startups with intense scientific linkage have attracted venture capital financing and entrepreneurship for further rd opportunities and commercialization. we hypothesized that variables composed of several features of papers, patents, research institutes, and nations related to this domain can well reflect researchers' 'startup readiness.' our logistic regression model based on our selected and constructed explanatory variables yielded good predictive and classifying performance, with an auc value of 0.73. results carried specific implications about what variables and their combinations demand attention, to encourage the 'startup readiness' of researchers. more than conventional research methods, our computational approach might provide global, comprehensive, but convenient and real-time understanding of the 'start-up readiness' of researchers in user-inspired fundamental research. © 2019 picmet."
"the smart wealth management platform (swmp) is an intelligent cloud-based system that aims to provide a unified wealth management service with smart risk-assessment and diversified products for institutional and retail investors in china and hong kong. an efficient computational architecture supports high performance data processing for financial analysis, huge amount of calculation for portfolio optimization and simulation, and know your customer (kyc) module based on behavioral finance. leading market core technologies from big data, ai, financial engineering, and financial real time data analysis are cohesively integrated to form a dynamic platform with excellent user experience to support rationalized investment and deliver specialized consultancy. © 2019 ieee."
"we present a new monte carlo methodology for the accurate estimation of the distribution of the sum of dependent log-normal random variables. the methodology delivers statistically unbiased estimators for three distributional quantities of significant interest in finance and risk management: the left tail, or cumulative distribution function; the probability density function; and the right tail, or complementary distribution function of the sum of dependent log-normal factors. for the right tail our methodology delivers a fast and accurate estimator in settings for which existing methodology delivers estimators with large variance that tend to underestimate the true quantity of interest. we provide insight into the computational challenges using theory and numerical experiments, and explain their much wider implications for monte carlo statistical estimators of rare-event probabilities. in particular, we find that theoretically strongly efficient estimators should be used with great caution in practice, because they may yield inaccurate results in the prelimit. further, this inaccuracy may not be detectable from the output of the monte carlo simulation, because the simulation output may severely underestimate the true variance of the estimator. © 2019, springer science+business media, llc, part of springer nature."
"a set of data can be obtained from different hierarchical levels in diverse domains, such as multi-levels of genome data in omics, domestic/global indicators in finance, ancestors/descendants in phylogenetics, genealogy, and sociology. such layered structures are often represented as a hierarchical network. if a set of different data is arranged in such a way, then one can naturally devise a network-based learning algorithm so that information in one layer can be propagated to other layers through interlayer connections. incorporating individual networks in layers can be considered as an integration in a serial/vertical manner in contrast with parallel integration for multiple independent networks. the hierarchical integration induces several problems on computational complexity, sparseness, and scalability because of a huge-sized matrix. in this paper, we propose two versions of an algorithm, based on semi-supervised learning, for a hierarchically structured network. the naïve version utilizes existing method for matrix sparseness to solve label propagation problems. in its approximate version, the loss in accuracy versus the gain in complexity is exploited by providing analyses on error bounds and complexity. the experimental results show that the proposed algorithms perform well with hierarchically structured data, and, outperform an ordinary semi-supervised learning algorithm. © 2019 elsevier ltd"
"in the financial industry, identifying useful information from big data becomes a key research topic. since the vast number of technical indicators can be captured nowadays, the indicator selection can be used to support investment decision for different financial products concurrently; however, this process is still required experience from investors. in this article, we propose a novel recommendation system which is incorporated with technical indicators. the method of fuzzy subset selection is used to feature relevant indicators which have more impact to the variation in the transaction history. the proposed method enables automatic customization of indicators for different financial products in different markets. in particular, the least absolute distance fuzzy regression with non-symmetric lower and upper bounds is proposed to avoid extreme values in dominating the model. furthermore, to reduce computational complexity in the subset selection, the selection algorithm operates in the frequency domain for identifying and matching key patterns and peaks in transacted volumes with the technical indicators. this method performs very effective although the number of factors is much greater than the sample size. the proposed method can benefit participants in the finance markets to customize their own trading dashboard as well as set up their own trading strategies. © 2019, taiwan fuzzy systems association."
"the original publication of the article has been published incorrectly with few textual errors. the correct text is given below: in section “general presentation of the contributions”, first paragraph, the sentence should read, “this special issue of computational economics publishes a selection of the best papers presented at the fourth international symposium in computational economics and finance (iscef), organized in paris in april 2016 (www.iscef.com).” in the same section, second paragraph, the sentence should read, “the first 13 papers investigate the dynamics of the financial markets, while the focus of the three last papers is on macroeconomic dynamics. in particular, these last papers in this special issue study the behavior of diverse macroeconomic variables”. © 2019, springer science+business media, llc, part of springer nature."
"the implementation of climate adaptation and mitigation policies depend on the development of green technologies whose diffusion is constrained by a number of barriers which prevent them to spread broadly and at a fast pace. by means of an agent-based computational model, the paper investigates the macro and micro economic dynamics considering the role of a “traditional” commercial bank and a state investment bank that explicitly supports green investments. simulation results emphasize that green finance matter and that the market diffusion of environmental innovation is more pronounced when the presence of the public investment bank is combined with strong consumers’ preferences oriented towards environmental quality. the relevance of the paper is twofold. besides contributing to the literature on the finance-innovation nexus by considering the role of climate finance within a complex systems framework, it provides a model that can be used as a tool to explore policies to foster environmental innovation diffusion. © 2018"
"we propose a local mesh-free method for the bates–scott option pricing model, a 2d partial integro-differential equation (pide) arising in computational finance. a wendland radial basis function (rbf) approach is used for the discretization of the spatial variables along with a linear interpolation technique for the integral operator. the resulting set of ordinary differential equations (odes) is tackled via a time integration method. a potential advantage of using rbfs is the small number of discrete equations that need to be solved. computational experiments are presented to illustrate the performance of the contributed approach. © 2018 wiley periodicals, inc."
"we show how a recently developed multivariate data fitting technique enables to solve a variety of scientific computing problems in filtering, queueing, networks, metamodelling, computational finance, graphics, and more. we can capture linear as well as nonlinear phenomena because the method uses a generalized multivariate rational model. the technique is a refinement of the basic ideas developed in salazar et al. (numer algorithms 45:375–388, 2007. https://doi.org/10.1007/s11075-007-9077-3) and interpolates interval data. intervals allow to take the inherent data error in measurements and simulation into consideration, whilst guaranteeing an upper bound on the tolerated range of uncertainty. the latter is the main difference with a best approximation or least squares technique which does as well as it can, but without respecting an a priori imposed threshold on the approximation error. compared to the best approximations, the interval interpolant is relatively easy to compute. in applications where industry standards need to be guaranteed, the interval interpolation technique may be a valuable alternative. © 2018, springer nature b.v."
"due to their aptitude in both accurate data processing and human comprehensible reasoning, neural fuzzy inference systems have been widely adopted in various application domains as decision support systems. especially in real-world scenarios such as decision making in financial transactions, the human experts may be more interested in knowing the comprehensive reasons of certain advices provided by a decision support system in addition to how confident the system is on such advices. in this paper, we apply an integrated autonomous computational model termed genetic algorithm and rough set incorporated neural fuzzy inference system (garsinfis) to predict underpricing in initial public offerings (ipos). the difference between a stock's potentially high value and its actual ipo price is referred as money-left-on-the-table, which has been extensively studied in the literature of corporate finance on its theoretical foundations, but surprisingly under-investigated in the field of computational decision support systems. specifically, we use garsinfis to derive interpretable rules in determining whether there is money-left-on-the-table in ipos to assist the investors in their decision making. for performance evaluations, we first demonstrate how to balance between accuracy and interpretability in garsinfis by simply altering the values of several coefficient parameters using well-known datasets. we then use garsinfis to investigate the ipo underpricing problem. the encouraging experimental results show that we may yield higher initial returns of ipos by following the advices provided by garsinfis than any other benchmarking model. therefore, our autonomous computational model is shown to be capable of offering the investors highly interpretable and reliable decision supports to grab the money-left-on-the-table in ipos. © 2018"
"multivariate time series forecasting involves the learning of historical multivariate information in order to predict the future values of several quantities of interests, accounting for interdependencies among them. in finance, several of this quantities of interests (stock valuations, return, volatility) have been shown to be mutually influencing each other, making the prediction of such quantities a difficult task, especially while dealing with an high number of variables and multiple horizons in the future. here we propose a machine learning based framework, the dfml, based on the dynamic factor model, to first perform a dimensionality reduction and then perform a multiple step ahead forecasting of a reduced number of components. finally, the components are transformed again into an high dimensional space, providing the desired forecast. our results, comparing the dfml with several state of the art techniques from different domanins (pls, rnn, lstm, dfm), on both traditional stock markets and cryptocurrencies market and for different families of volatility proxies show that the dfml outperforms the concurrent methods, especially for longer horizons. we conclude by explaining how we wish to further improve the performances of the framework, both in terms of accuracy and computational efficiency. © springer nature switzerland ag 2019."
"a blockchain as a trustworthy and secure decentralized and distributed network has been emerged for many applications such as in banking, finance, insurance, healthcare and business. recently, many communities in blockchain networks want to deploy machine learning models to get meaningful knowledge from geographically distributed large-scale data owned by each participant. to run a learning model without data centralization, distributed machine learning (dml) for blockchain networks has been studied. while several works have been proposed, privacy and security have not been sufficiently addressed, and as we show later, there are vulnerabilities in the architecture and limitations in terms of efficiency. in this paper, we propose a privacy-preserving dml model for a permissioned blockchain to resolve the privacy, security, and performance issues in a systematic way. we develop a differentially private stochastic gradient descent method and an error-based aggregation rule as core primitives. our model can treat any type of differentially private learning algorithm where non-deterministic functions should be defined. the proposed error-based aggregation rule is effective to prevent attacks by an adversarial node that tries to deteriorate the accuracy of dml models. our experiment results show that our proposed model provides stronger resilience against adversarial attacks than other aggregation rules under a differentially private scenario. finally, we show that our proposed model has high usability because it has low computational complexity and low transaction latency. © 2013 ieee."
"deep learning has been widely used in many software disciplines in both academia and industry including computer vision, speech recognition and translation, natural languages processing, search engine, bioinformatics, sensor data processing, finance, etc., due to its scalability in big data environments and accuracy at higher level than ever before. especially, deep neural networks can utilize the parallel computational power of gpu to accelerate the learning process and ensure higher efficiency for big data problems. © the institution of engineering and technology 2020."
"artificial intelligence (ai) provides many opportunities to improve private and public life. discovering patterns and structures in large troves of data in an automated manner is a core component of data science, and currently drives applications in computational biology, finance, law and robotics. however, such a highly positive impact is coupled with significant challenges: how do we understand the decisions suggested by these systems in order that we can trust them? how can they be held accountable for those decisions?. © biochemical society 2019."
"this paper presents an assessment, using global databases of start-up finances and academic research papers of 'start-up readiness' of research topics and researchers related to commercialization-oriented fundamental research. for research domains such as life sciences that pursue fundamental scientific understanding and applications intended for immediate use, academic entrepreneurship has played a pivotal role in commercialization. case studies of start-ups in the biopharmaceutical research domain suggest that the biopharmaceutical field has abundant opportunities stemming from scientific research, commercialization, and entrepreneurship in life sciences. this assessment method sorts specific industry segments by which financing activities are active, and by which related growing research topics attract increased academic attention. we constructed networks of author citation and co-authorship from paper citation networks related to research topics in industry segments in the biopharmaceutical domain. results obtained across all the research topics we surveyed demonstrated that authors in the top 10% of degree centrality ranking in both networks are far more likely to be start-up participants than other authors. more than conventional research methods, our computational approach might provide convenient, dynamic comprehensive, global and real-time understanding of 'start-up readiness' of research topics and researchers in user-inspired fundamental research. © 2018 portland international conference on management of engineering and technology, inc. (picmet)."
"we present a brief overview of random matrix theory (rmt) with the objectives of highlighting the computational results and applications in financial markets as complex systems. an oft-encountered problem in computational finance is the choice of an appropriate epoch over which the empirical cross-correlation return matrix is computed. a long epoch would smoothen the fluctuations in the return time series and suffers from non-stationarity, whereas a short epoch results in noisy fluctuations in the return time series and the correlation matrices turn out to be highly singular. an effective method to tackle this issue is the use of the power mapping, where a non-linear distortion is applied to a short epoch correlation matrix. the value of distortion parameter controls the noise-suppression. the distortion also removes the degeneracy of zero eigenvalues. depending on the correlation structures, interesting properties of the eigenvalue spectra are found. we simulate different correlated wishart matrices to compare the results with empirical return matrices computed using the s&p 500 (usa) market data for the period 1985–2016. we also briefly review two recent applications of rmt in financial stock markets: (i) identification of “market states” and long-term precursor to a critical state; (ii) characterization of catastrophic instabilities (market crashes). © 2019, springer nature switzerland ag."
"the relevance of the research topic is determined by the fact that in modern economic conditions the issue of cash flow formation and management directly influences the operation, and in some cases, the existence of a small and medium-sized business. the company needs a sufficient amount of cash for timely repayment of its obligations. however, cash excess reduces its turnover and, like shortage, negatively affects the performance of a company. the goal of this research is to develop recommendations for monitoring the operations of small and medium-sized businesses based on the analysis of their cash flows. to achieve this goal, the authors set the following objectives: to study the essence of cash flows; to evaluate the theoretical and practical methods of estimating cash flows in companies that are applied in financial management; to develop recommendations for optimizing financial decisions based on cash flow management. the subject matter of the research is russian small and medium-sized businesses. the scope of the study is the elements of their cash flows. the following research methods were used: retrospective analysis, statistical observation method, factor analysis, principles and methods of system analysis; computational procedures involving the apparatus of financial mathematics and financial management; optimization methods and models (excel nonlinear optimization technology). the methodological basis of the study is represented by the works of russian and international scientists on the management of cash flows and finance of small businesses. the scientific novelty of the study includes the development of a factor model for monitoring the activities of business entities which involved evaluating the various elements of their cash flows, as well as optimizing financial decisions by the selected optimality criterion. practical implementation of the proposed mechanism for monitoring the operations of a business entity enables to timely register the sufficiency of the cash flow and to adjust its value depending on the influence of factor indicators. © 2018 iaeme publication. all rights reserved."
"phase-type (ph) distributions are defined as distributions of lifetimes of finite continuous-time markov processes. their traditional applications are in queueing, insurance risk, and reliability, but more recently, also in finance and, though to a lesser extent, to life and health insurance. the advantage is that ph distributions form a dense class and that problems having explicit solutions for exponential distributions typically become computationally tractable under ph assumptions. in the first part of this paper, fitting of ph distributions to human lifetimes is considered. the class of generalized coxian distributions is given special attention. in part, some new software is developed. in the second part, pricing of life insurance products such as guaranteed minimum death benefit and high-water benefit is treated for the case where the lifetime distribution is approximated by a ph distribution and the underlying asset price process is described by a jump diffusion with ph jumps. the expressions are typically explicit in terms of matrix-exponentials involving two matrices closely related to the wiener-hopf factorization, for which recently, a lévy process version has been developed for a ph horizon. the computational power of themethod of the approach is illustrated via a number of numerical examples. © 2019 by the authors. licensee mdpi, basel, switzerland."
"this article investigates the role of the first digital computer in gdr’s socialist financial system. why did the gdr’s ministry of finance import a univac computer from the u.s. army in 1965, even though the country aimed at computational autarky and was restricted by embargo? the main argument is that the ministry of finance imported the computer to kickstart its program for electronic data processing. they succeeded because they not only imported a machine, but also reframed it ideologically. they drew on the notion of the computer as a universal machine and adapted it to local conditions. the process hints to the ambiguity of the later decision of the east bloc toward copying ibm’s system architecture. this article investigates this process by following the traces of an early computer and the ideas surrounding it through the iron curtain. it stresses the role of early computer users with the example of gdr’s financial system in contrast to better known producer stories. through the analysis of exclusive material, this is suggesting a different perspective on the import procedures of eastern european countries in the cold war. a policy change in the cold war towards détente becomes visible as early as in 1965. © 2019, ifip international federation for information processing."
"the ongoing global economic turmoil has got the asset management industry look into new ways of financial risk management. portfolio optimisation and risk budgeting are at the heart of most computational finance studies by academics and practitioners. in this paper, we introduce and analyse a method to construct an equity portfolio based on decomposition of marginal asset risk contribution of each stock in a given universe and then formulate a diversification problem for unsystematic risk as an optimisation problem. we have illustrated the performance of our method by comparing with another diversification technique, known as the risk parity portfolio, and then benchmark our results against the global major indices. © 2019, springer nature switzerland ag."
"computing equilibrium in incomplete-markets with long-lived assets is a challenging task especially because equilibrium may not exist due to the ‘bad-price’ problem. when algorithms fail to produce equilibrium outcomes, it is hard to differentiate between the existential failure and the algorithmic failure. moreover, algorithmic success can be misleading when algorithms work out a quasi-solution for the system of equations which may fail to have equilibrium. to address the computational dilemma, the paper provides a new approach to computing equilibrium in a multi-period, single-good general equilibrium model with incomplete asset markets (single-good gei model or stochastic finance model). the new approach is built on a notion of ‘pre-gei equilibrium’ which always exists in the economy. when the payoff matrix has full rank, equilibrium of the stochastic finance economy (gei equilibrium) coincides with pre-gei equilibrium in real terms. this implies full-rank gei equilibrium can be computed as pre-gei equilibrium. it is shown that pre-gei equilibrium is determined in a system of equations which can be encoded into diverse algorithms such as homotopy path-following methods. since pre-gei equilibrium always exists, the existential failure cannot occur and thus, computational failures imply algorithmic failures in the current computational framework. © 2017, springer science+business media, llc."
"the state of data in finance makes near real-time and consistent assessment of financial risks almost impossible today. the aggregate measures produced by traditional methods are rigid, infrequent, and not available when needed. in this chapter, we make the point that this situation can be remedied by introducing a suitable standard for data and algorithms at the deep technological level combined with the use of big data technologies. specifically, we present the actus approach to standardizing the modeling of financial contracts in view of financial analysis, which provides a methodological concept together with a data standard and computational algorithms. we present a proof of concept of actus-based financial analysis with real data provided by the european central bank. our experimental results with respect to computational performance of this approach in an apache spark based big data environment show close to linear scalability. the chapter closes with implications for data science. © springer nature switzerland ag 2019."
"graphic processing units (gpus) are unanimously considered as powerful computational resources. general-purpose computing on gpu (gpgpu), as well, is the de facto infrastructure for most of the today computationally intensive problems that researchers all over the globe dill with. high performance computing (hpc) facilities use state of the art gpus. many domains like deep learning, machine learning, and computational finance uses gpu’s for decreasing the execution time. gpus are widely used in data centers for high performance computing where virtualization techniques are intended for optimizing the resource utilization (e.g. gpu cloud computing). the gpu programming model requires for all the data to be stored in a global memory before it is used. this limits the dimension of the problem a gpu can handle. a system utilizing a cluster of gpu would have a bigger level of parallelization but also would eliminate the memory limitation imposed by a single gpu. these being just a few of the problems a programmer needs to handle. however, the ratio between specialists that are able to efficiently program such processors and the rest of programmers is very small. one important reason for this situation is the steepness of the gpu programming learning curve due to the complex parallel architecture of the processor. therefore, the tool presented in this article aims to provide visual support for a better understanding of the execution on gpu. with it, the programmers can easily observe the trace of the parallel execution on their own algorithm and, from that, they could determine the unused gpu capacity that could be better exploited. © 2019, national defence university - carol i printing house. all rights reserved."
"computational fluid dynamics (cfd) simulations are widely used; as these are cost effective in a manner that it saves finance for the preparation of experimental setups, and time involved in experimentation. this paper presents the cfd validation of l2s2 parabolic trough collector, which was determined by sandia laboratory usa in 1994. in this paper temperature variation of htf (heat transfer fluid) with the variation of mass flow rates are investigated. further the effects of receiver length and its diameter upon the temperature of htf and glass cover are also studied. the htf used here in the cfd model is same as that used in l2s2 parabolic trough collector (ptc) via dudley, and this fluid is syltherm-800. the novelty of this paper lies in the fact that it provides a cfd method to optimize a parabolic collector; that is, for a prescribed value of inlet temperature and solar flux with a particular fluid, what will be the optimized value of its geometrical dimensions and mass flow rate. © springer nature singapore pte ltd. 2019."
"the following interview with prof. james hamilton was conducted in april 2018 by dr. fredj jawadi with the assistance of professors jim smith and adonis yatchew during the 5th international symposium in computational economics and finance (iscef) held in paris, france. the interview includes 21 questions related to oil price dynamics. the aim of the discussion was, first, to help the reader gain a better understanding of the factors driving changes in oil prices, second, to examine the impact of oil price shocks on the economy and, third, to understand the dynamics of oil prices in the future. the recent related literature on oil price uncertainty is also discussed. we hope that this interview will give the reader clearer insights into the causes and consequences of oil price change and its evolution over time. copyright © 2019 by the iaee. all rights reserved."
"starting from seminal neglected work by rappeport (rappeport 1968 algorithms and computational procedures for the application of order statistics to queuing problems. phd thesis, new york university), we revisit and expand on the exact algorithms to compute the distribution of the maximum, the minimum, the range and the sum of the j largest order statistics of a multinomial random vector under the hypothesis of equiprobability. our exact results can be useful in all those situations in which the multinomial distribution plays an important role, from goodness-of-fit tests to the study of poisson processes, with applications spanning from biostatistics to finance. we describe the algorithms, motivate their use in statistical testing and illustrate two applications. we also provide the codes and ready-to-use tables of critical values. © 2019 the authors. published by the royal society"
"the proceedings contain 12 papers. the special focus in this conference is on enterprise applications, markets and services in the finance industry. the topics include: a semantic model based framework for regulatory reporting process management; applying ontology-informed lattice reduction using the discrimination power index to financial domain; a statistical learning ontology for managing analytics knowledge; open innovation effectiveness in the financial services sector; identification of financial statement fraud in greece by using computational intelligence techniques; what sort of asset? bitcoin analysed; blockchained sukuk-financing; the role of customer retention in business outcomes of online service providers; using nsia framework to evaluate impact of sentiment datasets on intraday financial market measures: a case study; financial data visualization in 3d on immersive virtual reality displays: a case-study for data storytelling and information visualization of financial data of australia’s energy sector."
"this iscef special issue of the energy journal presents new results in the area of energy economics to provide new insights on commodity markets, which will be helpful for investors, policymakers and analysts. in particular, this issue focuses on studies that use recent modeling techniques and empirical design. it introduces seven studies, presented at the fifth international symposium in computational economics and finance organized in paris on april 12-14th, 2018 (www.iscef. com). these studies focus on the investigation of the dynamics of commodity markets, discuss the consequences of uncertainty on energy prices and their effects on the real economy and financial markets, and use high frequency data and recent econometric methods to empirically investigate the interactions between commodity markets and financial markets.  copyright © 2019 by the iaee. all rights reserved."
"we propose the use of randomized (scrambled) quasirandom sequences for the purpose of providing practical error estimates for quasi-monte carlo (qmc) applications. one popular quasirandom sequence among practitioners is the halton sequence. however, halton subsequences have correlation problems in their highest dimensions, and so using this sequence for high-dimensional integrals dramatically affects the accuracy of qmc. consequently, qmc studies have previously proposed several scrambling methods; however, to varying degrees, scrambled versions of halton sequences still suffer from the correlation problem as manifested in two-dimensional projections. this paper proposes a modified halton sequence (mhalton), created using a linear digital scrambling method, which finds the optimal multiplier for the halton sequence in the linear scrambling space. in order to generate better uniformity of distributed sequences, we have chosen strong mhalton multipliers up to 360 dimensions. the proposed multipliers have been tested and proved to be stronger than several sets of multipliers used in other known scrambling methods. to compare the quality of our proposed scrambled mhalton sequences with others, we have performed several extensive computational tests that use l2-discrepancy and high-dimensional integration tests. moreover, we have tested mhalton sequences on mortgage-backed security (mbs), which is one of the most widely used applications in finance. we have tested our proposed mhalton sequence numerically and empirically, and they show optimal results in qmc applications. these confirm the efficiency and safety of our proposed mhalton over scrambling sequences previously used in qmc applications. © 2019 walter de gruyter gmbh. all rights reserved."
"neurofinance is a relatively new area of research that strives to understand financial decision making by combining insights from psychology and neuroscience with theories of finance. using behavioral experiments, neurofinance studies how we evaluate information about financial options that are uncertain, time-constrained, risky, and strategic in nature and how financial decisions are influenced by emotions, psychological biases, stress, and individual differences (such as gender, genes, neuroanatomy, and personality). in addition, it studies how the brain processes financial information and how individual decisions arise within it. finally, by combining these experiments with computational models, neurofinance aims to provide an alternative explanation for the apparent failure of classic finance theories. here we provide an introduction to neurofinance and look at how it is rooted in different fields of study. we review early findings and implications and conclude with open questions in neurofinance. © the author(s) 2017."
"quantitative trading is an automated trading system in which the trading strategies and decisions are conducted by a set of mathematical models. quantitative trading applies a wide range of computational approaches such as statistics, physics, or machine learning to analyze, predict, and take advantage of big data in finance for investment. this work studies core components of a quantitative trading system. machine learning offers a number of important advantages over traditional algorithmic trading. with machine learning, multiple trading strategies are implemented consistently and able to adapt to real-time market. to demonstrate how machine learning techniques can meet quantitative trading, linear regression and support vector regression models are used to predict stock movement. in addition, multiple optimization techniques are used to optimize the return and control risk in trading. one common characteristic for both prediction models is they effectively performed in short-term prediction with high accuracy and return. however, in short-term prediction, the linear regression model is outperform compared to the support vector regression model. the prediction accuracy is considerably improved by adding technical indicators to dataset rather than adjusted price and volume. despite the gap between prediction modeling and actual trading, the proposed trading strategy achieved a higher return than the s&p 500 etf-spy. © 2018 association for computing machinery."
"hydro storage system optimization is becoming one of the most challenging tasks in energy finance. while currently the state-of-the-art of the commercial software in the industry implements mainly linear models, we would like to introduce risk aversion and a generic utility function. at the same time, we aim to develop and implement a computational efficient algorithm, which is not affected by the curse of dimensionality and does not utilize subjective heuristics to prevent it. for the short term power market we propose a simultaneous solution for both dispatch and bidding problems. following the blomvall and lindberg (eur j oper res 143(2):452–461, 2002) interior point model, we set up a stochastic multiperiod optimization procedure by means of a “bushy”recombining tree that provides fast computational results. inequality constraints are packed into the objective function by the logarithmic barrier approach and the utility function is approximated by its second order taylor polynomial. the optimal solution for the original problem is obtained as a diagonal sequence where the first diagonal dimension is the parameter controlling the logarithmic penalty and the second one is the parameter for the newton step in the construction of the approximated solution. optimal intraday electricity trading and water values for hydroassets as shadow prices are computed. the algorithm is implemented in mathematica. © 2017, springer-verlag gmbh germany."
"stochastic differential equations (sde) play an important role in a range of application areas, including biology, physics, chemistry, epidemiology, mechanics, microelectronics, economics, and finance [1]. however, most sdes, especially nonlinear sdes, do not have analytical solutions, so that one must resort to numerical approximation schemes in order to simulate trajectories of the solutions to the given equation. the simplest effective computational method for approximation of ordinary differential equations is the euler’s method. the euler–maruyama method is the analogue of the euler’s method for ordinary differential equations for numerical simulation of the sdes [2]. another numerical scheme is the milstein method [3], which is a taylor method, meaning that it is derived from a truncation of the stochastic (itô) taylor expansion of the solution. the milstein scheme involves the derivatives. if it happens that the derivatives do not exist, then it leads to difficulties in it implementation. in that case we need a derivative-free method. in such cases one can consider an implicit schemes that avoid the use of derivatives [2]. it can be done by replacing the derivatives there by finite difference. these methods are known as runge-kutta schemes [4]. in this chapter, following the exposition of kloeden and platen [4], we describe numerical methods for solving the sde for diffusion and jump-diffusion processes. in addition, we provide a brief review of some computational packages in r and python for simulating stochastic diffusion and jump-diffusion processes. © 2019, springer nature switzerland ag."
"business statistics with solutions in r covers a wide range of applications of statistics in solving business related problems. it will introduce readers to quantitative tools that are necessary for daily business needs and help them to make evidence-based decisions. the book provides an insight on how to summarize data, analyze it, and draw meaningful inferences that can be used to improve decisions. it will enable readers to develop computational skills and problem-solving competence using the open source language, r. mustapha abiodun akinkunmi uses real life business data for illustrative examples while discussing the basic statistical measures, probability, regression analysis, significance testing, correlation, the poisson distribution, process control for manufacturing, time series analysis, forecasting techniques, exponential smoothing, univariate and multivariate analysis including anova and manova and more in this valuable reference for policy makers, professionals, academics and individuals interested in the areas of business statistics, applied statistics, statistical computing, finance, management and econometrics. © 2019 mustapha abiodun akinkunmi, published by walter de gruyter gmbh, berlin/boston."
"in this paper, we study the lookback option of the american style suggested in dai (journal of computational finance 4(2):63–68, 2000), and dai and kwok (siam journal on applied mathematics 66(1):206–227, 2005) under stochastic volatility. by the asymptotic analysis introduced in fouque et al. (derivatives in financial markets with stochastic volatility, cambridge university press, cambridge, 2000), we derive the explicit formula for the price and the optimal exercise value of the option with infinity maturity whose volatility follows the ornstein–uhlenbeck process. especially, we investigate the effects of the stochastic volatility on the perpetual american lookback option in comparison with the constant volatility [cf. (black and scholes in the journal of political economy 81(3):637–654, 1973] by using the results of the computational experiment. © 2018, springer science+business media, llc, part of springer nature."
"the last few decades have seen the advancement in information technology as no other period in history. the same period has also seen dramatic increase in computational power and a correspondingly significant decrease in the cost of electronics. in addition, internet and wireless telecommunication have exploded in the first decade of the twenty-first century. advancements in these areas have resulted in several synergies to deliver innovation in diverse applications leading to an era of digital transformation. the mining industry has been relatively slow in adopting these technologies compared to many other industries such as aviation, finance or even oil and gas. © springer nature switzerland ag 2019."
"attributed networks are ubiquitous and form a critical component of modern information infrastructure, where additional node attributes complement the raw network structure in knowledge discovery. recently, detecting anomalous nodes on attributed networks has attracted an increasing amount of research attention, with broad applications in various high-impact domains, such as cybersecurity, finance, and healthcare. most of the existing attempts, however, tackle the problem with shallow learning mechanisms by ego-network or community analysis, or through subspace selection. undoubtedly, these models cannot fully address the computational challenges on attributed networks. for example, they often suffer from the network sparsity and data nonlinearity issues, and fail to capture the complex interactions between different information modalities, thus negatively impact the performance of anomaly detection. to tackle the aforementioned problems, in this paper, we study the anomaly detection problem on attributed networks by developing a novel deep model. in particular, our proposed deep model: (1) explicitly models the topological structure and nodal attributes seamlessly for node embedding learning with the prevalent graph convolutional network (gcn); and (2) is customized to address the anomaly detection problem by virtue of deep autoencoder that leverages the learned embeddings to reconstruct the original data. the synergy between gcn and autoencoder enables us to spot anomalies by measuring the reconstruction errors of nodes from both the structure and the attribute perspectives. extensive experiments on real-world attributed network datasets demonstrate the efficacy of our proposed algorithm. copyright © 2019 by siam."
"this study aims to explore, via quasi-experiments, the effects of online externally-facilitated regulated learning (erl) and computational thinking (ct) on improving students’ computing skills in a blended learning environment. four classes in a one-semester course entitled ‘applied information technology: data processing’ were the samples for this research. the first class (c1, erl&ct group) simultaneously received the interventions regarding online erl and ct, the second class (c2, ct group) received the intervention regarding online ct, and the third class (c3, erl group) received the intervention regarding online ct, while the last group (c4, control group) received a traditional teaching method, although teaching was also conducted in a blended computing class. students in erl&ct group and ct group came from the department of finance, while the erl group and control group came from the department of law at a comprehensive university. according to the posttest analysis, the results indicate that students who received the intervention of online erl had statistically better development of computing skills for using excel by semester-end than those without. in addition, this study also reveals that the application of online ct alone could be helpful in students’ development of computing skills. furthermore, the results indicate that students’ computing skills could be improved under the condition of simultaneously applying erl and ct. based on the findings of this study, the authors present implications for online teachers and educators, particularly for those teaching computing courses. © 2017, springer-verlag berlin heidelberg."
"in an increasingly polarized world, demagogues who reduce complexity down to simple arguments based on emotion are gaining in popularity. are opinions and online discussions falling into demagoguery? in this work, we aim to provide computational tools to investigate this question and, by doing so, explore the nature and complexity of online discussions and their space of opinions, un¬ covering where each participant lies. more specifically, we present a modeling framework to construct latent representations of opinions in online discussions which are consistent with human judgments, as measured by online voting. if two opinions are close in the resulting latent space of opinions, it is because humans think they are similar. our framework is theoretically grounded and establishes a surprising connection between opinion and voting models and the sign-rank of matrices. moreover, it also provides a set of practical algorithms to both estimate the dimensionality of the latent space of opinions and infer where opinions expressed by the participants of an online discussion lie i n this space. experiments on a large dataset from yahoo! news, yahoo! finance, yahoo! sports, and the newsroom app show that many discussions are multisided, reveal a positive correlation between the complexity of a discussion, its linguistic diversity and its level of controversy, and show that our framework may be able to circumvent language nuances such as sarcasm or humor by relying on human judgments instead of textual analysis. © 2019 held by the owner/author(s). publication rights licensed to a c m ."
"the study of the impact of investor sentiment on stock returns has gained increasing momentum in the past few years. it has been widely accepted that public mood is correlated with financial markets. however, only a few studies discussed how the public mood would affect one of the fundamental problems of computational finance: portfolio management. in this study, we use public financial sentiment and historical prices collected from the new york stock exchange (nyse) to train multiple machine learning models for automatic wealth allocation across a set of assets. unlike previous studies which set as target variable the asset prices in the portfolio, the variable to predict here is represented by the best asset allocation strategy ex post. experiments performed on five portfolios show that long short-term memory networks are superior to multi-layer perceptron and random forests producing, in the period under analysis, an average increase in the revenue across the portfolios ranging between 5% (without financial mood) and 19% (with financial mood) compared to the equal-weighted portfolio. results show that our all-in-one and end-to-end approach for automatic portfolio selection outperforms the equal-weighted portfolio. moreover, when using long short-term memory networks, the employment of sentiment data in addition to lagged data leads to greater returns for all the five portfolios under evaluation. finally, we find that among the employed machine learning algorithms, long short-term memory networks are better suited for learning the impact of public mood on financial time series. © 2018, springer science+business media, llc, part of springer nature."
"nowadays there is a huge growth of massive amount of data generated by different sources at real time or soft real time. generally data is heterogeneous by its content and exists at every human sphere such as education, government, finance, medicine and so on. this paper is about the possibility to use the kolmogorov complexity as a technological innovation in such area as education. the article describes fundamental issues such as data storage, data security and high speed access to data as one of the main parameters for successful using of elearning platforms for education. it is considered comparisons of different elearning platforms and its functional principles. most elearning platforms based on supercomputer and distributed computing systems, however, the growing volumes of data, especially in educational sphere, required other approaches of data storing and access. nevertheless, educational platforms should be scalable and cross accessible. to increase data processing and storing efficiency it is considered data and operation partition idea among heterogeneous computational nodes which is the main infrastructure for elearning platforms and methods. kolmogorov complexity approach is investigated for such purposes. furthermore, kolmogorov complexity approach is examined for reduction a number of transmitting data between nodes. the possibility of using kolmogorov complexity for optimal way of data dividing and processing in distributed and heterogeneous computing system and nodes without quantity nodes limitations is considered. the advantages and disadvantages of kolmogorov complexity were considered applying to elearning study aims. experiments and results are given. the further plan investigation is described. © 2019, national defence university - carol i printing house. all rights reserved."
"computational building information modeling (bim) is an intelligent 3d model-based process that provides architecture, engineering, and construction professionals the insight to plan, design, construct, and manage buildings and infrastructure more efficiently. this paper aims at using bim in hospitals configurations protection. infrastructure projects are classified as huge structural projects taking advantage of many resources such as finance, materials, human labor, facilities and time. immense expenses in infrastructure programs should be allocated to estimating the expected results of these arrangements in domestic economy. hence, the significance of feasibility studies is inevitable in project construction, in this way the necessity in promoting the strategies and using global contemporary technologies in the process of construction maintenance cannot be neglected. this paper aims at using the building information modeling in covering imam khomeini hospital's equipment. first, the relationship between hospital constructions maintenance and repairing, using the building information modeling, is demonstrated. then, using library studies, the effective factors of constructions' repairing and maintenance were collected. finally, the possibilities of adding these factors in revit software, as one of the most applicable software within bim is investigated and have been identified in some items, where either this software can enter or the software for supporting the repairing and maintenance phase lacks them. the results clearly indicated that the required graphical factors in construction information modeling can be identified and applied successfully. copyright © 2018 techno-press, ltd."
"measuring the corporate default risk is broadly important in economics and finance. quantitative methods have been developed to predictively assess future corporate default probabilities. however, as a more difficult yet crucial problem, evaluating the uncertainties associated with the default predictions remains little explored. in this paper, we attempt to fill this blank by developing a procedure for quantifying the level of associated uncertainties upon carefully disentangling multiple contributing sources. our framework effectively incorporates broad information from historical default data, cor-porates’ financial records, and macroeconomic conditions by (a) characterizing the default mechanism, and (b) capturing the future dynamics of various features contributing to the default mechanism. our procedure overcomes the major challenges in this large scale statistical inference problem and makes it practically feasible by using parsimonious models, innovative methods, and modern computational facilities. by predicting the marketwide total number of defaults and assessing the associated uncertainties, our method can also be applied for evaluating the aggregated market credit risk level. upon analyzing a us market data set, we demonstrate that the level of uncertainties associated with default risk assessments is indeed substantial. more informatively, we also find that the level of uncertainties associated with the default risk predictions is correlated with the level of default risks, indicating potential for new scopes in practical applications including improving the accuracy of default risk assessments. © 2018, institute of mathematical statistics. all rights reserved."
"background and aims: there is always an interaction between man and his environment that can be the cause of physical, physiological and psychological stress on people and also cause discomfort, annoyance, and have direct and indirect effects on their performance and productivity, health and safety. people in their workplace are exposed to many factors related to work activities and environmental factors, which can affect their health. thus, workplace disruptions such as noisy environments can cause safety problems, reduce employee's morale and has undesirable effects on their performance, working memory, and attention. for these reasons, ergonomic designers must be capable of designing the physical environment because the design of the environment not only keeps people's performance high, but also provides a safe and healthy environment for improving employees' health. noise can also negatively affect communication between people. different sources of noise production can affect employees and negatively affect their comfortable, functional and communication aspects. noise also effects mental activities and can cause employees to become tired early and increase their computational errors. because of high workload and the banks' competition to attract customers in order to provide better services, quiet workplace environments are essential for staff comfort and optimum performance; so right designing workplace can have a high impact on people's comfort and productivity. speech interference level (sil) is a simple way to predict or evaluate speech intelligibility where the conversation occurs directly in a noisy environment. at open plan offices, employee and customer communicate face-to-face and information exchange is done in that way. in recent years, many banks in our country have established their own workstations as sited customers and face-to-face with bank tellers, but so far no study has been conducted on the changed employees' workstations and its effect on speech interference. a question is whether the workstation redesign increase the speech interference level and improve intelligibility ratings for speech communications? therefore, the purpose of this study was to study the effect of redesigned workstations on sil among bank tellers. methods: this is a descriptive-analytic and interventional study carried out for one year in three branches of one of the governmental banks of hamadan province (n= 12). twelve workstations were redesigned and installed after measuring the sound and calculating speech interference level, and calculating the distance between customer and employee. bank tellers are those who have a direct relationship with customers and perform banking tasks. they had to sit long during the day and they spent all their time working on computers and they were getting things done and receiving customer financial requests. they used the workstations were constructed in the present study. the tools for data collection in this study were: 1) demographic and occupational questionnaire, 2) sound measuring and frequency analyzer cel-450 ?3) a questionnaire with a question ""what noise sources in your workplace do you annoy?"" to determine annoying sound sources and 4) iso 9921-2003 standard to determine speech interference level of the noise at the listener's ear (lsil), speech level, speech interference level (sil) and intelligibility ratings for speech communications. sil is a simple way to predict or evaluate speech intelligibility, speech intelligibility also increases with increasing sil. speech intelligibility is divided into 5 rating (bad, poor, fair, good and excellent) based on sil, also in sensitive situations where short messages are sent include important numbers, ability to understand at least ""good"" is recommended by increasing speech effort; the communication will be ""fair"" if the difference between lsil and the speech level (ls, a, l) that both determined at the listener's position is greater than 10. measuring the sound within 1 meter of the speaker's mouth, according to iso 9921 (2003) for normal sound (60 db) was considered. it was measured for 15 minutes at 10 a.m. to 13 p.m. every shift that they had the highest number of customers, and measuring was done twice for each workstation (total 24 times). to calculate lsil, the sound pressure level was measured in four octave bands with the central frequencies 500 hz, 1000 hz, 2000 hz and 4000 hz at the listener position and during the normal communication period in time-weighting ""slow"". the sound pressure level was measured for 15 minutes and the average of these pressure levels was calculated. after collecting the preliminary data, bank teller workstations were redesigned and built. some actions were taken to reduce speech interference and improve communication between bank tellers and customers; for example, insert a glass between customer and bank teller and create 12 vents (diameter =2 cm) at a customer's mouth height to improve communication, changing the workstation so that the operator and customer interact face-to-face and insert chair for customers to sit in, reduce customer gathering in front of the workstations, reducing the distance between the customers and the bank tellers by creating an arc at the table, enhancing employee privacy by placing the glass between employees in beside workstations, increasing the glass height between the bank teller and the customer to prevent customer standing communicate with the bank teller. the mean distance between bank teller and customer prior to redesigning the workstations was 123 cm which was reduced to 103 cm in redesign; the mean distance between two employees was 95 cm, which increased to 115 cm after the redesign. environmental interventions also include the installation of perforated acoustic tiles made of plaster as a false ceiling, so that, by installing them the distance between the main and false ceilings was 25 cm and there was an empty space between them which absorbs more sound. the walls of the branches were also covered with wall plugs that made of pvc and mdf, and the floor was improved. data were collected before and after the interventions and they were compared. data analysis was performed using spss software, version 16. data were analyzed using descriptive statistics (mean and standard deviation) and analytical statistics. the paired t-test was used to compare the mean results of the lsil and sil. results: in the present study, all the bank tellers were male. everyone was married, their mean age was 38.25 (5.62) and work experience was 15.25 (6.87) years. each bank teller was doing 30-50 customers' finances task in one shift. all participants stated that their rest time was less than 30 minutes in a shift and they are working 7-8 hours in a sitting position; none of the participants had a history of hearing loss or other hearing problems. results from annoying sources of noise showed that the most annoying sources of noise in all three branches were the noise of customers (42.85%) and the sound of money counting machine (33.3%); printer noise was the lowest (4.76%). before the interventions, mean speech interference level (lsil) was 59.50 db (4.07) that was then reduced to 54.98 (5.52) db and this decrease was statistically significant (p <0.05), also the results of evaluation and comparison of sil in the workstations after and before the interventions showed that sil increased and it was statistically significant (p <0.05), the mean of sil before interventions was-1/00 (3.98) which reached to 5/09 (5/23) after interventions, lots of noise in the workplace reduces sil which in turn reduces the speech intelligibility between the bank tellers and the customers. therefore, the interventions have improved the speech intelligibility in all three branches. so that after interventions, the ""fair"" speech intelligibility increased from zero to 4 cases (16.7%); and it is expected that by improving speech intelligibility, peoples' efficiency and concentration will increase; so it can be said that high levels of workplace noise reduce sil, which in turn reduces the speech intelligibility between bank tellers and customers. 22 of the 24 measurements before the new workstations were in the ""bad"" speech intelligibility range, which was reduced to eight after the new workstations were installed, but after the interventions, speech intelligibility in most workstations (50%) were ""poor""; upgrading the speech intelligibility from bad to poor can be a sign of improvement; but further studies are needed to survey the sources that impair intelligibility in order to reach good or excellent level and reduce problems due to poor intelligibility such as longer speaker and listener speech and computational errors. conclusion: the overall purpose of this study was to examine the effect of redesigned on speech interference level (sil). the findings of this study showed that the interventions were effective on sil. according to the results of this study, redesigning work stations and environmental interventions in open plan offices can influence sil and improve the speech intelligibility, that it can prevent errors and increase concentration and reduce fatigue by peoples' privacy and controlling annoying sound sources. finally, it can be said that the workstation redesign and layout can improve the acoustic working environment. © 2019 iran university of medical sciences. all rights reserved."
"in this paper, a new computational method is proposed to solve a class of nonlinear stochastic differential equations (sdes) driven by fractional brownian motion (fbm). the method is based on a new class of orthogonal wavelets, namely the chebyshev cardinal wavelets. these new basis functions have many useful properties such as orthogonality, spectral accuracy, and cardinality. the operational matrices of the derivative and the integration of these new wavelets are derived and used in the implementation of the proposed method. based on the interpolation property of these new wavelets, a new algorithm is presented for computing nonlinear terms in such problems. the main advantage of the proposed method is the reduction of the problem to a simpler one, which consists of solving a system of nonlinear algebraic equations. the convergence and error analysis of the established method are investigated in the sobolev space. moreover, the reliability and applicability of the proposed method are demonstrated by solving several numerical examples. finally, the application of the proposed method is illustrated by solving two well-known fractional stochastic models in the mathematical finance and the mathematical ecology. © 2018 elsevier b.v."
"with the rapid growth of computational domains, bioinformatics finance, engineering, biometrics, and neuroimaging emphasize the necessity for analyzing high-dimensional data. many real-world datasets may contain hundreds or thousands of features. the common problem in most of the knowledge-based classification problems is quality and quantity of data. in general, the common problem with many high-dimensional data samples is that it contains missing or unknown attribute values, incomplete feature vectors, and uncertain or vague data which have to be handled carefully. due to the presence of a large segment of missing values in the datasets, refined multiple imputation methods are required to estimate the missing values so that a fair and more consistent analysis can be achieved. in this paper, three imputation (mi) methods, mean, imputations predictive mean, and imputations by additive lasso, are employed in cloud. results show that imputations by additive lasso are the preferred multiple imputation (mi) method. © 2019, springer nature singapore pte ltd."
"this paper introduces time-continuous numerical schemes to simulate stochastic differential equations (sdes) arising in mathematical finance, population dynamics, chemical kinetics, epidemiology, biophysics, and polymeric fluids. these schemes are obtained by spatially discretizing the kolmogorov equation associated with the sde in such a way that the resulting semi-discrete equation generates a markov jump process that can be realized exactly using a monte carlo method. in this construction the jump size of the approximation can be bounded uniformly in space, which often guarantees that the schemes are numerically stable for both finite and long time simulation of sdes. by directly analyzing the infinitesimal generator of the approximation, we prove that the approximation has a sharp stochastic lyapunov function when applied to an sde with a drift field that is locally lipschitz continuous and weakly dissipative. we use this stochastic lyapunov function to extend a local semimartingale representation of the approximation. this extension makes it possible to quantify the computational cost of the approximation. using a stochastic representation of the global error, we show that the approximation is (weakly) accurate in representing finite and infinite-time expected values, with an order of accuracy identical to the order of accuracy of the infinitesimal generator of the approximation. the proofs are carried out in the context of both fixed and variable spatial step sizes. theoretical and numerical studies confirm these statements, and provide evidence that these schemes have several advantages over standard methods based on time-discretization. in particular, they are accurate, eliminate nonphysical moves in simulating sdes with boundaries (or confined domains), prevent exploding trajectories from occurring when simulating stiff sdes, and solve first exit problems without time-interpolation errors. © 2018 american mathematical society"
the proceedings contain 78 papers. the special focus in this conference is on computational intelligence in data mining. the topics include: analysis of credit card fraud detection using fusion classifiers; an efficient swarm-based multicast routing technique—review; a new howard–crandall–douglas algorithm for the american option problem in computational finance; graph anonymization using hierarchical clustering; reevaluation of ball-race conformity effect on rolling element bearing life using pso; static cost-effective analysis of a shifted completely connected network; dynamic notifications in smart cities for disaster management; application of classification techniques for prediction and analysis of crime in india; improving accuracy of classification based on c4.5 decision tree algorithm using big data analytics; on understanding the release patterns of open source java projects; comparative study of mppt control of grid-tied pv generation by intelligent techniques; stability analysis in recs-integrated multi-area agc system with sos algorithm based fuzzy controller; log-based reward field function for deep-q-learning for online mobile robot navigation; integrated design for assembly approach using ant colony optimization algorithm for optimal assembly sequence planning; design and performance evaluation of fractional order pid controller for heat flow system using particle swarm optimization; land records data mining: a developmental tool for government of odisha; piecewise modeling of ecg signals using chebyshev polynomials; analysis of supplementary excitation controller for hydel power system gt dynamic using metaheuristic techniques; farthest smote: a modified smote approach; dkfcm: kernelized approach to density-oriented clustering; a study of high-dimensional data imputation using additive lasso regression model; prediction of gold price movement using discretization procedure.
"department of data science and business analytics established in january 2018 in the college of innovation and technology at florida polytechnic university. two highly innovative undergraduate degree programs are offered: bachelor of science in data science and bachelor of science in business analytics. data science is an interdisciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from data in various forms, both structured and unstructured. new computational and analytic approaches to a vast array of forms, scales, and sources of data are now critical to research, decision-making, and action. the rigorous curriculum in data science program focuses on the fundamentals of applied mathematics, computer science, probability, statistics, optimization, and machine learning while incorporating real-world examples. business analytics is analytics expertise with a business focus. it is a cutting-edge program especially designed to prepare students for top jobs in today's technology and data intensive business world. the curriculum provides extensive instruction in the disciplines of optimization, mathematical modeling, probabilistic and statistical analysis, simulation, computer programming, database, data and text mining, and cloud computing. the curriculum also includes classes in subject areas you would expect to find in a more traditional business program, such as economics, accounting, finance, operations management, supply chain management, entrepreneurship, business law, negotiation, project management, and strategy. the curricula of two programs are intertwined by first year common and several common courses and senior yearlong industry project. in this paper we highlight curriculum development along with learning objectives. © ieom society international."
"computational finance is an emerging application field of metaheuristic algorithms. in particular, these optimisation methods are becoming the solving approach alternative when dealing with realistic versions of several decision-making problems in finance, such as rich portfolio optimisation and risk management. this paper reviews the scientific literature on the use of metaheuristics for solving np-hard versions of these optimisation problems and illustrates their capacity to provide high-quality solutions under scenarios considering realistic constraints. the paper contributes to the existing literature in three ways. firstly, it reviews the literature on metaheuristic optimisation applications for portfolio and risk management in a systematic way. secondly, it identifies the linkages between portfolio optimisation and risk management and presents a unified view and classification of both problems. finally, it outlines the trends that have gradually become apparent in the literature and will dominate future research in order to further improve the state-of-the-art in this knowledge area. © 2019"
"fractional differential equations have attracted considerable attention because of their many applications in physics, geology, biology, chemistry, and finance. in this paper, a two-dimensional riesz space fractional diffusion equation on a convex bounded region (2d-rsfde-cbr) is considered. these regions are more general than rectangular or circular domains. a novel alternating direction implicit method for the 2d-rsfde-cbr with homogeneous dirichlet boundary conditions is proposed. the stability and convergence of the method are discussed. the resulting linear systems are toeplitz-like and are solved by the preconditioned conjugate gradient method with a suitable circulant preconditioner. by the fast fourier transform, the method only requires a computational cost of o(nlog⁡n) per time step. these numerical techniques are used for simulating a two-dimensional riesz space fractional fitzhugh–nagumo model. the numerical results demonstrate the effectiveness of the method. these techniques can be extended to three spatial dimensions, which will be the topic of our future research. © 2018 imacs"
"the internet of things (iot) has attracted much attention in many fields, such as healthcare, transportation, finance and other critical infrastructures. but how to ensure the security of whole iot system and the privacy of users has always been a challenge. physical layer as the cornerstone of iot, appears to be a promising direction for enhancing the security of iot. as a consequence, we propose a novel chaotic physical layer security transmission scheme in the ofdm-based iot transmission system. by encrypting the discrete fourier transform (dft) matrix, the proposed scheme not only provides the confidentiality of physical layer information transmission, but also effectively addresses the issues on the extreme high papr of the ofdm symbols. moreover, it does not require any additional sideband information and has low computational complexity in theory. compared with other existing schemes, the proposed scheme achieves the higher confidentiality of information transmission with the capability to resist chosen-plaintext attacks, statistical-attacks, and brute-force attacks simultaneously. © 2019 ieee."
"criminal money is disguised from law enforcement agencies using money laundering techniques to profit wrongdoers pockets or even worse to finance criminal and terrorism activities. to make the proceeds from their illegal activities look legitimate in the eyes of the rest of society they use complex and diverse money laundering techniques that takes advantage of poor policies and law loopholes. current countermeasures taken by financial organizations are insufficient because they aim to comply in several cases on minimum base of the legal requirements. in this paper we argue that research in the area of anti-money laundering (aml) is being hampered by the lack of available quality data sets. classic computational controls such as thresholds and modern techniques such as machine learning can benefit from using synthetic data generation methods to target more accurately and efficiently suspicious transactions. the introduction of simulators of financial synthetic data generation for aml research has several benefits for the fight against this crime. in this paper we present also additional capabilities that we have identified and suggestions in how to define and implement a triple-helix cooperation model of interaction between academy, financial institutions and law enforcement agencies. copyright © 2019 by the international institute ofinformatics and systemics."
"portfolio selection problem (psp) is one of the major interesting research areas in finance which have drawn interest of several researchers over the years. over time, the different approaches had been engaged in solving the psp ranging from computational techniques to metaheuristics techniques with varying results. in this paper, we engaged three different metaheuristics techniques under this same condition to solve extended markowitz mean-variance portfolio selection model. the three metaheuristics techniques are non-dominated sorting genetic algorithm ii (nsgaii), speed-constrained multi-objective particle swarm optimization (smpso) and generalized differential evolution 3 (gde3). a comparative analysis was carried out with results obtained with existing benchmark data available in literature. the outcome of the findings reveals that smpso shows superior performance, followed by nsgaii in many different instances; however, the mean execution time of gde3 was the fastest among the three techniques considered. © 2019 icai 2015 - worldcomp 2015. all rights reserved."
"this paper proposes a novel mixed-copula var (mcv) model for financial portfolio risk management and a novel investment strategy based on it. var (value at risk) is a traditional risk metric in computational finance to measure how much a set of investments might lose in a disadvantageous situation. previous var models assume that the yield rates follow a single distribution (e.g. normal distribution) for simplicity, which is far from reality. in order to improve the adaptivity and the extendability of the var method, this paper constructs an mcv model with several families of distributions and designs a fast em algorithm to compute the mixing weights. it further leads to a strategy for portfolio investment. experiments by monte carlo simulation verify the intention of mcv. besides, experiments on two real-world financial data sets indicate that mcv measures portfolio risk more accurately and adaptively, and delivers superior investing performance. © 2018 ieee"
"online portfolio selection is one of the fundamental problems in the field of computational finance. as financial markets are changing rapidly, investors need to dynamically adjust asset positions according to various kinds of information related to the capital market. online portfolio strategy with side information is studied without probability assumptions on the asset prices. by using the relative entropy function to measure the distance between two portfolios, an online portfolio exponential gradient strategy with side information is proposed, and moreover it is proved a universal portfolio, i.e., its average growth rate is asymptotically the same as that of the best state constant rebalanced portfolio, which is offline. the strategy is tested on actual stock data, and the effect of transaction costs on the strategy is also analyzed, which the results show that this strategy can obtain higher returns. © 2019, editorial board of journal of systems engineering society of china. all right reserved."
"we critically assess mainstream accounting and finance research applying methods from computational linguistics (cl) to study financial discourse. we also review common themes and innovations in the literature and assess the incremental contributions of studies applying cl methods over manual content analysis. key conclusions emerging from our analysis are: (a) accounting and finance research is behind the curve in terms of cl methods generally and word sense disambiguation in particular; (b) implementation issues mean the proposed benefits of cl are often less pronounced than proponents suggest; (c) structural issues limit practical relevance; and (d) cl methods and high quality manual analysis represent complementary approaches to analyzing financial discourse. we describe four cl tools that have yet to gain traction in mainstream af research but which we believe offer promising ways to enhance the study of meaning in financial discourse. the four tools are named entity recognition (ner), summarization, semantics and corpus linguistics. © 2019 the authors. journal of business finance & accounting published by john wiley & sons ltd"
"computational finance is one of the fastest-growing application areas for natural language processing technologies. already today, algorithmic trading funds are successfully using robo readers and sentiment analysis techniques to support adaptive algorithms that are capable of making automated decisions with little or no human intervention. however, these technologies are still in a nascent state and the competition to improve approaches within the industry is fierce. in this chapter, we discuss financial news analytics and learning strategies that help machines combine domain knowledge with other linguistic information that is extracted from text sources.we provide an overview of existing linguistic resources and methodological approaches that can be readily utilized to develop knowledge-driven solutions for financial news analysis. © springer nature singapore pte ltd. 2019."
"computationally-intensive tools play an increasingly important role in financial decisions. many financial problems-ranging from asset allocation to risk management and from option pricing to model calibration-can be efficiently handled using modern computational techniques. numerical methods and optimization in finance presents such computational techniques, with an emphasis on simulation and optimization, particularly so-called heuristics. this book treats quantitative analysis as an essentially computational discipline in which applications are put into software form and tested empirically. this revised edition includes two new chapters, a self-contained tutorial on implementing and using heuristics, and an explanation of software used for testing portfolio-selection models. postgraduate students, researchers in programs on quantitative and computational finance, and practitioners in banks and other financial companies can benefit from this second edition of numerical methods and optimization in finance. © 2019 elsevier inc. all rights reserved."
"in this paper we consider the pricing of american options, governed by a partial differential complementarity problem. the differential problem is first approximated by a semi-linear pde using two distinct penalty approaches which are well known in computational finance. we then initiate the two-grid algorithm by solving the nonlinear problem on a coarse grid and further the linearized in the interpolated coarse-grid solution problem on a fine grid. by means of the maximum principle the algorithm is shown to be of fourth order convergence rate in space. numerical experiments verify the presented two-grid approach where we draw some interesting conclusions. © 2017, sbmac - sociedade brasileira de matemática aplicada e computacional."
"computational human factors tools are often not fully-integrated during the early phases of product design. often, conventional ergonomic practices require physical prototypes and human subjects which are costly in terms of finances and time. ergonomics evaluations executed on physical prototypes has the limitations of increasing the overall rework as more iterations are required to incorporate design changes related to human factors that are found later in the design stage, which affects the overall cost of product development. this paper proposes a design methodology based on digital human modeling (dhm) approach to inform designers about the ergonomics adequacies of products during early stages of design process. this proactive ergonomics approach has the potential to allow designers to identify significant design variables that affect the human performance before full-scale prototypes are built. the design method utilizes a surrogate model that represents human product interaction. optimizing the surrogate model provides design concepts to optimize human performance. the efficacy of the proposed design method is demonstrated by a cockpit design study. copyright © 2018 asme."
"digital transformation (dt) inside higher education (he): in the new era, on the way to the iv industrial revolution corporations of all classes, are focused on the massive and optimal practice of ict in different organizational plans including human talent, organizational configurations, processes, inputs, outcomes, services and obviously the industry model. nowadays, 70% of the top companies have robust dt teams, and 40% of them are driven by high ict and artificial and computational intelligence (ai, ci). in other hand, very few colombian universities (u) have undertaken the problem of competitiveness in he, much less have attempted to face the iv industrial revolution and take the jump, thus suffering a tremendous disappointment due to he obsolescence. the u should play an important role in the reduction of the digital gap, and even more, must design new programs that face the formation of digital talent (i.e. cyber-programmers professional technicians) that is required in this era, or we will belate again to change. in the recent ict international congress in latin america1, numerous national cases of dt were presented from companies, but, unfortunately, not a single case from he. ¿will the colombian computerization case be replicated, which was left to the discretion of transnational computer elites? the dt of u, implies necessarily the cognimatics (the informatics for the knowledge society) of all the companies and their social insertion in this journey... university architecture: enterprise architecture (ea): architecting implies the organization of a system of subsystems (sos) or constituents to obtain better and/or new functionalities. if any piece is knowledge-based, an intelligent performance is obtained. the architecture-multilayer approach is a sos one, that ensures compliance with government policies, procedures and standards, in a highly complex establishment with intellectual capital and intelligent processes, which is a usual status quo in an academy institution; defines the subsystems at a higher level, where a system is made-up, and with the protocols by which they communicate. ea and university architecture provide an industry 360° vision map, and organization structure for business and technological changes. ea is used as a system overall configuration of subsystems or components organized in layers, where each one describes an ordered audience of structures and common functionalities assembled by a purposeful criterion inside the he institutions. as purposes we present a succinct outline of our architectural dt model of the digital university transformation. the pillars of the architecture are: finance, innovation and investigation, entrepreneurship and social projection; recognizing from the start, that knowledge has its ethos in the u; they correspond to: 1- productive ecosystem of the dt. 2- dt that enhances the knowledge and innovation in the universities for the habilitation of the digital capacities throughout the institution. 3- a new economy that requires technology transformation also supported on entrepreneurship. 4- new dt human talent required by the new ia and knowledge industry. the student hyper-personalization by competences and skills, bio-data, socioeconomic conditions and family composition, is required. we discuss the basic and conceptual aspects, and methodological dimension for the dt, applying some intelligent constructs that we have developed and documented during the last 10 years at fessanjose2 leading the dt in he. © 2018 latin american and caribbean consortium of engineering institutions. all rights reserved."
"data generated from sensors, iot environment and many real time applications is mainly spatial, temporal, or spatio-temporal. some of them include data generated from geospatial, geographical, medical, weather, finance and environmental applications. such data objects changes over time. conventional knowledge discovery techniques available do not address the need for analyzing such complex datasets and hence data analysis has become increasingly complex and challenging. soft computing principles such as fuzzy logic, evolutionary and nature inspired computations may be applied to analyze dynamically varying data. analyzing temporal trends of association patterns requires handling the temporal data, as prevalence values of temporal patterns are implicitly vectors. finding prevalence values of temporal association patterns and validating them for similarity using conventional approach increases the computational complexity. this makes it challenging as the conventional data mining algorithms do not address this need. in this research, we propose a novel approach for estimation of temporal association pattern prevalence values and a novel temporal fuzzy similarity measure which holds monotonicity to find similarity between any two temporal patterns. experiments are performed considering naive, sequential, spamine and proposed approach. the results obtained show the proposed approach is promising and reduces computational complexity in terms of computing true prevalence and optimizing execution times. © 2017 elsevier b.v."
"we demonstrate the flexibility and ease of use of ccc algorithmic differentiation (ad) tools based on overloading through application to numerical patterns (kernels) arising in computational finance. while adjoint methods and ad have been known in the finance literature for some time, there are few tools capable of handling and integrating with the ccc codes found in production. adjoint methods are also known to be very powerful but have potentially infeasible memory requirements. we present several techniques for dealing with this problem and demonstrate them on numerical kernels that occur frequently in finance. we build the discussion around the mature ad tool dco/c++, which is designed to handle arbitrary ccc codes and be highly flexible; however, the sketched concepts can certainly be transferred to other ad solutions including in-house tools. an archive of the source code for the numerical kernels as well as all the ad solutions discussed can be downloaded from an accompanying website. this includes documentation for the code and for dco/c++. trial licences for dco/c++ are available from numerical algorithms group ltd. © 2018 infopro digital risk (ip) limited."
"being published as a celebration of the 60th anniversary of john von neumann's ""theory of self-reproducing automata,"" this handbook attempts to provide a unique reflection on the nature of computational economics and finance (cef) in light of natural computationalism. we restructure cef by including both nature-inspired computing and natural computing. this new framework allows us to have a view of cef much broader than just the conventional algorithmic consideration. the book begins with a historical review of computational economics (ce), tracing its history far back to the era of analog computing. in these early days, advancements were mainly made using the idea of natural computing, and the subjects pursued by ce were the computing system as a whole, not just numerical computing. the handbook then is organized by distinguishing computing from computing systems. six chapters (chapters 2 to 7) are devoted to the former. they together present a review on the recent progresses in ce, as illustrated by the computation of rational expectations, general equilibrium, risk, and volatility. the subsequent 16 chapters are devoted to the computing-systemic view of ce, including natural-inspired computing (chapters 8 to 12) and network, agent-based computing and neural computing (chapters 13 to 23). in addition to providing alternative approaches to forecasting, investment strategies and risk management, etc., they enable us to have a 'natural' or more realistic description of the economy, starting from its decision makers; hence, market-design or policy-design issues involving different levels of the economy, be microscopic, mesoscopic and macroscopic, can be simultaneously addressed and coherently integrated. the handbook concludes with a chapter on what we may hope from ce by providing an in-depth review on the epistemological aspects of computation. © oxford university press 2018. all rights reserved."
"the classical black-scholes pricing model is based on standard geometric brownian motion, and the log-returns of this model are independent and gaussian. however, most of the recent researches on the statistical properties of the log-returns make this hypothesis not always consistent. one of the ongoing issues of mathematical finance today is to design an efficient numerical algorithm for the pricing model, which might be modified from the standard black-scholes diffusion equation and would have favorable empirical results. of those financial models that have been already proposed, the most interesting include the finite moment log-stable (fmls) process model and its fractional partial integral-differential equation. in this paper, we consider to use gauss-jacobi spectral method on a two-dimensional computation domain in order to discretize the fmls fractional partial integral-differential equation, and further illustrate the flexibility and accuracy of the method by comparing the first order finite difference scheme for the pricing examples of european and american-styled options. our results suggest that the global character of the gauss-jacobi method makes them well-suited to fractional partial integral-differential equations and can naturally take the global behavior of the solution into account and thus do not lead to an extra computational cost when moving from a second-order to a fractional-order diffusion model. © 2018 global-science press."
"deep learning machine that employs spiking neural network (snn) is currently one of the main techniques in computational intelligence to discover knowledge from various fields. it has been applied in many application areas include health, engineering, finances, environment, and others. this paper addresses a classification problem based on a functional magnetic resonance image (fmri) brain data experiment involving a subject who reads a sentence or looks at a picture. in the experiment, signal to noise ratio (snr) is used to select the most relevant features (voxels) before they were propagated in an snn-based learning architecture. the spatiotemporal relationships between spatio temporal brain data (stbd) are learned and classified accordingly. all the brain regions are taken from data with label star plus-04847-v7.mat. the overall results of this experiment show that the snr method helps to get the most relevant features from the data to produced higher accuracy for reading a sentence instead of looking a picture. © 2018 insight society."
"time series is widely found in various fields such as geoscience, medicine, finance, and social sciences. how to effectively extract the features of time series remains a challenge due to its potentially complex non-linear dynamics. recently, random projection filter bank (rpfb) [5] is proposed as a generic and simple approach to extract features from time series data. it generates the features by randomly generating numerous autoregressive filters that are convolved with input time series. such numerous random filters inevitably have redundancy and lead to the increased computational cost of the classifier. in this paper, we propose a distillation method of rpfb, named d-rpfb, to not only maintain the high level of quantity of the filters, but also reduce the redundancy of the filters while improving precision. we demonstrate the efficacy of the features extracted by d-rpfb via extensive experimental evaluation in three different areas of time series data with three traditional classifiers (i.e., logistic regression (lr) [2], support vector machine (svm) [14] and random forest (rf) [8]). © springer nature switzerland ag 2018."
"learning how to forecast is always important for traders, and divergent learning frequencies prevail among traders. the influence of the evolutionary frequency on learning performance has occasioned many studies of agent-based computational finance (e.g., lettau in j econ dyn control 21:1117–1147, 1997. doi:10.1016/s0165-1889(97)00046-8; szpiro in complexity 2(4):31–39, 1997. doi:10.1002/(sici)1099-0526(199703/04)2:4&lt;31::aid-cplx8&gt;3.0.co;2-3; cacho and simmons in aust j agric resour econ 43(3):305–322, 1999. doi:10.1111/1467-8489.00081). although these studies all suggest that evolving less frequently and, hence, experiencing more realizations help learning, this implication may result from their common stationary assumption. therefore, we first attempt to approach this issue in a ‘dynamically’ evolving market in which agents learn to forecast endogenously generated asset prices. moreover, in these studies’ market settings, evolving less frequently also meant having a longer time horizon. however, it is not true in many market settings that are even closer to the real financial markets. the clarification that the evolutionary frequency and the time horizon are two separate notions leaves the effect of the evolutionary frequency on learning even more elusive and worthy of exploration independently. we find that the influence of a trader’s evolutionary frequency on his forecasting accuracy depends on all market participants and the resulting price dynamics. in addition, prior studies also commonly assume that traders have identical preferences, which is too strong an assumption to apply to a real market. considering the heterogeneity of preferences, we find that converging to the rational expectations equilibrium is hardly possible, and we even suggest that agents in a slow-learning market learn frequently. we also apply a series of econometric tests to explain the simulation results. © 2017, springer science+business media new york."
"a practical, interdisciplinary guide to advanced mathematical methods for scientists and engineers. mathematical methods in science and engineering, second edition, provides students and scientists with a detailed mathematical reference for advanced analysis and computational methodologies. making complex tools accessible, this invaluable resource is designed for both the classroom and the practitioners; the modular format allows flexibility of coverage, while the text itself is formatted to provide essential information without detailed study. highly practical discussion focuses on the ""how-to"" aspect of each topic presented, yet provides enough theory to reinforce central processes and mechanisms. recent growing interest in interdisciplinary studies has brought scientists together from physics, chemistry, biology, economy, and finance to expand advanced mathematical methods beyond theoretical physics. this book is written with this multi-disciplinary group in mind, emphasizing practical solutions for diverse applications and the development of a new interdisciplinary science. © 2018 johnwiley & sons, inc. all rights reserved."
"selected inversion problems must be addressed in several research fields like physics, genetics, weather forecasting, and finance, in order to extract selected entries from the inverse of large, sparse matrices. state-of-the-art algorithms are either based on the lu factorization or on an iterative process. both approaches present computational bottlenecks related to prohibitive memory requirements or extremely high running time for large-scale matrices. in recent years, in order to overcome such limitations, an alternative approach for computing stochastic estimates of the inverse entries has been developed. in this work, we present a stochastic estimator for the diagonal of the inverse and test its performance on a dataset of symmetric, positive semidefinite matrices coming from the field of atomistic quantum transport simulations with nonequilibrium green's functions (negf) formalism. in such a framework, it is required to solve the schrödinger equation thousands of times, demanding the computation of the diagonal of the retarded green's function, i.e., the inverse of a large, sparse matrix including open boundary conditions. given the nature and the structure of the negf matrices, our stochastic estimation framework exploits the capabilities of a stencil-based, matrix-free code, avoiding the fill-in and lack of scalability that the lv-based methods present for three-dimensional nanoelectronic devices. we also illustrate the impact of the stochastic estimator by comparing its accuracy against existing methods and demonstrate its scalability performance on the 'piz daint' cluster at the swiss national supercomputing center, preparing for postpetascale three-dimensional nanoscale calculations. © 2018 ieee."
"in the last decade parallel computation (cuda in particular), has accelerated the world. applications used in astronomy, biology, chemistry, physics, data mining, manufacturing, finance, machine-learning and other computational intense fields are increasingly using cuda to deliver the benefits of gpu acceleration. also, cuda is used in boosting the performance of database-oriented problems, speeding up the sql performance on a huge dataset resulting in acquiring the data in rapid speed. this paper focuses on reading data from a. csv file, implementing kernels and performing the basic sql aggregation functions on the gpu, and comparing their performance with the mysql counterpart, showing the worthiness and importance of the concept. © 2018 ieee."
"these lecture notes provide a comprehensive, self-contained introduction to the analysis of wishart matrix moments. this study may act as an introduction to some particular aspects of random matrix theory, or as a self-contained exposition of wishart matrix moments. random matrix theory plays a central role in statistical physics, computational mathematics and engineering sciences, including data assimilation, signal processing, combinatorial optimization, compressed sensing, econometrics and mathematical finance, among numerous others. the mathematical foundations of the theory of random matrices lies at the intersection of combinatorics, non-commutative algebra, geometry, multivariate functional and spectral analysis, and of course statistics and probability theory. as a result, most of the classical topics in random matrix theory are technical, and mathematically difficult to penetrate for non-experts and regular users and practitioners. the technical aim of these notes is to review and extend some important results in random matrix theory in the specific context of real random wishart matrices. this special class of gaussian-type sample covariance matrix plays an important role in multivariate analysis and in statistical theory.we derive non-asymptotic formulae for the full matrix moments of real valued wishart random matrices. as a corollary, we derive and extend a number of spectral and trace-type results for the case of non-isotropic wishart random matrices. we also derive the full matrix moment analogues of some classic spectral and trace-type moment results. for example, we derive semi-circle and marchencko-pastur-type laws in the non-isotropic and full matrix cases. laplace matrix transforms and matrix moment estimates are also studied, along with new spectral and trace concentration-type inequalities. © 2018 now publishers inc. all rights reserved."
"expert and intelligent systems understand the underlying information behind the data by relying on a wide range of machine learning techniques. the interpretation of machine learning models is often the key to success in research areas such as business, finance, medical and health science, and bioinformatics; such research areas demand human understanding of the obtained model. the logistic model tree (lmt) algorithm is a popular classification method that combines a decision tree and logistic regression models. the combination of two complementary algorithms produces an accurate and interpretable classifier by combining the advantages of both logistic regression and tree induction. however, lmt has the disadvantage of high computational cost, which makes the algorithm undesirable in practice. in this paper, we propose an efficient method to learn the logistic regression models in the tree. we employ least angle regression to update the regression model in logitboost so that the algorithm efficiently learns sparse logistic regression models composed of relevant input variables. we compare the performance of our proposed method with the original lmt algorithm using 14 benchmark datasets and show that the training time dramatically decreases while the accuracy is preserved. our proposed algorithm is not only accurate and intuitively interpretable but also computationally efficient. it helps users in making the best possible use of the data that are included in expert and intelligent systems. © 2017 elsevier ltd"
the proceedings contain 18 papers. the topics discussed include: continuous business model planning with the value management platform; modeling communities in e3value; ascribing value; contract modeling utilizing demo co-creation co-production model; economic exchange in a regulated shared ledger; towards an ontology of competition; using mathematical model theory to align conceptual and operational ontologies in fibo; analysing transaction codes with data requirements in information systems for compliance monitoring - a manufacturing case study for computational auditing; on the ontological nature of rea core relations; the pattern of patterns: what is a pattern in conceptual modeling?; implementation of rea contracts as blockchain smart contracts: an exploratory example; ontorea © accounting and finance model: including option contracts in the mdd-context; on the development of a modelling framework for value cocreation; brand identity ontology; value exchange and formation of coalitions; an introduction to commitment based smart contracts using reactionruleml; and configuring value networks based on subjective business values.
"we present a model and a computational procedure for dealing with seasonality and regime changes in time series. in this work we are interested in time series which in addition to trend display seasonality in mean, in autocorrelation and in variance. these type of series appears in many areas, including hydrology, meteorology, economics and finance. the seasonality is accounted for by subset par modelling, for which each season follows a possibly different autoregressive model. levels, trend, autoregressive parameters and residual variances are allowed to change their values at fixed unknown times. the identification of number and location of structural changes, as well as par lags indicators, is based on genetic algorithms, which are suitable because of high dimensionality of the discrete search space. an application to italian industrial production index time series is also proposed. © springer international publishing ag."
"this paper explores the effects of fiscal policy in the presence of a vat evasion channel, and then compares and contrasts two regimes-the exogenous vs. optimal policy case. to this end, a dynamic general-equilibrium model, calibrated to bulgarian data (1999-2014), is augmented with a government sector. the main findings from the computational experiments performed in the paper are: (i) the optimal steady-state income tax rate is zero; (ii) the benevolent ramsey planner provides the optimal amount of the valuable public services, which are now three times lower; (iii) the size of the grey sector is twice lower; (iv) optimal steady-state consumption tax needed to finance the optimal level of government spending is twice lower, as compared to the exogenous policy case. © 2018, faculty of social sciences. all rights reserved."
"marketing practices have adopted the use of computational approaches in order to optimize the performance of their promotional emails and site advertisements. in the case of promotional emails, subject lines have been found to offer a reliable signal of whether the recipient will open an email or not. clickbait headlines are also known to drive reader engagement. in this study, we explore the differences in recipients' preferences for subject lines of marketing emails from different industries, in terms of their clickthrough rates on marketing emails sent by different businesses in finance, cosmetics and television industries. different stylistic strategies of subject lines characterize high clickthroughs in different commercial verticals. for instance, words providing insight and signaling cognitive processing lead to more clickthroughs for the finance industry; on the other hand, social words yield more clickthroughs for the movies and television industry. domain adaptation can further improve predictive performance for unseen businesses by an average of 16.52% over generic industry-specific predictive models. we conclude with a discussion on the implications of our findings and suggestions for future work. © 2018 association for computing machinery."
"the objective of this work is to present a quick gbest guided artificial bee colony (abc) learning algorithm to train the feedforward neural network (qggabc-ffnn) model for the prediction of the trends in the stock markets. as it is quite important to know that nowadays, stock market prediction of trends is a significant financial global issue. the scientists, finance administration, companies, and leadership of a given country struggle towards developing a strong financial position. several technical, industrial, fundamental, scientific, and statistical tools have been proposed and used with varying results. still, predicting an exact or near-to-exact trend of the stock market values behavior is an open problem. in this respect, in the present manuscript, we propose an algorithm based on abc to minimize the error in the trend and actual values by using the hybrid technique based on neural network and artificial intelligence. the presented approach has been verified and tested to predict the accurate trend of saudi stock market (ssm) values. the proposed qggabc-ann based on bio-inspired learning algorithm with its high degree of accuracy could be used as an investment advisor for the investors and traders in the future of ssm. the proposed approach is based mainly on ssm historical data covering a large span of time. from the simulation findings, the proposed qggabc-ffnn outperformed compared with other typical computational algorithms for prediction of ssm values. © 2018 by the author."
"transboundary has become not only a trend but also an important requirement and feature of modern service industry development. in the internet era, transboundary service not only changed people's daily life, but also had a profound impact on the business model of traditional industry. china has proposed the 'internet+' development plan, hoping to use the internet for the transformation of traditional industries, including retail, manufacturing, catering, agriculture, finance, and health care. however, the transboundary impact of internet on the operating model of traditional industry is decided by a combination of a variety of factors. currently, the appropriate assessment model and analysis methods are absent in this field. based on this, the impact evaluation method of internet (service bridge) is proposed from the prospect of supply and demand matching, which includes three main parts: the capability model of supply side, the characteristic model of demand side, and the service bridge model. based on the proposed method, the corresponding computational experiment system is built to evaluate the impact of internet mode in different industries. finally, this paper verifies the method with actual cases, and compares the transboundary impact of internet in different daily consumption industries (online to offline in beauty service and take-out food service industry). the results showed that the 'service bridge' method can introduce a new idea for the transboundary impact evaluation of internet, which can provide some decision support to the reconstruction of demand value chain in some traditional industries. © 2014 ieee."
"quantization techniques have been applied in many challenging finance applications, including pricing claims with path dependence and early exercise features, stochastic optimal control, filtering problems and efficient calibration of large derivative books. recursive marginal quantization (rmq) of the euler scheme has recently been proposed as an efficient numerical method for evaluating functionals of solutions of stochastic differential equations. this method involves recursively quantizing the conditional marginals of the discrete-time euler approximation of the underlying process. by generalizing this approach, we show that it is possible to perform rmq for two higher-order schemes: the milstein scheme and a simplified weak order 2.0 scheme. we further extend the applicability of rmq by showing how absorption and reflection at the zero boundary may be incorporated, when necessary. to illustrate the improved accuracy of the higher-order schemes, various computations are performed using geometric brownian motion and the constant elasticity of variance model. for both models, we provide evidence of improved weak order convergence and computational efficiency. by pricing european, bermudan and barrier options, further evidence of improved accuracy of the higher-order schemes is demonstrated. © 2018 informa uk limited, trading as taylor & francis group."
"hardware accelerators are an effective solution to increase the performance of algorithms in a wide array of disciplines, from data science to computational finance. however, data scientists and mathematicians often do not have the required knowledge or time to fully exploit these accelerators, and they perceive them as difficult and frustrating to use. opencl was created to simplify the creation of computational pipelines with heterogeneous hardware, but as of today, its integration with high-level languages commonly used in data science is limited. in this paper, we propose a framework to integrate opencl kernels running on field programmable gate arrays (fpgas) with python, r, and matlab, the most common languages used in data science. our framework can automatically generate all the interfaces needed to wrap an opencl kernel into these high-level languages and provide the user with a transparent access to the kernel itself. © 2018 ieee."
"the practical adoption of the solvency ii regulatory framework in 2016, together with increasing property and casualty (pc) claims in recent years and an overall reduction of treasury yields across more developed financial markets have profoundly affected traditional risk management approaches by insurance institutions. the adoption of firm-wide risk capital methodologies to monitor the companies’ overall risk exposure has further consolidated the introduction of risk-adjusted performance measures to guide the management medium and long-term strategies. relying on a dynamic stochastic programming formulation of a 10 year asset-liability management (alm) problem of a pc company, we analyse in this article the implications on capital allocation and risk-return trade-offs of an optimization problem developed for a global insurance company based on a pair of risk-adjusted return functions. the analysis is relevant for any institutional investor seeking a high risk-adjusted performance as for regulators in their structuring of stress-tests and effective regulatory frameworks. the introduction of the concept of risk capital, or economic capital, in the definition of medium and long term insurance strategies poses a set of modeling and methodological issues tackled in this article. of particular interest is the study of optimal alm policies under different assets’ correlation assumptions. from a computational viewpoint it turns out that, depending on the assumed correlation matrix, the stochastic program is linear or of second order conic type. a case study from a real-world company development is presented to highlight the effectiveness of applied stochastic programming in capturing complex risk and return dynamics arising in modern corporate finance and lead to an efficient long-term financial allocation process. © 2018, springer-verlag gmbh germany, part of springer nature."
"in domains such as health care and finance, shortage of labeled data and computational resources is a critical issue while developing machine learning algorithms. to address the issue of labeled data scarcity in training and deployment of neural network-based systems, we propose a new technique to train deep neural networks over several data sources. our method allows for deep neural networks to be trained using data from multiple entities in a distributed fashion. we evaluate our algorithm on existing datasets and show that it obtains performance which is similar to a regular neural network trained on a single machine. we further extend it to incorporate semi-supervised learning when training with few labeled samples, and analyze any security concerns that may arise. our algorithm paves the way for distributed training of deep neural networks in data sensitive applications when raw data may not be shared directly. © 2018 elsevier ltd"
"stock market forecasting is very important in the planning of business activities. stock price prediction has attracted many researchers in multiple disciplines including computer science, statistics, economics, finance, and operations research. recent studies have shown that the vast amount of online information in the public domain such as wikipedia usage pattern, news stories from the mainstream media, and social media discussions can have an observable effect on investors' opinions towards financial markets. the reliability of the computational models on stock market prediction is important as it is very sensitive to the economy and can directly lead to financial loss. in this paper, we retrieved, extracted, and analyzed the effects of news sentiments on the stock market. our main contributions include the development of a sentiment analysis dictionary for the financial sector, the development of a dictionary-based sentiment analysis model, and the evaluation of the model for gauging the effects of news sentiments on stocks for the pharmaceutical market. using only news sentiments, we achieved a directional accuracy of 70.59% in predicting the trends in short-term stock price movement. © 2018 ieee."
"this paper explores the effects of fiscal policy in an economy based on indirect taxes, and one that is constrained to taxing all (labor and capital) income at the same rate. the focus of the paper is on the relative importance of consumption vs. income taxation, as well as on the provision of utility-enhancing public services. to this end, a real-business-cycle model, calibrated to bulgarian data (1999-2014), was set up with a richer public finance side. bulgarian economy was chosen as a case study due to its major dependence on consumption taxation as a source of tax revenue. to illustrate the effects of fiscal policy, two regimes were compared and contrasted to one another - exogenous vs. optimal (ramsey) policy case. the main findings from the computational experiments performed are: (i) the optimal steady-state (capital and labor income) tax rate is zero, as it is the most distortionary tax to use; (ii) the optimal steady-state consumption tax (the only source of revenue) has to almost double to finance the optimally-set level of government purchases. © 2018 oldenbourg wissenschaftsverlag gmbh, published by de gruyter oldenbourg, berlin/boston."
"forecasting time series data is an important subject in economics, business, and finance. traditionally, there are several techniques to effectively forecast the next lag of time series data such as univariate autoregressive (ar), univariate moving average (ma), simple exponential smoothing (ses), and more notably autoregressive integrated moving average (arima) with its many variations. in particular, arima model has demonstrated its outperformance in precision and accuracy of predicting the next lags of time series. with the recent advancement in computational power of computers and more importantly development of more advanced machine learning algorithms and approaches such as deep learning, new algorithms are developed to analyze and forecast time series data. the research question investigated in this article is that whether and how the newly developed deep learning-based algorithms for forecasting time series data, such as 'long short-term memory (lstm)', are superior to the traditional algorithms. the empirical studies conducted and reported in this article show that deep learning-based algorithms such as lstm outperform traditional-based algorithms such as arima model. more specifically, the average reduction in error rates obtained by lstm was between 84 - 87 percent when compared to arima indicating the superiority of lstm to arima. furthermore, it was noticed that the number of training times, known as 'epoch' in deep learning, had no effect on the performance of the trained forecast model and it exhibited a truly random behavior. © 2018 ieee."
"— the binomial-tree model is a numerical method widely used in finance with a computational complexity which is quadratic with respect to the solution accuracy. the existing research has employed reconfigurable computing to provide faster solutions compared with general-purpose processors, but they require low-level manual design by a hardware engineer, and can only solve american options. this paper presents a formal mathematical framework that captures a large class of binomial-tree problems, and provides a systolic data-movement template that maps the framework into digital hardware. this paper also presents a fully automated design flow, which takes c-level user descriptions of binomial trees, with custom data types and tree operations, and automatically generates fully pipelined reconfigurable hardware solutions in field-programmable gate array (fpga) bit-stream files. on a xilinx virtex-7 xc7vx980t fpga at a 100-mhz clock frequency, we require 54-µs latency to solve three 876-step 32-bit fixed-point american option binomial trees, with a pricing rate of 114k trees/s. from the same device and in comparison to the existing solutions with equivalent fpga technology, we always achieve better throughput. this ranges from 1.4× throughput compared with a hand-tuned register-transfer level systolic design, to 9.1× and 5.6× improvement with respect to scalar and vector architectures, respectively. © 2017 ieee."
"solve engineering and scientific partial differential equation applications using the pde2d software developed by the author solving partial differential equation applications with pde2d derives and solves a range of ordinary and partial differential equation (pde) applications. this book describes an easy-to-use, general purpose, and time-tested pde solver developed by the author that can be applied to a wide variety of science and engineering problems. the equations studied include many time-dependent, steady-state and eigenvalue applications such as diffusion, heat conduction and convection, image processing, math finance, fluid flow, and elasticity and quantum mechanics, in one, two, and three space dimensions. the author begins with some simple ""0d” problems that give the reader an opportunity to become familiar with pde2d before proceeding to more difficult problems. the book ends with the solution of a very difficult nonlinear problem, which requires a moving adaptive grid because the solution has sharp, moving peaks. this important book: describes a finite-element program, pde2d, developed by the author over the course of 40 years derives the ordinary and partial differential equations, with appropriate initial and boundary conditions, for a wide variety of applications offers free access to the windows version of the pde2d software through the author’s website at www.pde2d.com offers free access to the linux and macosx versions of the pde2d software also, for instructors who adopt the book for their course and contact the author at www.pde2d.com written for graduate applied mathematics or computational science classes, solving partial differential equation applications with pde2d offers students the opportunity to actually solve interesting engineering and scientific applications using the accessible pde2d. © 2018 johnwiley & sons, inc. all rights reserved."
"in this work, we propose a smart idea to couple importance sampling and multilevel monte carlo (mlmc). we advocate a per level approach with as many importance sampling parameters as the number of levels, which enables us to handle the different levels independently. the search for parameters is carried out using sample average approximation, which basically consists in applying deterministic optimisation techniques to a monte carlo approximation rather than resorting to stochastic approximation. our innovative estimator leads to a robust and efficient procedure reducing both the discretization error (the bias) and the variance for a given computational effort. in the setting of discretized diffusions, we prove that our estimator satisfies a strong law of large numbers and a central limit theorem with optimal limiting variance, in the sense that this is the variance achieved by the best importance sampling measure (among the class of changes we consider), which is however non tractable. finally, we illustrate the efficiency of our method on several numerical challenges coming from quantitative finance and show that it outperforms the standard mlmc estimator. © 2017, springer science+business media, llc."
"the book is a monograph in the cross disciplinary area of computational intelligence in finance and elucidates a collection of practical and strategic portfolio optimization models in finance, that employ metaheuristics for their effective solutions and demonstrates the results using matlab implementations, over live portfolios invested across global stock universes. the book has been structured in such a way that, even novices in finance or metaheuristics should be able to comprehend and work on the hybrid models discussed in the book. © iste ltd 2018. all rights reserved."
"using the weighted and shifted lubich difference (wsld) operator, we propose an efficient and stable difference scheme for solving space tempered fractional diffusion equations. the scheme has fourth order accuracy in space and second order accuracy in time. to improve the computational efficiency further, a fourth order backward euler scheme is used to achieve fourth order accuracy in time. the fourth order backward euler method is shown to be stable under a minor requirement. for two-dimensional problems, the crank-nicolson alternating direction implicit (cn-adi) scheme is used to reduce computational complexity. these proposed methods are applied to the carr-geman-madan-yor (cgmy) model in finance, and several numerical experiments are designed to verify the efficiency of the proposed methods. © 2018 society for industrial and applied mathematics."
"in this study we look at one wheel in the machinery of modern finance that may help evaluate piketty's contributions in his best-seller capital in the 21st century (c21c): the constant-growth equity model, also known as gordon's model. we first briefly review piketty's text, and highlight two theories advanced by piketty: one about the relationship between return on capital and economic growth (r − g) associated with the ratio between two variables (k/y), and another theory in which the same (k/y) relationship is associated with a ratio between the growth rate of savings-to-economic growth (i.e., k/y = s/g). piketty uses these two devices, s/g and r > g to warn readers about a possible future of secular stagnation (a continued age of very low or even negative g's), in which the inequality r > g may create inequality levels not seen since the xix century, or worse. the constant growth model, however, provides what piketty's analysis does not include: transitional dynamics, the adjustments agents would make in such dire low growth scenario and system responses. furthermore, the constant growth model shows why r > g, r − g > 0 is both a logical and a computational condition valid for all times. in sum, we show that piketty's theoretical devices cannot support his contentions. © 2017"
"in real companies engaged in economic activities through transactions involving consumer items, such as retail, distribution, finance, and information materials, supplying an opportunity to customers to choose specialized items is an important factor that can improve customer satisfaction and convenience allowing their diverse and time-dependent needs to be met. however, capturing the specialized needs of customers accurately is a difficult task because their needs depend on time, context, situation, and meaning. recently, physical computational environments have been developing rapidly, thereby allowing easy implementation to sense a customer's action and deal with it sequentially. in this paper, we propose a personalized method to predict individual interests and demands appropriately. in particular, the system learns the customers' situation, meaning, and action from their action history, and reflects a feedback of the result to predict the next action. to realize this method, we utilize the following two methodologies: the mathematical model of meaning (mmm), which is a semantic associative search technology; and the local variational inference (lvi), which is an approximation of the bayesian inference. a numerical experiment shows that the proposed method performed better than a typical method. © 2018 ieee."
"given a large volume of multi-dimensional data streams, such as that produced by iot applications, finance and online web-click logs, how can we discover typical patterns and compress them into compact models? in addition, how can we incrementally distinguish multiple patterns while considering the information obtained from a pattern found in a streaming setting? in this paper, we propose a streaming algorithm, namely streamscope, that is designed to find intuitive patterns efficiently from event streams evolving over time. our proposed method has the following properties: (a) it is effective: it operates on semi-infinite collections of co-evolving streams and summarizes all the streams into a set of multiple discrete segments grouped by their similarities. (b) it is automatic: it automatically and incrementally recognizes such patterns and generates models for each of them if necessary; (c) it is scalable: the complexity of our method does not depend on the length of the data streams. our extensive experiments on real datasets demonstrate that streamscope can find meaningful patterns and achieve great improvements in terms of computational time and memory space over its full batch method competitors. © 2018 acm."
"high-performance oscillators, atomic clocks for instance, are important in modern industries, finance and scientific research. in this paper, the authors study the estimation and prediction of long-term stability based on convex optimization techniques and compressive sensing. to take frequency drift into account, its influence on allan and modified allan variances is formulated. meanwhile, expressions for the expectation and variance of discrete-time hadamard variance are derived. methods that reduce the computational complexity of these expressions are also introduced. tests against gps precise clock data show that the method can correctly predict one-week frequency stability from 14-day measured data. © 2018 by the authors. licensee mdpi, basel, switzerland."
"iterative stencils represent the core computational kernel of many applications belonging to different domains, from scientific computing to finance. given the complex dependencies and the low computation to memory access ratio, this kernels represent a challenging acceleration target on every architecture. this is especially true for fpgas, whose direct hardware execution offers the possibility for high performance and power efficiency, but where the non-fixed architecture can lead to very large solutions spaces to be explored. in this work, we build upon an fpga-based acceleration methodology for iterative stencil algorithms previously presented, where we provide a dataflow architectural template that implements optimal on-chip buffering and is able to increase almost linearly in performance using a scaling technique denoted as iterations queuing. in particular, we propose a set of design improvements and we elaborate an accurate analytical performance model that can be used to support the exploration of the design space. experimental results obtained implementing a set of benchmarks from different application domains on a xilinx vc707 board show an average performance and power efficiency increase over the previous work of respectively 22.8x and 9.9x, and a prediction error that is on average 0.5%. © 2018 ieee."
"optimization methods play a central role in financial modeling. this textbook is devoted to explaining how state-of-the-art optimization theory, algorithms, and software can be used to efficiently solve problems in computational finance. it discusses some classical mean-variance portfolio optimization models as well as more modern developments such as models for optimal trade execution and dynamic portfolio allocation with transaction costs and taxes. chapters discussing the theory and efficient solution methods for the main classes of optimization problems alternate with chapters discussing their use in the modeling and solution of central problems in mathematical finance. this book will be interesting and useful for students, academics, and practitioners with a background in mathematics, operations research, or financial engineering. the second edition includes new examples and exercises as well as a more detailed discussion of mean-variance optimization, multi-period models, and additional material to highlight the relevance to finance. reviews review of first edition:‘this book will be useful as a textbook for students in financial engineering at the ms level…. the book will also be of interest to researchers and graduate students in optimization who are interested in applications of optimization to financial problems.' brian borchers source: journal of online mathematics and its applications review of first edition:‘this book would certainly appeal to someone with a mathematical background, perhaps in operations research, wishing to update and apply their knowledge to the financial world.' source: mathematics today review of first edition:‘until now, there has been no comprehensive optimization book aimed at quantitative analysts in the financial industry. the book by cornuejols and tutuncu fills this void… an excellent source for quantitative financial analysts and graduate students to learn about basic optimization theory, computational methods, and available software. at the same time, it can be used by academic researchers and students in optimization as an introduction to various interesting problems in financial applications.' source: international review of economics and finance. © gérard cornuéjols and reha tütüncü 2007."
"data streams are a new class of data that is becoming pervasively important in a wide range of applications, ranging from sensor networks, environmental monitoring to finance. in this article, we propose a novel framework for the online diagnosis of evolution of multidimensional streaming data that incorporates recursive wavelet density estimators into the context of velocity density estimation. in the proposed framework changes in streaming data are characterized by the use of local and global evolution coefficients. in addition, we propose for the analysis of changes in the correlation structure of the data a recursive implementation of the pearson correlation coefficient using exponential discounting. two visualization tools, namely temporal and spatial velocity profiles, are extended in the context of the proposed framework. these are the three main advantages of the proposed method over previous approaches: (1) the memory storage required is minimal and independent of any window size; (2) it has a significantly lower computational complexity; and (3) it makes possible the fast diagnosis of data evolution at all dimensions and at relevant combinations of dimensions with only one pass of the data. with the help of the four examples, we show the framework’s relevance in a change detection context and its potential capability for real world applications. © 2018 copyright is held by the owner/author(s)."
"in light of the structural role of computational technology in the expansion of modern global finance, this essay investigates the ontology of contemporary markets starting from a reformulation of liquidity—one of the tenets of financial trading. focusing on the nexus between financial and algorithmic flows, the paper complements contemporary philosophies of the market with insights into recent theories of computation, emphasizing the functional role of contingency, both for market trading and algorithmic processes. considering the increasing adoption of advanced computational methods in automated trading strategies, this article argues that the event of price is the direct manifestation of the incomputability at the heart of market exchange. in doing so, it questions the ontological assumptions of “flow” and “fluidity” underpinning traditional conceptions of liquidity and challenges the notion of rationality in market behavior. ultimately, the paper gestures toward some of the social and political consequences of this reformulation. © 2017, springer science+business media dordrecht."
"currently, forecasting and feature selection tasks are attracting considerable attention from various scientific fields including global solar radiation forecasting, signal processing, microarray data analysis, finance, medicine and others. however, both selection inconsistency and the intractable computational cost pose critical difficulties when implementing forecasting tasks. although artificial neural networks (anns) are useful for forecasting, a large number of nuisance features are employed. to establish an interpretable forecasting model, feature selection techniques are combined with anns to reduce the number of inputs and the complexity of network structures. however, these approaches shrink the estimate, which results in inaccurate forecasting results. to overcome these drawbacks, this paper successfully investigates a novel soft computing approach referred to as a two-stage feature selection procedure using the orthogonal greedy algorithm (tsoga) to select the important features as inputs of anns. a simple-to-implement and efficient computational algorithm is designed, and the theoretical analysis is also provided. furthermore, the high dimensional bayesian information criterion (hdbic) is utilized to select the optimal forecasting model. real data experiments directly demonstrate the outstanding forecasting performances of the proposed tsoga approach compared with other state-of-the-art techniques. © 2017 elsevier b.v."
"the tremendous explosion of data has led to the ""big data challenge"" in the various domains of the current digital age including financial analytics, weather forecasting and bioinformatics. the processing requirements of the voluminous and complex data sets produced by the current data explosion are outpacing the computational capacity of traditional hardware platforms and thus necessitating adoption of high performance computing architectures such as clusters, cloud computing and customisable processing hardware such as field programmable gate arrays. in particular, fpgas offer excellent flexibility, massive parallel computational capacity and good power efficiency which can meet the high processing demands of big data applications. however, despite their excellent processing merits, fpgas are still suffering from low adoption by designers. standard fpga languages and tools are difficult and exclusive to users with digital hardware design expertise. multiple high-level languages and design flows targeted at different application domains have been developed to meet the fpga design challenge. however, there is a lack of a standardised specification that defines clearly how a high-level fpga design flow should be and what it should be capable of. this paper employs a system engineering approach to design and prototype an ideal high-level fpga design toolflow for the computational finance domain which utilises a simple standard software programming language to program the fpga. the detailed specification of the ideal high-level fpga toolflow is presented and discussed. preliminary results between a purely software design in comparison to a hardware design generated using the prototyped high-level fpga toolflow are presented. © 2018 ieee."
"nonlinear chaotic systems yield many interesting features related to different physical phenomena and practical applications. these systems are very sensitive to initial conditions at each time-iteration level in a numerical algorithm. in this article, we study the behavior of some nonlinear chaotic systems by a new numerical approach based on the concept of galerkin–petrov time-discretization formulation. computational algorithms are derived to calculate dynamical behavior of nonlinear chaotic systems. dynamical systems representing weather prediction model and finance model are chosen as test cases for simulation using the derived algorithms. the obtained results are compared with classical rk-4 and rk-5 methods, and an excellent agreement is achieved. the accuracy and convergence of the method are shown by comparing numerically computed results with the exact solution for two test problems derived from another nonlinear dynamical system in two-dimensional space. it is shown that the derived numerical algorithms have a great potential in dealing with the solution of nonlinear chaotic systems and thus can be utilized to delineate different features and characteristics of their solutions. © 2017, springer science+business media b.v., part of springer nature."
"estimation of agent-based models is currently an intense area of research. recent contributions have to a large extent resorted to simulation-based methods mostly using some form of simulated method of moments estimation (smm). there is, however, an entire branch of statistical methods that should appear promising, but has to our knowledge never been applied so far to estimate agent-based models in economics and finance: markov chain monte carlo methods designed for state space models or models with latent variables. this latter class of models seems particularly relevant as agent-based models typically consist of some latent and some observable variables since not all the characteristics of agents would mostly be observable. indeed, one might often not only be interested in estimating the parameters of a model, but also to infer the time development of some latent variable. however, agent-based models when interpreted as latent variable models would be typically characterized by non-linear dynamics and non-gaussian fluctuations and, thus, would require a computational approach to statistical inference. here we resort to sequential monte carlo (smc) estimation based on a particle filter. this approach is used here to numerically approximate the conditional densities that enter into the likelihood function of the problem. with this approximation we simultaneously obtain parameter estimates and filtered state probabilities for the unobservable variable(s) that drive(s) the dynamics of the observable time series. in our examples, the observable series will be asset returns (or prices) while the unobservable variables will be some measure of agents’ aggregate sentiment. we apply smc to two selected agent-based models of speculative dynamics with somewhat different flavor. the empirical application to a selection of financial data includes an explicit comparison of the goodness-of-fit of both models. © 2018 elsevier b.v."
"after a brief review of natural computationalism, this introductory chapter presents a new skeleton of computational economics and finance (cef) along with an overview of the handbook. it begins with a conventional pursuit focusing on the algorithmic or numerical aspect of cef such as computational efforts devoted to rational expectations, (dynamic) general equilibrium, and volatility. it then moves toward an automata- or organism-based perspective of cef, involving nature-inspired intelligence, algorithmic trading, automated markets, network- and agent-based computing, and neural computing. as an alternative way to introduce this novel skeleton, the chapter starts with a view of computation or computing, addressing what computational economics intends to compute and what kinds of economics make computation so hard, and then it turns to a view of computing systems in which the walrasian kind of computational economics is replaced by the wolframian kind due to computational irreducibility. © oxford university press 2018. all rights reserved."
"in the recent years, computational neuroscience which is a study on the brain functions was frequently applied to discover interesting patterns in the investment decisions. emotions in neurofinance study have been measured by sentiments analysis but not measured by biosignal. behavioural finance affects investors‘ performance which is also influenced by their emotional or cognitive errors in taking the investment decisions. this paper focused on the eeg-based emotion recognition recorded while making decisions that can also be helpful in investment’s returns. the features were extracted by using mel frequency cepstal coefficient (mfcc) and the classification used the multi-layer perceptron (mlp) classifier. the eeg-based emotion recognition was tested by using the dimensional models of emotions, 12-pac and rsasm, and also the radboud faces database (rafd). results show that investment decisions can be driven by the emotions of the investor and some measurement should be taken before they lose their money. © 2017 american scientific publishers all rights reserved."
"this special issue on computational and algorithmic finance showcases contemporary developments ranging from advanced numerical methods to machine learning techniques and efficient parallel implementations in finance and insurance. this, in particular, includes: calibration of various asset pricing models (local volatility, stochastic volatility, jumps) to market data; development of new approaches in constructing efficient finite difference and radial basis function methods; study of models and machine learning techniques, like bayesian and neural networks, for asset liability management and limit order books; analysis of bond quote inconsistencies; and also implementation issues on gpu of a monte carlo insurance balance sheet projection. © 2017"
"like almost all fields of science, hydrology has benefited to a large extent from the tremendous improvements in scientific instruments that are able to collect long-time data series and an increase in available computational power and storage capabilities over the last decades. many model applications and statistical analyses (e.g., extreme value analysis) are based on these time series. consequently, the quality and the completeness of these time series are essential. preprocessing of raw data sets by filling data gaps is thus a necessary procedure. several interpolation techniques with different complexity are available ranging from rather simple to extremely challenging approaches. in this paper, various imputation methods available to the hydrological researchers are reviewed with regard to their suitability for filling gaps in the context of solving hydrological questions. the methodological approaches include arithmetic mean imputation, principal component analysis, regression-based methods and multiple imputation methods. in particular, autoregressive conditional heteroscedasticity (arch) models which originate from finance and econometrics will be discussed regarding their applicability to data series characterized by non-constant volatility and heteroscedasticity in hydrological contexts. the review shows that methodological advances driven by other fields of research bear relevance for a more intensive use of these methods in hydrology. up to now, the hydrological community has paid little attention to the imputation ability of time series models in general and arch models in particular. © 2018, springer-verlag gmbh germany, part of springer nature."
"a large number of conventional financial institutions, especially banks are moving to an islamic financial model that's comply with the shari'a law with little change to current conventional practices (reverse eningineer current business processes) to accommodate the market for islamic finance. this study is an attempt to design and develop the business processes for the islamic financial institutions' (ifis) products by investigating and collecting information through islamic literature, surveys and interviews of experts in islamic jurisprudence, regulators, academic and islamic finance and banking practitioners. then the findings will be assessed and evaluated using a qur'anic financial corpus and use computational and analytical approaches to mine the qur'an and the hadith to find hidden knowledge on islamic financial business processes. the knowledge acquired from this investigation will be translated into an islamic financial process model to be adapted by islamic and non-islamic financial institutions. the outcome of this research will influence the future development, growth and diversification of islamic financial services worldwide. © 2017 ieee."
"exascale computing is facing a gap between the ever increasing demand for application performance and the underlying chip technology that does no longer deliver the expected exponential increases in cpu performance. the industry is now progressively moving towards dedicated accelerators to deliver high performance and better energy efficiency. however, the question of programmability still remains. to address this challenge we propose a dedicated high-level accelerator programming and execution model where performance and efficiency are primary targets. our model splits the computation into a conventional cpu-oriented part and a highly efficient fully programmable data flow part. we present a number of systematic transformations and optimisations targeting maxeler dataflow systems that typically yield one to two orders of magnitude improvements in terms of both performance and energy efficiency. these significant gains are enabled by addressing fundamental algorithmic properties and on-demand numerical requirements. this approach is demonstrated by a case study from computational finance. © 2017 ieee."
"the prediction of stock price directions is important in finance. the majority voting ensemble method is superior in prediction accuracy to single classifier models including logistic regression, decision tree, k-nearest neighbors and support vector machine, but the computational cost is very expensive since it considers all the hyperparameters of single classifier models. the current study proposes a revision of the majority voting method to improve the computational efficiency. the proposed method lets each single classifier model find its own hyperparameter values and this modification speeds up the computation by 500 times compared to the standard majority voting method while maintaining the accuracy. the numerical experiments show the ranking of the classifier models in the order of the proposed majority voting, the standard majority voting, and then other single classifier models including the support vector machine. this improvement will allow the majority voting ensemble method to be applied in the financial market in practice. the algorithms are tested on 7 national indices from 3 continents for the past 3 years, and the performance is measured in two criteria, the area under the receiver operating characteristic curve and the percent correctly classified. © 2018, bucharest university of economic studies. all rights reserved."
"appropriately identifying outlier data is a critical requirement in the decision-making process of many expert and intelligent systems deployed in a variety of fields including finance, medicine, and defense. classical outlier detection schemes typically rely on the assumption that normal/background data of interest are distributed according to an assumed statistical model and search for data that deviate from that assumption. however, it is frequently the case that performance is reduced because the underlying distribution does not follow the assumed model. manifold learning techniques offer improved performance by learning better models of the background but can be too computationally expensive due to the need to calculate a distance measure between all data points. here, we study a general framework that allows manifold learning techniques to be used for unsupervised anomaly detection by reducing computational expense via a uniform random sampling of a small fraction of the data. a background manifold is learned from the sample and then an out-of-sample extension is used to project unsampled data into the learned manifold space and construct an anomaly detection statistic based on the prediction error of the learned manifold. the method works well for unsupervised anomaly detection because, by definition, the ratio of anomalous to non-anomalous data points is small and the sampling will be dominated by background points. however, a variety of parameters that affect detection performance are introduced so we use here a low-dimensional toy problem to investigate their effect on the performance of four learning algorithms (kernel pca, two versions of diffusion map, and the parzen density estimator). we then apply the methods to the detection of watercraft in an ensemble of 22 infrared maritime scenes where we find kernel pca to be superior and show that it outperforms a commonly employed baseline algorithm. the framework is not limited to the tested image processing example and can be used for any unsupervised anomaly detection task. © 2017"
"data driven machine learning for predictive modeling problems (classification, regression, or survival analysis) typically involves a number of steps beginning with data preprocessing and ending with performance evaluation. a large number of packages providing tools for the individual steps are available for r, but there is a lack of tools for facilitating rigorous performance evaluation of the complete procedures assembled from them by means of cross-validation, bootstrap, or similar methods. such a tool should strictly prevent test set observations from influencing model training and meta-parameter tuning, so-called information leakage, in order to not produce overly optimistic performance estimates. here we present a new package for r denoted emil (evaluation of modeling without information leakage) that offers this form of performance evaluation. it provides a transparent and highly customizable framework for facilitating the assembly, execution, performance evaluation, and interpretation of complete procedures for classification, regression, and survival analysis. the components of package emil have been designed to be as modular and general as possible to allow users to combine, replace, and extend them if needed. package emil was also developed with scalability in mind and has a small computational overhead, which is a key requirement for analyzing the very big data sets now available in fields like medicine, physics, and finance. first package emil’s functionality and usage is explained. then three specific application examples are presented to show its potential in terms of parallelization, customization for survival analysis, and development of ensemble models. finally a brief comparison to similar software is provided. © 2018, american statistical association. all rights reserved."
"many domains like deep learning, machine learning, and computational finance uses gpu's for decreasing the execution time. gpus are widely used in data centers for high performance computing where virtualization techniques are intended for optimizing the resource utilization (e.g. gpu cloud computing). however, these data centers have some downsides like energy consumption and acquisition cost. © 2017 ieee."
"the pricing of financial derivatives based on stochastic volatility models has been a popular subject in computational finance. although exact or approximate closed form formulas of the prices of many options under stochastic volatility have been obtained so that the option prices can be easily computed, such formulas for exchange options leave much to be desired. in this paper, we consider two different risky assets with two different scales of mean-reversion rate of volatility and use asymptotic analysis to extend the classical margrabe formula, which corresponds to a geometric brownian motion model, and obtain a pricing formula under a stochastic volatility. the resultant formula can be computed easily, simply by taking derivatives of the margrabe price itself. based on the formula, we show how the stochastic volatility corrects the margrabe price behavior depending on the moneyness and the correlation coefficient between the two asset prices. © 2017 elsevier ltd"
"one popular approach to option pricing in ĺevy models is through solving the related partial integro differential equation (pide). for the numerical solution of such equations, powerful galerkin methods have been put forward, e.g., by hilber et al. [computational methods for quantitative finance, springer, 2013]. as in practice, large classes of models are maintained simultaneously, and flexibility in the driving ĺevy model is crucial for the implementation of these powerful tools. in this article we provide a tool that enables the implementation of finite element galerkin methods flexibly in the model. to this end we exploit the fourier representation of the infinitesimal generator, i.e., the related symbol, which is explicitly available for the most relevant ĺevy models. empirical studies for the merton, nig, and cgmy models confirm the numerical feasibility of the tool. © 2018 society for industrial and applied mathematics."
"feedback controllers are introduced to help manage an individual's or household's financial life and build savings. the controllers can be viewed as financial advisors for an individual's resource allocation problem, which is modeled as a nonlinear discrete time stochastic system with income uncertainties, asset losses, and constraints on cash flow and credit. we introduce a model predictive controller (mpc) and a proportional-integral-derivative (pid) controller, and compare them with a benchmark method employed in finance and economics, stochastic dynamic programming (dp). both mpc and pid produce similar consistency in financial management compared with dp. they also offer the advantage of low computational complexity relative to dp, which allows us to efficiently perform assessments of robustness (reliability) and disturbance rejection (e.g., effects of uncertainties), both of which are of significant practical engineering importance. in addition, this flexibility enables us to uncover the system's properties, such as the existence of a 'poverty trap' caused by constraints in the control space and dynamics. the effectiveness of a pid-based aid intervention for low-skilled and low-endowed agents that lie within the trap is assessed and contrasted with other existing cash transfer programs, with results that support a further implementation analysis. these assessments constitute a novel application of feedback controllers that, besides effectively dealing with scarcity constraints, present practical advantages that are shown to translate into an ability to implement the mpc or pid controller in a variety of ways for low-income individuals via computer-assistive methods (e.g., a cell phone). © 1993-2012 ieee."
"the advancement of information technology and research in finance have recently led to flash decision making and actions by computer algorithms in order to respond to fast events occurring in the stock markets. this new area of technology involves the implementation of high-speed trading strategies which have generated significant amount of activity and information for financial research. in this study, we proposed an improved version of a computational intelligence model using genetic algorithms (ga) for the forecasting of stock price at the microscopic level. our empirical results show that the new version of the ga-based system is able to improve the accuracy of the prediction significantly, which shall advance the current state of research for artificial intelligence and relevant financial applications. © 2017 ieee."
"in this paper, we present a novel methodology for stock investment using the technique of high utility episode mining and genetic algorithms. our objective is to devise a profitable episode-based investment model to reveal hidden events that are associated with high utility in the stock market. the time series data of stock price and the derived technical indicators, including moving average, moving average convergence and divergence, random index and bias index, are used for the construction of episode events. we then employ the genetic algorithm for the simultaneous optimization on parameters and selection of subsets of models. the empirical results show that our proposed method significantly outperforms the state-of-the-art methods in terms of annualized returns of investment and precision. we also provide a set of z-tests to statistically validate the effectiveness of our proposed method. based upon the promising results obtained, we expect this novel methodology can advance the research in data mining for computational finance and provide an alternative to stock investment in practice. © 2017 elsevier b.v."
"ordinary differential equations (odes) are equalities involving a function and its derivatives that define the evolution of the function over a prespecified domain. the applications of odes range from simulation and prediction to control and diagnosis in diverse fields such as engineering, physics, medicine, and finance. parameter estimation is often required to calibrate these theoretical models to data. while there are many methods for estimating ode parameters from partially observed data, they are invariably subject to several problems including high computational cost, complex estimation procedures, biased estimates, and large sampling variance. we propose a method that overcomes these issues and produces estimates of the ode parameters that have less bias, a smaller sampling variance, and a 10-fold improvement in computational efficiency. the package genpen containing the matlab code to perform the methods described in this article is available online. © 2017 american statistical association, institute of mathematical statistics, and interface foundation of north america."
"with the rapid growth of computational domains, like bioinformatics finance, engineering, biometrics and neuro-imaging, emphasizes the necessity for analysing high dimensional data. many real world data sets may contain hundreds or thousands of features. a common problem that may occur in many knowledge classification systems is presence of incomplete data samples with missing or unknown attribute values which will downside the quality of classification results. due to the presence of a large segment of missing values in the datasets, refined multiple imputation methods are required to estimate the missing values, so that, a fair and more consistent analysis can be achieved. this study is implemented in horton works sandbox on microsoft azure. three imputation (mi) methods are employed, i.e., imputation by mean, imputation by predictive mean and imputation by additive lasso. results show that imputations by additive lasso is the preferred multiple imputation (mi) method. © medwell journals, 2018."
"understanding the powerful computational and graphics capabilities of microsoft excel is an enormous benefit to engineers and technical professionals in almost any field and at all levels of experience. what every engineer should know about excel is a practical guide to unlocking the features and functions of this program, using examples and screenshots to walk readers through the steps to build a strong understanding of the material. this second edition is updated to reflect the latest version of excel (2016) and expands its scope to include data management, connectivity to external data sources, and integration with ""the cloud"" for optimal use of the excel product. it also introduces the ribbon bar navigation prevalent in microsoft products beginning with the 2007 version of ms office. covering a variety of topics in self-contained chapters, this handy guide will also prove useful for professionals in it, finance, and real estate. © 2018 by taylor & francis group, llc. all rights reserved."
"purpose: this paper aims to demonstrate the importance of a cash flow generating standard for individual financial contract level data and the ability to create such a standard. design/methodology/approach: the authors analyze the importance for such a standard of software that turns natural language contracts into cash flow generating algorithms; a data dictionary that standardizes contract terms; and access to variables that represent the state of the world (e.g. market risk, counterparty risk, etc.) that affect contractual obligations. findings: the ability to realize benefits from the use of such a contract level algorithmic standard depends on the following: making the standard’s software open source; fully testing the software to have complete confidence in its accuracy; and enabling the software to use of a wide range of models of various sources of risk (market, credit and behavior risk) to support forward-looking analysis. such a standard would solve the disconnect that exists in financial firms between the representation of financial contracts for transaction processing and analysis. the actus financial research foundation is building, testing and making available such a standard that represents almost all financial contracts extant in markets. practical implications: the adoption of such a standard would reduce the costs of operations of financial firms, provide the computational infrastructure for more effective regulatory oversight, reduce regulatory reporting costs and improve financial market transparency. it would also enable the assessment of systemic risk by directly quantifying the interconnectedness of firms. originality/value: this is a new approach to financial analytics that clearly separates the deterministic components of finance, which can be standardized from the stochastic elements that cannot be standardized. © 2018, emerald publishing limited."
"there is no theory in strategic management and other related fields for identifying decision problems that cannot be solved by organizations using rational analytical technologies of the type typically taught in mba programs. furthermore, some and perhaps many scholars in strategic management believe that the alternative of heuristics or “rules of thumb” is little more than crude guesses for decision making when compared to rational analytical technologies. this is reflected in a paucity of research in strategic management on heuristics. i propose a theory of “organizational intractability” based roughly on the metaphor provided by “computational intractability” in computer science. i demonstrate organizational intractability for a common model of the joint strategic planning and resource allocation decision problem. this raises the possibility that heuristics are necessary for deciding many important decisions that are intractable for organizations. this possibility parallels the extensive use of heuristics in artificial intelligence for computationally intractable problems, where heuristics are often the most powerful approach possible. some important managerial heuristics are documented from both the finance and strategic management literatures. based on all of this, i discuss some directions for theory of and research on organizational intractability and heuristics in strategic management. © 2016, © the author(s) 2016."
"the repayment policy for multiple loans is about a given set of loans and a monthly incoming cash flow: what is the best way to allocate the monthly income to repay such loans? in this article, we close the almost 20-year-old open question about how to model the repayment policy for multiple loans problem together with its computational complexity. thus, we propose a mixed integer linear programming model that establishes an optimal repayment schedule by minimizing the total amount of cash required to repay the loans. we prove that the most employed repayment strategies, such as the highest interest debt and the debt snowball methods, are not optimal. experimental results on simulated cases based on real data show that our methodology obtains on average more than 4% of savings, that is, the debtor pays approximately 4% less to the bank or loaner, which is a considerable amount in finances. in certain cases, the debtor can save up to 40%. © 2017 rios-solis et al. this is an open access article distributed under the terms of the creative commons attribution license, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited."
"inspired by giles and glasserman [1], algorithmic differentiation (ad) [2,3] has been gaining popularity in computational finance over recent years. adjoint ad (aad) in particular facilitates a paradigm shift in financial modelling through provision of first-order sensitivities at a relative computational cost which is independent of the number of sensitivities asked for. © 2018 by taylor & francis group, llc."
"this chapter surveys the state-of-art of heterogeneous agent models (hams) in finance using a jointly theoretical and empirical analysis, combined with numerical analysis from the latest development in computational finance. it provides supporting evidence on the explanatory power of hams to various stylized facts and market anomalies through model calibration, estimation, and economic mechanisms analysis. it presents hams with the mainstream finance a unified framework in continuous time to study the impact of historical price information on price dynamics, profitability and optimality of fundamental and momentum trading. it demonstrates how hams can help to understand stock price co-movements and evolutionary capm. it also introduces a new hams perspective on house price dynamics and an integrate approach to study dynamics of limit order markets. the survey provides further insights into the complexity and efficiency of financial markets and policy implications. © 2018 elsevier b.v."
"forecasting the term structure of interest rates plays a crucial role in portfolio management, household finance decisions, business investment planning, and policy formulation. this paper aims to address yield curve forecasting and evolving fuzzy systems modelling using data from us and brazilian fixed income markets. evolving fuzzy models provide a high level of system adaptation and learn the system dynamic continuously, which is essential for uncertain environments as interest rate markets. computational experiments show that the evolving fuzzy modelling approaches describe the interest rate behaviour accurately, outperforming traditional econometric techniques in terms of error measures and statistical tests. moreover, evolving models provide promising results for short and long-term maturities and for both fixed income markets evaluated, highlighting its potential to forecast complex nonlinear dynamics in uncertain environments. copyright © 2018 inderscience enterprises ltd."
"in recent years, the interest for the automatic evaluation of the state of civil structures is increased. the development of structural health monitoring is allowed by the low costs of the hardware and the improving of the computational capacity of computers that can analyze considerable amount of data in short time. a structural health monitoring (shm) system should continuously monitor structures, extracting and processing relevant information, to efficiently allocate the resources for maintenance and ensure the security of the structure. considering the latest developments in this field, great attention has been paid to data-based approaches, especially to autoregressive models; these econometric models, born in the field of finance, are usually used to analyze the vibration time series provided by the sensors applied on the monitored structures. indexes based on these autoregressive models can be used as features by which the structural integrity can be assessed. this work proposes the application of a multivariable analysis, principal component analysis (pca), to the set of the autoregressive model parameters estimated on the vibration responses of a real structure under operational conditions. this approach reduces a complex set of data to a lower dimension, by representing the behavior of the structure through the few variables. this work uses the principal components of the autoregressive model parameters as indicators that can effectively describe different operational levels and some important environmental effects. the strategy is applied for the first time on the data collected by the long-time monitoring system installed on the stands of the g. meazza stadium in milan. the results will show that this procedure is effective in representing the status of the structure and can be used in a structural health monitoring prospective. © 2017 elsevier ltd"
"savin et al. [savin, g., p. weller, and j. zvingelis. 2007. ""the predictive power of ""head-and-shoulders"" price patterns in the us stock market."" journal of financial econometrics 5: 243-265.] and lo et al. [lo, a. w., h. mamaysky, and j. wang. 2000. ""foundations of technical analysis: computational algorithms, statistical inference, and empirical implementation."" journal of finance 55: 1705-1765.] analysed the predictive power of head-and-shoulders (hs) patterns in the u.s. stock market. the algorithms in both studies ignored the relative position of the hs pattern in a price trend. in this paper, a filter that removes invalid hs patterns is proposed. it is found that the risk-adjusted excess returns for the hst pattern generally improve through the use of our filter. © 2017 walter de gruyter gmbh, berlin/boston."
"in this article we consider recursive approximations of the smoothing distribution associated to partially observed stochastic differential equations (sdes), which are observed discretely in time. such models appear in a wide variety of applications including econometrics, finance, and engineering. this problem is notoriously challenging, as the smoother is not available analytically and hence requires numerical approximation. this usually consists of applying a time-discretization to the sde, for instance the euler method, and then applying a numerical (e.g., monte carlo) method to approximate the smoother. this has led to a vast literature on methodology for solving such problems, perhaps the most popular of which is based upon the particle filter (pf), e.g., [a. doucet and a. johansen, handbook of nonlinear filtering, oxford university press, 2011]. in the context of filtering for this class of problems, it is well known that the particle filter can be improved upon in terms of cost to achieve a given mean squared error (mse) for estimates. this is in the sense that the computational effort can be reduced to achieve this target mse by using multilevel methods [m. b. giles, oper. res., 56 (2008), pp. 607–617; m. b. giles, acta numer., 24 (2015), pp. 259–328; s. heinrich, in large-scale scientific computing, springer, new york, 2001] via the multilevel particle filter (mlpf) [a. gregory, c. cotter, and s. reich, siam j. sci. comp., 38 (2016), pp. a1317–a1338; a. jasra, k. kamatani, k. j. law, and y. zhou, siam j. numer. anal., 55 (2017), pp. 3068–3096; a. jasra, k. kamatani, p. osei, and y. zhou, stat. comp., 28 (2018), pp. 47–60]. for instance, to obtain a mse of o(2) for some > 0 when approximating filtering distributions associated with euler-discretized diffusions with constant diffusion coefficients, the cost of the pf is o(−3) while the cost of the mlpf is o(−2 log()2). in this article we consider a new approach to replace the pf, using transport methods in [a. spantini, d. bigoni, and y. marzouk, inference via low-dimensional couplings, preprint, arxiv:1703.06131, 2017]. in the context of filtering, one expects that the proposed method improves upon the mlpf by yielding, under assumptions, a mse of o(2) for a cost of o(−2). this is established theoretically in an “ideal” example and numerically in numerous examples. © 2018 society for industrial and applied mathematics."
"we consider cover’s universal portfolio and the problem of risk management in a distribution-free setting when learning from experts. we aim to find optimal portfolios without modelling the financial market at the outset. although it exists, the price distribution of the constituent assets is neither known nor given as part of the input. we consider the portfolio selection problem from the perspective of online algorithms that process input piece-by-piece in a serial fashion. under the minimax regret criterion, we propose two risk-adjusted algorithms that track the expert with the lowest maximum drawdown. we obtain upper bounds on the worst-case performance of our algorithms that equal the bounds obtained by cover (math finance 1(1):1–29, 1991). we also present computational evidence using nyse data over a 22-year period, which shows superior performance of investment strategies that take risk management into account. © 2016, springer science+business media new york."
"the concept of event processing is established as a generic computational paradigm in various application fields, ranging from data processing in web environments, over maritime and transport, to finance and medicine. events report on state changes of a system and its environment. complex event recognition (cer) in turn, refers to the identification of complex/composite events of interest, which are collections of simple events that satisfy some pattern, thereby providing the opportunity for reactive and proactive measures. examples include the recognition of attacks in computer network nodes, human activities on video content, emerging stories and trends on the social web, traffic and transport incidents in smart cities, fraud in electronic marketplaces, cardiac arrhythmias, and epidemic spread. in each scenario, cer allows to make sense of big event data streams and react accordingly. the goal of this tutorial is to provide a step-by-step guide for realizing cer in the big data era. to do so, it elaborates on major challenges and describes algorithmic toolkits for optimized manipulation of event streams characterized by high volume, velocity and/or lack of veracity, placing emphasis on distributed cer over potentially heterogeneous (data variety) event sources. finally, we highlight future research directions in the field. © 2017 vldb."
"open science big data is emerging as an important area of research and software development. although there are several high quality frameworks for big data, additional capabilities are needed for open science big data. these include data provenance, citable reusable data, data sources providing links to research literature, relationships to other data and theories, transparent analysis/reproducibility, data privacy, new optimizations/advanced algorithms, data curation, data storage and transfer. an important part of science is explanation of results, ideally leading to theory formation. in this paper, we examine means for supporting the use of theory in big data analytics as well as using big data to assist in theory formation. one approach is to fit data in a way that is compatible with some theory, existing or new. functional data analysis allows precise fitting of data as well as penalties for lack of smoothness or even departure from theoretical expectations. this paper discusses principal differential analysis and related techniques for fitting data where, for example, a time-based process is governed by an ordinary differential equation. automation in theory formation is also considered. case studies in the fields of computational economics and finance are considered. © 2017 ieee."
"open science big data is emerging as an important area of research and software development. although there are several high quality frameworks for big data, additional capabilities are needed for open science big data. these include data provenance, citable reusable data, data sources providing links to research literature, relationships to other data and theories, transparent analysis/reproducibility, data privacy, new optimizations/advanced algorithms, data curation, data storage and transfer. an important part of science is explanation of results, ideally leading to theory formation. in this paper, we examine means for supporting the use of theory in big data analytics as well as using big data to assist in theory formation. one approach is to fit data in a way that is compatible with some theory, existing or new. functional data analysis allows precise fitting of data as well as penalties for lack of smoothness or even departure from theoretical expectations. this paper discusses principal differential analysis and related techniques for fitting data where, for example, a time-based process is governed by an ordinary differential equation. automation in theory formation is also considered. case studies in the fields of computational economics and finance are considered. © 2017 ieee."
"computational finance is a challenging application domain with ever-increasing performance requirements. driven by the competition between companies, computational finance pushes high performance computing (hpc) technology to its limits. in this paper, we consider asian options which are financial derivatives whose payoff is determined by the average price of their underlying asset at predetermined observation points rather than on the single value at expiration time. due to this path dependency, their pricing is computationally expensive and is therefore a suitable candidate for dataflow acceleration. this paper introduces an application for asian option pricing based on curran's approximation method that exploits a dataflow-oriented development approach, employing dedicated optimisations and replacing conventional floating-point with fixed-point formats wherever possible. the implementation targets a maxeler server-class hpc system consisting of a cpu server node and maxeler dataflow engines encapsulating altera stratix v fpgas. the application has been evaluated on two different data sets and achieves a speed-up of 111x and 278.3x compared to a single-threaded software implementation, and 4x and 9.2x compared to a multi-threaded software implementation running on a dual socket cpu server with 12-core intel xeon e5-2697 v2 cpus with up to 48 hyper-threads in total. © 2017 ieee."
"cointegration is a fundamental principle in technical finance market analysis and refers to the existence of a complex equilibrium in a set of different time series. its stable long-term appearance for a given set of assets enables to predict their prices by evaluating deviations from an expected pattern. a significant part of indicators that are based on multivariate time series systems utilizes this effect to estimate delayed market reactions. thus, the identification of cointegrated assets is a widely discussed topic. this research has the objective to determine cointegrated portfolios with a fixed cardinality by evaluating each combination by canonical decomposition. due to its computational complexity, the introduced method is implemented on gpgpus. the method is evaluated on all assets that constitute the german national index dax during 2014 and 2015. © 2017 ieee."
"the development of offshore oil and gas production systems demands a multi-disciplinary team to investigate a multitude of design uncertainties extending from the reservoir to the production facilities, and commodity product sales. the generation of concept options usually follows a linear and technically oriented process, where each specialized discipline determines the technical requirements and boundary conditions to the next discipline. the process may be repeated as new information becomes available, and some design uncertainties are progressively reduced, as most of key design parameters are stochastic and non-deterministic. this methodology may vary between oil companies regarding the logical sequence, accuracy range and concept coverage, but is strongly driven by the company's culture and by industrial capacity available in each region. due to limitations of time and availability of costly multi-disciplinary specialized resources, the generation of field development concept options is often focused primarily on generating concepts considering the highest technological maturity and the lowest capital expenditure (capex). following this approach, the generation of ranked concept options with a focus on the net present value (npv) is usually performed on a limited workable number of options. the npv check is a project finance tool to assess and rank the concepts generated at the end of the engineering process. further design phase iterations tend towards these same options, resulting in either slow and incremental improvement, or worse through propagation of the initial uncertainties, inadequate facilities design and reduced project reward. on a process digitization perspective, an integrated and automated design computational model enables specialized disciplines to probe and visualize the concept search space, including lower maturity concepts with potentially higher npv reward. automation accelerates the design process, allowing the necessary time to focus efforts either on maturing concepts from an earlier phase, or on minimizing uncertainty in propagation, by reconsidering full concept search space in subsequent design phase iterations, ensuring more adequate facilities design. the objective of this paper is to present the results of an integrated design approach applying a computationally developed model, focusing on automating the process of offshore full field concept generation and ranking during early project phases to fetch optimized designs. based on the use of the computational model, a case study was developed, highlighting the sensitivity analysis of capex, opex (operational expenditures) and npv, by varying specific parameters in typical subsea production system architectures. the hypothetical scenario is representative of the ultra-deepwater environment in the brazilian pre-salt. © 2018, offshore technology conference."
"the cumulative sum (cusum) control chart is widely used in a great variety of practical applications such as finance, medicine, engineering, psychology and in other areas. there are many situations in which the process is serially correlation such as in the manufacturing industry, for example, the dynamics of the process will induce correlations in observations that are closely spaced in time. the average run length (arl) is a traditional measurement of the performance of control chart. in this paper we derive explicit formula for the arl of cusum control chart when observations are seasonal autoregressive and moving average, sarma(1,1)l process with exponential white noise. we use fredholm integral equation approach to derive an explicit formula for the arl and use the gauss-legendre quadrature rule to approximate the numerical integration which both methods based on the banach’s fixed point theorem which is used to guarantee the existence and uniqueness of the solution. finally, we compare numerical results obtained from the explicit formula for the arl of sarma(1,1)l processes with results obtained from a numerical solution of an integral equation for the arl. the results show that the arl from explicit formula is close to the numerical integration with an absolute percentage difference less than 0.1%. in addition, the explicit formula can reduce in the computational time better than the numerical integration. © 2017, thailand statistician. all right reserved."
"in this paper, an analytic approximation method for highly nonlinear equations, namely the homotopy analysis method (ham), is employed to solve some backward stochastic differential equations (bsdes) and forward-backward stochastic differential equations (fbsdes), including one with high dimensionality (up to 12 dimensions). by means of the ham, convergent series solutions can be quickly obtained with high accuracy for a fbsde in a 6-dimensional case, within less than 1 % cpu time used by a currently reported numerical method for the same case [34]. especially, as dimensionality enlarges, the increase of computational complexity for the ham is not as dramatic as this numerical method. all of these demonstrate the validity and high efficiency of the ham for the backward/forward-backward stochastic differential equations in science, engineering, and finance. © 2017, springer science+business media new york."
"we introduce a simulation scheme for brownian semistationary processes, which is based on discretizing the stochastic integral representation of the process in the time domain. we assume that the kernel function of the process is regularly varying at zero. the novel feature of the scheme is to approximate the kernel function by a power function near zero and by a step function elsewhere. the resulting approximation of the process is a combination of wiener integrals of the power function and a riemann sum, which is why we call this method a hybrid scheme. our main theoretical result describes the asymptotics of the mean square error of the hybrid scheme, and we observe that the scheme leads to a substantial improvement of accuracy compared to the ordinary forward riemann-sum scheme, while having the same computational complexity. we exemplify the use of the hybrid scheme by two numerical experiments, where we examine the finite-sample properties of an estimator of the roughness parameter of a brownian semistationary process and study monte carlo option pricing in the rough bergomi model of bayer et al. (quant. finance 16:887–904, 2016), respectively. © 2017, the author(s)."
"we analyse the relationship between the complexity of corporate social responsibility (csr) disclosure and actual csr performance, and postulate a positive association between actual csr performance and readability and the size of csr disclosure documents. using several readability and disclosure size measures from computational linguistics, we test our hypotheses using a cross-sectional sample of stand-alone csr reports issued by large u.s. companies. we find that increased csr disclosure and more readable csr reports are associated with better csr performance. our findings suggest that extending csr disclosure increases transparency regarding firms’ social and environmental performance, while using less-readable language in csr reports increases obfuscation. this study contributes to the disclosure literature by documenting that the complexity indices that have been used as measures of obfuscation in prior finance and accounting research can help shareholders, financial analysts, and investors determine the credibility of csr disclosure. © 2017 the authors"
"sharing and working on sensitive data in distributed settings from healthcare to finance is a major challenge due to security and privacy concerns. secure multiparty computation (smc) is a viable panacea for this, allowing distributed parties to make computations while the parties learn nothing about their data, but the final result. although smc is instrumental in such distributed settings, it does not provide any guarantees not to leak any information about individuals to adversaries. differential privacy (dp) can be utilized to address this; however, achieving smc with dp is not a trivial task, either. in this paper, we propose a novel secure multiparty distributed differentially private (sm-ddp) protocol to achieve secure and private computations in a multiparty environment. specifically, with our protocol, we simultaneously achieve smc and dp in distributed settings focusing on linear regression on horizontally distributed data. that is, parties do not see each others' data and further, can not infer information about individuals from the final constructed statistical model. any statistical model function that allows independent calculation of local statistics can be computed through our protocol. the protocol implements homomorphic encryption for smc and functional mechanism for dp to achieve the desired security and privacy guarantees. in this work, we first introduce the theoretical foundation for the sm-ddp protocol and then evaluate its efficacy and performance on two different datasets. our results show that one can achieve individual-level privacy through the proposed protocol with distributed dp, which is independently applied by each party in a distributed fashion. moreover, our results also show that the sm-ddp protocol incurs minimal computational overhead, is scalable, and provides security and privacy guarantees. © 2017 ieee."
"nonparametric regression has recently become important in quantitative finance due to its distribution-free property. however, this advantage does not come without any cost. as large sample sizes are always required to adequately estimate local structures, nonparametric regression is computationally intensive in real applications. this paper proposes an online method to decrease the computational cost of nonparametric regression for estimating stationary stochastic diffusion models. we establish asymptotic behaviours of the proposed estimators under appropriate conditions. numerical examples and an empirical study of us 3-month treasury bill rates are illustrated. the application to financial risk management is also taken into consideration. © 2016 informa uk limited, trading as taylor & francis group."
"in this chapter, we will argue that there is a parallelism imperative: quants must learn to write effective parallel code in order to take advantage of future computing hardware. we provide a grounding in the basic computer science and hardware considerations needed to explore effective parallelism in more depth. these are, fortunately, far simpler than the typical financial mathematics encountered in computational finance. we spend the bulk of the chapter applying parts of these foundational points in working a detailed example of coding a nontrivial early exercise lmm problem on the gpu, which highlights the key issues of writing highly parallel code. the results clearly highlight the parallelism imperative-quants who leverage parallel execution well can gain a significant advantage over competitors who do not. © 2018 by taylor & francis group, llc."
"fractional diffusion equations (fdes) are a mathematical tool used for describing some special diffusion phenomena arising in many different applications like porous media and computational finance. in this paper, we focus on a two-dimensional space-fde problem discretized by means of a second order finite difference scheme obtained as combination of the crank–nicolson scheme and the so-called weighted and shifted grünwald formula. by fully exploiting the toeplitz-like structure of the resulting linear system, we provide a detailed spectral analysis of the coefficient matrix at each time step, both in the case of constant and variable diffusion coefficients. such a spectral analysis has a very crucial role, since it can be used for designing fast and robust iterative solvers. in particular, we employ the obtained spectral information to define a galerkin multigrid method based on the classical linear interpolation as grid transfer operator and damped-jacobi as smoother, and to prove the linear convergence rate of the corresponding two-grid method. the theoretical analysis suggests that the proposed grid transfer operator is strong enough for working also with the v-cycle method and the geometric multigrid. on this basis, we introduce two computationally favourable variants of the proposed multigrid method and we use them as preconditioners for krylov methods. several numerical results confirm that the resulting preconditioning strategies still keep a linear convergence rate. © 2017 elsevier inc."
"dynamic programming (dp) is a mathematical programming approach for optimizing a system that changes over time and is a common approach for developing intelligent systems. expert systems that are intelligent must be able to adapt dynamically over time. an optimal dp policy identifies the optimal decision dependent on the current state of the system. hence, the decisions controlling the system can intelligently adapt to changing system states. although dp has existed since bellman introduced it in 1957, exact dp policies are only possible for problems with low dimension or under very limiting restrictions. fortunately, advances in computational power have given rise to approximate dp (adp). however, most adp algorithms are still computationally-intractable for high-dimensional problems. this paper specifically considers continuous-state dp problems in which the state variables are multicollinear. the issue of multicollinearity is currently ignored in the adp literature, but in the statistics community it is well known that high multicollinearity leads to unstable (high variance) parameter estimates in statistical modeling. while not all real world dp applications involve high multicollinearity, it is not uncommon for real cases to involve observed state variables that are correlated, such as the air quality ozone pollution application studied in this research. correlation is a common occurrence in observed data, including sources in meteorology, energy, finance, manufacturing, health care, etc. adp algorithms for continuous-state dp achieve an approximate solution through discretization of the state space and model approximations. typical state space discretizations involve full-dimensional grids or random sampling. the former option requires exponential growth in the number of state points as the state space dimension grows, while the latter option is typically inefficient and requires an intractable number of state points. the exception is computationally-tractable adp methods based on a design and analysis of computer experiments (dace) approach. however, the dace approach utilizes ideal experimental designs that are (nearly) orthogonal, and a multicollinear state space will not be appropriately represented by such ideal experimental designs. while one could directly build approximations over the multicollinear state space, the issue of unstable model approximations remains unaddressed. our approach for handling multicollinearity employs data mining methods for two purposes: (1) to reduce the dimensionality of a dp problem and (2) to orthogonalize a multicollinear dp state space and enable the use of a computationally-efficient dace-based adp approach. our results demonstrate the risk of ignoring high multicollinearity, quantified by high variance inflation factors representing model instability. our comparisons using an air quality ozone pollution case study provide guidance on combining feature selection and feature extraction to guarantee orthogonality while achieving over 95% dimension reduction and good model accuracy. © 2017 elsevier ltd"
"the discrete procedures for pricing parisian/parasian options depend, in general, on three dimensions: time, space, time spent over the barrier. here we present some combinatorial and lattice procedures which reduce the computational complexity to second order. in the european case the reduction was already given by lyuu and wu (decisions econ finance 33(1):49–61, 2010) and li and zhao (j deriv 16(4):72–81, 2009), in this paper we present a more efficient procedure in the parisian case and a different approach (again of order 2) in the parasian case. in the american case we present new procedures which decrease the complexity of the pricing problem for the parisian/parasian knock-in options. the reduction of complexity for parisian/parasian knock-out options is still an open problem. © 2017, springer-verlag berlin heidelberg."
"in this study, we compared three types of word assignment to investigate topic coherence and task performance in latent dirichlet allocation (lda). we randomly selected 18,000 news articles consists of the categories of finance, life, and politics with 176,224 unigram, 863,680 compound, and 938,206 mixture in total. the result shows that our unigram-based model has high interpretability and topic coherence which the traditional lda has been blamed for its shortage. the compound and mixture models illustrate more precise and accurate meaning and demonstrate computational efficiency than the unigram based model. this result suggests a well-planned word preprocessing is a crucial factor for supporting the superior topic clustering function of lda. © 2017 ieee."
"it is prevalently known that markov chains have been successfully applied to many fields such as digital communications, queuing theory, and finance. however, when the system becomes massive and complex, the computational load will become too heavy to be handled by conventional approaches. to address this issue, molecular computation, which is inherently parallel, has been considered in this paper to synthesize markov chain. here, a succinct and systematic approach based on reversible unimolecular reactions is proposed for any time-homogeneous markov chain, no matter it is discrete or continuous. for the chemical reaction networks (crns), molecular concentrations at time t reflect the probability distribution of the continuous-time markov chain at time t. the final concentrations indicate the steady state of the markov chain. numerical results based on deterministic mass action kinetics have demonstrated the robustness and accuracy of this method. it is worth noting that an already mathematically-proven conclusion, which states that nearly an arbitrary set of uni-or bimolecular reactions can be implemented by dna strand displacement reactions, ensures the meaningfulness of our work. © 2017 ieee."
"we present *k-means clustering algorithm and source code by expanding statistical clustering methods applied in https://ssrn.com/abstract=2802753 to quantitative finance. *k-means is statistically deterministic without specifying initial centers, etc. we apply *k-means to extracting cancer signatures from genome data without using nonnegative matrix factorization (nmf). *k-means’ computational cost is a fraction of nmf's. using 1389 published samples for 14 cancer types, we find that 3 cancers (liver cancer, lung cancer and renal cell carcinoma) stand out and do not have cluster-like structures. two clusters have especially high within-cluster correlations with 11 other cancers indicating common underlying structures. our approach opens a novel avenue for studying such structures. *k-means is universal and can be applied in other fields. we discuss some potential applications in quantitative finance. © 2017 elsevier gmbh"
"in this paper, we assess the magnitude of model uncertainty of credit risk portfolio models, that is, what is the maximum and minimum value-at-risk (var) of a portfolio of risky loans that can be justified given a certain amount of available information. puccetti and rüschendorf [2012a. “computation of sharp bounds on the distribution of a function of dependent risks”. journal of computational and applied maths 236, 1833–1840] and embrechts, puccetti, and rüschendorf [2013. “model uncertainty and var aggregation”. journal of banking and finance 37, 2750–2764] propose the rearrangement algorithm (ra) as a general method to approximate var bounds when the loss distributions of the different loans are known but not their interdependence (unconstrained bounds). their numerical results show that the gap between worst-case and best-case var is typically very high, a feature that can only be explained by lack of using dependence information. we propose a modification of the ra that makes it possible to approximate sharp var bounds when besides the marginal distributions also higher order moments of the aggregate portfolio such as variance and skewness are available as sources of dependence information. a numerical study shows that the use of moment information makes it possible to significantly improve the (unconstrained) var bounds. however, var assessments of credit portfolios that are performed at high confidence levels (as it is the case in solvency ii and basel iii) remain subject to significant model uncertainty and are not robust. © 2015 informa uk limited, trading as taylor & francis group."
"since giles introduced the multilevel monte carlo path simulation method [18], there has been rapid development of the technique for a variety of applications in computational finance. this paper surveys the progress so far, highlights the key features in achieving a high rate of multilevel variance convergence, and suggests directions for future research. © 2018 by taylor & francis group, llc."
"high-performance computing (hpc) delivers higher computational performance to solve problems in science, engineering and finance. there are various hpc resources available for different needs, ranging from cloud computing- that can be used without much expertise and expense - to more tailored hardware, such as field-programmable gate arrays (fpgas) or d-wave’s quantum computer systems. high-performance computing in finance is the first book that provides a state-of-the-art introduction to hpc for finance, capturing both academically and practically relevant problems. © 2018 by taylor & francis group, llc."
"the proceedings contain 204 papers. the topics discussed include: the holographic electron density theorem, de-quantization, re-quantization, and nuclear charge space extrapolations of the universal molecule model; role of cooperative network interaction in transition region of roaming reactions: non-equilibrium steady state vs. thermal equilibrium reaction scheme; structure and dynamics of gas phase ions: interplay between experiments and computations in irmpd spectroscopy; spin networks and sturmian orbitals: orthogonal complete polynomial sets in molecular quantum mechanics; theoretical determination of the ionization potential and the electron affinity of organic semiconductors; development of density-functional tight-binding repulsive potentials for bulk zirconia using particle swarm optimization algorithm; one electron changes the entire story: nmr versus pnmr; towards scalable and accurate molecular dynamics using the sibfa polarizable force field; computational modeling of carbohydrate recognition in protein complex; an attempt at ab initio crystal orbital calculation of electronic structure of b-type model-dna; the electronic structure of negatively charged fullerenes: from monomers to dimers; enhancement of field-free molecular orientation of ocs by intense laser fields; tailoring plasmonic properties of nanobeam composites by the sliding disorder; fractional time stepping for unsteady engineering calculations on parallel computer systems; stabilization of business cycles of finance agents using nonlinear optimal control; flatness-based adaptive fuzzy control of chaotic finance dynamics; a boundary pde feedback control approach for the stabilization of mortgage price dynamics; and flatness-based control and kalman filtering for a continuous-time macroeconomic model."
"algorithms have become a widespread trope for making sense of social life. science, finance, journalism, warfare, and policing—there is hardly anything these days that has not been specified as “algorithmic.” yet, although the trope has brought together a variety of audiences, it is not quite clear what kind of work it does. often portrayed as powerful yet inscrutable entities, algorithms maintain an air of mystery that makes them both interesting and difficult to understand. this article takes on this problem and examines the role of algorithms not as techno-scientific objects to be known, but as a figure that is used for making sense of observations. following in the footsteps of harold garfinkel’s tutorial cases, i shall illustrate the implications of this view through an experiment with algorithmic navigation. challenging participants to go on a walk, guided not by maps or gps but by an algorithm developed on the spot, i highlight a number of dynamics typical of reasoning with running code, including the ongoing respecification of rules and observations, the stickiness of the procedure, and the selective invocation of the algorithm as an intelligible object. the materials thus provide an opportunity to rethink key issues at the intersection of the social sciences and the computational, including popular concerns with transparency, accountability, and ethics. © the author(s) 2017."
"a pair of variables that tend to rise and fall either together or in opposition are said to be monotonically associated. for certain phenomena, this tendency is causally restricted to a subpopulation, as, e.g., the severity of an allergic reaction trending with the concentration of an air pollutant. previously, yu et al. (stat methodol 2011, 8:97–111) devised a method of rearranging observations to test paired data to see if such an association might be present in a subpopulation. however, the computational intensity of the method limited its application to relatively small samples of data, and the test itself only judges if association is present in some subpopulation; it does not clearly identify the subsample that came from this subpopulation, especially when the whole sample tests positive. the present study adds a ‘top-k’ feature (sampath s, verducci js. stat anal data min 2013, 6:458–471) based on a multistage ranking model, that identifies a concise subsample that is likely to contain a high proportion of observations from the subpopulation in which the association is supported. computational improvements incorporated into this top-k tau-path algorithm now allow the method to be extended to thousands of pairs of variables measured on sample sizes in the thousands. a description of the new algorithm along with measures of computational complexity and practical efficiency help to gauge its potential use in different settings. simulation studies catalog its accuracy in various settings, and an example from finance illustrates its step-by-step use. wires comput stat 2016, 8:206–218. doi: 10.1002/wics.1382. for further resources related to this article, please visit the wires website. © 2016 wiley periodicals, inc."
"uncertainty is present in industrial sectors such as transportation and production logistics, supply chain management, computer and telecommunication networks, or economics and finance. thus, in order to cope with their stochastic components, simulation methods and techniques are frequently employed in the analysis of complex systems related to these sectors. however, simulation is not an optimization tool, so it needs to be combined with optimization methods whenever the goal is to maximize the system performance or to minimize the associated costs. a large number of these real-life optimization problems are n p-hard and large scale in nature, which makes it necessary the use of metaheuristic approaches to solve them in an efficient way. this paper provides an introductory tutorial to simheuristic algorithms, i.e.: the combination of simulation methods with metaheuristics to efficiently deal with stochastic optimization problems. after motivating the need for simheuristics in today’s world and reviewing some related work, a series of key methodological and computational aspects are discussed in detail. then, several examples of application to different industries are provided. the paper also describes open research lines and future trends in this emerging area. © 2017 or society. all rights reserved."
"in interactive data analysis processes, the dialogue between the human and the computer is the enabling mechanism that can lead to actionable observations about the phenomena being investigated. it is of paramount importance that this dialogue is not interrupted by slow computational mechanisms that do not consider any known temporal human-computer interaction characteristics that prioritize the perceptual and cognitive capabilities of the users. in cases where the analysis involves an integrated computational method, for instance to reduce the dimensionality of the data or to perform clustering, such non-optimal processes are often likely. to remedy this, progressive computations, where results are iteratively improved, are getting increasing interest in visual analytics. in this paper, we present techniques and design considerations to incorporate progressive methods within interactive analysis processes that involve high-dimensional data. we define methodologies to facilitate processes that adhere to the perceptual characteristics of users and describe how online algorithms can be incorporated within these. a set of design recommendations and according methods to support analysts in accomplishing high-dimensional data analysis tasks are then presented. our arguments and decisions here are informed by observations gathered over a series of analysis sessions with analysts from finance. we document observations and recommendations from this study and present evidence on how our approach contribute to the efficiency and productivity of interactive visual analysis sessions involving high-dimensional data. © 2016 ieee."
"we introduce pseudonet, a new pseudolikelihood-based estimator of the inverse covariance matrix, that has a number of useful statistical and computational properties. we show, through detailed experiments with synthetic as well as real-world finance and wind power data, that pseudonet outperforms related methods in terms of estimation error and support recovery, making it well-suited for use in a downstream application, where obtaining low estimation error can be important. we also show, under regularity conditions, that pseudonet is consistent. our proof assumes the existence of accurate estimates of the diagonal entries of the underlying inverse covariance matrix; we additionally provide a two-step method to obtain these estimates, even in a high-dimensional setting, going beyond the proofs for related methods. unlike other pseudolikelihood-based methods, we also show that pseudonet does not saturate, i.e., in high dimensions, there is no hard limit on the number of nonzero entries in the pseudonet estimate. we present a fast algorithm as well as screening rules that make computing the pseudonet estimate over a range of tuning parameters tractable. copyright 2017 by the author(s)."
"we present a novel method for extracting cancer signatures by applying statistical risk models (http://ssrn.com/abstract=2732453) from quantitative finance to cancer genome data. using 1389 whole genome sequenced samples from 14 cancers, we identify an “overall” mode of somatic mutational noise. we give a prescription for factoring out this noise and source code for fixing the number of signatures. we apply nonnegative matrix factorization (nmf) to genome data aggregated by cancer subtype and filtered using our method. the resultant signatures have substantially lower variability than those from unfiltered data. also, the computational cost of signature extraction is cut by about a factor of 10. we find 3 novel cancer signatures, including a liver cancer dominant signature (96% contribution) and a renal cell carcinoma signature (70% contribution). our method accelerates finding new cancer signatures and improves their overall stability. reciprocally, the methods for extracting cancer signatures could have interesting applications in quantitative finance. © 2016 elsevier b.v."
"this paper is a short introduction on (big) data science and intelligence for the rda educational corner. its purpose is to motivate a greater discussion of what is big data, how it is transforming the future of finance and what are the essential opportunities and concerns when using big data. ""intelligence"" in big data is used to emphasize that mathematics is an essential part of the algorithmic and the statistical approaches we use when searching, estimating or seeking answers to our problems. when we use the power of it, mathematical and statistical intelligence embedded in numerous applications and studies seek to bridge theoretical constructs and their computational realizations. their integration is a complete system of automatic and learning know how (we may call ai, learning machines or what not and by any other name). it is now expanded by systemic computing, data analytics and management to do much more with a lot less. however, in the long run, doing more without intelligence, replacing intentionality by machine rationality, lead to an evolution where choices are no longer made but instead, are imposed by a data complexity and expert systems that may embed far greater risks than we can expect. in this case, without the power of a human intelligence and a mathematical (objective) rationality, our use of big data without science are similar to seeking to go from one place to another without a map. © 2017 - ios press and the authors. all rights reserved."
"over the last decade, cloud environments have gained significant attention by the scientific community, due to their flexibility in the allocation of resources and the various applications hosted in such environments. recently, high performance computing applications are migrating to cloud environments. efficient methods are sought for solving very large sparse linear systems occurring in various scientific fields such as computational fluid dynamics, n-body simulations and computational finance. herewith, the parallel multi-projection type methods are reviewed and discussions concerning the implementation issues for iaas-type cloud environments are given. moreover, phenomena occurring due to the 'noisy neighbor' problem, varying interconnection speeds as well as load imbalance are studied. furthermore, the level of exposure of specialized hardware residing in modern cpus through the different layers of software is also examined. finally, numerical results concerning the applicability and effectiveness of multi-projection type methods in cloud environments based on openstack are presented. © 2016 ieee."
"this article presents an overview of literature on behavioural and experimental asset pricing theory. we systematically review the evolution and current development of behavioural asset pricing models as an alternate approach to asset pricing in financial economics literature. a review and synthesis of research carried out in behavioural finance spreading across theoretical, empirical and experimental approaches are presented to understand the behavioural dimension of pricing of financial assets. from theoretical perspective, behavioural asset pricing models try to adopt additional behavioural variables into asset pricing process. in terms of empirical investigation perspective, it is documented that econometric and computational advancement takes its biggest place ever in financial literature when compared with the other field. our review underlines the fact that the direction of advancing a methodology is changing from financial literature to economics due to the fact that there is huge account of raw data available to analyze. future research direction should be judging the empirical power of the asset pricing models and their role in practice for incorporating a new dimension to the model. the distinctiveness of the study is that this is the first attempt to review literature written on behavioural asset pricing models in the form of structural empirical review. in doing so, the historical perspective of the concept and the place it will take in future are clarified and the way further researches will be conducted are explored. jel: e03, g02, g12 © 2017 sage publications india pvt. ltd."
"for decades, the streaming architecture of fpgas has delivered accelerated performance across many application domains, such as option pricing solvers in finance, computational fluid dynamics in oil and gas, and packet processing in network routers and firewalls. however, this performance has come at the significant expense of programmability, i.e., the performance-programmability gap. in particular, fpga developers use a hardware design language (hdl) to implement the application data path and to design hardware modules for computation pipelines, memory management, synchronization, and communication. this process requires extensive low-level knowledge of the target fpga architecture and consumes significant development time and effort. to address this lack of programmability of fpgas, opencl provides an easy-to-use and portable programming model for cpus, gpus, apus, and now, fpgas. however, this significantly improved programmability can come at the expense of performance, that is, there still remains a performance-programmability gap. to improve the performance of opencl kernels on fpgas, and thus, bridge the performance-programmability gap, we apply and evaluate the effect of various optimization techniques on gem, an n-body method from the opendwarfs benchmark suite. © 2016 ieee."
"we introduce a pathwise integration for volterra processes driven by lévy noise or martingale noise. these processes are widely used in applications to turbulence, signal processes, biology, and in environmental finance. indeed they constitute a very flexible class of models, which include fractional brownian and lévy motions and it is part of the so-called ambit fields. a pathwise integration with respect of such volterra processes aims at producing a framework where modelling is easily understandable from an information perspective. the techniques used are based on fractional calculus and in this there is a bridging of the stochastic and deterministic techniques. the present paper aims at setting the basis for a framework in which further computational rules can be devised. our results are general in the choice of driving noise. additionally we propose some further details in the relevant context subordinated wiener processes. © 2016 diogenes co., sofia 2016."
"computational finance is a domain where performance is in high demand. in this work, we investigate the suitability of intel xeon phi for computational finance simulations. specifically, we use a scenario based alm (asset liability management) model and propose a novel opencl implementation for xeon phi. to further improve the performance of the application, we apply several optimization techniques (data layout and data locality improvement, loop unrolling) and study their effects. our results show that the optimized opencl code deployed on the phi can run up to 135x faster than the original scalar code running on an intel i7 gpp. furthermore, we also show that choosing the optimal work-item/work-group distribution has a compelling effect on massively parallel and heavily-branching code. overall, these results are significant for the computational finance specialists, as they enable a major increase in model accuracy, because 10x more simulations can be performed in less than a 10th of the original time. © springer international publishing ag 2017."
"the present article solves the classic problem of optimization of investment portfolios, using the model of average-variance and proposing a way to calculate the volatility through the generalized autoregressive conditional heteroskedasticity (garch) models. the problem is solved through a bio-inspired metaheuristic, called artificial bee colony (abc), whose objective is to reduce the computational execution times present in other solutions. the results were counteracted by a previous work, solved with lagrange multipliers, finding a similar investment boundary, but with a notably lower reduction in execution time. finally, reference is made to future work within the area of computer finance. © 2017 universidad icesi. published by elsevier españa, s.l.u. this is an open access article under the cc by license (http://creativecommons.org/licenses/by/4.0/)."
"one of the main concerns in construction projects is finance. proper cash-flow management is necessary to insure that a construction project finishes within time, on budget, and yielding a satisfying profit. poor financial management might put the contractor or the owner in a situation where they are unable to finance the project due to insufficient liquidity, or where they are engaged in excessive loans to finance the project, decreasing the profit, and even creating unsettled debts. engagement with a portfolio of large construction projects, like infrastructure projects, makes attention to finance more critical, due to large budgets and long project durations. moreover, the impact of the time value of money when the project spans over many years must be factored. this paper aims at the analysis and optimization of the cash-flow requirements for large engineering portfolios from the contractor's point of view. a computational model, with a friendly user interface, is developed to achieve that. the user is able to create a portfolio of multiple projects with corresponding activities using different relationship types, lags, constraints. parameters necessary for the remuneration are also considered, which include the down payment percentage, duration between invoices, duration for payment, and retention percentage. the model takes into consideration the time value of money, calculated with an interest rate assigned to the projects by the user, this could be the inflation rate or the minimum attractive rate of return (marr) of the contractor. optimization is performed with the objective of maximizing the net present value (npv) for the projects as a whole, discounted at the start of the portfolio. the variables for the optimization are lags that are assigned for each activity, which, after rescheduling, delays the activities after their early start with the value of those lags, and thus creates a modified cash flow for the project. the model is verified and validated to demonstrate its flexibility and adaptability. © asce."
"this note introduces a set of papers that have been presented to the third international symposium on computational economics and finance (iscef), organized in paris on april 10–12, 2014, focusing on topics in banking and financial markets. this selection of papers emphasizes the role of the development of research in quantitative finance that benefited from the progress in econometric modeling and the availability of high frequency data. these studies carried in the context of the global financial crisis provide different interesting findings enabling to better understand financial market dynamics and banking sectors. we briefly analyze in this note their methodologies and discuss their empirical findings. © 2016"
"deep learning (dl) took artificial intelligence (ai) by storm and has infiltrated into business at an unprecedented rate. access to vast amounts of data extensive computational power and a new wave of efficient learning algorithms, helped artificial neural networks to achieve state-of-the-art results in almost all ai challenges. dl is the cornerstone technology behind products for image recognition and video annotation, voice recognition, personal assistants, automated translation and autonomous vehicles. dl works similarly to the brain by extracting high-level, complex abstractions from data in a hierarchical and discriminative or generative way. the implications of dl supported ai in business is tremendous, shaking to the foundations many industries. in this chapter, ipresent the mostsignificantalgorithms and applications, including natural language processing (nlp), image and video processing and finance. © 2017 by igi global. all rights reserved."
"this study simulates the evolution of artificial economies in order to understand the tax relevance of administrative boundaries in the quality of life of its citizens. the modeling involves the construction of a computational algorithm, which includes citizens, bounded into families; firms and governments; all of them interacting in markets for goods, labor and real estate. the real estate market allows families to move to dwellings with higher quality or lower price when the families capitalize property values. the goods market allows consumers to search on a flexible number of firms choosing by price and proximity. the labor market entails a matching process between firms (given its location) and candidates, according to their qualification. the government may be configured into one, four or seven distinct sub-national governments, which are all economically conurbated. the role of government is to collect taxes on the value added of firms in its territory and invest the taxes into higher levels of quality of life for residents. the results suggest that the configuration of administrative boundaries is relevant to the levels of quality of life arising from the reversal of taxes. the model with seven regions is more dynamic, but more unequal and heterogeneous across regions. the simulation with only one region is more homogeneously poor. the study seeks to contribute to a theoretical and methodological framework as well as to describe, operationalize and test computer models of public finance analysis, with explicitly spatial and dynamic emphasis. several alternatives of expansion of the model for future research are described. moreover, this study adds to the existing literature in the realm of simple microeconomic computational models, specifying structural relationships between local governments and firms, consumers and dwellings mediated by distance. © 2016, university of surrey. all rights reserved."
"this paper develops the procedure of multivariate subordination for a collection of independent markov processes with killing. starting from d independent markov processes xi with killing and an independent d-dimensional time change t, we construct a new process by time, changing each of the markov processes xi with a coordinate ti. when t is a d-dimensional lévy subordinator, the time changed process (yi ≔ xi(ti(t))) is a time-homogeneous markov process with state-dependent jumps and killing in the product of the state spaces of xi. the dependence among jumps of its components is governed by the d-dimensional lévy measure of the subordinator. when t is a d-dimensional additive subordinator, y is a time-inhomogeneous markov process. when ti = ∫t0 visds with vi forming a multivariate markov process, (yi, vi) is a markov process, where each vi plays a role of stochastic volatility of yi. this construction provides a rich modeling architecture for building multivariate models in finance with time- and state-dependent jumps, stochastic volatility, and killing (default). the semigroup theory provides powerful analytical and computational tools for securities pricing in this framework. to illustrate, the paper considers applications to multiname unified credit-equity models and correlated commodity models. © 2014 wiley periodicals, inc."
"this article integrates the company operations decisions (i.e. location, production, inventory, distribution, and transportation) and finance decisions (i.e. cash, accounts payable and receivable, debt, securities, payment delays, and discounts) in which the demands and return rate are uncertain, defined by a set of scenarios. the cash flow and budgeting model will be coupled with supply chain network design using a mixed integer linear programming formulation. the article evaluates two financial criteria, that is, the change in equity and the profit as objective functions. the results indicate that objective functions are partially interdependent, that is, they conflict in certain parts. this fact illustrates the inadequacy of treating process operations and finances in isolated environments and pursuing objective myopic performance indicators such as profit or cost. due to the importance of the supply chain network design problem, a multi-objective robust optimization with the max-min version is extended to cope with the uncertainty. a solution approach integrating benders' decomposition method with the scenario relaxation algorithm is also proposed in this research. the improved algorithm has been applied to solve a number of numerical experiments. all results illustrate significant improvement in computation time of the improved algorithm over existing approaches. for a problem, the proposed algorithm shows a significant reduction in computational time compared with the benders' decomposition and scenario relaxation that shows the efficiency of the proposed solution method. © imeche 2015."
"in this paper, lower-order mathematical thinking skills within finance were studied from the viewpoint of financial employees in the iranian bank of industry and mine. to conduct this research, a questionnaire was developed after reviewing lower-order mathematical thinking skills in finance. in accordance with the revised bloom's taxonomy, the skills considered in the questionnaire were ""remembering mathematics in finance"", ""understanding mathematics in finance"", and ""applying mathematics in finance"". in order to develop the questionnaire, we conducted interviews with employees and scholars, then a suitable sample familiar with mathematics and finance, consisting of 141 bank employees, was studied. descriptive and inferential statistics were used for data analysis. our findings show a hierarchical relationship between the first three mathematical thinking skills in finance, which confirms the revised bloom's taxonomy. in addition, the attitudes of participants were positive concerning the importance of these skills. participants believed that, in order to achieve proper functioning, it is essential to improve the skills of financial employees. the results were analysed with a t-test and anova. this showed that the gender, position, experience, department, degree, and field of study of participants did not affect their attitudes. this research indicates that, for the successful utilisation of skills, it is essential to form an effective relationship between mathematical science and its practical application in the banking world. it is recommended to hold on-the-job training courses for financial employees in banks to empower them to use their computational skills. © 2017 by academic publishing house researcher s.r.o."
"developing health promotion programs that support healthy lifestyle behaviors require comprehensive understanding of mechanisms that drive such complex social systems. policy makers can use models and theories to guide this process at the individuals, groups, and communities levels. individuals can have multiple risky health behaviors including physical inactivity, unhealthy diets, smoking, and alcohol drinking that are often shaped by social and ecological factors. collective understanding of these factors can provide ability to design and evaluate intervention programs that can change unhealthy or risky behaviors over long period of time. however, it is overwhelming task to optimize intervention based on only empirical and/or cross-sectional studies. effective long lasting intervention needs a thorough understanding of the role of social and environmental mechanisms at multiple scales on the dynamics of health behaviors. recent mathematical and computational methods developed in other fields, such as epidemiology and finance, can provide systematic and in-depth understanding of mechanisms. however, the use of such methods in social and behaviors sciences have been limited. in this chapter, some real life working examples of social health behaviors problems are provided which uses some cutting edge methods from dynamical systems and data mining to uncertainty quantification. © 2017 elsevier b.v."
"in recent years, the interest for the automatic evaluation of the state of civil structures is increased. the development of structural health monitoring is allowed by the low costs of the hardware and the increasing of the computational capacity of computers that can analyze considerable amount of data in short time. a structural health monitoring (shm) system should continuously monitor structures, extracting and processing relevant information, in order to efficiently allocate the resources for maintenance and ensure the security of the structure. considering the latest developments in this field, great attention has been paid to data-based approaches, especially to autoregressive models; these econometric models, born in the field of finance, are usually used to analyze the vibration time series provided by the sensors applied on the monitored structures. indexes based on these autoregressive models can be used as features by which the structural integrity can be assessed. this work proposes the application of multivariable analysis, the principal component analysis (pca), to the parameters of autoregressive models estimated on the vibration responses of a real structure under operational conditions. this approach reduces a complex set of data to a lower dimension, by representing the behavior of the structure through the few variables. this work uses the principal components of the autoregressive model parameters as indicators that can effectively describe some important environmental effects. the strategy is applied for the first time on the data collected by the long-time monitoring system installed on the stands of the g. meazza stadium in milan. the results will show that this procedure is effective in representing the status of the structure and can be used in a structural health monitoring prospective. © 2017 the authors. published by elsevier ltd."
"users of heterogeneous computing systems face two problems: first, in understanding the trade-off relationships between the observable characteristics of their applications, such as latency and quality of the result, and second, how to exploit knowledge of these characteristics to allocate work to distributed computing platforms efficiently. a domain specific approach addresses both of these problems. by considering a subset of operations or functions, models of the observable characteristics or domain metrics may be formulated in advance, and populated at run-time for task instances. these metric models can then be used to express the allocation of work as a constrained integer program. these claims are illustrated using the domain of derivatives pricing in computational finance, with the domain metrics of workload latency and pricing accuracy. for a large, varied workload of 128 black-scholes and heston model-based option pricing tasks, running upon a diverse array of 16 multicore cpus, gpus and fpgas platforms, predictions made by models of both the makespan and accuracy are generally within 10 percent of the run-time performance. when these models are used as inputs to machine learning and milp-based workload allocation approaches, a latency improvement of up to 24 and 270 times over the heuristic approach is seen. © 1990-2012 ieee."
copulas are known to provide a flexible tool for analyzing the dependence structure between random events. here we apply the newly introduced notion of evolution of copulas to real data of exchange rates so that we ensure the quality of practically employing our theory. results show that our algorithm provides a prospective handy method in computational finance. © springer international publishing ag 2017.
"computational intelligence methods are widely used in a diverse range of scientific and industrial applications, including economic modeling, finance, networks and transportation, database design, design and control, scheduling and others. on the other hand, collective intelligence methods aim at harvesting the power of people's mobile devices for gathering and sending useful information about the values and the variations of environmental variables in a target area or the macroscopic behavior of a target population. in this paper we propose a computation paradigm, which combines computational, collective intelligence and privacy respecting techniques, towards the creation of a flexible, low-cost, secure, massively parallel virtual computing platform based on people's own devices. examples of how such an infrastructure can be the platform of the deployment of innovative, computationally demanding, applications and services that demand peer-to-peer connections and users' device collaboration are presented. the privacy respecting techniques enhance people's trust in providing their data through their personal devices. © 2016 ieee."
"prediction of stock market is a challenging task that has attracted researchers in various fields including the computational intelligence and finance. since stock market data sets are intrinsically large, nonlinear and time-varying, it is extremely difficult to design models for forecasting the future directions with an acceptable accuracy. in this paper, an integrative and intelligent machine learning framework is proposed through combining cloud computing, machine learning and heuristic optimization. essentially, the support vector machine (svm) method is extended with the grid search (gs) or chemical reaction optimization (cro) as a heuristic optimization method together with principal component analysis (pca) and feature noise filter (fnf) to construct quantitative investment forecasting models for efficient executions on cloud computing platforms. to demonstrate the effectiveness of the proposed framework, the hang seng index and some major stocks listed on the hong kong exchange are predicted using the constructed models on a daily basis. the empirical results clearly indicate that the proposed integrative approach is promising and gives impressive performance in terms of the prediction accuracy. © 2016 ieee."
"as the first volume of world scientific encyclopedia with semantic computing and robotic intelligence, this volume is designed to lay the foundation for the understanding of the semantic computing (sc), as a core concept to study robotic intelligence in the subsequent volumes. this volume aims to provide a reference to the development of semantic computing, in the terms of ""meaning"", ""context"", and ""intention"". it brings together a series of technical notes, in average, no longer than 10 pages in length, each focuses on one topic in semantic computing; being review article or research paper, to explain the fundamental concepts, models or algorithms, and possible applications of the technology concerned. this volume will address three core areas in semantic computing: understanding the (possibly naturally-expressed) intentions (semantics) of users and expressing them in a machine-processable format: semantics description languages, ontology integration, interoperability understanding the meanings (semantics) of computational content (of various sorts, including, but is not limited to, text, video, audio, process, network, software and hardware) and expressing them in a machine-processable format in multimedia, iot, sdn, wearable computing, interfacable with mobile computing, search engines, question answering, web services, to support applications in biomedicine, healthcare, manufacturing, engineering, education, finance, entertainment, business, science and humanity mapping the semantics of the user in context for content retrieval, management, creation in the form of structured data, image and video, audio and speech, big data, natural language, deep learning. © 2018 by world scientific publishing co. pte. ltd. all rights reserved."
"neuro-fuzzy models are being increasingly employed in the domains like weather forecasting, stock market prediction, computational finance, control, planning, physics, economics and management, to name a few. these models enable one to predict system behavior in a more human-like manner than their crisp counterparts. in the present work, an interval type-2 neuro-fuzzy evolutionary subsethood based model has been proposed for its use in finding solutions to some well-known problems reported in the literature such as regression analysis, data mining and research problems relevant to expert and intelligent systems. a novel subsethood based interval type-2 fuzzy inference system, named as interval type-2 subsethood neural fuzzy inference system (it2sunfis) is proposed in the present work. mathematical modeling and empirical studies clearly bring out the efficacy of this model in a wide variety of practical problems such as truck backer-upper control, mackey-glass time-series prediction, narazaki-ralescu and bell function approximation. the simulation results demonstrate intelligent decision making capability of the proposed system based on the available data. the major contribution of this work lies in identifying subsethood as an efficient measure for finding correlation in interval type-2 fuzzy sets and applying this concept to a wide variety of problems pertaining to expert and intelligent systems. subsethood between two type-2 fuzzy sets is different from the commonly used sup-star methods. in the proposed model, this measure assists in providing better contrast between dissimilar objects. this method, coupled with the uncertainty handling capacity of type-2 fuzzy logic system, results in better trainability and improved performance of the system. the integration of subsethood with type-2 fuzzy logic system is a novel idea with several advantages, which is reported for the first time in this paper. © 2016 elsevier ltd. all rights reserved."
"modern computing resources provide substantial processing power but require that the programmatic implementation of used numerical algorithms is fine-tuned to the target cpu architecture. the main performance gains could be obtained minimizing cache traffic and by efficient using of multiple levels of cpu parallel units. monte carlo estimation of libor models is an example of challenging computational finance/computer programming problem. this paper compares the efficiency of multicore (intel core haswell) and manycore platforms (intel xeon phi knight corner and intel xeon phi knight landing) for the calculation european libor-based swaptions. target optimized programmatic implementations of libor calculation using the intel cilk plus and openmp standards are presented and benchmarked. results show that intel xeon phi knight landing evaluate payoff european libor-based swaptions faster, economically profitable and more energy efficient than the intel xeon phi knight corner coprocessor and intel core haswell."
"the following sections are included: positive definite kernels: where do they fit in the mathematical landscape? a historical perspective the fundamental application: scattered data fitting the haar-mairhuber-curtis theorem: why using kernels is a ""natural"" approach variations of scattered data fitting other applications statistical data fitting machine learning numerical solution of pdes computational finance topics we do not cover. © 2016 world scientific publishing co. pte. ltd."
"chiu and zhou [quant. finance, 2011, 11, 115–123] show that the inclusion of a risk-free asset strictly boosts the sharpe ratio in a continuous-time setting, which is in sharp contrast to the static single-period case. in this paper, we extend their work to a discrete-time setting. specifically, we prove that the multi-period mean-variance efficient frontier generated by both risky and risk-free assets is strictly separated from that generated by only risky assets. as a result, we demonstrate that the inclusion of a risk-free asset strictly enhances the best sharpe ratio of the efficient frontier in a multi-period discrete-time setting. furthermore, we offer an explicit expression for the enhancement of the best sharpe ratio, which was referred to as the premium of dynamic trading by chiu and zhou [op. cit.], although they do not present a computational formula for it. our results further show that, in the case with a risk-free asset, if an investor can extract some money from his initial wealth at time 0, the efficient frontier with a risk-free asset can be tangent to that without a risk-free asset. finally, based on real data from the american market, a numerical example is provided to illustrate the results obtained in this paper; a numerical comparison between the discrete-time case and the continuous-time case is also provided. our numerical results reveal that the continuous-time model can be considered to be a limit of the discrete-time model. © 2016 informa uk limited, trading as taylor & francis group."
"we consider the problem where a manager aims to minimize the probability of his portfolio return falling below a threshold while keeping the expected return no worse than a target, under the assumption that stock returns are log-normally distributed. this assumption, common in the finance literature for daily and weekly returns, creates computational difficulties because the distribution of the portfolio return is difficult to estimate precisely. we approximate it with a single log-normal random variable using the fenton–wilkinson method and investigate an iterative, data-driven approximation to the problem. we propose a two-stage solution approach, where the first stage requires solving a classic mean-variance optimization model and the second step involves solving an unconstrained nonlinear problem with a smooth objective function. we suggest an iterative calibration method to improve the accuracy of the method and test its performance against a generalized pareto distribution approximation. we also extend our results to the design of basket options. © 2016, springer-verlag berlin heidelberg."
"cash forecasting is one of the important tasks in the domain of computational finance. a number of tools have been developed by various groups of researchers and are being used by banks or corporate to identify future cash needs. however, due to the high degree of non-linearity of the problem and surrounded by many local optimal solutions, this paper propose a multi-layer locally tuned perceptron (mltp) to forecast the future needs and at the same time reduce the users frustration. it uses a fine tuned mltp to forecast a daily cash demand of an automated teller machine (atm). further, potential indicators are used to making the model robust in terms of its efficiency and accuracy. the accuracy is compared against a traditional time series method. furthermore, it is validated using the past data collected from the sbi atm of bhadrak district of odisha, india. the performance of the method is encouraging. this system can be scaled for all branches of a bank in an area by incorporating historical data from these branches. © 2017 by igi global. all rights reserved."
"dynamic time warping (dtw) is a popular and efficient distance measure used in classification and clustering algorithms applied to time series data. by computing the dtw distance not on raw data but on the time series of the (first, discrete) derivative of the data, we obtain the so-called derivative dynamic time warping (ddtw) distance measure. ddtw, used alone, is usually inefficient, but there exist datasets on which ddtw gives good results, sometimes much better than dtw. to improve the performance of the two distance measures, we can combine them into a new single (parametric) distance function. the literature contains examples of the combining of dtw and ddtw in algorithms for supervised classification of time series data. in this paper, we demonstrate that combination of dtw and ddtw can also be applied in a method of time series clustering (unsupervised classification). in particular, we focus on a hierarchical clustering (with average linkage) of univariate (one-dimensional) time series data. we construct a new parametric distance function, combining dtw and ddtw, where a single real number parameter controls the contribution of each of the two measures to the total value of the combined distances. the parameter is tuned in the initial phase of the clustering algorithm. using this technique in clustering methods requires a different approach (to address certain specific problems) than for supervised methods. in the clustering process we use three internal cluster validation measures (measures which do not use labels) and three external cluster validation measures (measures which do use clustering data labels). internal measures are used to select an optimal value of the parameter of the algorithm, where external measures give information about the overall performance of the new method and enable comparison with other distance functions. computational experiments are performed on a large real-world data base (ucr time series classification archive: 84 datasets) from a very broad range of fields, including medicine, finance, multimedia and engineering. the experimental results demonstrate the effectiveness of the proposed approach for hierarchical clustering of time series data. the method with the new parametric distance function outperforms dtw (and ddtw) on the data base used. the results are confirmed by graphical and statistical comparison. © 2016elsevierltd.allrightsreserved."
"it is worthwhile to investigate abnormal performance of ipos by incorporating investor sentiment. using the method of agent-based computational finance (acf), we analyze the effect from different kinds of investor sentiment on ipos first-day underpricing and long-term performance. the results show that individual investor's sentiment is positively correlated with the ipo's first-day underpricing and its long-run performance. in the long run, along with the rising of individual investor sentiment, ipos' long-term performance will change from underperforming to outperforming. this conclusion provides a more reasonable explanation for the different ipos long-term performance. © authors."
"this paper presents a novel approach for automatic optimisation of reconfigurable design parameters based on knowledge transfer. the key idea is to make use of insights derived from optimising related designs to benefit future optimisations. we show how to use designs targeting one device to speed up optimisation of another device. the proposed approach is evaluated based on various applications including computational finance and seismic imaging. it is capable of achieving up to 35% reduction in optimisation time in producing designs with similar performance, compared to alternative optimisation methods. © 2016 ieee."
"this paper confirms that, as originally reported in seneta (journal of applied probability 41:177–187, 2004, p. 183), it is impossible to replicate madan et al. (european finance review 2:135–156, 1998) results using log daily returns on s&p 500 index from january 1992 to september 1994. this failure leads to a close investigation of the computational problems associated with finding maximum likelihood estimates of the parameters of the popular vg model. both standard econometric software, such as r, low level programming languages, such as matlab®, and non-standard optimization software, such as ezgrad described in tucci (journal of economic dynamics and control 26:1739–1764, 2002), are used. the complexity of the log-likelihood function is studied. it is shown that it looks very complicated, with many local optima, and may be incredibly sensitive to very small changes in the sample used. adding or removing a single observation may cause huge changes both in the maximum of the log-likelihood function and in the estimated parameter values. an intuitive procedure which works nicely both when implemented in r and in matlab® is presented. © 2016, springer science+business media new york."
"nowadays, machine trading contributes significantly to activities in the equity market, and forecasting market movement under high-frequency scenario has become an important topic in finance. a key challenge in high-frequency market forecasting is modeling the dependency structure among stocks and business sectors, with their high dimensionality and the requirement of computational efficiency. as a group of powerful models, neural networks (nns) have been used to capture the complex structure in many studies. however, most existing applications of nns only focus on forecasting with daily or monthly data, not with minute-level data that usually contains more noises. in this article, we propose a novel double-layer neural (dnn) network for high-frequency forecasting, with links specially designed to capture dependence structures among stock returns within different business sectors. various important technical indicators are also included at different layers of the dnn framework. our model framework allows update over time to achieve the best goodness-of-fit with the most recent data. the model performance is tested based on 100 stocks with the largest capitals from the s&p 500. the results show that the proposed framework outperforms benchmark methods in terms of the prediction accuracy and returns. our method will help in financial analysis and trading strategy designs. © 2017 acm."
"financial forecasting is an important area in computational finance. evolutionary dynamic data investment evaluator (eddie) is an established genetic programming (gp) financial forecasting algorithm, which has successfully been applied to a number of international financial datasets. the purpose of this paper is to further improve the algorithm’s predictive performance, by incorporating heuristics in the search. we propose the use of two heuristics: a sequential covering strategy to iteratively build a solution in combination with the gp search and the use of an entropy-based dynamic discretisation procedure of numeric values. to examine the effectiveness of the proposed improvements, we test the new eddie version (eddie 9) across 20 datasets and compare its predictive performance against three previous eddie algorithms. in addition, we also compare our new algorithm’s performance against c4.5 and ripper, two state-of-the-art classification algorithms. results show that the introduction of heuristics is very successful, allowing the algorithm to outperform all previous eddie versions and the well-known c4.5 and ripper algorithms. results also show that the algorithm is able to return significantly high rates of return across the majority of the datasets. © 2015, springer-verlag berlin heidelberg."
the proceedings contain 62 papers. the topics discussed include: re-emphasizing the dimensions and impacts of complementary it resources through governance; factors affecting instructors' adoption of learning management systems: a theoretical framework; a comprehensive knowledge management process framework for healthcare information systems in healthcare industry of pakistan; certificate-based strategy to auction model for e-procurement in indonesia: a review on local ethics and the future challenges; a review for future research and practice in using computer assisted instruction on vocabulary learning among children with autism spectrum disorder; my emergency assistant device: a conceptual solution in enhancing the quality of life for the disabled and elderly; testing sphinx's language model fault-tolerance for the holy quran; tcp skudai: a high performance tcp variant for collaborative virtual environment systems; eliminating unanswered questions from question answering system for khulafaa al-rashidin history; system architecture conceptual for applying malik bennabi's ruler on intellectual property of islamic finance and banking; frameworks for a computational isnad authentication and mechanism development; watermarking in protecting and validating the integrity of digital information: a case study of the holy scripture; predicting obesity from grocery data: a conceptual process framework; and the effect of e-crm features on customers satisfaction for airline e-ticket services in malaysia.
"t+0 trading system, or day trading system, which allows the investors to buy and sell shares in one day, is a universal trading system in international markets. by contrast, t+1 trading system, which is implemented in china's stock market, allows investors to sell shares which are bought today only on the next day. t+1 trading system in spot market is a chinese problem left over by history, and a characteristic in chinese market. stock index futures, the financial derivatives based on the stock, are the products of the development of the capital market at certain stage. for the stock index futures are generated based on the stock, they must be closely related. on that basis, what's the impact of the chinese special t+1 trading system in spot market on the pricing efficiency, market liquidity and market volatility of the stock index futures market? the paper adopted the method based on the agent-based computational finance which's different from the traditional method, and built the agent-based computational cross-market platform which includes both several stocks and stock index futures based on mason. the paper simulated the experiment on the platform to look out the effect on the market quality of the stock index futures market when t+1 trading system in spot market turned into t+0 trading system, consisting mainly of the efficiency of price discovery, market liquidity, market volatility and arbitrage investors' order submission behavior. the results showed that, compared with the t+1 trading system in spot market, t+0 trading system improved the efficiency of price discovery and market liquidity of stock index futures market, did not raise market volatility, enhance the enthusiasm of arbitrage investors' order submission behavior. from the perspective of the stock index futures market, we believe that we should restore the t+0 trading system in stock market timely, so as to active the market and improve the efficiency of market information diffusion, to promote the steady and sound environment of the stock index futures market. © authors."
"graph similarity can contribute to the solutions of a wide variety of real-life problems. effective graph similarity measures, therefore, are in high demand in areas such as communication network, biology, medicine, finance, etc. existing similarity methods present key shortcomings including high run-time computational cost and/or strong dependence on feature selection and feature engineering. many of existing methods also suffer from ambiguity in the interpretation of resultant measurement. in this paper, we propose a fast approximation of graph similarity grounded on convolutional neural network based image embedding. graph similarity (distance) is, therefore, translated into the quantitative comparison of the corresponding images faithfully encoding structural information of the graphs. the proposed method is validated with purposely built test data. in addition, we have also carried out a preliminary evaluation, that has demonstrated highly promising results: confirming the viability of proposed""imagisation"" based graph distance measure. © 2016 acm."
"multi-agent-based simulation for artificial stock market (asm) is an important method in behavioural finance. the social network in asm will influence the coordination and decision making of the intelligent agents. to improve the performance of an asm with evolving social networks in a distributed computing environment, the computational load balancing and inter-nodes communication should be considered jointly. this paper proposes a scheduling algorithm called lbmic to partition the agents onto different computing nodes while keeping the degree of load imbalance lower than a given threshold with minimized inter-nodes communication between agents. lbmic models the scheduling into a graph partitioning problem and uses the multi-level graph partitioning algorithm to achieve an efficient scheduling. when the network evolves, lbmic refines the partitioning by migrating parts of the agents. the experiments indicate that lbmic can efficiently improve the performance of communication-intensive asms by both initial partitioning and refining partitioning. © 2016 operational research society ltd. all rights reserved."
"since the introduction of the modern portfolio theory by markowitz in the journal of finance in 1952, it has been the underlying theory in several portfolio optimization techniques. with the advancement of computers, most portfolio optimization are done by cpus. over the years, there have been papers that introduce various optimization methods including those introduced by markowitz, implemented on the cpus. in the recent years, gpus have taken the front seat as a technology to take computational speeds to new levels. however, very few papers published about portfolio optimization utilize gpus. our paper is about accelerating the open source critical line algorithm for portfolio optimization by using gpu’s, more precicely using nvidia’s gpus and cuda api, for time consuming parts of the algorithm. ï¿½ springer international publishing switzerland 2016."
"the wiener-hopf factorization of a complex function arises in a variety of fields in applied mathematics such as probability, finance, insurance, queuing theory, radio engineering and fluid mechanics. the factorization fully characterizes the distribution of functionals of a random walk or a lévy process, such as the maximum, the minimum and hitting times. here we propose a constructive procedure for the computation of the wiener-hopf factors, valid for both single and double barriers, based on the combined use of the hilbert and the z-transform. the numerical implementation can be simply performed via the fast fourier transform and the euler summation. given that the information in the wiener-hopf factors is strictly related to the distributions of the first passage times, as a concrete application in mathematical finance we consider the pricing of discretely monitored exotic options, such as lookback and barrier options, when the underlying price evolves according to an exponential lévy process. we show that the computational cost of our procedure is independent of the number of monitoring dates and the error decays exponentially with the number of grid points. © 2015 elsevier b. v. all rights reserved."
"given the complexity of over-the-counter derivatives and structured products, almost all derivatives pricing today is based on numerical methods. large financial institutions typically have their own teams of developers who maintain state-of-the-art financial libraries, but until a few years ago, none of that sophistication was available for use in teaching and research. however, for the past decade, quantlib, a reliable c++ open source library, has been available. in this article, the authors introduce quantlib for pricing derivatives and document their experiences using its python extension, quantlib-python, in their computational finance course at the indian institute of management, ahmedabad. the fact that quantlib is available in python makes it possible to harness the power of c++ with the ease of ipython notebooks for use in both the classroom and student projects. © 1999-2011 ieee."
"this paper discusses a novel conceptual formulation of the fractional-order euler-lagrange equation for the fractional-order variational method, which is based on the fractional-order extremum method. in particular, the reverse incremental optimal search of the fractional-order variational method is based on the fractional-order steepest descent approach. fractional calculus has been applied to the solution of a necessary condition for the fractional-order fixed boundary optimization problems in signal processing and image processing mainly because of its inherent strengths in terms of long-term memory, non-locality, and weak singularity. at first, for the convenience of comparison, the first-order euler-lagrange equation for the first-order variational method is derived based on the first-order green formula. second, the fractional-order euler-lagrange equation for the fractional-order variational method is derived based on wiener-khintchine theorem. third, in order to directly and easily achieve the fractional-order variational method in the spatial domain or the time domain, the fractional-order green formula and the fractional-order euler-lagrange equation based on the fractional-order green formula are derived, respectively. fourth, the solution procedure of the fractional-order euler-lagrange equation is derived. finally, a fractional-order inpainting algorithm and a fractional-order denoising algorithm based on the fractional-order variational method are illustrated, respectively. the capability of restoring and maintaining the edges and textural details of the fractional-order image restoration algorithm based on the fractional-order variational method is superior to that of the integer-order image restoration algorithm based on the classical first-order variational method, especially for images rich in textural details. the fractional-order euler-lagrange equation for the fractional-order variational method proposed by this paper is a necessary condition for the fractional-order fixed boundary optimization problems, which is a basic mathematical method in the fractional-order optimization and can be widely applied to the fractional-order field of signal analysis, signal processing, image processing, machine intelligence, automatic control, biomedical engineering, intelligent transportation, computational finance and so on. © 2017 ieee."
"observations for the future our objective in conducting this congress was to explore the challenges and opportunities in harnessing big data for the environment. big data has three primary components: 1) data generated in massive quantities by electronic sensors, 2) computational capacity that permits unprecedented speeds and the manipulation of huge datasets, and 3) technology that provides inexpensive storage of more data than ever before. the combination of these three advances has created a new technological capability of unprecedented power and speed. big data has already seen extensive use in finance, market research, social media, manufacturing, healthcare (records and technology management), math-intensive science, and applications heavily dependent upon binary measurement.most big data used for environmental assessment andmonitoring are collected by satellites that record surface and atmospheric conditions (such as landsat and noaa weather satellites), and noaa's 32,000 data collection stations. satellite images provide inventory and condition data - both current status and trends over time. however, harnessing big data for environmental decision-making presents difficult challenges. big data versus big judgment typical decisions about the use and conservation of natural resources, including land-use and environmentalstandards decisions, must consider social, economic and political factors. as has been the case since people first began debating the value of a duck or open space or how much pollution is permissible, social factors are considered in addition to physical assessment data. however, it is the case that social data are not available as big data, and concerted efforts to apply big data processes to environmental decision-making are nearly non-existent. thus, most environmental decisions will continue to be made through human integration of social and physical data - big judgment. most historical environmental data is big data incompatible in the absence of recently developed big data technology, environmental data that was collected in the past is not big data compatible. during the rnrf congress, the representative from the u.s. geological survey observed that none of thewater research data that has been collected by the agency is big data compatible. amajor impediment to using previously gathered data in big data analyses is the need to integrate disparate datasets. with many scientists in many fields collecting and analyzing data, comprehensive data integration is problematic. data sets must be managed fromthe time of origination so that they can be integrated into larger or more complete datasets. interoperability of datasets requires use of similar units, similar collection methods, and open access. as future data is made interoperable, more groups will be able to contribute to and collaborate on projects through the input of their data. it is likely thatmuch historical environmental data will be not available for use in big-data processes. the science community will need to devote resources to promote interoperability. has big data redefined the value of data? the generation of data by electronicmonitors and sensors has changed the nature of data. historically, data was gathered by investigators for a specific purpose and to answer specific questions. data was considered uniquely valuable among scientists. data is now being generated in torrents by machines. more than 90 percent of existing data has been generated in the past two years. there is so much data that only 0.5% is currently being used and that percentage is destined to drop. the gap between generation and use of data is growing. somuch data is being generated that it cannot be stored. this characteristic of big data will change notions of the value and necessity of storing and maintaining datasets. the science community will need to come to terms with what data should be preserved. defining the public sector - private sector collaboration both public and private sectors collect and utilize big data. potential benefits from public-private partnerships promoting the use of big data for the environment are intuitive, however, the history of such partnerships is relatively brief. delegates at the rnrf congress recognized that there is a need and opportunity for conversations among representatives of the public and private sectors to develop ideas and approaches for advancing big data for the environment. there also was a strong consensus that publicly-financed big data for the environment is a significant public good, and the need for robust advocacy for such data has never been greater. a meeting of the interested parties should be convened."
"over the last five decades, clustering has established itself as a primary unsupervised learning technique. in most major data mining projects clustering can serve as a first step in understanding the available data. clustering is used for creating meaningful profiles of entities in an application. it can also be used to compress the dataset into more manageable granules. the initial methods of crisp clustering objects represented using numeric attributes have evolved to address the demands of the real-world. these extensions include the use of soft computing techniques such as fuzzy and rough set theory, the use of centroids and medoids for computational efficiency, modes to accommodate categorical attributes, dynamic and stream clustering for managing continuous accumulation of data, and meta-clustering for correlating parallel clustering processes. this paper uses applications in engineering, web usage, retail, finance, and social networks to illustrate some of the recent advances in clustering and their role in improved profiling, as well as augmenting prediction, classification, association mining, dimensionality reduction, and optimization tasks. © springer international publishing ag 2016."
"handbook of big data provides a state-of-the-art overview of the analysis of large-scale datasets. featuring contributions from well-known experts in statistics and computer science, this handbook presents a carefully curated collection of techniques from both industry and academia. thus, the text instills a working understanding of key statistical and computing ideas that can be readily applied in research and practice. offering balanced coverage of methodology, theory, and applications, this handbook: describes modern, scalable approaches for analyzing increasingly large datasets defines the underlying concepts of the available analytical tools and techniques details intercommunity advances in computational statistics and machine learning handbook of big data also identifies areas in need of further development, encouraging greater communication and collaboration between researchers in big data sub-specialties such as genomics, computational biology, and finance. © 2016 by taylor & francis group, llc. all rights reserved."
"the merton problem determines the optimal intertemporal portfolio choice by maximizing the ex- pected utility and is the basis of modern portfolio theory in continuous-time finance. however, its empirical performance is disappointing. the estimation errors of the expected rates of returns make the optimal policy degenerate, resulting in an extremely low (or unbounded) expected utility value for a high-dimensional portfolio. we further prove that the estimation error of the variance-covariance matrix leads to the degenerated policy of solely investing in the risk-free asset. this study proposes a constrained 1-minimization approach to resolve the degeneracy in the high-dimensional setting and stabilize the performance in the low-dimensional setting. the proposed scheme can be implemented with simple linear programming and involves negligible additional computational time, compared to standard estimation. we prove the consistency of our framework that our estimate of the optimal control tends to be the true one. we also derive the rate of convergence. simulation studies are provided to verify the finite-sample properties. an empirical study using s&p 500 component stock data demonstrates the superiority of the proposed approach. © 2016 society for industrial and applied mathematics."
"as today's financial products have become more complex, quantitative analysts, financial engineers, and others in the financial industry now require robust techniques for numerical analysis. covering advanced quantitative techniques, computational methods in finance explains how to solve complex functional equations through numerical methods. the first part of the book describes pricing methods for numerous derivatives under a variety of models. the book reviews common processes for modeling assets in different markets. it then examines many computational approaches for pricing derivatives. these include transform techniques, such as the fast fourier transform, the fractional fast fourier transform, the fourier-cosine method, and saddlepoint method; the finite difference method for solving pdes in the diffusion framework and pides in the pure jump framework; and monte carlo simulation. the next part focuses on essential steps in real-world derivative pricing. the author discusses how to calibrate model parameters so that model prices are compatible with market prices. he also covers various filtering techniques and their implementations and gives examples of filtering and parameter estimation. developed from the author's courses at columbia university and the courant institute of new york university, this self-contained text is designed for graduate students in financial engineering and mathematical finance as well as practitioners in the financial industry. it will help readers accurately price a vast array of derivatives. © 2013 by taylor & francis group, llc. all rights reserved."
"due to the scale and complexity of data sets currently being collected in areas such as health, transportation, environmental science, engineering, information technology, business and finance, modern quantitative analysts are seeking improved and appropriate computational and statistical methods to explore, model and draw inferences from big data. this book aims to introduce suitable approaches for such endeavours, providing applications and case studies for the purpose of demonstration. computational and statistical methods for analysing big data with applications starts with an overview of the era of big data. it then goes onto explain the computational and statistical methods which have been commonly applied in the big data revolution. for each of these methods, an example is provided as a guide to its application. five case studies are presented next, focusing on computer vision with massive training data, spatial data analysis, advanced experimental design methods for big data, big data in clinical medicine, and analysing data collected from mobile devices, respectively. the book concludes with some final thoughts and suggested areas for future research in big data. © 2016 elsevier ltd. all rights reserved."
"the global financial crisis, which peaked in late 2008, accentuated the urgent need for reconciliation between the theoretical and practical components of macroeconomic theory. worldwide, apart from a few, economists faced the profession’s dire challenge to develop an ability to think outside conventional macroeconomic equilibrium models. such ability, of course, pre-supposes an open approach to dynamic macroeconomic modeling, one founded in a profound understanding of the diverse currents within the many traditions of economic thought. one of such economists of the highest caliber is professor willi semmler, in whose honor this book is compiled, celebrating his life and grand scientific accomplishments. his work encompasses research on problems of dynamic modeling, computational methods in economics and finance, financial economics, exchange rate, credit risk, and ultimately climate change and the linkages between financial and “real” economic activity. offering a rundown of the book’s structure, this chapter mainly discusses dr. semmler’s achievements across three pillars of academia and science: teaching, research, and mentorship. © springer international publishing switzerland 2016."
"rapidly processing raw data and effectively extracting underlining information from huge volumes of multivariate data become essential to all decision-making processes in sectors like finance, government, medical care, climate analysis, industries, science, etc. remarkably, visualisation is recognised as a fundamental technology that props up human comprehension, cognition and utilisation of burgeoning amounts of heterogeneous data. this paper presents a computational visualisation system, named dataview, which has been developed for graphically displaying and capturing outcomes of multiphysics problem-solvers widely used in engineering fields. the dataview is functionally composed of techniques for table/diagram representation, and graphical illustration of scalar, vector and tensor fields. the field visualisation techniques are implemented on the basis of a range of linear and non-linear meshes, which flexibly adapts to disparate data representation schemas adopted by a variety of disciplinary problem-solvers. the visualisation system has been successfully applied to a number of engineering problems, of which some illustrations are presented to demonstrate effectiveness of the visualisation techniques. © 2014 taylor & francis."
"many important applications require a continuous computation of statistics over data streams. activities monitoring, surveillance and fraud detections are some settings where it is crucial for the monitoring applications to protect user’s sensitive information in addition to efficiently compute the required statistics. in the last two decades, a broad range of techniques for time-series and stream data monitoring has been developed to provide provable privacy guarantees employing the formal notion of differential privacy. although these solutions are well established, they are mostly limited to count based statistics (e.g. number of distinct elements, heavy hitters) and do not apply in settings where more complex statistics are needed. in this paper, we consider a more general problem of estimating the sortedness of a data stream by privately computing the length of the longest increasing subsequence (lis). this important statistic can be used to detect surprising trends in time-series data (e.g. finance) and perform approximate string matching in computational biology domains. our proposed approaches employ the differential privacy notion which provides strong and provable privacy guarantees. our solutions estimate the length of the lis using block decomposition and local approximation techniques. we provide a rigorous analysis to bound the approximation error of our algorithms in terms of privacy level and length of the stream. furthermore, we extend our solutions to computing the length of the lis over sliding windows and we show the beneficial effects of this formulation on the final utility. an extensive experimental evaluation of our proposed solutions on real-world data streams demonstrates the effectiveness of our approaches for computing accurate statistics and detecting surprising trends. © 2016, iiia-csic. all rights reserved."
"larger amounts of variable renewable energy sources bring about larger amounts of uncertainty in the form of forecast errors. when taking operational and planning decisions under uncertainty, a trade-off between risk and costs must be made. today's deterministic operational tools, such as n-1-based methods, cannot directly account for the underlying risk due to uncertainties. instead, several definitions of operating risks, which are probabilistic indicators, have been proposed in the literature. estimating these risks require estimating very low probabilities of violations of operating constraints. crude monte-carlo simulations are very computationally demanding for estimating very low probabilities. in this paper, an importance sampling technique from mathematical finance is adapted to estimate very low operating risks in power systems given probabilistic forecasts for the wind power and the load. case studies in the ieee 39 and 118 bus systems show a decrease in computational demand of two to three orders of magnitude. © 2015 elsevier b.v. all rights reserved."
"in this paper, we propose a handy approximation technique (hat) for obtaining both closed-form and approximate solutions of time-fractional heat and heat-like equations with variable coefficients. the method is relatively recent, proposed via the modification of the classical differential transformation method (dtm). it devises a simple scheme for solving the illustrative examples, and some similar pdes. besides being handy, the results obtained converge faster to their exact forms. this shows that this modified dtm (mdtm) is very efficient and reliable. it involves less computational work, even without given up accuracy. therefore, we strongly recommend it for solving both linear and nonlinear time-fractional partial differential equations (pdes) with applications in other aspects of pure and applied sciences, management, and finance."
"the black-scholes model is one of the most famous and useful models for option valuation as regards option pricing theory. in this paper, we propose a semi-analytical method referred to as he's polynomials for solving the classical black-scholes pricing model with stock as the underlying asset. the proposed method gives the exact solution of the solved problem in a very simple and quick manner even with less computational work while still maintaining high level of accuracy. hence, we recommend an extension and adoption of this method for solving problems arising in other areas of financial engineering, finance, and applied sciences."
"‘@ap: breaking: two explosions in the white house and barack obama is injured’. so read a tweet sent from a hacked associated press twitter account @ap, which affected financial markets, wiping out $136.5 billion of the standard & poor's 500 index's value. while the speed of the associated press hack crash event and the proprietary nature of the algorithms involved make it difficult to make causal claims about the relationship between social media and trading algorithms, we argue that it helps us to critically examine the volatile connections between social media, financial markets, and third parties offering human and algorithmic analysis. by analyzing the commentaries of this event, we highlight two particular currents: one formed by computational processes that mine and analyze twitter data, and the other being financial algorithms that make automated trades and steer the stock market. we build on sociology of finance together with media theory and focus on the work of christian marazzi, gabriel tarde and tony sampson to analyze the relationship between social media and financial markets. we argue that twitter and social media are becoming more powerful forces, not just because they connect people or generate new modes of participation, but because they are connecting human communicative spaces to automated computational spaces in ways that are affectively contagious and highly volatile. © 2015, sage publications. all rights reserved."
"a local meshless radial basis function collocation differential quadrature (lmrbfcdq) is proposed for the numerical solution of a single and multi-asset option pricing pde models arising in computational finance. spatial discretization is performed by both local and a standard global meshless collocation procedures coupled with a set of different time integrators based on the forward euler difference formula (fedf), the fully implicit method (fim), the crank-nicolson method (cnm), the explicit runge-kutta method of order two (erk2), the crank-nicolson runge-kutta method of order two (cnrk2), the fully implicit runge-kutta method of order two (irk2), the runge-kutta method of order four (rk4), the embedded runge-kutta method (rk23). operator splitting techniques like the ordinary operator splitting (oos), the lie-trotter splitting, the additive splitting and the strang splitting are also tested for time integration. the proposed hybrid schemes are the amalgamation of the meshless differential quadrature procedure and the finite difference approximations. different types of radial basis functions (rbfs) i.e. the multiquadric (mq), the inverse quadric (iq) and the gaussian (ga) are utilized for the spatial discretization of the pde models. numerical analysis of a range of computational finance related models are shown to demonstrate accuracy, efficiency and ease of implementation of the proposed meshless-finite difference procedure. © 2016 elsevier ltd. all rights reserved."
"uncertainty is one of the characteristic properties in the area of high-tech engineering and the environment, but also in finance and insurance, as the given data, in both input and output variables, are affected with noise of various kinds, and the scenarios which represent the developments in time, are not deterministic either. since the global environmental and economic crisis has caused the necessity for an essential restructuring of the approach to risk and regulation in these areas, core elements of new global regulatory frameworks for serving the requirements of the real life have to be established in order to make regulatory systems more robust and suitable. the integration of uncertain is a significant issue for the reliability of any model of a highly interconnected system as the presence of noise and data uncertainty raises serious problems to be coped with on the theoretical and computational side. therefore, nowadays, robustification has started to attract more attention with regard to complex interdependencies of global networks and robust optimization (ro) has gained great importance as a modeling framework for immunizing against parametric uncertainties. in this book, robust (conic) multivariate adaptive regression splines (r(c)mars) approach has worked out through ro in terms of polyhedral uncertainty which brings us back to cqp naturally. by conducting a robustification in (c)mars, the estimation variance is aimed to be reduced. data uncertainty of real-world models is also integrated into regulatory systems and they are robustified by applying r(c)mars. in (r)mars and (r)cmars, however, an extra problem has to be solved (by software mars, etc.), namely, the knot selection, which is not needed for the linear model part. therefore, robust (conic) generalized partial linear models (r(c)gplms) are also developed and introduced by using the contributions of both regression models linear model/logistic regression and r(c)mars. as semiparametric models, (c)gplm and r(c)gplm lead to reduce the complexity of (c)mars and r(c)mars in terms of the number of variables used in (c)mars and r(c)mars. © springer international publishing switzerland 2016."
"information systems with the objective to make forecasts for financial time series and negotiate from these are subject to various risks, because the stock market is influenced by different sources continuously. the study of quantitative finance addresses methods for treating problems such as these, a fact which occurs mainly through the use of computational intelligence. this paper presents an automated strategy (investor robot) that combines predictions made by artificial neural networks and econometric predictors in a second neural network, this acts like a ensemble. the predictions are used to generate purchase or sell signals through a negotiation model built into the algorithm. the experiments were conducted with real series of three assets with high liquidity, a commodity and a market index. the financial results are compared against the individual application of each predictor and also the classical market techniques."
"what distinguishes this book from other texts on mathematical finance is the use of both probabilistic and pdes tools to price derivatives for both constant and stochastic volatility models, by which the reader has the advantage of computing explicitly a large number of prices for european, american and asian derivatives. the book presents continuous time models for financial markets, starting from classical models such as black-scholes and evolving towards the most popular models today such as heston and var. a key feature of the textbook is the large number of exercises, mostly solved, which are designed to help the reader to understand the material. the book is based on the author's lectures on topics on computational finance for senior and graduate students, delivered in usa (princeton university and emu), taiwan and kuwait. the prerequisites are an introductory course in stochastic calculus, as well as the usual calculus sequence. the book is addressed to undergraduate and graduate students in masters of finance programs as well as to those who wish to become more efficient in their practical applications. topics covered: interest rates and bonds forward rates and yield curves risk-neutral valuation martingale measures black-scholes analysis american options stochastic volatility models (heston, ar, garch) stochastic return models (var). © 2017 by world scientific publishing co. pte. ltd. all rights reserved."
"the extension of traditional data mining methods to time series has been effectively applied to a wide range of domains such as finance, econometrics, biology, security, and medicine. many existing mining methods deal with the task of change points detection, but very few provide a flexible approach. querying specific change points with linguistic variables is particularly useful in crime analysis, where intuitive, understandable, and appropriate detection of changes can significantly improve the allocation of resources for timely and concise operations. in this paper, we propose an on-line method for detecting and querying change points in crime-related time series with the use of a meaningful representation and a fuzzy inference system. change points detection is based on a shape space representation, and linguistic terms describing geometric properties of the change points are used to express queries, offering the advantage of intuitiveness and flexibility. an empirical evaluation is first conducted on a crime data set to confirm the validity of the proposed method and then on a financial data set to test its general applicability. a comparison to a similar change-point detection algorithm and a sensitivity analysis are also conducted. results show that the method is able to accurately detect change points at very low computational costs. more broadly, the detection of specific change points within time series of virtually any domain is made more intuitive and more understandable, even for experts not related to data mining. © 2015 elsevier b.v. all rights reserved."
"linear computational problems emerge from various fields such as engineering and finance. due to the large scale of these problems, they are often hard to be processed by resource-constrained devices. thus, outsourcing becomes a natural solution. in this paper, we propose a secure outsourcing scheme (linsos) for linear computations. the proposed scheme (linsos) is based on affine mapping and imposes only linear operations at local environment. as a result, the end-users can enjoy impressive computational gains from outsourcing. we also provide a verification scheme such that end-users can always receive valid results. our extensive security and complexity analysis and performance comparison with existing schemes show that linsos is both secure and efficient. © 2016 ieee."
"very long and noisy sequence data arise from biological sciences to social science including high throughput data in genomics and stock prices in econometrics. often such data are collected in order to identify and understand shifts in trends, for example, from a bull market to a bear market in finance or from a normal number of chromosome copies to an excessive number of chromosome copies in genetics. thus, identifying multiple change points in a long, possibly very long, sequence is an important problem. in this article, we review both classical and new multiple change-point detection strategies. considering the long history and the extensive literature on the change-point detection, we provide an in-depth discussion on a normal mean change-point model from aspects of regression analysis, hypothesis testing, consistency and inference. in particular, we present a strategy to gather and aggregate local information for change-point detection that has become the cornerstone of several emerging methods because of its attractiveness in both computational and theoretical properties. © institute of mathematical statistics, 2016."
"measuring the information content of news text is useful for decision makers in their investments since news information can influence the intrinsic values of companies. we propose a model to automatically measure the information content given news text, trained using news and corresponding cumulative abnormal returns of listed companies. existing methods in finance literature exploit sentiment signal features, which are limited by not considering factors such as events. we address this issue by leveraging deep neural models to extract rich semantic features from news text. in particular, a novel tree-structured lstm is used to find target-specific representations of news text given syntax structures. empirical results show that the neural models can outperform sentiment-based models, demonstrating the effectiveness of recent nlp technology advances for computational finance. © 1963-2018 acl."
"at this time optimization has an important role in various fields as well as between other operational research, industry, finance and management. optimization problem is the problem of maximizing or minimizing a function of one variable or many variables, which include unimodal and multimodal functions. differential evolution (de), is a random search technique using vectors as an alternative solution in the search for the optimum. to localize all local maximum and minimum on multimodal function, this function can be divided into several domain of fitness using niching method. species-based niching method is one of method that build sub-populations or species in the domain functions. this paper describes the modification of species-based previously to reduce the computational complexity and run more efficiently. the results of the test functions show species-based modifications able to locate all the local optima in once run the program. © 2015 aip publishing llc."
"the challenges of the current financial environment have revealed the need for a new generation of professionals who combine training in traditional finance disciplines with an understanding of sophisticated quantitative and analytical tools. risk management and simulation shows how simulation modeling and analysis can help you solve risk management problems related to market, credit, operational, business, and strategic risk. simulation models and methodologies offer an effective way to address many of these problems and are easy for finance professionals to understand and use. drawing on the author's extensive teaching experience, this accessible book walks you through the concepts, models, and computational techniques. how simulation models can help you manage risk more effectively. organized into four parts, the book begins with the concepts and framework for risk management. it then introduces the modeling and computational techniques for solving risk management problems, from model development, verification, and validation to designing simulation experiments and conducting appropriate output analysis. the third part of the book delves into specific issues of risk management in a range of risk types. these include market risk, equity risk, interest rate risk, commodity risk, currency risk, credit risk, liquidity risk, and strategic, business, and operational risks. the author also examines insurance as a mechanism for risk management and risk transfer. the final part of the book explores advanced concepts and techniques. the book contains extensive review questions and detailed quantitative or computational exercises in all chapters. use of matlab® mathematical software is encouraged and suggestions for matlab functions are provided throughout. learn step by step, from basic concepts to more complex models. packed with applied examples and exercises. © 2014 by taylor & francis group, llc. all rights reserved."
"the following sections are included: raffles college, the university of malaya, the university of singapore, nanyang university, the national university of singapore, expansion of the science deanery, computer science and computational science, materials science, the srp, usrp and the special programme in science, the zoological reference collection, tmsi and the school of biological sciences, clinical pharmacy, the diamond jubilee celebration, computational finance, crisp and ssls, the growth of the faculty, towards the millennium, 70th anniversary of the faculty and the ims, centre for financial engineering, quantum information and ciba, computational biology and nusnni, cqt and science communication, mechanobiology and environmental studies, the centre for advanced 2d materials and the lee kong chian natural history museum, references. © 2017 by world scientific publishing co. pte. ltd."
"copulas are distribution functions with standard uniform univariate marginals. copulas are widely used for studying dependence among continuously distributed random variables, with applications in finance and quantitative risk management; see, e.g., the pricing of collateralized debt obligations (hofert and scherer, quantitative finance, 11(5), 775–787, 2011). the ability to model complex dependence structures among variables has recently become increasingly popular in the realm of statistics, one example being data mining (e.g., cluster analysis, evolutionary algorithms or classification). the present work considers an estimator for both the structure and the parameters of hierarchical archimedean copulas. such copulas have recently become popular alternatives to the widely used gaussian copulas. the proposed estimator is based on a pairwise inversion of kendall’s tau estimator recently considered in the literature but can be based on other estimators as well, such as likelihood-based. a simple algorithm implementing the proposed estimator is provided. its performance is investigated in several experiments including a comparison to other available estimators. the results show that the proposed estimator can be a suitable alternative in the terms of goodness-of-fit and computational efficiency. additionally, an application of the estimator to copula-based bayesian classification is presented. a set of new archimedean and hierarchical archimedean copula-based bayesian classifiers is compared with other commonly known classifiers in terms of accuracy on several well-known datasets. the results show that the hierarchical archimedean copula-based bayesian classifiers are, despite their limited applicability for high-dimensional data due to expensive time consumption, similar to highly-accurate classifiers like support vector machines or ensemble methods on low-dimensional data in terms of accuracy while keeping the produced models rather comprehensible. © 2014, springer science+business media new york."
"learn to trade algorithmically with your existing brokerage, from data management, to strategy optimization, to order execution, using free and publicly available data. connect to your brokerage’s api, and the source code is plug-and-play. automated trading with r explains automated trading, starting with its mathematics and moving to its computation and execution. you will gain a unique insight into the mechanics and computational considerations taken in building a back-tester, strategy optimizer, and fully functional trading platform. the platform built in this book can serve as a complete replacement for commercially available platforms used by retail traders and small funds. software components are strictly decoupled and easily scalable, providing opportunity to substitute any data source, trading algorithm, or brokerage. this book will: provide a flexible alternative to common strategy automation frameworks, like tradestation, metatrader, and cqg, to small funds and retail traders offer an understanding of the internal mechanisms of an automated trading system standardize discussion and notation of real-world strategy optimization problems what you will learn understand machine-learning criteria for statistical validity in the context of time-series optimize strategies, generate real-time trading decisions, and minimize computation time while programming an automated strategy in r and using its package library best simulate strategy performance in its specific use case to derive accurate performance estimates understand critical real-world variables pertaining to portfolio management and performance assessment, including latency, drawdowns, varying trade size, portfolio growth, and penalization of unused capital who this book is for traders/practitioners at the retail or small fund level with at least an undergraduate background in finance or computer science; graduate level finance or data science students. © 2016 by chris conlan."
"volatility analysis plays a major role in finance and economics. it is the key input for many financial topics including risk management, option and derivative pricing. one pressing computational hurdle in high frequency financial statistics is the tremendous amount of data and the optimization procedures that require computing power beyond the currently available desktop systems. in this article, we focus on the statistical inference problem on large volatility matrix using high-frequency financial data, and propose a regularization approach to achieve lower prediction errors. we also applied a hybrid parallelization solution to carry out efficient computations for high dimensional statistical methods via intra-day high-frequency data. a variety of hardware and software based hpc techniques, including parallel r, intel math kernel library, and automatic offloading to intel xeon phi coprocessor are applied to speed up the statistical computations. our numerical studies are based on high-frequency price data on stocks traded in new york stock exchange in 2013. the analysis results show that the constructed estimator using regularization approach generally achieves higher prediction power while enjoying faster convergence rate. we demonstrate significant performance improvement on statistical inference for high-frequency financial data by combining both software and hardware parallelism. © 2015 ieee."
"computational finance has become one of the emerging application fields of metaheuristic algorithms. in particular, these optimization methods are quickly becoming the solving approach alternative when dealing with realistic versions of financial problems, such as the popular portfolio optimization problem (pop). this paper reviews the scientific literature on the use of metaheuristics for solving rich versions of the pop and illustrates, with a numerical example, the capacity of these methods to provide high-quality solutions to complex pops in short computing times, which might be a desirable property of solving methods that support real-time decision making. © springer international publishing switzerland 2016."
"automatic differentiation (ad) is a practical field of computational mathematics that is of growing interest across many industries, including finance. the use of reverse-mode ad is particularly interesting, since it allows for the computation of gradients in the same time required to evaluate the objective function itself. however, it requires excessive memory. this memory requirement can make reverse-mode ad infeasible in some cases (depending on the function complexity and available ram) and slower than expected in others, due to the use of secondary memory and nonlocalized memory references. however, it turns out that many complex (expensive) functions in finance exhibit a natural substitution structure. in this paper, we illustrate this structure in computational finance as it arises in calibration and inverse problems, and determine greeks in a monte carlo setting. in these cases, the required memory is a small fraction of that required by reverse-mode ad, but the computing time complexity is the same. in fact, our results indicate a significant realized speedup compared with straight reverse-mode ad. © 2016 incisive risk information (ip) limited."
"in recent years, there has been rising interest in a field called behavioral finance, which incorporates psychological methods in analysing investor behavior. the aim of this chapter is to study the technical and the fundamental investing strategy of financial market participants dealing with assets. the motivation of the presented research is to simulate the financial market in the form of agent-based model and to investigate various impacts of risk and transaction costs on its stability. computational social science involves the use of agent based modeling and simulation to study complex social systems. it is related to a variety of other simulation techniques, including discrete event simulation and distributed artificial intelligence or multi-agent systems (mas). in practice, each agent has only partial knowledge of other agents and each agent makes its own decisions based on the partial knowledge about other agents in the system. for purposes of this chapter, a mas will be implemented as a simulation framework in jade development platform. the hypothesis was that transaction costs introduction will stabilize the financial market. the results obtained show that in the case of risk involvement into the system the hypothesis can be fulfilled only partially. © springer international publishing switzerland 2016."
"monte carlo is a simple and flexible tool that is widely used in computational finance. in this context, it is common for the quantity of interest to be the expected value of a random variable defined via a stochastic differential equation. in 2008, giles proposed a remarkable improvement to the approach of discretizing with a numerical method and applying standard monte carlo. his multilevel monte carlo method offers a speed up of (formula presented.) , where ε is the required accuracy. so computations can run 100 times more quickly when two digits of accuracy are required. the ‘multilevel philosophy’ has since been adopted by a range of researchers and a wealth of practically significant results has arisen, most of which have yet to make their way into the expository literature. in this work, we give a brief, accessible, introduction to multilevel monte carlo and summarize recent results applicable to the task of option evaluation. © 2015 taylor & francis."
"financial fraud is an issue with far reaching consequences in the finance industry, government, corporate sectors, and for ordinary consumers. increasing dependence on new technologies such as cloud and mobile computing in recent years has compounded the problem. traditional methods involving manual detection are not only time consuming, expensive and inaccurate, but in the age of big data they are also impractical. not surprisingly, financial institutions have turned to automated processes using statistical and computational methods. this paper presents a comprehensive review of financial fraud detection research using such data mining methods, with a particular focus on computational intelligence (ci)-based techniques. over fifty scientific literature, primarily spanning the period 2004-2014, were analysed in this study; literature that reported empirical studies focussing specifically on ci-based financial fraud detection were considered in particular. research gap was identified as none of the existing review articles addresses the association among fraud types, ci-based detection algorithms and their performance, as reported in the literature. we have presented a comprehensive classification as well as analysis of existing fraud detection literature based on key aspects such as detection algorithm used, fraud type investigated, and performance of the detection methods for specific financial fraud types. some of the key issues and challenges associated with the current practices and potential future direction of research have also been identified. © 2015 elsevier ltd."
"in optimization problems appearing in fields such as economics, finance, or engineering, it is often important that a risk measure of a decision-dependent random variable stays below a prescribed level. at the same time, the underlying probability distribution determining the risk measure's value is typically known only up to a certain degree and the constraint should hold for a reasonably wide class of probability distributions. in addition, the constraint should be computationally tractable. in this paper we review and generalize results on the derivation of tractable counterparts of such constraints for discrete probability distributions. using established techniques in robust optimization, we show that the derivation of a tractable robust counterpart can be split into two parts, one corresponding to the risk measure and the other to the uncertainty set. this holds for a wide range of risk measures and uncertainty sets for probability distributions defined using statistical goodness-of-fit tests or probability metrics. in this way, we provide a unified framework for reformulating this class of constraints, extending the number of solvable risk measureuncertainty set combinations considerably, also including risk measures that are nonlinear in the probabilities. to provide a clear overview for the user, we provide the computational tractability status for each of the uncertainty set-risk measure pairs, some of which have been solved in the literature. examples, including portfolio optimization and antenna array design, illustrate the proposed approach in a theoretical and numerical setting. © 2016 society for industrial and applied mathematics."
"about 80% of the financial market investors fail, the main reason for this being their poor investment decisions. without advanced financial analysis tools and the knowledge to interpret the analysis, the investors can easily make irrational investment decisions. moreover, investors are challenged by the dynamism of the market and a relatively large number of indicators that must be computed. in this paper we propose e-fast, an innovative approach for on-line technical analysis for helping small investors to obtain a greater efficiency on the market by increasing their knowledge. the e-fast technical analysis platform prototype relies on high performance computing (hpc), allowing to rapidly develop and extensively validate the most sophisticated finance analysis algorithms. in this work, we aim at demonstrating that the e-fast implementation, based on the cloudpower hpc infrastructure, is able to provide small investors a realistic, low-cost and secure service that would otherwise be available only to the large financial institutions. we describe the architecture of our system and provide design insights. we present the results obtained with a real service implementation based on the exponential moving average computational method, using cloudpower and grid5000 for the computations’ acceleration. we also elaborate a set of interesting challenges emerging from this work, as next steps towards high performance technical analysis for small investors. © springer international publishing switzerland 2016."
"computational finance using c and c#: derivatives and valuation, second edition provides derivatives pricing information for equity derivatives, interest rate derivatives, foreign exchange derivatives, and credit derivatives. by providing free access to code from a variety of computer languages, such as visual basic/excel, c++, c, and c#, it gives readers stand-alone examples that they can explore before delving into creating their own applications. it is written for readers with backgrounds in basic calculus, linear algebra, and probability. strong on mathematical theory, this second edition helps empower readers to solve their own problems. features new programming problems, examples, and exercises for each chapter. includes freely-accessible source code in languages such as c, c++, vba, c#, and excel. includes a new chapter on the history of finance which also covers the 2008 credit crisis and the use of mortgage backed securities, cdss and cdos. emphasizes mathematical theory. features new programming problems, examples, and exercises with solutions added to each chapter. includes freely-accessible source code in languages such as c, c++, vba, c#, excel. includes a new chapter on the credit crisis of 2008. emphasizes mathematical theory. © 2016, 2008 elsevier ltd. all rights reserved."
"in this paper, an efficient and accurate computational method based on the legendre wavelets (lws) together with the galerkin method is proposed for solving a class of nonlinear stochastic itô–volterra integral equations. for this purpose, a new stochastic operational matrix (som) for lws is derived. a collocation method based on hat functions (hfs) is employed to derive a general procedure for forming this matrix. the lws and their operational matrices of integration and stochastic itô-integration and also some useful properties of these basis functions are used to transform such problems into corresponding nonlinear systems of algebraic equations, which can be simply solved to achieve the solution of such problems. moreover, the efficiency of the proposed method is shown for some concrete examples. the results reveal that the proposed method is very accurate and efficient. furthermore as some useful applications, the proposed method is applied to obtain approximate solutions for some stochastic problems in the mathematics finance, biology, physics and chemistry. © 2016, springer science+business media dordrecht."
"although market feedback on investor sentiment effect has been conceptually identified in the existing finance literature and investment strategies have been designed to explore this effect, there lacks systematic analysis in a quantified manner on such effect. digitization of news articles and the advancement of computational intelligence applications have led to a growing influence of news sentiment over financial markets in recent years. news sentiment has often been used as a proxy for gauging investor sentiment and reflecting the aggregate confidence of the society toward future market. previous studies have primarily focused on elucidating the unidirectional impact of news sentiment on market returns and not vice versa. in this study, we analyze more than 12 millions of news articles and document the presence of a significant feedback effect between news sentiment and market returns across the major indices in the us financial market. more specifically, we find that news sentiment exhibits a lag-5 effect on market returns and conversely market returns elicit consistent lag-1 effects on news sentiment. this aligns well with our intuition that news sentiment drives trading activity and investment decisions. in turn, heightened investment activity further stimulates involuntary responses, which manifest in the form of more news coverage and publications. the evidence presented highlights the strong correlation between news sentiment and market returns and demonstrates the benefits of advancing knowledge in data-driven modeling and its interaction with market movements. © 2016, springer science+business media new york."
the black-scholes equation is applied in pharmaceutical engineering. the black-scholes equation is a classic equation in computational finance. in this work certain case of a modified black-scholes equation is analytically solved in the context of a problem of absorption of a drug by a tissue. the analytical solution is obtained using computer algebra specifically maple. the solution is written as one series of associated laguerre polynomials. in the procedure the kummer m functions are used. the analytical solution is numerically tested and using experimental data is possible to estimate the pharmacological parameters of the tissue. we claim that our analytical solution will have important applications in pharmaceutical engineering. © 2016 spie.
"a succession of data-bases, advanced information processing (aip) and intelligent decision-making technologies (idt) have evolved rapidly over the past five decades. this cumulative evolution of intelligent decision support systems (idsss) has served to stimulate industrial activity and enhanced the lives of most people that the modern world. this publication highlights a series of current contributions to enhance the collective body of knowledge. artificial intelligence (ai) and computational intelligence (ci) techniques continue to be successfully employed to generate human-like decision-making, while simultaneously providing greater access to information to solve data intensive problems. this book documents innovative contributions that represent advances in knowledge-based and intelligent information and engineering systems. new research recognises that society is familiar with modern aip and increasingly expect richer idt systems. today there is a growing reliance on automatically processing information into knowledge with less human input. society has already digitised its past and continues to progressively automate knowledge management for the future and increasingly expect to access this information using mobile devices. this book seeks to inform and enhance the exposure of research on intelligent systems and intelligence technologies that utilize novel and leading-edge techniques to improve decision-making. each chapter concentrates on the theory, design, development, implementation, testing or evaluation of idt techniques or applications. these approaches have the potential to support decision making in the areas of management, international business, finance, accounting, marketing, healthcare, production, networks, traffic management, crisis response, human interfaces and, military applications. all fourteen chapters represent a broad spread of topics across the domain and highlight how research is being realised to benefit society. students, professionals and interested observers within the knowledge-based and intelligent information management domain will benefit from the diversity and richness of the content. © springer international publishing switzerland 2016."
"many infrastructures, such as those of finance and banking, transportation, military and telecommunications, are highly dependent on the internet. however, as the internet's underlying structural protocols and governance can be disturbed by intruders, for its smooth operation, it is important to minimize such disturbances. of the available techniques for achieving this, computational intelligence methodologies, such as evolutionary algorithms and swarm intelligence approaches, are popular and have been successfully applied to detect intrusions. in this paper, we present an overview of these techniques and related literature on intrusion detection, analyze their research contributions, compare their approaches and discuss new research directions which will provide useful insights for intrusion detection researchers and practitioners. © 2015 imperial college press."
"this note provides an overview on recent theoretical and empirical developments in decision-making under uncertainty, monetary policy and financial markets. it introduces in particular a special issue that contains a selection of papers presented at the third international symposium in computational economics and finance (iscef) in april 2014 in paris (www.iscef.com). the papers, both theoretical and empirical, discuss issues that improve our understanding of how computational tools can be used to facilitate our understanding of the agents' behaviors and policies. © 2015."
"generation of pseudorandom numbers from different probability distributions has been studied extensively in the monte carlo simulation literature. two standard generation techniques are the acceptance-rejection and inverse transformation methods. an alternative approach to monte carlo simulation is the quasi-monte carlo method, which uses low-discrepancy sequences, instead of pseudorandom numbers, in simulation. low-discrepancy sequences from different distributions can be obtained by the inverse transformation method, just like for pseudorandom numbers. in this paper, we present an acceptance-rejection algorithm for low-discrepancy sequences. we prove a convergence result, and present error bounds. we then use this acceptance-rejection algorithm to develop quasi-monte carlo versions of some well-known algorithms to generate beta and gamma distributions, and investigate the efficiency of these algorithms numerically. we also consider the simulation of the variance gamma model, a model used in computational finance, where the generation of these probability distributions are needed. our results show that the acceptance-rejection technique can result in significant improvements in computing time over the inverse transformation method in the context of low-discrepancy sequences. © 2016 by de gruyter."
"homotopy type theory has been developed by us for over two decades and is applied to big data to eliminate the inevitable bottleneck of big data implementation originating from inherent combinatorial explosion. an incrementally modular abstraction hierarchy, imah in short, is used in 7 layers starting from homotopy, then type, and ending with presentation. big data is in cyber worlds that are being formed in cyberspaces as computational spaces. now cyberspaces are rapidly expanding on the web either intentionally or spontaneously, with or without design. it is quite different in its emphasis of homotopy type theory as recently reported with emphasis on mathematical proof automation and computer verification. widespread and intensive local activities are melting each other on the web as big data globally to create cyber worlds. the major key players of big data in cyber worlds include e-finance that trades a gdp-equivalent a day and e-manufacturing that is transforming industrial production into web shopping of product components and assembly factories. lacking proper theory and design, big data has continued to grow chaotic and are now out of human understanding and control. this research first presents a generic theoretical framework as an incrementally modular abstraction hierarchy, based on homotopy type theory, provides an axiomatic approach to theorize the potentials of big data in cyber worlds, and shows that the incrementally modular abstraction hierarchy automate big data application development and eliminates the need for design verification and validation. it also makes the systems developed secure from the all sort of attacks. © 2015 ieee."
"the modified craig-sneyd scheme is an alternating direction implicit(adi) type scheme that was introduced by in 't hout and welfert (2009) [12] in order to numerically solve multidimensional convection-diffusion equations with mixed-derivative terms. it is one of the most prominent adi schemes currently known for their efficiency in solving above type of problems. this paper deals with a useful stability result for the modified craig-sneyd scheme when applied to two-dimensional convection-diffusion equations with mixed derivative term. the stability of the scheme is analyzed in the von neumann framework, effectively taking into account the actual size of the mixed derivative term. this study is relevant to an observation of apparent discrepancy in a real world application of the scheme, i.e., in computational finance. the obtained results not only generalize some of the existing stability results, but also clearly justify this surprising observation theoretically. © 2016 elsevier inc. all rights reserved."
"with the rapid growth of usage of social network, the patterns, the scales, and the rate of information exchange have brought profound impacts on research and practice in finance. one important topic is the stock market efficiency analysis. traditional schemes in finance focus on identifying significant abnormal returns triggered by important events. however, those events are merely identified by regular financial announcements such as mergers, equity issuances, and financial reports. related data-driven approaches mainly focus on developing trading strategies using social media data, while the results are usually lack of theoretical explanations. in this paper, we fill the gap between the usage of social media data and financial theories. we propose a degree of social attention (dsa) framework for stock analysis based on influence propagation model. specifically, we define the self-influence for users in a social network and the dsa for stocks. a recursive process is also designed for dynamic value updating. furthermore, we provide two modified approaches to reduce the computational cost. our testing results from the chinese stock market suggest that the proposed framework effectively captures stock abnormal returns based on the related social media data, and dsa is verified to be a key factor to link social media activities to the stock market. © 2015 ieee."
"prediction of earnings per share (eps) is the fundamental problem in finance industry. various data mining technologies have been widely used in computational finance. this research work aims to predict the future eps with previous values through the use of data mining technologies, thus to provide decision makers a reference or evidence for their economic strategies and business activity. we created three models lr, rbf and mlp for the regression problem. our experiments with these models were carried out on the real datasets provided by a software company. the performance assessment was based on correlation coefficient and root mean squared error. these algorithms were validated with the data of six different companies. some differences between the models have been observed. in most cases, linear regression and multilayer perceptron are effectively capable of predicting the future eps. but for the high nonlinear data, mlp gives better performance. © 2015 by scitepress - science and technology publications, lda."
"the availability of data on digital traces is growing to unprecedented sizes, but inferring actionable knowledge from large-scale data is far from being trivial. this is especially important for computational finance, where digital traces of human behaviour offer a great potential to drive trading strategies. we contribute to this by providing a consistent approach that integrates various datasources in the design of algorithmic traders. this allows us to derive insights into the principles behind the profitability of our trading strategies. we illustrate our approach through the analysis of bitcoin, a cryptocurrency known for its large price fluctuations. in our analysis, we include economic signals of volume and price of exchange for usd, adoption of the bitcoin technology and transaction volume of bitcoin. we add social signals related to information search, word of mouth volume, emotional valence and opinion polarization as expressed in tweets related to bitcoin for more than 3 years. our analysis reveals that increases in opinion polarization and exchange volume precede rising bitcoin prices, and that emotional valence precedes opinion polarization and rising exchange volumes. we apply these insights to design algorithmic trading strategies for bitcoin, reaching very high profits in less than a year. we verify this high profitability with robust statistical methods that take into account risk and trading costs, confirming the long-standing hypothesis that trading-based social media sentiment has the potential to yield positive returns on investment. © 2015 the authors."
"an artificial neural network (ann) is an information processing technique that is inspired by biological nervous systems. this technique tries to simulate its learning process and uses a mathematical or computational model to process information. artificial neural networks (anns) are utilized in many scientific disciplines extensively. there are many problems in mathematics, medicine, business and finance can be solved by using anns. the application areas of anns are classification, clustering, forecast, function approximation, optimization etc. the prediction of the motion responses for marine systems is computationally very expensive and highly time-consuming. several methods such as anns can be used in order to eliminate high computational costs for these systems. this paper employs anns to optimize multiple point mooring systems (spread mooring systems). these systems are often affected from harsh environmental conditions at open seas. to reduce effects of environmental conditions, these systems are supported by mooring lines. in this study, at first orcaflex software is employed to model and simulate spread mooring systems. 4 points mooring system is designed in orcaflex to calculate mooring tensions and tanker motion displacements. these simulation results are then used to train ann structure and an algorithm is created. after creating algorithm, the results of training, test and validation are obtained from ann. utilization of the ann tool can provide physical results and can process information in extremely rapid mode promising high accuracy of prediction. © 2015 taylor and francis group."
"this paper discusses the implementation of the trading competition held at the 2014 ieee computational intelligence in financial engineering conference (cifer 2014). participants in the competition were asked to hedge a simulated portfolio of assets, worth approximately $54 million. the winner was the individual whose portfolio most closely generated a 1% annualized return based on daily tracking. the goal of the competition was to provide participants with the opportunity to learn portfolio management and hedging skill. self-assessments indicate that contestants improved their portfolio management skills and enjoyed their experience. this paper discusses methods used to generate the simulated stock and option prices and to construct the trading platform. all of the software used in the competition is being made open source in the hope that students, professors, and practitioners improve on the idea of the competition, thereby facilitating project-based learning for the future practitioners of economics, finance, and financial engineering. © 2014 ieee."
"we study an optimal high frequency trading problem within a market microstructure model designed to be a good compromise between accuracy and tractability. the stock price is driven by a markov renewal process (mrp), as described in [p. fodra and h. pham, appl. math. finance, to appear], while market orders arrive in the limit order book via a point process correlated with the stock price itself. in this framework, we can reproduce the adverse selection risk, appearing in two different forms: the usual form due to big market orders impacting the stock price and penalizing the agent, and the weak form due to small market orders and reducing the probability of a profitable execution. we solve the market making problem by stochastic control techniques in this semi-markov model. in the no risk-aversion case, we provide an explicit formula for the optimal controls and characterize the value function as a simple linear pde. in the general case, we derive the optimal controls and the value function in terms of the previous result and illustrate how the risk aversion influences the trader strategy and her expected gain. finally, by using a perturbation method, approximate optimal controls for small risk aversions are explicitly computed in terms of two simple pdes, drastically reducing the computational cost and enlightening the financial interpretation of the results. © 2015 society for industrial and applied mathematics."
"aircraft conceptual design is a challenging task that requires not only the understanding of many different disciplines, but also how they interact with each other, leading to many trade-off analyses. the complexity of these interactions grows fast with the number of variables, disciplines and goals of the problem. multidisciplinary analysis and optimization tools can be very helpful to explore the design space, but it is up to the engineering team to define the objective function: optimal with respect to what? for commercial aircraft segment there are two well defined objectives that mostly represent what costumers of this segment desire: minimization of block fuel or minimization of direct operating costs. the objective function is not so clear for the executive jets segment and it is usual to apply minimization of mtow. this paper proposes an architecture that includes costs, market share and finances disciplines in the optimization loop, treating the market specifications provided by company's intelligence as constraints and maximizes the financial return to the shareholders. this way, the conceived solution complies with all customer needs and also provides the most attractive investment to the shareholders. a comparison is made with the traditional optimization strategy and the results shows that, although both strategies are not exactly conflicting objectives, to maximize the financial return can lead to a different design with significant improvement in financial return. another key optimization issue treated in this work is the reliability and robustness of the design. estimation methods have inherent model uncertainties that can not be mitigated even with in-house data calibration. uncertainty quantification and robust design is held in this work by the use of monte carlo simulations using triangular distributions and superposition of effects. results for the robust optimization showed that slight changes in the design can improve robustness in the outcomes of interest. also, the proposed methodology reduces the computational cost of the robust design to almost the level of a deterministic design, presenting a significant improvement to this process. © 2015, american institute of aeronautics and astronautics inc."
"compared with conventional weighted voting methods, class-specific soft voting (cssv) system has several advantages. on one hand, it not only deals with the soft class probability outputs but also refines the weights from classifiers to classes. on the other hand, the class-specific weights can be used to improve the combinative performance without increasing much computational load. this paper proposes two weight optimization based ensemble methods (cssv-elm and spacssv-elm) under the framework of cssv scheme for multiple extreme learning machines (elms). the designed two models are in terms of accuracy and sparsity aspects, respectively. firstly, cssv-elm takes advantage of the condition number of matrix, which reveals the stability of linear equation, to determine the weights of base elm classifiers. this model can reduce the unreliability induced by randomly input parameters of a single elm, and solve the ill-conditioned problem caused by linear system structure of elm simultaneously. secondly, sparse ensemble methods can lower memory requirement and speed up the classification process, but only for classifier-specific weight level. therefore, a spacssv-elm method is proposed by transforming the weight optimization problem to a sparse coding problem, which uses the sparse representation technique for maintaining classification performance with less nonzero weight coefficients. experiments are carried out on twenty uci data sets and finance event series data and the experimental results show the superior performance of the cssv based elm algorithms by comparing with the state-of-the-art algorithms. © 2014 elsevier b.v."
"praise for the first edition ""a nice, self-contained introduction to simulation and computational techniques in finance"" - mathematical reviews. simulation techniques in financial risk management, second edition takes a unique approach to the field of simulations by focusing on techniques necessary in the fields of finance and risk management. thoroughly updated, the new edition expands on several key topics in these areas and presents many of the recent innovations in simulations and risk management, such as advanced option pricing models beyond the black-scholes paradigm, interest rate models, mcmc methods including stochastic volatility models simulations, model assets and model-free properties, jump diffusion, and state space modeling. the second edition also features: updates to primary software used throughout the book, microsoft office® excel® vba. new topical coverage on multiple assets, model-free properties, and related models. more than 300 exercises at the end of each chapter, with select answers in the appendix, to help readers apply new concepts and test their understanding. extensive use of examples to illustrate how to use simulation techniques in risk management. practical case studies, such as the pricing of exotic options; simulations of greeks in hedging; and the use of bayesian ideas to assess the impact of jumps, so readers can reproduce the results of the studies. a related website with additional solutions to problems within the book as well as excel vba and s-plus computer code for many of the examples within the book. simulation techniques in financial risk management, second edition is an invaluable resource for risk managers in the financial and actuarial industries as well as a useful reference for readers interested in learning how to better gauge risk and make more informed decisions. the book is also ideal for upper-undergraduate and graduate-level courses in simulation and risk management. © 2015 by john wiley & sons, inc. all rights reserved."
"in computational finance, high dimensional problems typically arise when pricing basket options, foreign-exchange (fx) options, etc. since the number of grid points grows exponentially with the dimension, the so called curse of dimensionality shows its effect very quickly. sparse grids and the combination technique have proven their great ability to reduce the computational effort. in this article, we introduce a fourth order scheme for the combination technique to solve efficiently high dimensional partial differential equation problems. in order to linearly combine the sub-solutions, we propose a tensor-based interpolation method. we show that our approach can preserve the error splitting structure of the sub-solutions and lead to a highly accurate sparse grid solution. © 2015 univerzita komenskeho. all rights reserved."
"people vary widely in their temporal orientation-how often they emphasize the past, present, and future-and this affects their finances, health, and happiness. traditionally, temporal orientation has been assessed by self-report questionnaires. in this paper, we develop a novel behavior-based assessment using human language on facebook. we first create a past, present, and future message classifier, engineering features and evaluating a variety of classification techniques. our message classifier achieves an accuracy of 71.8%, compared with 52.8% from the most frequent class and 58.6% from a model based entirely on time expression features. we quantify a users' overall temporal orientation based on their distribution of messages and validate it against known human correlates: conscientiousness, age, and gender. we then explore social scientific questions, finding novel associations with the factors openness to experience, satisfaction with life, depression, iq, and one's number of friends. further, demonstrating how one can track orientation over time, we find differences in future orientation around birthdays. © 2015 association for computational linguistics."
"the proceedings contain 70 papers. the topics discussed include: automating customized computing; is high level synthesis ready for business? a computational finance case study; comparing performance, productivity and scalability of the tilt overlay processor to opencl hls; size aware placement for island style fpgas; analyzing the impact of heterogeneous blocks on fpga placement quality; low-latency option pricing using systolic binomial trees; collaborative processing of least-square monte carlo for american options; accelerating transfer entropy computation; fpga-accelerated monte-carlo integration using stratified sampling and brownian bridges; time sharing of runtime coarse-grain reconfigurable architectures processing elements in multi-process systems; and architectural synthesis of computational pipelines with decoupled memory access."
"we develop efficient meshfree method based on radial basis functions (rbfs) to solve european and american option pricing problems arising in computational finance. the application of rbfs leads to system of differential equations which are then solved by a time integration θ -method. the main difficulty in pricing the american options lies in the fact that these options are allowed to be exercised at any time before their expiry. such an early exercise right purchased by the holder of the option results into a free boundary problem. following the approach of nielsen et al. [b.f. nielsen, o. skavhaug and a. tveito, penalty methods for the numerical solution of american multi-asset option problems. j. comput. appl. math. 222, 3-16 (2008)], we use a small penalty term to remove the free boundary. the method is analyzed for stability. numerical results describing the payoff functions and option values are also present. we also compute the two important greeks, delta and gamma, of these options. © springer india 2015."
"in the modern age of the digital world, gigantic amounts of data have been recorded or collected. it remains a great challenge to process and analyze the ""big data"". many neurophysiological, physiological, clinical and behavioral data are dynamic by the nature of the experiments or the way they are collected. these signals could be complex, noisy, and often multivariate and multimodal. how to develop efficient statistical methods to characterize these data and extract information that reveals underlying biological or physiological mechanisms remains an active and important research topic. in recent years, numerous advanced computational statistics, signal processing, and machine-learning methods have been developed and there is rapidly growing interest in applying these methods to data analysis in neuroscience, physiology and medicine. the state space model (ssm) is referred to a class of probabilistic graphical models (koller & friedman 2009), which describe the probabilistic dependence between the latent state variable and the observed measurement. the state or the measurement can be either continuous or discrete. the term ""state space"" originated in 1960s in the area of control engineering (kalman 1960). ssm provides a general framework for analyzing deterministic and stochastic dynamical systems that are measured or observed through a stochastic process. the ssm framework has been successfully applied in engineering, statistics, computer science and economics to solve a broad range of dynamical systems problems. the most celebrated examples of ssm include the linear dynamical system and the associated inference algorithm: kalman filter (kalman 1960), and the hidden markov model (hmm) (rabiner 1989). despite plenty of successful examples applying state space analyses to neural and clinical data, there remain many challenges in data analysis, for either developing new mathematical theories and statistical models, or developing efficient algorithms tuned for large-scale data sets, or catering for highly complex (multimodal or multiscale) and nonstationary data. in order to pave the way for further advancement in these research areas, it is important to recognize these challenges and exchange new ideas among researchers and practitioners. it is important to point out that the modeling and analysis principles discussed in this book are general and equally valuable for time series analyses in many other disciplines, such as climatology, politics, finance, chemical engineering, consumer marketing and computer networking. © cambridge university press 2015"
"in high performance computing, monte carlo methods are widely used to solve problems in various areas of computational physics, finance, mathematics, electrical engineering and many other fields. we present monte carlo methods for the intel xeon phi coprocessor, to compute feynman loop integrals in high energy physics, and integrals arising in stochastic geometry as two types of sample problems. the intel xeon phi is based on a many integrated core (mic) architecture to gain extreme performance. we use two modes, 'offload' and 'native', to implement the simulations. we compare the parallel performance of our applications running on intel xeon phi, in terms of time and speedup, with a sequential execution on the cpu. in addition, the applications are designed in both single and double precision. © 2015 ieee."
"recently, real options have gained more importance in computational finance studies. it has already been shown that the compound option pricing can be formulated as a two-pass boundary pde arising from black-scholes model. radial basis function (rbf) as a meshfree approximation method is widely used for numerical study of the time dependent pdes. in this paper, the aim is to introduce the robust numerical approach based on rbf-qr to compute the price of european compound options such as the popular put on put options. we also extend the proposed approach to american compound option pricing. the numerical experiments will show the efficiency of the performance for european and american compound option with single asset and multi-asset cases. © 2015 elsevier ltd. all rights reserved."
"fast pricing of american-style options has been a difficult problem since it was first introduced to the financial markets in 1970s, especially when the underlying stocks’ prices follow some jump-diffusion processes. in this paper, we extend the ‘true martingale algorithm’ proposed by belomestny et al. [math. finance, 2009, 19, 53–71] for the pure-diffusion models to the jump-diffusion models, to fast compute true tight upper bounds on the bermudan option price in a non-nested simulation manner. by exploiting the martingale representation theorem on the optimal dual martingale driven by jump-diffusion processes, we are able to explore the unique structure of the optimal dual martingale and construct an approximation that preserves the martingale property. the resulting upper bound estimator avoids the nested monte carlo simulation suffered by the original primal–dual algorithm, therefore significantly improving the computational efficiency. theoretical analysis is provided to guarantee the quality of the martingale approximation. numerical experiments are conducted to verify the efficiency of our algorithm. © 2014 taylor & francis."
"risk management is a classical problem in finance. value at risk (var), is used as an important measure to quantify market risk. a common approach for estimating var is referred as historical value-at-risk (hvar). the hvar algorithm is simplistic in nature, however with large number of instruments or assets and frequent revaluations makes it a significant computational task. in this paper, we show that with the advent of multicore cpus and gpus and with parallel computing, acceleration can be achieved in hvar estimation for large portfolios. hvar computations are repeated many times in the tasks like back testing, deal synthesis and batch jobs, which can run for days, a significant reduction in turn around time can be achieved. these state-of-the-art platforms not only enables fast computations but also reduces the cost in terms of energy requirement. we present our approach for optimization and parallelization for hvar estimation and report significant reduction in overall application time."
"a substantial and diverse literature in economics traces its intellectual roots to charles tiebout’s 1956 article, “the pure theory of local expenditure.” its present recognition—frequently attributed to originating the idea of “voting with your feet”—contrasts sharply with its obscurity during tiebout's academic career, which was tragically cut short by his passing in 1968. penned as a qualification to paul samuelson's “pure theory,” the article failed to influence the stabilization of postwar public good theory despite tiebout's engagement with key figures in its construction. moreover, his death preceded the application of its central mechanism to public, urban, and environmental topics via hedonic, sorting, and computational general equilibrium models. viewed in this way, the history of tiebout's article, and thereby the history of public economics, has remarkably little to do with tiebout himself. in consequence, this article seeks to repersonalize and contextualize tiebout's scientific work. professionally, tiebout 1956 reflected its author's lifelong interest in local economies and governance. the social and political context of urban sprawl and political fragmentation that accompanied the rapid growth of metropolitan areas, such as chicago, los angeles, and seattle, raised novel questions in local public finance for researchers before a knowledge community existed to credit their work. for tiebout, it stimulated his collaboration with vincent ostrom and robert warren and later involvement in the interdisciplinary field of regional science. © 2015 by duke university press."
"when fourier techniques are applied to specific problems from computational finance with nonsmooth functions, the so-called gibbs phenomenon may become apparent. this seriously affects the efficiency and accuracy of the numerical results. for example, the variance gamma asset price process gives rise to algebraically decaying fourier coefficients, resulting in a slowly converging fourier series. we apply spectral filters to achieve faster convergence. filtering is carried out in fourier space; the series coefficients are pre-multiplied by a decreasing function. this does not add any significant computational costs. tests with different filters show how the algebraic index of convergence is improved. © 2015 incisive risk information (ip) limited."
"natural language processing employs computational techniques for the purpose of learning, understanding, and producing human language content. early computational approaches to language research focused on automating the analysis of the linguistic structure of language and developing basic technologies such asmachine translation, speech recognition, and speech synthesis.today's researchers refine and make use of such tools in real-world applications, creating spoken dialogue systems and speech-to-speech translation engines, mining social media for information about health or finance, and identifying sentiment and emotion toward products and services.we describe successes and challenges in this rapidly advancing area."
"in this study, we verify the existence of predictability in the brazilian equity market. unlike other studies in the same sense, which evaluate original series for each stock, we evaluate synthetic series created on the basis of linear models of stocks. following the approach of burgess (computational finance, 1999; 99, 297-312), we use the 'stepwise regression' model for the formation of models of each stock. we then use the variance ratio profile together with a monte carlo simulation for the selection of models with potential predictability using data from 1 april 1999 to 30 december 2003. unlike the approach of burgess, we carry out white's reality check (econometrica, 2000; 68, 1097-1126) in order to verify the existence of positive returns for the period outside the sample from 2 january 2004 to 28 august 2007. we use the strategies proposed by sullivan, timmermann and white (journal of finance, 1999; 54, 1647-1691) and hsu and kuan (journal of financial econometrics, 2005; 3, 606-628) amounting to 26,410 simulated strategies. finally, using the bootstrap methodology, with 1000 simulations, we find strong evidence of predictability in the models, including transaction costs. © 2015 john wiley & sons, ltd."
"with the aim to sequentially determine optimal allocations across a set of assets, online portfolio selection (olps) has significantly reshaped the financial investment landscape. online portfolio selection: principles and algorithms supplies a comprehensive survey of existing olps principles and presents a collection of innovative strategies that leverage machine learning techniques for financial investment. the book presents four new algorithms based on machine learning techniques that were designed by the authors, as well as a new back-test system they developed for evaluating trading strategy effectiveness. the book uses simulations with real market data to illustrate the trading strategies in action and to provide readers with the confidence to deploy the strategies themselves. the book is presented in five sections that: 1.introduce olps and formulate olps as a sequential decision task 2.present key olps principles, including benchmarks, follow the winner, follow the loser, pattern matching, and meta-learning 3.detail four innovative olps algorithms based on cutting-edge machine learning techniques 4.provide a toolbox for evaluating the olps algorithms and present empirical studies comparing the proposed algorithms with the state of the art 5.investigate possible future directions complete with a back-test system that uses historical data to evaluate the performance of trading strategies, as well as matlab® code for the back-test systems, this book is an ideal resource for graduate students in finance, computer science, and statistics. it is also suitable for researchers and engineers interested in computational investment. readers are encouraged to visit the authors’ website for updates: http://olps.stevenhoi.org. © 2016 by taylor & francis group, llc."
"computational finance is an area that includes many algorithms in trading and analytics that are both computationally very complex and performance critical. as financial institutions intend to perform a steadily increasing number of computations and obtain the results as quickly as possible, computer systems are expected to satisfy these growing performance demands. however, recent years have brought the end of “free” processors speed-ups, and single-thread performance is no longer the driving force behind automatic performance gains enjoyed by the industry for many decades. nowadays, high-performance computing systems have to increasingly rely on parallel programming models where the original application has to be modified to exploit many parallel cores. this requires considerable redesign efforts and yet, the desired performance improvements are not guaranteed. some financial applications may also reach practical physical limits imposed by the space and power provisions available in the data centre. a solution to the above problems can be the use of custom accelerators implemented in reconfigurable hardware. reconfigurable implementations can deliver both high computational throughput and low compute latency in addition to superior energy efficiency. however, porting applications for such devices requires a special skill set in hardware design, complicating their practical adoption. maxeler technologies offers conveniently programmable, high-performance computing systems and a software toolchain that exploit the sheer computational power of reconfigurable devices while abstracting the programming into a high-level data-flow model. our vision is to empower domain experts with the necessary means to create highly customised, efficient hardware/software implementations for their specific applications. this approach enables vertical optimisations across the different layers of abstraction that are typically not exposed to an application designer. the final result is a productive application development process that often delivers speed-ups by orders of magnitudes over traditional cpu implementations. © springer international publishing switzerland 2015."
"this article revisits the roots of modern portfolio theory. instead of isolating the systematic component of risk by recasting the risk in terms of a stock’s beta coefficient, i decompose the sd directly into its systematic and unsystematic components. from this decomposed sd, an ‘adjusted capital market line (cml)’ can be derived. it is easily shown that the adjusted cml is equivalent to sharpe’s security market line (sml). i evaluate the effectiveness of these alternative measures of systematic and unsystematic risk using empirical data and find that beta often deviates from my systematic risk measure and, in general, tends to overestimate a portfolio’s risk. this alternative way of looking at systematic and unsystematic risk offers easily accessible insights into the very nature of risk. implications include reducing the computational complexities in calculating the relevant portion of a portfolio’s volatility, facilitating sophisticated dispersion trades, estimating risk-adjusted returns and improving risk-adjusted performance measurement. this article offers new ideas that may influence the teaching of economics and finance. © 2014, taylor & francis."
"portfolio optimization is one of the most important problems in the finance field. the traditional mean-variance model has its drawbacks since it fails to take the market uncertainty into account. in this work, we investigate a two-stage stochastic portfolio optimization model with a comprehensive set of real world trading constraints in order to capture the market uncertainties in terms of future asset prices. a hybrid approach, which integrates genetic algorithm (ga) and a linear programming (lp) solver is proposed in order to solve the model, where ga is used to search for the assets selection heuristically and the lp solver solves the corresponding sub-problems of weight allocation optimally. scenarios are generated to capture uncertain prices of assets for five benchmark market instances. the computational results indicate that the proposed hybrid algorithm can obtain very promising solutions. possible future research directions are also discussed. © 2015 ieee."
"graphics processing unit (gpu) computing has become popular in computational finance, and many financial institutions are moving their cpu-based applications to the gpu platform. since most monte carlo algorithms are embarrassingly parallel, they benefit greatly from parallel implementations, and consequently monte carlo has become a focal point in gpu computing. gpu speed-up examples reported in the literature often involve monte carlo algorithms, and there are software tools commercially available that help migrate monte carlo financial pricing models to gpu. we present a survey of monte carlo and randomized quasi-monte carlo methods, and discuss existing (quasi) monte carlo sequences in gpu libraries. we discuss specific features of gpu architecture relevant for developing efficient (quasi) monte carlo methods. we introduce a recent randomized quasi-monte carlo method, and compare it with some of the existing implementations on gpu, when they are used in pricing caplets in the libor market model and mortgage-backed securities. © 2015 taylor & francis."
"one of the most important advantage of abm (agent-based modeling) used in social and economic calculation simulation is that the critical behavioral characteristics of the micro agents can be deeply depicted by the approach. why, what and how real behavior(s) should be incorporated into abm and is it appropriate and effective to use abm with hs-ca collaboration and micro-macro link features for complex economy/finance analysis? through deepening behavioral analysis and using computational experimental methods incorporating hs (human subject) into ca (computational agent), which is extended abm, based on the theory of behavioral finance and complexity science as well, we constructed a micro-macro integrated model with the key behavioral characteristics of investors as an experimental platform to cognize the conduction mechanism of complex capital market and typical phenomena in this paper, and illustrated briefly applied cases including the internal relations between impulsive behavior and the fluctuation of stock’s, the asymmetric cognitive bias and volatility cluster, deflective peak and fat-tail of china stock market. © 2015 sciendo. all rights reserved."
"in december 2013 the european commissioner barnier, presenting the single resolution mechanism for the resolution and recovery of banking crises, said it will “break the vicious circle between banks and their sovereigns”. but is there any vicious circle? and if so, will resolution tools be able to break it? in literature, the circular nature of the relationship between banking and sovereign debt crises has not yet been properly addressed. indeed, most papers exclusively focus on one channel of transmission, either quantifying the effects that banking crises have on public finances or analyzing when banking crises may cause sovereign debt crises (or vice-versa). in this paper we propose a computational approach to quantify the effects of this circular relationship and to highlight how sovereign and bank riskiness may increase because of their interconnection. we quantify the effects of bank distress on the banking system itself, passing through the higher public deficit induced by state support, and the subsequent haircut in government bonds. we then test the effectiveness of the bail-in tool proposed in the single resolution mechanism context. the method is tested on four european countries. results show that, while limited crises tend to be absorbed by the system, serious crises tend to exacerbate at each turn, so that it becomes impossible to stop them without external intervention. moreover, results show that a bail-in of 8 % of total bank balance sheet can be really effective in breaking the vicious circle and preventing contagion between banks and public finances. this finding supports the bail-in as a valid instrument to avoid taxpayers paying the bill of banking crises. © 2014, springer science+business media new york."
"since the emergence of complex multi-objective problems in the finance and economics areas, dealing with multi-objective problems has gained increasing attention. how to improve the quality of generating solutions is the key in solving such problems. although a number of moeas (multi-objective evolution algorithms) have been proposed over the last several years to solve the complex financial and economic multi-objective problems, not much effort has been made to deal with generating solutions in multi-objective optimization. recently, we have suggested a modea_dacr (multi-objective difference evolution algorithm via dynamic allocation of computational resource) to improve the quality of generating solutions. the proposed algorithm uses two populations with different convergence rates to extract convergence information for the pareto set, and then adjusts the parameter and difference evolution selection strategy according to the obtained convergence rate. in addition,based on the convergence rate of the population the proposed algorithm dynamically allocates the computational resources. the proposed algorithm is compared with two state-of-the-art algorithms, ε-moea and moea/d-dra, on a suite of test problems with a complex pareto set. experimental results have shown the effectiveness of the proposed algorithm. ©, 2015, science press. all right reserved."
"an artificial neural network (ann) is an information processing technique that is inspired by biological nervous systems. this technique tries to simulate its learning process and uses a mathematical or computational model to process information. artificial neural networks (anns) are utilized in many scientific disciplines extensively. there are many problems in mathematics, medicine, business and finance can be solved by using anns. the application areas of anns are classification, clustering, forecast, function approximation, optimization etc. the prediction of the motion responses for marine systems is computationally very expensive and highly time-consuming. several methods such as anns can be used in order to eliminate high computational costs for these systems. this paper employs anns to optimize multiple point mooring systems (spread mooring systems). these systems are often affected from harsh environmental conditions at open seas. to reduce effects of environmental conditions, these systems are supported by mooring lines. in this study, at first orcaflex software is employed to model and simulate spread mooring systems. 4 points mooring system is designed in orcaflex to calculate mooring tensions and tanker motion displacements. these simulation results are then used to train ann structure and an algorithm is created. after creating algorithm, the results of training, test and validation are obtained from ann. utilization of the ann tool can provide physical results and can process information in extremely rapid mode promising high accuracy of prediction. © 2015 taylor & francis group, london."
"recent finance and economic forecasting and risk calculation failures made obvious that macro-modelling without micro-foundation may be treacherous. reliable macro-modelling requires the consistent bundling of individual actions into intermediate and macro-variables exploiting the individual actions’ coordination and its dynamics. the degree of coordination may range from chaos – absence of coordination – to determined situations caused by macro-level equilibrium dictating any agent's actions and inhibiting interactions. coordination clusters individual actions into real decision units such as companies, political parties and unions. it structures the emergent intermediate and macro-level situations vitally.the paper presents first a centennial history of prominent scholars’ quotes questioning the equilibrium paradigm, a short survey of prevailing paradigm's deficiencies laid bare once again by the latest financial crises.it proposes second discrete choice (dc) – successfully applied in different fields – to model the individual agent's decision. dcs innovative integration into a markov process provides a steady foundation to model interactions of individual agents consistently.the final section justifies the actions’ proposed interactive bundling by referring to recent advances in data processing and network topology. the dynamic modelling of the actions’ and interactions’ coordination breaks fresh grounds both with regards to mathematical, computational and economic modeling requirements. the combination of latest developments in data processing like big data and the recently (re)discovered network topology capabilities may cope with these challenges. © 2014, taylor & francis."
"this is one of the first books that describe all the steps that are needed in order to analyze, design and implement monte carlo applications. it discusses the financial theory as well as the mathematical and numerical background that is needed to write flexible and efficient c++ code using state-of-the art design and system patterns, object-oriented and generic programming models in combination with standard libraries and tools. includes a cd containing the source code for all examples. it is strongly advised that you experiment with the code by compiling it and extending it to suit your needs. support is offered via a user forum on www.datasimfinancial.com where you can post queries and communicate with other purchasers of the book. this book is for those professionals who design and develop models in computational finance. this book assumes that you have a working knowledge of c ++. © 2009 john wiley & sons, ltd."
"as today’s organizations are capturing exponentially larger amounts of data than ever, now is the time for organizations to rethink how they digest that data. through advanced algorithms and analytics techniques, organizations can harness this data, discover hidden patterns, and use the newly acquired knowledge to achieve competitive advantages. presenting the contributions of leading experts in their respective fields, big data: algorithms, analytics, and applications bridges the gap between the vastness of big data and the appropriate computational methods for scientific and social discovery. it covers fundamental issues about big data, including efficient algorithmic methods to process data, better analytical strategies to digest data, and representative applications in diverse fields, such as medicine, science, and engineering. the book is organized into five main sections: 1. big data management—considers the research issues related to the management of big data, including indexing and scalabilityaspects. 2. big data processing—addresses the problem of processing big data across a wide range of resource-intensive computational settings: 3. big data stream techniques and algorithms—explores research issues regarding the management and mining of big data in streaming environments. 4. big data privacy—focuses on models, techniques, and algorithms for preserving big data privacy. 5. big data applications—illustrates practical applications of big data across several domains, including finance, multimedia tools, biometrics, and satellite big data processing. overall, the book reports on state-of-the-art studies and achievements in algorithms, analytics, and applications of big data. it provides readers with the basis for further efforts in this challenging scientific field that will play a leading role in next-generation database, data warehousing, data mining, and cloud computing research. it also explores related applications in diverse sectors, covering technologies for media/data communication, elastic media/data storage, cross-network media/data fusion, and saas. © 2015 by taylor & francis group, llc."
"we combine a dynamic programming approach (stochastic optimal control) with a multi-stage stochastic programming approach (msp) in order to solve various problems in personal finance and pensions. both optimization methods are integrated into one msp formulation, making it possible to achieve a solution within a short computational time. the solution takes into account the entire lifetime of an individual, while focusing on practical constraints, such as limits on portfolio composition, limits on the sum insured, inclusion of transaction costs, and taxes on capital gains, during the first years of a contract. two applications are considered: (a) optimal investment, consumption and sum insured for an individual maximizing the expected utility of consumption and bequest, and (b) optimal investment for a pension saver who wishes to maximize the expected utility of retirement benefits. numerical results show that among the considered practical constraints, the presence of taxes affects the optimal controls the most. furthermore, the individual’s preferences, such as impatience level and risk aversion, have even a higher impact on the controlled processes than the taxes on capital gains. © 2014, springer-verlag berlin heidelberg."
"cities are complex entities, and their studies are interdisciplinary. some hunt for finding urban variables; others say a few variables can little capture the essence of urbanization. the reason is urbanization is not only a global phenomenon of physical and cultural restructuring; it has itself become a spatial effect of the distributed networks of communication, resources, finance, and migration that characterize contemporary city life. in this fight, the key question of urbanization has remained unanswered: are there scientific reasons behind the development of a human city that well exists within its physical form? this paper develops a discussion explaining how a human city interacts with its physical form by revisiting the meaning of city configurations with the help of a newly developed syntax-based accessibility analysis model known as unit-segment model. the discussion also points out that, with current increases in computational power, the unit-segment model can contribute to the field further by answering a fascinating question that syntax configurational studies have helped to frame: what makes a city complex entity while dealing with its behavior and, therefore, the reasoning of its formation? © 2015, springer science+business media new york."
"the majority of existing artificial intelligence (ai) studies in computational finance literature are devoted solely to predicting market movements. in this paper we shift the attention to how ai can be applied to control risk-based money management decisions. we propose an innovative fuzzy logic approach which identifies and categorizes technical rules performance across different regions in the trend and volatility space. the model dynamically prioritizes higher performing regions at an intraday level and adapts money management policies with the objective to maximize global risk-adjusted performance. by adopting a hybrid method in conjunction with a popular neural network (nn) trend prediction model, our results show significant performance improvements compared with both standard nn and buy-and-hold approaches. © 2015 john wiley & sons, ltd."
"financial fraud is an issue with far reaching consequences in the finance industry, government, corporate sectors, and for ordinary consumers. increasing dependence on new technologies such as cloud and mobile computing in recent years has compounded the problem. traditional methods of detection involve extensive use of auditing, where a trained individual manually observes reports or transactions in an attempt to discover fraudulent behaviour. this method is not only time consuming, expensive and inaccurate, but in the age of big data it is also impractical. not surprisingly, financial institutions have turned to automated processes using statistical and computational methods. this paper presents a comprehensive investigation on financial fraud detection practices using such data mining methods, with a particular focus on computational intelligence-based techniques. classification of the practices based on key aspects such as detection algorithm used, fraud type investigated, and success rate have been covered. issues and challenges associated with the current practices and potential future direction of research have also been identified. © institute for computer sciences, social informatics and telecommunications engineering 2015."
"despite the s&op (sales and operations planning) process not being a new procedure in enterprises, especially the large size, it is still a research subject for their practices and computational tools support. computational models and tools focused to aid the s&op process are used and can contribute to improve the quality of its implementation and results. the most common tools are those based on spreadsheets; and the most sophisticated ones are based on operational research techniques inserted in advanced planning systems (aps). the literature review on theory and computational techniques used to aid the s&op process identified a lack of studies related to the use of system dynamics. additionally, the review indicates the need for economic and financial analysis integration studies during the preparation of aggregates plans and the use of probabilistic variables in the process enabling the statistical analysis in order to provide more reliable plans. another relevant issue observed is that managers involved in the s&op process do not always have a global vision of all variables and constraints involved in the process, therefore, the techniques and tools used to implement the s&op process do not provide a holistic view of the activities and variables involved. hence, this paper proposes a system dynamics model simulation that provides a holistic vision for the s&op process, allowing the integration of finance processes and the use of probabilistic variables. the evaluation of the simulations results performed with the model, supported by a design of experiments (doe), shows that the generated plans are compatible if compared to business procedure, with the advantage of dealing simultaneously with economic and financial analysis and still allows the generation of numerous scenarios of s&op plans."
"purpose – the purpose of this paper is to compare the performance of the genetic algorithm (ga), simulate annealing (sa) and shuffled frog-leaping algorithm (sfla) in solving discrete versus continuous-variable optimization problems of the finance-based scheduling. this involves the minimization of the project duration and consequently the time-related cost components of construction contractors including overheads, finance costs and delay penalties. design/methodology/approach – the meta-heuristics of the ga, sa and sfla have been implemented to solve non-deterministic polynomial-time hard (np-hard) finance-based scheduling problem employing the objective of minimizing the project duration. the traditional problem of generating unfeasible solutions in scheduling problems is adequately tackled in the implementations of the meta-heuristics in this paper. findings – the obtained results indicated that the sa outperformed the sfla and ga in terms of the quality of solutions as well as the computational cost based on the small-size networks of 30 activities, whereas it exhibited the least total duration based on the large-size networks of 120 and 210 activities after prolonged processing time. research limitations/implications – from researchers’ perspective, finance-based scheduling is one of the few domain problems which can be formulated as discrete and continuous-variable optimization problems and, thus, can be used by researchers as a test bed to give more insight into the performance of new developments of meta-heuristics in solving discrete and continuous-variable optimization problems. practical implications – finance-based scheduling discrete-variable optimization problem is of high relevance to the practitioners, as it allows schedulers to devise finance-feasible schedules of minimum duration. the minimization of project duration is focal for the minimization of time-related cost components of construction contractors including overheads, finance costs and delay penalties. moreover, planning for the expedient project completion is a major time-management aspect of construction contractors towards the achievement of the objective of client satisfaction through the expedient delivery of the completed project for clients to start reaping the anticipated benefits. social implications – planning for the expedient project completion is a major time-management aspect of construction contractors towards the achievement of the objective of client satisfaction. originality/value – sfla represents a relatively recent meta-heuristic that proved to be promising, based on its limited number of applications in the literature. this paper is to implement sfla to solve the discrete-variable optimization problem of the finance-based scheduling and assess its performance by comparing its results against those of the ga and sa. © emerald group publishing limited."
"abstract the positive relation between stock price changes and trading volume (price-volume relationship) as a stylized fact has attracted significant interest among finance researchers and investment practitioners. however, until now, consensus has not been reached regarding the causes of the relationship based on real market data because extracting valuable variables (such as information-driven trade volume) from real data is difficult. this lack of general consensus motivates us to develop a simple agent-based computational artificial stock market where extracting the necessary variables is easy. based on this model and its artificial data, our tests have found that the aggressive trading style of informed agents can produce a price-volume relationship. therefore, the information spreading process is not a necessary condition for producing price-volume relationship. © 2015 elsevier b.v. all rights reserved."
"purpose - the purpose of this paper is to measure the financial risk and optimal capital structure of a corporation. design/methodology/approach - irregular disjunctive programming problems arising in firm models and risk management can be solved by the techniques presented in the paper. findings - parallel processing and mathematical modeling provide a fruitful basis for solving ultra-scale non-convex general disjunctive programming (gdp) problems, where the computational challenge in direct mixed-integer non-linear programming (minlp) formulations or single processor algorithms would be insurmountable. research limitations/implications - the test is limited to a single firm in an experimental setting. repeating the test on large sample of firms in future research will indicate the general validity of monte-carlo-based var estimation. practical implications - the authors show that the risk surface of the firm can be approximated by integrated use of accounting logic, corporate finance, mathematical programming, stochastic simulation and parallel processing. originality/value - parallel processing has potential to simplify large-scale minlp and gdp problems with non-convex, multi-modal and discontinuous parameter generating functions and to solve them faster and more reliably than conventional approaches on single processors. keywords artificial intelligence, algorithms, disjunctive programming, multi-modality, non-convexity, parallel processing © emerald group publishing limited."
"twitter has been widely used for human behavior research with its applications in public opinion mining (cf. chapter 3), studying well-being (cf. chapter 4), disease monitoring (cf. chapter 5), and disaster mapping (cf. chapter 6). in this chapter, we review existing work on using twitter data to measure socioeconomic indicators, including unemployment rate, consumer confidence, social mood, investor sentiment, and financial markets. moreover, to complement research with twitter data, we use several examples to illustrate the use of other large-scale data sources (e.g., web search queries, mobile phone calls) for socioeconomic measurement and prediction. at the end, we discuss challenges with existing research and identify several directions for future work introduction there has been considerable success in leveraging large-scale social media data at the intersection of social sciences and computational sciences with myriad applications in socioeconomic measurement and prediction. an early study (antweiler & frank, 2004) finds that the message volume of stock message boards on yahoo! finance and raging bull can predict market volatility, and disagreement among posted messages is related to high trading volume. public mood indicators extracted from social networks such as facebook (karabulut, 2011), livejournal (gilbert & karahalios, 2010), and twitter (bollen, mao, & zeng, 2011) can predict stock market fluctuations. zhang, fuehres, and gloor (2010) study the correlation between emotional tweets and financial market indicators. they find that the percentage of emotional tweets is negatively correlated with dow jones, nasdaq, and standard and poor's (s&p) 500 values, but positively correlated with volatility index (vix). bollen, mao, and zeng (2011) develop a multidimensional mood analysis model that can track twitter mood in six dimensions (i.e., calm, alert, sure, vital, kind, and happy) and find that twitter calmness has significant predictive power on daily dow jones industrial average (djia) price changes. © cambridge university press 2015."
"as it is known in the finance risk and macroeconomics literature, risk-sharing in large portfolios may increase the probability of creation of default clusters and of systemic risk. we review recent developments on mathematical and computational tools for the quantification of such phenomena. limiting analysis such as law of large numbers and central limit theorems allow to approximate the distribution in large systems and study quantities such as the loss distribution in large portfolios. large deviations analysis allow us to study the tail of the loss distribution and to identify pathways to default clustering. sensitivity analysis allows to understand the most likely ways in which different effects, such as contagion and systematic risks, combine to lead to large default rates. such results could give useful insights into how to optimally safeguard against such events. © springer international publishing switzerland 2015."
"we present a high-order compact finite difference approach for a class of parabolic partial differential equations with time- and space-dependent coefficients as well as with mixed second-order derivative terms in n spatial dimensions. problems of this type arise frequently in computational fluid dynamics and computational finance. we derive general conditions on the coefficients which allow us to obtain a high-order compact scheme which is fourth-order accurate in space and second-order accurate in time. moreover, we perform a thorough von neumann stability analysis of the cauchy problem in two and three spatial dimensions for vanishing mixed derivative terms, and also give partial results for the general case. the results suggest unconditional stability of the scheme. as an application example we consider the pricing of european power put basket options in the multidimensional black-scholes model for two and three underlying assets. due to the low regularity of typical initial conditions we employ the smoothing operators of kreiss, thomee, and widlund [comm. pure appl. math., 23(1970), pp. 241-259] to ensure high-order convergence of the approximations of the smoothed problem to the true solution. © 2015 society for industrial and applied mathematics."
"order cancellation process plays a crucial role in the dynamics of price formation in order-driven stock markets and is important in the construction and validation of computational finance models. based on the order flow data of 18 liquid stocks traded on the shenzhen stock exchange in 2003, we investigate the empirical statistical properties of inter-cancellation durations in units of events defined as the waiting times between two consecutive cancellations. the inter-cancellation durations for both buy and sell orders of all the stocks favor a q-exponential distribution when the maximum likelihood estimation method is adopted; in contrast, both cancelled buy orders of 6 stocks and cancelled sell orders of 3 stocks prefer weibull distribution when the non-linear least-squares estimation is used. applying detrended fluctuation analysis (dfa), centered detrending moving average (cdma) and multifractal detrended fluctuation analysis (mf-dfa) methods, we unveil that the inter-cancellation duration time series process long memory and multifractal nature for both buy and sell cancellations of all the stocks. our findings show that order cancellation processes exhibit long-range correlated bursty behaviors and are thus not poissonian. © 2014  gu, xiong, zhang, zhang and zhou."
"in this paper, we study the problem of privately computing ordered statistics with the goal of monitoring sequential data streams. despite the broad series of techniques for time-series monitoring, only few works provide provable privacy guarantees employing the formal notion of differential privacy. while these solutions are well established, their focus is mostly limited to count based statistics (e.g. number of distinct elements, heavy hitters). in this paper, we consider a more general problem of privately computing the length of the longest increasing subsequence (lis) in the data stream model. this important statistic can be used to detect trends in time-series data (e.g. finance) and perform approximate string matching in computational biology domains. our proposed approaches employ the differential privacy notion which provides strong and provable privacy guarantees. our solutions estimate the length of the lis using block decomposition and local approximation techniques. we provide a rigorous analysis to bound the approximation error of our algorithms in terms of privacy level and length of the stream. © 2015, copyright is with the authors."
"the use of robo-readers to analyze news texts is an emerging technology trend in computational finance. recent research has developed sophisticated financial polarity lexicons for investigating how financial sentiments relate to future company performance. however, based on experience from fields that commonly analyze sentiment, it is well known that the overall semantic orientation of a sentence may differ from that of individual words. this article investigates how semantic orientations can be better detected in financial and economic news by accommodating the overall phrasestructure information and domain-specific use of language. our three main contributions are the following: (a) a human-annotated finance phrase bank that can be used for training and evaluating alternative models; (b) a technique to enhance financial lexicons with attributes that help to identify expected direction of events that affect sentiment; and (c) a linearized phrase-structure model for detecting contextual semantic orientations in economic texts. the relevance of the newly added lexicon features and the benefit of using the proposed learning algorithm are demonstrated in a comparative study against general sentiment models as well as the popular word frequency models used in recent financial studies. the proposed framework is parsimonious and avoids the explosion in feature space caused by the use of conventional n-gram features. © 2013 asis&t."
"it is well-known in empirical finance that virtually all asset returns, whether monthly, daily, or intraday, are heavy-tailed and, particularly for stock returns, are mildly but often significantly negatively skewed. however, the tail indices, or maximally existing moments of the returns, can differ markedly across assets. to accommodate these stylized facts when modeling the joint distribution of asset returns, an asymmetric extension of the meta-elliptical t distribution is proposed. while the likelihood is tractable, for high dimensions it will be impractical to use for estimation. to address this, a fast, two-step estimation procedure is developed, based on a saddlepoint approximation to the noncentral student's t distribution. the model is extended to support a ccc-(i)garch structure and demonstrated by modeling and forecasting the return series comprising the djia. the techniques of shrinkage, time-varying tail dependence, and weighted likelihood are employed to further enhance the forecasting performance of the model with no added computational burden. © 2015 elsevier inc."
"on-line portfolio selection, a fundamental problem in computational finance, has attracted increasing interest from artificial intelligence and machine learning communities in recent years. empirical evidence shows that stock's high and low prices are temporary and stock prices are likely to follow the mean reversion phenomenon. while existing mean reversion strategies are shown to achieve good empirical performance on many real datasets, they often make the single-period mean reversion assumption, which is not always satisfied, leading to poor performance in certain real datasets. to overcome this limitation, this article proposes a multiple-period mean reversion, or so-called ""moving average reversion"" (mar), and a new on-line portfolio selection strategy named ""on-line moving average reversion"" (olmar), which exploits mar via efficient and scalable online machine learning techniques. from our empirical results on real markets, we found that olmar can overcome the drawbacks of existing mean reversion algorithms and achieve significantly better results, especially on the datasets where existing mean reversion algorithms failed. in addition to its superior empirical performance, olmar also runs extremely fast, further supporting its practical applicability to a wide range of applications. finally, we have made all the datasets and source codes of this work publicly available at our project website: http://olps.stevenhoi.org/. © 2015 elsevier b.v. all rights reserved."
"this portfolio selection problem (psp) remains an intractable research problem in finance and economics and often regarded as np-hard problem in optimization and computational intelligence. this paper solved the extended markowitz mean-variance portfolio selection model with an efficient metaheuristics method of generalized differential evolution 3 (gde3). the extended markowitz mean-variance portfolio selection model consists of four constraints: bounds on holdings, cardinality, minimum transaction lots, and expert opinion. there is no research in literature that had ever engaged the set of four constraints with gde3 to solve psp. this paper is the first to conduct the study in this direction. the first three sets of constraints have been presented in other researches in literatures. this paper introduced expert opinion constraint to existing portfolio selection models and solved with gde3. the computational results obtained in this research study show improved performance when compared with other metaheuristics methods of genetic algorithm (ga), simulated annealing (sa), tabu search (ts) and particle swarm optimization (pso). © 2015 a. adebiyi ayodele and k. ayo charles."
"in this paper we present the semeval-2014 task 2 on spoken dialogue grammar induction. the task is to classify a lexical fragment to the appropriate semantic category (grammar rule) in order to construct a grammar for spoken dialogue systems. we describe four subtasks covering two languages, english and greek, and three speech application domains, travel reservation, tourism and finance. the classification results are compared against the groundtruth. weighted and unweighted precision, recall and f-measure are reported. three sites participated in the task with five systems, employing a variety of features and in some cases using external resources for training. the submissions manage to significantly beat the baseline, achieving a f-measure of 0.69 in comparison to 0.56 for the baseline, averaged across all subtasks. © 8th international workshop on semantic evaluation, semeval 2014 - co-located with the 25th international conference on computational linguistics, coling 2014, proceedings. all rights reserved."
"this chapter aims at reviewing complex network models and methods that were either developed for or applied to socioeconomic issues, and pertinent to the theme of new economic geography. after an introduction to the foundations of the field of complex networks, the present summary adds insights on the statistical mechanical approach, and on the most relevant computational aspects for the treatment of these systems. as the most frequently used model for interacting agent-based systems, a brief description of the statistical mechanics of the classical ising model on regular lattices, together with recent extensions of the same model on small-world watts–strogatz and scale-free albert-barabási complex networks is included. other sections of the chapter are devoted to applications of complex networks to economics, finance, spreading of innovations, and regional trade and developments. the chapter also reviews results involving applications of complex networks to other relevant socioeconomic issues, including results for opinion and citation networks. finally, some avenues for future research are introduced before summarizing the main conclusions of the chapter. © 2015, springer international publishing switzerland."
"risk analysis in engineering and economics is required reading for decision making under conditions of uncertainty. the authordescribes the fundamental concepts, techniques, and applications of the subject in a style tailored to meet the needs of students and practitioners of engineering, science, economics, and finance. drawing on his extensive experience in uncertainty and risk modeling and analysis, the author covers everything from basic theory and key computational algorithms to data needs, sources, and collection. he emphasizes practical use of the methods presented and carefully examines the limitations, advantages, and disadvantages of each to help readers translate the discussed techniques into real-world solutions. this second edition: • introduces the topic of risk finance • incorporates homeland security applications throughout • offers additional material on predictive risk management • includes a wealth of new and updated end-of-chapter problems • delivers a complementary mix of theoretical background and risk methods • brings together engineering and economics on balanced terms to enable appropriate decision making • presents performance segregation and aggregation within a risk framework • contains contemporary case studies, such as protecting hurricane-prone regions and critical infrastructure • provides 320+ tables and figures, over 110 diverse examples, numerous end-of-book references, and a bibliography unlike the classical books on reliability and risk management, risk analysis in engineering and economics, second edition relates underlying concepts to everyday applications, ensuring solid understanding and use of the methods of risk analysis. © 2014 by taylor & francis group, llc."
"the analytic hierarchy process (ahp) is one of the popular methods to support human decision making. prioritization method leading to the possibility of rank reversal is still one of the unsettled issues of the ahp, although many applications of this method have been made. in this study, 10 important prioritization methods are reviewed and compared. to evaluate the fitness of the prioritization operators (pos), on the basis of existing po measurement criteria, this research proposes the root mean penalty weighted square variance (rmpwsv) as a reasonable measurement criterion. graphical solution demonstrates the computational concept for the rmpwsv. on the basis of the rmpwsv, two least penalty optimization (lpo) pos are proposed as the new perspectives for solving the prioritization problems: least product of penalty and direct squares (lppds) and least product of penalty and weighted squares (lppws). the comprehensive numerical analyses verify that lppds is the fittest po with respect to the rmpwsv. to demonstrate improvement of the proposed method, a medical decision problem for organ transplantation is revised by using 11 developed pos and two proposed pos with respect to rmpwsv measurement. lppds is the ideal po, instead of eigenvector method, to generate a priority vector for a decision problem in various fields, such as social science, politics, business, finance, medical treatment, resource management, and engineering management. © 2014 wiley periodicals, inc."
"this article traces chemometrics back to its origins in scientific computing in the 1960s. its development is compared in other computational disciplines such as bioinformatics. the change in geographical origins of papers published in the core chemometrics literature is discussed. it is concluded that the level of core activities in this area has hardly changed over several decades, whilst there has been a significant expansion in non-expert users of packages over this period. it is estimated that around 2% of people encountering chemometrics in their research can be considered real experts. the problems of non-experts using chemometrics methods with limited knowledge of the statistical fundamentals are explored. the contrasting development of chemometrics compared with, for example, computational chemistry and bioinformatics, is interpreted in terms of the changing financial pressures on research over its key developmental phase, as illustrated by the change in academic finance in the uk over the past 50years. © 2014 john wiley & sons, ltd."
"emerging technologies are largely engaged in processing big data using different computational environments especially in different x-information systems such as astronomy, biology, biomedicine, business, chemistry, climate, computer science, earth science, electronics, energy, environment, finance, health, intelligence, lifestyle, market engineering, mechanics, medicine, pathology, physics, policy making, radar, security, social issues, wealth, wellness and so on for different visual and graphical modelling. these frameworks of different scientific modelling will help government, industry, research and different other communities for their decisionmaking and strategic planning. in this paper we will discuss about different x-informatics systems, trends of different emerging technologies, how big data processing will help in different decision making and different models available in the parallel paradigms and the probable way out to work with high dimensional data. © 2015 the authors. published by elsevier b.v."
"a step-by-step introduction to modeling, training, and forecasting using wavelet networks. wavelet neural networks: with applications in financial engineering, chaos, and classification presents the statistical model identification framework that is needed to successfully apply wavelet networks as well as extensive comparisons of alternate methods. providing a concise and rigorous treatment for constructing optimal wavelet networks, the book links mathematical aspects of wavelet network construction to statistical modeling and forecasting applications in areas such as finance, chaos, and classification. the authors ensure that readers obtain a complete understanding of model identification by providing in-depth coverage of both model selection and variable significance testing. featuring an accessible approach with introductory coverage of the basic principles of wavelet analysis, wavelet neural networks: with applications in financial engineering, chaos, and classification also includes: methods that can be easily implemented or adapted by researchers, academics, and professionals in identification and modeling for complex nonlinear systems and artificial intelligence. multiple examples and thoroughly explained procedures with numerous applications ranging from financial modeling and financial engineering, time series prediction and construction of confidence and prediction intervals, and classification and chaotic time series prediction. an extensive introduction to neural networks that begins with regression models and builds to more complex frameworks. coverage of both the variable selection algorithm and the model selection algorithm for wavelet networks in addition to methods for constructing confidence and prediction intervals. ideal as a textbook for mba and graduate-level courses in applied neural network modeling, artificial intelligence, advanced data analysis, time series, and forecasting in financial engineering, the book is also useful as a supplement for courses in informatics, identification and modeling for complex nonlinear systems, and computational finance. in addition, the book serves as a valuable reference for researchers and practitioners in the fields of mathematical modeling, engineering, artificial intelligence, decision science, neural networks, and finance and economics. © 2014 john wiley & sons, inc. all rights reserved."
"learn two popular programming languages in a single volume widely used by scientists and engineers, well-established matlab® and open-source octave are similar software programs providing excellent capabilities for data analysis, visualization, and more. by means of straightforward explanations and examples from different areas in mathematics, engineering, finance, and physics, essential matlab and octave explains how matlab and octave are powerful tools applicable to a variety of problems. this text provides an introduction that reveals basic structures and syntax, demonstrates the use of functions and procedures, outlines availability in various platforms, and highlights the most important elements for both programs. effectively implement models and prototypes using computational models this text requires no prior knowledge. self-contained, it allows the reader to use the material whenever needed rather than follow a particular order. compatible with both languages, the book material incorporates commands and structures that allow the reader to gain a greater awareness of matlab and octave, write their own code, and implement their scripts and programs within a variety of applicable fields. it is always made clear when particular examples apply only to matlab or only to octave, allowing the book to be used flexibly depending on readers’ requirements. includes brief, simple code that works in both matlab and octave provides exercise sections at the end of each chapter introduces framed examples and discussions with a scientific twist exercises are provided at the end of each chapter essential matlab and octave offers an introductory course in matlab and octave programming and is an authoritative resource for students in physics, mathematics, statistics, engineering, and any other subjects that require the use of computers to solve numerical problems. © 2015 by taylor & francis group, llc."
"the term monte carlo simulation was first coined in the 1940s by a group of mathematicians while working on the manhattan project in tribute to the famous monte carlo casino. the simulation consists of a series of computational algorithms that work by repeated sampling of a range of possible values in calculating a series of probability distributions. the advantage of this technique is that it enables greater accuracy to be achieved in describing the uncertainty in the variables being used. this type of technique is primarily used when it is not feasible to compute exact results. because of the broad application of this method, monte carlo techniques are used in various simulations involving physical and mathematical systems. for example, in mathematics, monte carlo simulations are used for evaluating multidimensional definite integrals with complicated boundary conditions. monte carlo methods have also been applied in simulating systems involving numerous coupled degrees of freedom, such as, for example, disordered materials, fluids, strongly joined solids, and cellular structures. among some of the many other applications for monte carlo simulations are the calculation of various risks assessments in business, research and development, engineering, manufacturing, project management, insurance, finance, transportation, and even space exploration. © 2014 elsevier inc. all rights reserved."
"identification of a set of key players in a given social network is of interest in many disciplines such as sociology, politics, finance, economics, etc. although many algorithms have been proposed to identify a set of key players, each emphasizes a single objective of their interest. consequently, the prevailing deficiency of each of these methods is that they perform well only when we consider their objective of interest as the only characteristic the set of key players should have. but in complicated real life applications, we need a set of key players which can perform well with respect to multiple objectives of interest. in this paper, we propose a new perspective for key player identification, based on optimizing multiple objectives of interest. this method allows us to compare other methods of key player identification. the sets of key players identified by this method are better when multiple objectives must be addressed. in addition we propose an algorithm to select the most suitable sets of key players when multiple choices are available. to reduce the computational complexity of the proposed approach for large networks, we propose a new sampling approach based on degree centrality. we apply these algorithms in eventual influence limitation (eil) problem and immunization problem and show that our multi-objective methodology outperforms previous key player identification approaches. © 2015, springer-verlag wien."
"the vast volume of financial data that exists and the globalization of financial markets create new challenges for researchers and practitioners in economics and finance. computational data analysis techniques can contribute significantly within this context, by providing a rigorous analytic framework for decision-making and support, in areas such as financial times series analysis and forecasting, risk assessment, trading, asset management, and pricing. the aim of this edited volume is to present, in a unified context, some recent advances in the field, covering the theory, the methodologies, and the applications of computational data analysis methods in economics and finance. the volume consists of papers published in the fifth volume of the journal of computational optimization in economics and finance (published by nova science publishers). the contents of this volume cover a wide range of topics, including among others stock market applications, corporate finance, corporate performance, as well as macroeconomic issues. © 2015 by nova science publishers, inc. all rights reserved."
"transfer entropy is a recently introduced information-theoretic measure quantifying directed statistical coherence between spatiotemporal processes, and is widely used in diverse fields ranging from finance to neuroscience. however, its relationships to fundamental limits of computation, such as landauer's limit, remain unknown. here we show that in order to increase transfer entropy (predictability) by one bit, heat flow must match or exceed landauer's limit. importantly, we generalise landauer's limit to bi-directional information dynamics for non-equilibrium processes, revealing that the limit applies to prediction, in addition to retrodiction (information erasure). furthermore, the results are related to negentropy, and to bremermann's limit and the bekenstein bound, producing, perhaps surprisingly, lower bounds on the computational deceleration and information loss incurred during an increase in predictability about the process. the identified relationships set new computational limits in terms of fundamental physical quantities, and establish transfer entropy as a central measure connecting information theory, thermodynamics and theory of computation."
"high-accuracy optimiser is the success of resolution-sensitive applications such as computational finance and scientific computing. however, if the cost function is complicated with a large number of peaks, it is computationally expensive for the optimiser to reach high-accuracy and to satisfy the needs of these applications. in this paper, by the novel idea of single-gpu-based iterative discrete approximation, we develop a high-accuracy non-gradient optimiser, iterative discrete approximation monte carlo search (single-gpu ida-mcs), with the style of single instruction multiple data by cuda 5.0, and we illustrate the performance of the algorithm by finding the optimum of a cost function up to hundreds of peaks. computational results show that the accuracy of optima from a single-gpu ida-mcs with ten iterations and 104 elements is significantly higher than the conventional method monte carlo search with 1,000 iterations and 108 elements. computational results also show that, by the same number of iterations and elements, the accuracy of a single-gpu ida-mcs is higher than (weighted) discrete approximation monte carlo search. © 2015 inderscience enterprises ltd."
"what follows is an account of the concepts of information and noise as they apply to an analysis of high-frequency trading according to ‘heterodox economics’. the text proposes a framework according to which finance can best be understood as a complex technical system tightly coupled to other social, economic systems. to be more precise, the paper attempts to show how finance is not just any complex system but it can be understood as an ecology of evolving socio-technical systems, sub-systems such as investment banks, hedge funds, high-frequency trading traders, retail investors, pensions funds, etc. all of these are the technical building blocks of our financial markets. moreover, we attempt to show how concepts from other disciplines, such as entropy, information and noise, can be useful in opening up the world of finance from its traditional economic milieu. although the following text is confined to the discursive realm of humanities/social sciences, it echoes the analytical approaches of econophysics and experimental economics and particularly the ongoing research around ‘computational evolutionary economics’ [mirowski, p. 2007. “markets come to bits: evolution, computation and markomata in economic science.” journal of economic behavior & organization 63: 209–242; mirowski, p. 2010. “inherent vice: minsky, markomata, and the tendency of markets to undermine themselves.” journal of institutional economics 6: 415–443]. this becomes particularly relevant in the context of the so-called robot phase transition from human-dominated trading to the more automatic electronic trading. the current microstructure of automatic market-making can be understood as an ‘ecological niche’ developed by ultra-fast trading algorithms which ‘feed’ on the asymmetries and disparities of the wider ‘financial ecology’. they do this by dissipating noise and adding to the complexity of market microstructure a behaviour that can push the whole ecology to critical thresholds, sometimes referred to as flash crashes. this whole process can ultimately be described by philip mirowski's notion of ‘inherent vice’, as well as by sir robert may's concept of instability ‘which develops in ecosystems upon increasing bio-diversity’ [caccioli, f., m. marsili, and p. vivo. 2007. “eroding market stability by proliferation of financial instruments.” the european physical journal b–condensed matter and complex systems 71: 467–479; haldane, a., and r. may. 2011. “systemic risk in banking ecosystems.” nature 469: 351–355]. © 2014, © 2014 taylor & francis."
"in this paper, we introduce a mean-variance criterion for production optimization of oil reservoirs and suggest the sharpe ratio as a systematic procedure to optimally trade-off risk and return. we demonstrate by open-loop simulations of a two-phase synthetic oil field that the mean-variance criterion is able to mitigate the significant inherent geological uncertainties better than the alternative certainty equivalence and robust optimization strategies that have been suggested for production optimization. in production optimization, the optimal water injection profiles and the production borehole pressures are computed by solution of an optimal control problem that maximizes a financial measure such as the net present value (npv). the npv is a stochastic variable as the reservoir parameters, such as the permeability field, are stochastic. in certainty equivalence optimization, the mean value of the permeability field is used in the maximization of the npv of the reservoir over its lifetime. this approach neglects the significant uncertainty in the npv. robust optimization maximizes the expected npv over an ensemble of permeability fields to overcome this shortcoming of certainty equivalence optimization. robust optimization reduces the risk compared to certainty equivalence optimization because it considers an ensemble of permeability fields instead of just the mean permeability field. this is an indirect mechanism for risk mitigation as the risk does not enter the objective function directly. in the mean-variance bi-criterion objective function risk appears directly, it also considers an ensemble of reservoir models, and has robust optimization as a special extreme case. the mean-variance objective is common for portfolio optimization problems in finance. the markowitz portfolio optimization problem is the original and simplest example of a mean-variance criterion for mitigating risk. risk is mitigated in oil production by including both the expected npv (mean of npv) and the risk (variance of npv) for the ensemble of possible reservoir models. with the inclusion of the risk in the objective function, the sharpe ratio can be used to compute the optimal water injection and production borehole pressure trajectories that give the optimal return-risk ratio. by simulation, we investigate and compare the performance of production optimization by mean-variance optimization, robust optimization, certainty equivalence optimization, and the reactive strategy. the optimization strategies are simulated in open-loop without feedback while the reactive strategy is based on feedback. the simulations demonstrate that certainty equivalence optimization and robust optimization are risky strategies. at the same computational effort as robust optimization, mean-variance optimization is able to reduce risk significantly at the cost of slightly smaller return. in this way, mean-variance optimization is a powerful tool for risk management and uncertainty mitigation in production optimization. © 2014 elsevier b.v."
"funahashi and kijima (in press, a chaos expansion approach for the pricing of contingent claims, journal of computational finance) have proposed an approximation method based on the wiener-ito chaos expansion for the pricing of european-style contingent claims. in this paper, we extend the method to the multi-asset case with general local volatility structure for the pricing of exotic basket options such as asian basket options. through ample numerical experiments, we show that the accuracy of our approximation remains quite high even for a complex basket option with long maturity and high volatility. © 2013 taylor & francis."
"monte carlo simulations have become widely used in computational finance. standard error is the basic notion to measure the quality of a monte carlo estimator, and the square of standard error is defined as the variance divided by the total number of simulations. variance reduction methods have been developed as efficient algorithms by means of probabilistic analysis. gpu acceleration plays a crucial role of increasing the total number of simulations. we show that the total effect of combining variance reduction methods as efficient software algorithms with gpu acceleration as a parallel-computing hardware device can yield a tremendous speed up for financial applications such as evaluation of option prices and estimation of joint default probabilities. © 2014 ieee."
"new consumer products such as personal care products, mobile phones, smart windows, led lamps, ev batteries, thin film solar cells, printed electronics, air purifiers, and medical devices, are being introduced to the market at an increasingly rapid pace. this is driven partly by consumer demands and partly by the emergence of new molecules, nanomaterials, advanced materials, and innovative processing technologies. this presentation describes a framework for the design of chemical products in which the chemical engineer drives the product design project in collaboration with personnel in marketing, finance, business development, chemistry, physics, mechanical engineering, electronic engineering, and so on. in this framework, the many product design and development tasks are classified into management, business and marketing, research and design, manufacturing, and finance and economics. these are performed in three phases - product conceptualization, product design and prototyping, and product manufacturing and launch. the framework includes rule-based methods such as quality function deployment and rat2io, and model-based methods such as computer-aided molecular design, and transport models. it also includes databases for chemicals and equipment, and computer-aided tools for property prediction, process simulation, computational fluid dynamics, etc. experiments are used whenever model predictions are impossible or not sufficiently accurate. this framework is illustrated with two examples - a sunscreen cream and a die attach adhesive for led lamps. © 2015 elsevier b.v."
"the analyses of dividends paid by firms and decisions to repurchase their own shares require an econometric approach because of the complex dynamic interrelationships. this chapter begins by, first, highlighting the importance of developing comprehensive econometric models for these interrelationships. it is common in finance research to spell out “specific hypotheses” and conduct empirical research to investigate validity of the hypotheses. however, such an approach can be misleading in situations where variables are simultaneously determined as is often the case in financial applications. second, financial and accounting databases such as compustat are complex and contain useful longitudinal information on variables that display considerable heterogeneity across the firms. empirical analyses of financial databases demand the use of econometric and computational methods in order to draw robust inferences. for example, using longitudinal data on the same us firms, it was found that dividends were neither “disappearing” nor “reappearing” but were relatively stable in the period 1992–2007 bhargava (journal of the royal statistical society a, 173, 631–656, 2010). third, the econometric methodology tackled the dynamics ofrelationships and investigated endogeneity of certain explanatory variables. identification of the model parameters is achieved in such models by exploiting the cross-equations restrictions on the coefficients in different time periods.moreover, the estimation entails using nonlinear optimization methods to compute the maximum likelihood estimates of the dynamic random effects models and for testing statistical hypotheses using likelihood ratio tests. for example, share repurchases were treated as endogenous explanatory variable in the models for dividend payments, and dividends were treated as endogenous variables in the models for share repurchases.theempirical results showed that dividends are decided quarterly at the first stage, and higher dividends payments lowered share repurchases by firms that are made at longer intervals. these findings cast some doubt on evidence for the simple “substitution” hypothesis between dividends and share repurchases. the appendix outlines some of the econometric estimation techniques and tests that are useful for research in finance. © springer science+business media new york 2015."
"high-dimensional option pricing, which plays an important role in complex financial activities, presents a great computational challenge in practice. randomized quasi monte carlo (rqmc) algorithm is of practical significance for forecasting option prices or other finance derivatives. in this paper, we present an improved parallel rqmc algorithm to forecast asian option prices using many integrated core (mic) architecture. the improved algorithm employs novel data structure, independent random generator, vectorization technology, and data alignment. numerical experiments were conducted on mic architecture and the parallel performance was then analyzed. a speedup of 1.37 was achieved on mic over cpu. efficiency of 70.85% was achieved by using 64 openmp threads of a mic card. an average speedup of 3.38 can be obtained by mixing the cpu and mic computation in comparison with a single core of the cpu. ample evidences proved the rqmc algorithm can benefit enormously from mic architecture. © 2014 ieee."
"option pricing is one of the most challenging problems in computational finance and derivative modeling. as a result, one has to resort to computational approaches since it is difficult to obtain closed form solution for options other than simple options such as european style options. also, due to the complex nature of the governing mathematics, several numerical approaches have been proposed in the past to price american style options as well as complex options. in the current study, we apply trinomial lattice which has been used in many scientific and engineering applications to model option pricing for assets with high volatility. the three novelties of this paper include, the formulation of cloud asset price using stochastic process, the improvement of the american style option pricing algorithm by integrating our option pricing factor (pf) into the algorithm, and the presentation of the computed option values for various strike price. with carefully select strike-price spacing, we guarantee a fine-grain integration of pf into the trinomial lattice. © 2014 ieee."
"few financial mathematical books have discussed mathematically acceptable boundary conditions for the degenerate diffusion equations in finance. in the time-discrete method of lines for options and bonds, gunter h meyer examines pde models for financial derivatives and shows where the fichera theory requires the pricing equation at degenerate boundary points, and what modifications of it lead to acceptable tangential boundary conditions at non-degenerate points on computational boundaries when no financial data are available. extensive numerical simulations are carried out with the method of lines to examine the influence of the finite computational domain and of the chosen boundary conditions on option and bond prices in one and two dimensions, reflecting multiple assets, stochastic volatility, jump diffusion and uncertain parameters. special emphasis is given to early exercise boundaries, prices and their derivatives near expiration. detailed graphs and tables are included which may serve as benchmark data for solutions found with competing numerical methods. © 2015 by world scientific publishing co. pte. ltd. all rights reserved."
"we propose a computationally efficient fully polynomial-time approximation scheme (fptas) to compute an approximation with arbitrary precision of the value function of convex stochastic dynamic programs, using the technique of k-approximation sets and functions introduced by halman et al. [math. oper. res., 34, (2009), pp. 674-685]. this paper deals with the convex case only, and it has the following contributions. first, we improve on the worst-case running time given by halman et al. second, we design and implement an fptas with excellent computational performance and show that it is faster than an exact algorithm even for small problem instances and small approximation factors, becoming orders of magnitude faster as the problem size increases. third, we show that with careful algorithm design, the errors introduced by floating point computations can be bounded, so that we can provide a guarantee on the approximation factor over an exact infinite-precision solution. we provide an extensive computational evaluation based on randomly generated problem instances coming from applications in supply chain management and finance. the running time of the fptas is both theoretically and experimentally linear in the size of the uncertainty set. © 2015 society for industrial and applied mathematics."
"this book presents solutions to the general problem of single period portfolio optimization. it introduces different linear models, arising from different performance measures, and the mixed integer linear models resulting from the introduction of real features. other linear models, such as models for portfolio rebalancing and index tracking, are also covered. the book discusses computational issues and provides a theoretical framework, including the concepts of risk-averse preferences, stochastic dominance and coherent risk measures. the material is presented in a style that requires no background in finance or in portfolio optimization; some experience in linear and mixed integer models, however, is required. the book is thoroughly didactic, supplementing the concepts with comments and illustrative examples. © springer international publishing switzerland 2015."
"portfolio optimization is an important problem based on the modern portfolio theory (mpt) in the finance field. the idea is to maximize the portfolio expected return as well as minimizing portfolio risk at the same time. in this work, we propose a combinatorial algorithm for the portfolio optimization problem with the cardinality and bounding constraints. the proposed algorithm hybridizes a metaheuristic approach (particle swarm optimization, pso) and a mathematical programming method where pso is used to deal with the cardinality constraints and the math programming method is used to deal with the rest of the model. computational results are given for the benchmark datasets from the or-library and they indicate that it is a useful strategy for this problem. we also present the solutions obtained by the cplex mixed integer program solver for these instances and they can be used as the criteria for the comparison of algorithms for the same problem in the future. © 2014 ieee."
"experimental computational simulation environments (e.g., galas, 2014) are increasingly being developed by major financial institutions to model their analytic algorithms; this includes evaluation of algorithm stability, estimation of its optimal parameters, and the expected risk and performance profiles. such environments rely on big data analytics (e.g., mcafee and brynjolfsson, 2012), as part of their software infrastructure, to enable large-scale testing, optimization, and monitoring of algorithms running in virtual or real mode.at ucl, we believe that such environments will have a profound impact on the way research is conducted in social sciences. consequently, for the last few years, we have been working on our dracus system, a state of-the-art computational simulation environment, believed to be the first available for academic research in computational finance, specifically financial economics. as part of the dracus project, we work to support simulations in algorithmic trading, systemic risk, and sentiment analysis. © 2015 elsevier b.v."
"the sail-grs system is based on a widely used approach originating from information retrieval and document indexing, the tf-idf measure. in this implementation for spoken dialogue system grammar induction, rule constituent frequency and inverse rule frequency measures are used for estimating lexical and semantic similarity of candidate grammar rules to a seed set of rule pattern instances. the performance of the system is evaluated for the english language in three different domains, travel, tourism and finance and in the travel domain, for greek. the simplicity of our approach makes it quite easy and fast to implement irrespective of language and domain. the results show that the sail-grs system performs quite well in all three domains and in both languages. © 8th international workshop on semantic evaluation, semeval 2014 - co-located with the 25th international conference on computational linguistics, coling 2014, proceedings. all rights reserved."
"funding for humanitarian operations in the global health sector is highly variable and unpredictable. we study the problem of managing inventory in the presence of funding constraints over a finite planning period. our goal is to determine the optimal procurement policy given the complexities associated with funding and also to analyze the impact of funding amount, funding schedule, and uncertainty around the funding timing on operations. we use a multiperiod stochastic inventory model with financial constraints and demonstrate that despite the funding complexities, the optimal replenishment policy is a state-independent policy that can be easily implemented. we also provide analytical results and several insights based on our computational study regarding the effect of funding timing uncertainty and variability on the operating costs and fill rates. among other results, we find that receiving funding early is beneficial in underfinanced systems while avoiding funding delays is critical in fully financed systems. our analysis also indicates that receiving less overall funding in a timely manner might actually be better than delayed full funding. © 2014 informs."
"the traditional finance theory based on rational investors and the efficient market hypothesis was difficult to make a reasonable explanation to the phenomenon of fluctuation and crash in the stock market. these phenomena were formed by the behavior of investors transaction by transaction. that is, the behavior characteristics of investors had important influence on the abnormal phenomenon of the stock market fluctuation. in this paper, the investor behavior was analyzed. the model of investor decision based on prospect theory was built, which is more in line with the process of human practical decision-making. at the same time, the artificial stock market model was constructed combined with the agent-based computational finance method. finally, the model was verified by the simulation program of object-c based on swarm. the study explores intrinsic formation mechanism of stock price volatility from the perspective of behavioral finance, in order to put forward some valuable suggestions for the risk management of investor and the policy regulation of regulatory. © 2015 taylor & francis group, london."
"pairs trading is an important and challenging research area in computational finance, in which pairs of stocks are bought and sold in pair combinations for arbitrage opportunities. traditional methods that solve this set of problems mostly rely on statistical methods such as regression. in contrast to the statistical approaches, recent advances in computational intelligence (ci) are leading to promising opportunities for solving problems in the financial applications more effectively. in this paper, we present a novel methodology for pairs trading using genetic algorithms (ga). our results showed that the ga-based models are able to significantly outperform the benchmark and our proposed method is capable of generating robust models to tackle the dynamic characteristics in the financial application studied. based upon the promising results obtained, we expect this ga-based method to advance the research in computational intelligence for finance and provide an effective solution to pairs trading for investment in practice. © 2015 chien-feng huang et al."
"efficient kernel summation is an active research topic in machine learning and computational physics. fast multipole methods (fmms) in particular are known as efficient computational methods in these fields, but they have not gained much attention in computational finance. in this paper,we apply the improved fast gauss transform (ifgt), a version of an fmm, to the computation of european-type option prices under merton’s jump-diffusion model. ifgt is applied to computing the nonlocal integral terms in partial integrodifferential equations, and our results indicate that ifgt is useful for the fast computation of option pricing under this model. © incisive media ltd. all rights reserved."
"in the finance industry, obtaining stable estimates for sensitivities of derivatives to price changes in an underlying asset is very important from a practical point of view. however, this aim is often hindered by the absence of closed-form expressions for greeks or the requirement of an excessive computational workload due to the complexities of various exotic derivative structures. however, ad hoc numerical schemes to produce stable greeks such as nonlinear regression can result in nonsensical values. this article proposes a fairing algorithm designed for the computation of gamma values of exotic derivatives. examples are presented at exotic derivatives to which the algorithm is applied and some analytical and numerical results are provided that show its usefulness in reducing the mean square error of gamma estimates. © 2014 iie."
"portfolio asset selection (pas) is a challenging and interesting multiobjective task in the field of computational finance, and is receiving the increasing attention of researchers, fund management companies and individual investors in the last few decades. selecting a subset of assets and corresponding optimal weights from a set of available assets, is a key issue in the pas problem. a markowitz model is generally used to solve this optimization problem, where the total profit is maximized, while the total risk is to be minimized. however, this model does not consider the practical constraints, such as the minimum buy in threshold, maximum limit, cardinality etc. the practical constraints are incorporated in this study to meet a real world financial scenario. in the proposed work, the pas problem is formulated in a multiobjective framework, and solved using the multiobjective bacteria foraging optimization (mobfo) algorithm. the performance of the proposed approach is compared with a set of competitive multiobjective evolutionary algorithms using six performance metrics, the pareto front and computational time. on examining the performance metrics, it is concluded that the proposed mobfo algorithm is capable of identifying a good pareto solution, maintaining adequate diversity. the proposed algorithm is also successfully applied to different cardinality constraint conditions, for six different market indices. © 2013 springer-verlag berlin heidelberg."
"the 2014 special issue of concurrency and computation: practice and experience presents papers from the xsede13 conference held in san diego, july 2013. the conference is an annual gathering of the extended community of those interested in advancing research cyber infrastructure (ci) and digital resources. one paper looked at ecc available on many of the modern graphics processing units (gpus) used in hpc machines. it looks at the penalty involved in utilizing ecc and compares molecular dynamics simulation results regarding ecc events triggered during such simulations. making campus bridging work for researchers proposes utilization of campus bridging experts to make this transition easier while requiring minimal investment from the organizing body. the paper on computational finance is looking into the impact of high frequency trading on the stock market. it discusses how the simulation, for this research, was speeded up by two orders of magnitude on xsede hpc resources."
"both mathematical characteristics and computational aspects of dynamic optimization in finance have potential for extensions. various proposed extensions are presented in this paper for dynamic optimization modelling in finance, adapted from developments in other areas of economics and mathematics. they show the need and potential for further areas of study and extensions in financial modelling. the extensions discussed and made concern (a) incorporation of the elements of a dynamic optimization model, (b) an improved model including physical capital, (c) some computational experiments. these extensions make dynamic financial optimisation relatively more organized, coherent and coordinated. these extensions are relevant for applications of financial models to academic and practical exercises. this paper reports initial efforts in providing some useful extensions; further work is necessary to complete the research agenda."
"mining contrast sequential patterns, which are sequential patterns that characterize a given sequence class and distinguish that class from another given sequence class, has a wide range of applications including medical informatics, computational finance and consumer behavior analysis. in previous studies on contrast sequential pattern mining, each element in a sequence is a single item or symbol. this paper considers a more general case where each element in a sequence is a set of items. the associated contrast sequential patterns will be called itemsetbased distinguishing sequential patterns (itemset-dsp). after discussing the challenges on mining itemset-dsp, we present idsp-miner, a mining method with various pruning techniques, for mining itemset-dsps that satisfy given support and gap constraint. in this study, we also propose a concise border-like representation (with exclusive bounds) for sets of similar itemset-dsps and use that representation to improve efficiency of our proposed algorithm. our empirical study using both real data and synthetic data demonstrates that idsp-miner is effective and efficient. © 2015, springer international publishing switzerland, all rights reserved."
the proceedings contain 125 papers. the special focus in this conference is on civil engineering and material engineering. the topics include: experimental study on the solidification of mswi fly ash; analyses of nugget and expulsion formation process during resistance spot welding between aluminum alloy and steel; research and development of high grade gear steel scm420h for automobile; study on hydraulic characteristics of opposite folded plate reactor; reconstruction of key parameters of marine supercharged boiler based on pls-svm; numerical study of vertical axis wind turbine rotor configuration; parameter analysis and shaking table test based on mechanics analysis in seismic isolation system of transformer with bushings; computational fluid dynamics simulation optimization research based on hydraulic torque converter; experimental study on workability and strength of green high performance concrete with high volume fly ash; experimental study on mechanical properties of steel fiber reinforced high performance concrete; calculation methods of cfrp tendons stress in two-span prestressed continuous beams; research on the wear resistance of high-chromium white cast iron and multi-component white cast iron; research on the shear strength of high-strength concrete beams with web bars by concentrated load; numerical solution of vehicle-bridge coupling vibration; research on shaking force with ground-roll suppression based on fast discrete curvelet transform; the study on used properties of mine tailings sand; finite element analysis of the subsidence of cap rocks during underground coal gasification process; seismic performance of reinforced concrete rectangular hollow bridge piers; optimal design of double sheet piles in deep foundation pit based on ud-svm; fe modeling of elliptical concrete-filled steel tubular members subjected to pure bending; interface structure of eps and pores effect on properties of eps lightweight concrete; research on tensile bearing capacity of self-drilling screw joints for thick faced roof sandwich panels; study on the static and dynamic load test of skew bridge; research on monitoring and control for suspender cable tension of half-through concrete filled steel tube arch bridge; study on the load test of variable cross-section box girder bridge; acoustic detection on analysis of rock mass integrality; the mechanical mechanism analysis for mortar arch framework slope protection structure; study on the static load test of bridge inspection; analysis of influence to primary support forces by stress release in loess tunnel; plastic analysis on buckling restrained braced steel frame; modal parameter identification of civil engineering structures under seismic excitations; structural modal parameter identification based on natural excitation technique; the test of dynamic characteristics of concrete under biaxial compression state; analysis of the tunnel disease considering the unsaturated loess matric suction effects; a discussion on numerical simulation of axial reinforcements in rc and rcft columns; research on civil engineering with constructing eco-village development model in shaanxi province; research on civil engineering with ecological impact assessment of construction activities; study progress on modern green high performance concrete at domestic and foreign; application of dynamic load test in the bridge detection; the stability control of soft soil foundations of urban roads according to the surface subsidence monitoring data; the application and development of ultra-high-performance concrete in bridge engineering; comparison analysis of gfrp anchor pile performance in expansive soil foundation; reservoir volume estimation in exploration phase by monte carlo simulation; modal analysis on steel frame-energy dissipation support structure; a numerical infiltration model in civil engineering and its comparison with geo-seep software; research on pedestrian crossing characteristics and crossing facilities of university's passageway; analysis on energy conservation materials' influence on energy saving construction cost; study on application of monte carlo simulation in construction schedule management; an urgent slope reinforcement for a power transmission tower foundation; influence analysis of ground asymmetric load to existing open trench tunnel based on soil arching effect; study on synchronized grouting pressure to segments of shallow epb shield tunnel; theoretical and experimental research on load transfer of anchorage stress in soil anchor; application of svm to reliability analysis of foundation excavations system; preparation and characterization of la-b co-doped tio2 photocatalyst; the influencing research for acid pollution to laterite intensity; an efficient synthesis of an akt inhibitor-iv analog labeled with biotin; a practical synthesis of 4-azaindole; removal of nonylphenol from water by ozone; experimental study on high salt acid pretreatment of refinery wastewater; study on applied technology with soft measurement for lance position based on mlr and pcr; research on applied technology and information technology in news publishing and dissemination based on information age; new tabu search algorithm with applied technology for capacitated vehicle routing problem; applied technology in analysis of status quo of academic theses duplicate checking of china and the treatment countermeasures; research on web application technology in distance education personalized recommendation system; applied technology in business credit in the enterprise's short-term finance; the control system of automated stereoscopic warehouse with application of web information technology; cloud computing technology in arts integrated experiment teaching resource management; applied technology in a composite and definable saas billing mechanism supporting optimized utilization of resource; applied technology in the information management system for the degree of master of arts education in china; applied technology on artificial neural network in fault diagnosis system; applied technology in robust optimization model for airport flow allocation; a study on applied technology in safety and protection of the android os; research of classification retrieval technology in remote video communication network; research on information technology with moving object segmentation based on memory matrix and kalman filter; research on applied technology in human action recognition based on skeleton information; applied technology in the performance of university computer labs; applied technology in design of random human information interactive system based on android; comparison on applied technology in web application in chinese and american education; a brief study on web application technology in autonomous learning of learners; information technology in e-commerce capabilities and firm performance; applied technology in the grid workflow quality of service calculation and estimation; applied technology in design of smart terminal for new energy micro grid of island; the electronic transports technology in a t-shaped double quantum dot; information technology in the e-commerce marketing strategy of oaway website; study on influence of information technology on reform practice of public hospitals in china; research of real estate appraisal based on gis technology; development of mobile e-commerce based on 3g mobile and rfid technology; design and implementation of the safety monitoring system with information technology for pork product; research on information technology with detecting the fraudulent clicks using classification method; comparison studies on prediction methods of network information system survivability situation; the optimal information rates of the graph access structures on seven participants; interaction motivation driven virtual human emotion modeling in computer engineering and application and influence of web application technology on the public hospital reform in china.
"computational intelligence, a sub-branch of artificial intelligence, is a field which draws on the natural world and adaptive mechanisms in order to study behaviour in changing complex environments. this book provides an interdisciplinary view of current technological advances and challenges concerning the application of computational intelligence techniques to financial time-series forecasting, trading and investment. the book is divided into five parts. the first part introduces the most important computational intelligence and financial trading concepts, while also presenting the most important methodologies from these different domains. the second part is devoted to the application of traditional computational intelligence techniques to the fields of financial forecasting and trading, and the third part explores the applications of artificial neural networks in these domains. the fourth part delves into novel evolutionary-based hybrid methodologies for trading and portfolio management, while the fifth part presents the applications of advanced computational intelligence modelling techniques in financial forecasting and trading. this volume will be useful for graduate and postgraduate students of finance, computational finance, financial engineering and computer science. practitioners, traders and financial analysts will also benefit from this book. © 2014 selection and editorial matter, christian dunis, spiros likothanassis, andreas karathanasopoulos, georgios sermpinis and konstantinos theofilatos. all rights reserved."
"stochastic process models play a prominent role in a range of application areas, including biology, chemistry, epidemiology, mechanics, microelectronics, economics, and finance. in mathematical modeling, if we use stochastic systems then we will assume that the system follows a probabilistic rule and the future behavior of the system will not be known for sure. idea of modeling chemical reactions in terms of ordinary and stochastic differential equations can be exposed to a range of modern ideas in applied and computational mathematics. in this paper, we will introduce some fundamental concepts of stochastic processes and simulate them with r saftware. also, we peresent a numerical solution of chemical langevin equation as a stochastic differential system with applications in chemistry and physics."
"we introduce a numerically efficient simulation algorithm for hawkes process with exponentially decaying intensity, a special case of general hawkes process that is most widely implemented in practice. this computational method is able to exactly generate the point process and intensity process, by sampling interarrival-times directly via the underlying analytic distribution functions without numerical inverse, and hence avoids simulating intensity paths and introducing discretisation bias. moreover, it is flexible to generate points with either stationary or non-stationary intensity, starting from any arbitrary time with any arbitrary initial intensity. it is also straightforward to implement, and can easily extend to multi-dimensional versions, for further applications in modelling contagion risk or clustering arrival of events in finance, insurance, economics and many other fields. simulation algorithms for one dimension and multi-dimension are represented, with numerical examples of univariate and bivariate processes provided as illustrations."
"partial differential equation (pde) models are commonly used to model complex dynamic systems in applied sciences such as biology and finance. the forms of these pde models are usually proposed by experts based on their prior knowledge and understanding of the dynamic system. parameters in pde models often have interesting scientific interpretations, but their values are often unknown and need to be estimated from the measurements of the dynamic system in the presence of measurement errors. most pdes used in practice have no analytic solutions, and can only be solved with numerical methods. currently, methods for estimating pde parameters require repeatedly solving pdes numerically under thousands of candidate parameter values, and thus the computational load is high. in this article, we propose two methods to estimate parameters in pde models: a parameter cascading method and a bayesian approach. in both methods, the underlying dynamic process modeled with the pde model is represented via basis function expansion. for the parameter cascading method, we develop two nested levels of optimization to estimate the pde parameters. for the bayesian method, we develop a joint model for data and the pde and develop a novel hierarchical model allowing us to employ markov chain monte carlo (mcmc) techniques to make posterior inference. simulation studies show that the bayesian method and parameter cascading method are comparable, and both outperform other available methods in terms of estimation accuracy. the two methods are demonstrated by estimating parameters in a pde model from long-range infrared light detection and ranging data. supplementary materials for this article are available online. © 2013 american statistical association."
"quantitative finance: an object-oriented approach in c++ provides readers with a foundation in the key methods and models of quantitative finance. keeping the material as self-contained as possible, the author introduces computational finance with a focus on practical implementation in c++. through an approach based on c++ classes and templates, the text highlights the basic principles common to various methods and models while the algorithmic implementation guides readers to a more thorough, hands-on understanding. by moving beyond a purely theoretical treatment to the actual implementation of the models using c++, readers greatly enhance their career opportunities in the field. the book also helps readers implement models in a trading or research environment. it presents recipes and extensible code building blocks for some of the most widespread methods in risk management and option pricing. web resource the author's website provides fully functional c++ code, including additional c++ source files and examples. although the code is used to illustrate concepts (not as a finished software product), it nevertheless compiles, runs, and deals with full, rather than toy, problems. the website also includes a suite of practical exercises for each chapter covering a range of difficulty levels and problem complexity. © 2014 by taylor & francis group, llc. all rights reserved."
"online portfolio selection is a fundamental problem in computational finance, which has been extensively studied across several research communities, including finance, statistics, artificial intelligence, machine learning, and data mining. this article aims to provide a comprehensive survey and a structural understanding of online portfolio selection techniques published in the literature. from an onlinemachine learning perspective, we first formulate online portfolio selection as a sequential decision problem, and then we survey a variety of state-of-the-art approaches, which are grouped into several major categories, including benchmarks, follow-the-winner approaches, follow-the-loser approaches, pattern-matching-based approaches, and meta-learning algorithms. in addition to the problem formulation and related algorithms, we also discuss the relationship of these algorithms with the capital growth theory so as to better understand the similarities and differences of their underlying trading ideas. this article aims to provide a timely and comprehensive survey for both machine learning and data mining researchers in academia and quantitative portfolio managers in the financial industry to help them understand the state of the art and facilitate their research and practical applications. we also discuss some open issues and evaluate some emerging new trends for future research. © 2014 acm."
the proceedings contain 228 papers. the special focus in this conference is on bionic mechatronics and information technology. the topics include: two-camera phase measuring profilometry system; detection device construction and analysis of urinary sediment; research on the multi-parameter modeling of submarine sediment prediction; urinary sediment detection device design and test; the research and exploration of impurity removal by laser sensor in tobacco production; overview of micro-force sensing methods; an overhead conductor weighing sensor based on fiber bragg grating; design of lower computer system in wireless sensor network for monitoring of sf6 moisture content; study of the transducer fault diagnosis module for fire alarm system; a new approach for solving cluster head uneven distribution in wireless sensor network; detection technology of electronic tongue in the liquior quality; a review of the research in measurement error models; application of computer vision technology in agricultural field; fault diagnosis of induction motors based on rbf neural network; a grating generator for 3d scanner using cpld; the real-time monitoring technology research of deep foundation pit displacement; design of light intensity detection system based on stm32; an ant-colony routing algorithm for wireless sensor network; design on safety monitoring system of oilcan during transportation; a simple and effective method for traffic flow density detection; hot water billing system based on smart ic card; the research and application of power grid transmission interface dynamic capacity-increase technology; sorting 4dct images using locally linear embedding; film defects of lithium battery recognition based on brightness and one-against-all support vector machine; anti-jamming performance simulation of pn-lfm combined ranging system; a switch on/off relaying scheme in tdmb cooperative system; weld pool weld width prediction based on artificial neural network; research on cerebellar contributions to speech acquisition and production based on diva model; parameter selection of svr based on improved k-fold cross validation; a novel llr-bp algorithm for ldpc codes based on taylor series and least squares; user plane for volte; state evaluation of diesel engine using genetic algorithm for the feature selection and optimize; research summary of dispatching system based on multi-agent technology; the application of multiple classifier system for environmental audio classification; blind recognition algorithm of ttandc signals of satellite based on fast-ica algorithm; analysis of markov models; the blurring and restoring process of the moving object images; 2d-3d medical image registration based on ant colony algorithm; design of vehicle monitoring software based on video; signal processing based on mathematical morphology; a new spiht image coder based on fast lifting wavelet transform; an equipment fault sound location system design; a new preprocessing algorithm of hand vein image; fast fuzzy search for mixed data using locality sensitive hashing; evaluation methods for medical endoscope's characteristics; image restoration with lucy-richardson algorithm; fusion of sar image using stationary contourlet transform; the fault diagnosis algorithm of pls-lssvm process based on base vector space; estimation of salt-pepper noise in images with correlation inspection; research on big data management for high-speed railway equipment; exploiting saliency filters and domain knowledge for saliency estimation; a fast implementation of dpm-based facial landmark localization; a background subtraction method for defect detection of printed image; vibration analysis of different micro-beams with laser ablation; a retrieval method of ancient chinese character images; the payment solutions for campus card based on improved set protocol; webrtc-based video communication application; proxy-based security-feedback trust model in mp2p network; test of the run-time infrastructure software; design and realization of music player based on android system; design on hevc streaming media player based for android; design and implementation of a cache system in web search engines; a method of numbered stl file format for recording data; study of the establishment of a local fitness network system platform; architecture model of fms based on multi-agent technology; optimal dispatching methods for unit commitment based on hybrid genetic-simulated annealing algorithm; a middle-ranking cadre of universities evaluation management information system based on soa; an improved anti-collision algorithm based on binary in rfid system; evaluation fitness of footwear using hybrid simulation; a 18.4m triangles/s 122.6 mw tile co-processor for embedded gpu systems; anomaly detection based on chi-square statistic technology in computer information system; architecture for vertex transformation and triangle clipping in 3d graphics; improvements and implementation of integrated in-vehicle monitoring system; primary investigation of networking industry's development; design and implementation of data migration on housing fund archives system; design of mobile learning system based on cloud computing; design of ecg acquisition system based on matlab; improved congestion control algorithm in wide bandwidth and long delay network; anaglyph 3d display based on a computational stereo system; a non-repudiation e-mail transmission protocol and its formal analysis; based on the experimental platform of litchi picking vision calibration; the financial management system based on android and sqlite; the research on the adaptive network english learning of guizhou university; analysis software application status of enterprise human resource management; an algorithm to update k-dominant skyline; the loan pricing study of supply chain finance based on cloud muster warehouse (cmw); development of graphic low voltage line loss calculation and load optimized strategy software; the nac system in the smart grid systems; mapping a real-time system graphics design model to windows ce; a platform for agent-based hybrid intelligent systems; the existing problems and countermeasures in the development of accounting informatization in china; research on the choice of quality signal of b2c enterprise under asymmetric information; multiple-input single-output wireless power transmission system for coal mine application; research on the equivalence of pulsed interference signal and cw signal in dsss communication system; study of cdr real-time query based on big data technologies; research on mobile business value network and model construction; automatic white balance based on gray world method and retinex; improve dynamic performance on high speed by application of electromagnetic valve actuation; research of garlic umbilical (root) cutting machine based on image processing; research on backlash nonlinearity in servo precision drive system; a design of electric control and plc system for nc milling machine based on sinumerik 802d solution line; vehicle power grid voltage's influence on anti-disturbance ability of maglev train; a control architecture for mission re-planning and plan repair of auv; study on chaotic operation and control system of ultrasonic motor; research on dc motor intelligent control algorithm; path planning for nonholonomic mobile robot in dynamic environment; position synchronization of the biaxial system with a pid neural networks control; review of odor source localization robot based on bionic olfaction; a dds-based interoperation framework of large-scale simulation system; design of dynamic vision system for modular reconfigurable robots; design of switched reluctance motor speed control system base on dsp; ldpc codes with the layered llr-bp algorithm for 3gpp; the application of network communication tools in information service of university library; chaotic communication based on chaotic pilot signal for synchronization control; cooperative retransmission based on network coding with fourier matrix for wireless ad hoc networks; 1-4 power splitter based on cascaded multimode interference photonic crystal waveguide; video-conferencing-device to engineering; the study on led accelerated life testes and failure mechanism and intelligent and compositive protector for motor based on avr single-chip microcomputer.
"financial forecasting is a vital area in computational finance. this importance is reflected in the literature by the continuous development of new algorithms. eddie is well-established genetic programming financial forecasting tool, which has successfully been applied to a variety of international datasets. recently, we introduced hyper-heuristics to eddie. this was the first time in the literature that hyper-heuristics were used for financial forecasting. results showed that this introduction significantly benefited the performance of the algorithm. however, an issue was encountered in the way that low-level heuristics were selected during the search process, because it was considered to be a static way. to address this issue, in this paper we further improve our algorithm by introducing a choice function, which is a score based technique that offers a more dynamic selection of the low-level heuristics. this paper presents preliminary results, after having tested the choice function approach with 10 datasets. these results show that the introduction of the choice function is beneficial to eddie, thus making it a very promising tool for future investigation on financial forecasting problems. © 2013 ieee."
"we develop a cross-market simulation trading platform of stock and stock index future on the basis of mason system with agent-base computational finance method. we analyze the impact of the number of arbitragers on stock index future market volatility using 5-second high frequent data. we find that too many or too few arbitragers will both make market uctuate, so keeping appropriate number of arbitragers is very important for reducing market volatility and price discovery. regulatory agencies should strengthen the effciency and quality of supervision, to ensure reasonable investor structure, and keep the diversity of the types of investors."
"the purpose of this paper is to evaluate the performance of vector evaluated differential evolution (vede) and vector evaluated particle swarm optimization (vepso) in solving a real world financial optimization problem. the algorithms have been applied to the reinsurance contract problem, which is a challenging problem in computational finance, and their performance has been evaluated in terms of metrics including the average number of solutions, the average hypervolume and the coverage. results have shown that both algorithms can reach good solutions, however vepso tends to perform better. © 2014 ieee."
"this essay focuses on the investor structure of the stock index futures market and uses agent-based computational finance method to discuss how the volume-synchronized probability of informed trading (vpin) affects market absolute yield, information dissemination efficiency, and liquidity with different ratios of informed traders in the market. the result shows that the higher the proportion of informed traders is, the more the volatility of the market is. furthermore, the result indicates that when the proportion of informed traders in the stock index futures market accounts for 1/3-1/2, the transparency and liquidity of the market will be better. © 2014 hongli che et al."
"unlike most online social networks where explicit links among individual users are defined, the relations among commercial entities (e.g. firms) may not be explicitly declared in commercial web sites. one main contribution of this article is the development of a novel computational model for the discovery of the latent relations among commercial entities from online financial news. more specifically, a crf model which can exploit both structural and contextual features is applied to commercial entity recognition. in addition, a point-wise mutual information (pmi)-based unsupervised learning method is developed for commercial relation identification. to evaluate the effectiveness of the proposed computational methods, a prototype system called conet has been developed. based on the financial news articles crawled from google finance, the conet system achieves average f-scores of 0.681 and 0.754 in commercial entity recognition and commercial relation identification, respectively. our experimental results confirm that the proposed shallow natural language processing methods are effective for the discovery of latent commercial networks from online financial news. © 2013 copyright taylor and francis group, llc."
"since, with increased volatility and further uncertainties, financial crises translated a high ""noise"" within data from financial markets and economies into the related models, recent years' events in the financial world have led to radically untrustworthy representations of the future. hence, robustification started to attract more attention in finance. the presence of noise and data uncertainty raises critical problems to be dealt with on the theoretical and computational side. for immunizing against parametric uncertainties, robust optimization has gained greatly in importance as a modeling framework from both a theoretical and a practical point of view. consequently, we include the existence of uncertainty considering future scenarios in the multivariate adaptive regression spline (mars) that has an apparent success in modeling real-life data in a variety of application fields, and robustify it through robust optimization proposed to cope with data and resulting model parameter uncertainty. we represent the new robust mars (rmars) in theory and method and apply rmars on financial market data. we demonstrate its good performance with a simulation study and a numerical experience that refers to basic economic indicators. results indicate that models from rmars have much less variability in parameter estimates and in accuracy measures, to the cost of just a slightly lower accuracy than mars. © 2013 elsevier b.v. all rights reserved."
"in this paper, we present a novel methodology for stock investment using episode mining and technical indicators. the time-series data of stock price and the derived moving average, a class of well-known technical indicators, are used for the construction of complex episode events and rules. our objective is to devise a profitable episodebased investment model to mine associated events in the stock market. using taiwan capitalization weighted stock index (taiex), the empirical results show that our proposed model significantly outperforms the benchmark in terms of cumulative total returns. we also show that the level of the precision by our model is close to 60%, which is better than random guessing. based upon the results obtained, we expect this novel episodebased methodology will advance the research in data mining for computational finance and provide an alternative to stock investment in practice."
"we present a new portfolio default model based on a conditionally independent and identically distributed (ciid) structure of the default times. it combines an intensity-based ansatz in the spirit of duffie and gârleanu (2001). risk and valuation of collateralized debt obligations. financial analysts journal, 57(1), 41-59. with the lévy subordinator concept introduced in mai and scherer (2009). a tractable multivariate default model based on a stochastic time-change. international journal of theoretical and applied finance, 12(2), 227-249. we aim at exploiting the computational advantages of the ciid framework for evaluating multiname credit derivatives, while incorporating two central drivers for credit products. more precisely, we allow for both a dynamic evolution of the portfolio credit default swap (cds) spread (unlike static copula models) and cataclysmic events allowing for simultaneous defaults (unlike intensity-based portfolio loss processes). while the former feature is considered to be crucial for consistently hedging credit products, the second property is supposed to take into account default clusters and the market's fear of extreme events. for applications, the model is approximated by a related top-down representation of the portfolio loss process. it is shown how to coherently calculate hedging deltas for collateralized debt obligations (cdos) w.r.t. portfolio cds and how to consistently calibrate the model to the two products. both tasks solely require the computation of one-dimensional (laplace inversion) integrals and can be carried out within fractions of a second. illustrating the stability and functionality of the pricing approach, the new model and the models it is related to are calibrated to a daily time-series of itraxx europe index cds and cdos. we find the fitting results of the presented model to be very promising and conclude that it may be used for the dynamic pricing and hedging of credit derivatives. © 2013 © taylor & francis."
"ledermann et al. (2011) propose random orthogonal matrix (rom) simulation for generating multivariate samples matching means and covariances exactly. its computational efficiency compared to standard monte carlo methods makes it an interesting alternative. in this paper we enhance this method[u+05f3]s attractiveness by focusing on applications in finance. many financial applications require simulated asset returns to be free of arbitrage opportunities. we analytically derive no-arbitrage bounds for expected excess returns to be used in the context of rom simulation, and we establish the theoretical relation between the number of states (i.e., the sample size) and the size of (no-)arbitrage regions. based on these results, we present a no-arbitrage rom simulation algorithm, which generates arbitrage-free random samples by purposefully rotating a simplex. hence, the proposed algorithm completely avoids any need for checking samples for arbitrage. compared to the alternative of (potentially frequent) re-sampling followed by arbitrage checks, it is considerably more efficient. as a by-product, we provide interesting geometrical insights into affine transformations associated with the no-arbitrage rom simulation algorithm. © 2014 elsevier b.v."
"finance-related problems require more and more computations; therefore,the problem of finding efficient implementations for option pricing models on modern architectures has become an important challenge. although there are numerous implementations of the monte carlo method on central processing units,many of them face limitations arising from the necessary increased computational power. in this paper, we have implemented the monte carlo approach to option pricing using the compute unified device architecture and its optimizationsolutions."
the proceedings contain 21 papers. the topics discussed include: price variation limits and financial market bubbles: artificial market simulations with agents' learning process; analysis on the number of xcs agents in agent-based computational finance; empirical analysis of liquidity provision of an order driven market; a study of dark pool trading using an agent-based model; portfolio optimization using improved artificial bee colony approach; a new approach for time series prediction using ensembles of anfis models with interval type-2 and type-1 fuzzy integrators; cluster analysis of high-dimensional high-frequency financial time series; simplified evolving rule-based fuzzy modeling of realized volatility forecasting with jumps; and empirical analysis of model selection criteria for genetic programming in modeling of time series system.
"the bayesian analysis of a state-space model includes computing the posterior distribution of the system's parameters as well as its latent states. when the latent states wander around rn there are several well-known modeling components and computational tools that may be profitably combined to achieve this task. when the latent states are constrained to a strict subset of rn these models and tools are either impaired or break down completely. state-space models whose latent states are covariance matrices arise in finance and exemplify the challenge of devising tractable models in the constrained setting. to that end, we present a state-space model whose observations and latent states take values on the manifold of symmetric positive-definite matrices and for which one may easily compute the posterior distribution of the latent states and the system's parameters as well as filtered distributions and one-step ahead predictions. employing the model within the context of finance, we show how one can use realized covariance matrices as data to predict latent time-varying covariance matrices. this approach out-performs factor stochastic volatility. © 2014 international society for bayesian analysis."
"in this work we propose a highly optimized version of a simulated annealing (sa) algorithm adapted to the more recently developed graphic processor units (gpus). the programming has been carried out with compute unified device architecture (cuda) toolkit, specially designed for nvidia gpus. for this purpose, efficient versions of sa have been first analyzed and adapted to gpus. thus, an appropriate sequential sa algorithm has been developed as starting point. next, a straightforward asynchronous parallel version has been implemented and then a specific and more efficient synchronous version has been developed. a wide appropriate benchmark to illustrate the performance properties of the implementation has been considered. among all tests, a classical sample problem provided by the minimization of the normalized schwefel function has been selected to compare the behavior of the sequential, asynchronous and synchronous versions, the last one being more advantageous in terms of balance between convergence, accuracy and computational cost. also the implementation of a hybrid method combining sa with a local minimizer method has been developed. note that the generic feature of the sa algorithm allows its application in a wide set of real problems arising in a large variety of fields, such as biology, physics, engineering, finance and industrial processes. © 2012 springer science+business media, llc."
"we propose a computationally efficient fully polynomial-time approximation scheme (fptas) for convex stochastic dynamic programs using the technique of k-approximation sets and functions introduced by halman et al. this paper deals with the convex case only, and it has the following contributions: first, we improve on the worst-case running time given by halman et al. second, we design an fptas with excellent computational performance, and show that it is faster than an exact algorithm even for small problem instances and small approximation factors, becoming orders of magnitude faster as the problem size increases. third, we show that with careful algorithm design, the errors introduced by floating point computations can be bounded, so that we can provide a guarantee on the approximation factor over an exact infinite-precision solution. our computational evaluation is based on randomly generated problem instances coming from applications in supply chain management and finance. © 2013 springer-verlag."
"using the agent-based computational finance (acf) method, we build an artificial stock market with heterogeneous adaptive investors and investigate the evolutionary and interacting relationship between rational investors and irrational investors. we find that with strategy switching, there is a symbiosis among the three kinds of investors in the acf experiments, although the rational investors are often dominant in the market. and our main findings are robust with agents' scale. when the initial values change, the market ecology achieves new equilibrium. © 2014 world scientific publishing company."
"this article surveys the recent developments in computational methods for second order fully nonlinear partial differential equations (pdes), a relatively new subarea within numerical pdes. due to their ever increasing importance in mathematics itself (e.g., differential geometry and pdes) and in many scientific and engineering fields (e.g., astrophysics, geostrophic fluid dynamics, grid generation, image processing, optimal transport, meteorology, mathematical finance, and optimal control), numerical solutions to fully nonlinear second order pdes have garnered a great deal of interest from the numerical pde and scientific communities. significant progress has been made for this class of problems in the past few years, but many problems still remain open. this article intends to introduce these current advancements and new results to the siam community and generate more interest in numerical methods for fully nonlinear pdes. © 2013 society for industrial and applied mathematics."
the internet provides the opportunity for investors to post online opinions that they share with fellow investors. sentiment analysis of online opinion posts can facilitate both investors' investment decision making and stock companies' risk perception. this paper develops a novel sentiment ontology to conduct context-sensitive sentiment analysis of online opinion posts in stock markets. the methodology integrates popular sentiment analysis into machine learning approaches based on support vector machine and generalized autoregressive conditional heteroskedasticity modeling. a typical financial website called sina finance has been selected as an experimental platform where a corpus of financial review data was collected. empirical results suggest solid correlations between stock price volatility trends and stock forum sentiment. computational results show that the statistical machine learning approach has a higher classification accuracy than that of the semantic approach. results also imply that investor sentiment has a particularly strong effect for value stocks relative to growth stocks. © 2014 ieee.
"since its modest inception as the statistical laboratory in 1947, the department of statistics, purdue university has grown to one of the largest and most diverse in the country supported by a distinguished list of alumni, outstanding contributions in research, and major advances in statistical education. its current (2011) size of 62 faculty, 125 graduate students, and almost 400 undergraduate students reflects its commitment to developing statistical sciences research for the present and the future, and to providing high quality education to students, both in statistics and in other disciplines. historically, the department of statistics at purdue university has been an important center for diverse areas of statistical research. its strong presence in probability, theory, and education set the stage for its expansion in the mid-1990s. as the field of statistics expanded to include many interdisciplinary areas that require specialization (statistical genetics and bioinformatics, computational finance, machine learning, etc.), purdue statistics engaged in an aggressive program of hiring well-prepared faculty with diverse backgrounds who are playing leading roles in the development of the field as it expands its scope. today purdue statistics stands strong as the highest ranked department in the college of science at purdue university, and is enjoying its place among the top ranked departments in the united states. © 2013 springer science+business media new york. all rights reserved."
"financial forecasting is a vital area in computational finance, where several studies have taken place over the years. one way of viewing financial forecasting is as a classification problem, where the goal is to find a model that represents the predictive relationships between predictor attribute values and class attribute values. in this paper we present a comparative study between two bio-inspired classification algorithms, a genetic programming algorithm especially designed for financial forecasting, and an ant colony optimization one, which is designed for classification problems. in addition, we compare the above algorithms with two other state-of-the-art classification algorithms, namely c4.5 and ripper. results show that the ant colony optimization classification algorithm is very successful, significantly outperforming all other algorithms in the given classification problems, which provides insights for improving the design of specific financial forecasting algorithms. © springer-verlag berlin heidelberg 2014."
"anomaly detection based on spectral features is applicable to a diverse range of problems including prognostic and health management, vibration analysis, astronomy, biomedicai engineering and computational finance. the input data could be regularly sampled, as in the case of a standard analogue to digital converter sampling a bandlimited signal at above the nyquist rate, or irregularly sampled, as in the case of stock quotes or astronomical data. in this paper, we present new online algorithms for the computation of power spectra for regularly or irregularly sampled data, and performing anomaly detection on time series data. both algorithms allow hardware implementations with o(l) time complexity, this being the minimum for any system that considers all the samples. we combine the two algorithms to form a power spectrum-based anomaly detector (sad). we also describe an implementation of sad which has minimal hardware requirements, and achieves one to two orders of magnitude improvement in speed, latency, power and energy over a traditional processor-based design. © 2014 ieee."
"the design presented in this paper is a genetic algorithm (ga) written in vhsic hardware description language (vhdl) and intended for a hardware implementation which is used for real time applications. the hardware implementation exploits the reprogrammability of certain types of field-programmable gate arrays (fpgas) like those from xilinx and provides a good optimization using all kinds of parallelism that allows the exploitation of several promising areas of the solution space at the same time. gas are a family of computational models inspired by evolution allowing the optimization of many problems related to digital solution, econometrics and finance. these algorithms encode a potential solution to a specific problem on a simple chromosome like data structure and apply recombination operators to these structures to preserve critical information. © 2013 ieee."
"an increase in research over the past 60 years in the field of machine learning widened its areas of application from merely making computers learn to play board games to analysis of big data. many algorithms have been developed that are now commonly used in various fields ranging from natural language processing to computational finance and has been brought to use commercially as well. recently, there has been an increase in research on machine learning application in the area of automated video surveillance systems. most of these algorithms assume that both the training data and test data belong to same feature space with same distribution which might not always be true. this constraint gave rise to the concept of transfer learning which uses the knowledge from the preoccupied knowledge from other related task. this paper aims at improving the efficiency of a transfer learning based machine learning technique for object classification, mktl framework. it can be brought to use for multiclass object classification in automated video surveillance systems. © 2014 pes institute of technology, bangalore."
"this book gives a comprehensive introduction to numerical methods and analysis of stochastic processes, random fields and stochastic differential equations, and offers graduate students and researchers powerful tools for understanding uncertainty quantification for risk analysis. coverage includes traditional stochastic odes with white noise forcing, strong and weak approximation, and the multi-level monte carlo method. later chapters apply the theory of random fields to the numerical solution of elliptic pdes with correlated random data, discuss the monte carlo method, and introduce stochastic galerkin finite-element methods. finally, stochastic parabolic pdes are developed. assuming little previous exposure to probability and statistics, theory is developed in tandem with state-of-the-art computational methods through worked examples, exercises, theorems and proofs. the set of matlab codes included (and downloadable) allows readers to perform computations themselves and solve the test problems discussed. practical examples are drawn from finance, mathematical biology, neuroscience, fluid flow modelling and materials science. © gabriel j. lord, catherine e. powell and tony shardlow 2014."
"while the overall performance of buildings has been established to be heavily impacted by design decisions made during the early stages of the design process, design professionals are typically unable to explore design alternatives, or their impact on energy profiles, in a sufficient manner during this phase. the research presents a new design simulation methodology based on incorporating a prototype tool (h.d.s. beagle) that combines parametric modeling with multi-objective optimization through an integrated platform for enabling rapid iteration and trade-off analysis across the domains of design, energy use intensity, and finance. the research evaluates how the proposed method impacts design simulation processes, by either enabling and/or disrupting the early stages of design decision making. this simulation technology is presented through two major experiment sets: (1) a series of hypothetical cases emulating the architecture, engineering, and construction (aec) design modeling and simulation process using our integrated simulation framework and technology; and (2) a pedagogically based experiment used for establishing benchmarks. through these experiment data sets, both quantitative and qualitative data are collected, including human designer and computational analysis speeds, quantity of generated design alternatives, and quality of resulting solution space as defined by the evaluation metric of this research. the affordances for incorporation of real world design complexity into our computational design prototype and simulation methodology are discussed through both the enabling and the disruptive impact on the early stages of the design process. © 2013, the society for modeling and simulation international. all rights reserved."
"the methods addressing volatility in computational finance and econometrics have been recently reported in financial literature. recently peiris et al. [8] have introduced doubly stochastic volatility models with garch innovations. random coefficient autoregressive sequences are special case of doubly stochastic time series. in this paper, we consider doubly stochastic stationary time series with asymmetric garch errors. some general properties of process, like variance and kurtosis are derived."
"financial forecasting is a really important area in computational finance, with numerous works in the literature. this importance can be reflected in the literature by the continuous development of new algorithms. hyper-heuristics have been successfully used in the past for a number of search and optimization problems, and have shown very promising results. to the best of our knowledge, they have not been used for financial forecasting. in this paper we present pioneer work, where we use different hyper-heuristics frameworks to investigate whether we can improve the performance of a financial forecasting tool called eddie 8. eddie 8 allows the gp (genetic programming) to search in the search space of indicators for solutions, instead of using pre-specified ones; as a result, its search area has dramatically increased and sometimes solutions can be missed due to ineffective search. we apply 14 different low-level heuristics to eddie 8, to 30 different datasets, and examine their effect to the algorithm's performance. we then select the most prominent heuristics and combine them into three different hyper-heuristics frameworks. results show that all three frameworks are competitive, and are able to show significantly improved results, especially in the case of best results. lastly, analysis on the weights of the heuristics shows that there can be a constant swinging among some of the low-level heuristics, which denotes that the hyper-heuristics frameworks are able to 'know' the appropriate time to switch from one heuristic to the other, based on their effectiveness. © 2012 springer science+business media b.v."
"we present a framework for obtaining fully polynomial time approximation schemes (fptass) for stochastic univariate dynamic programs with either convex or monotone single-period cost functions. this framework is developed through the establishment of two sets of computational rules, namely, the calculus of k-approximation functions and the calculus of k-approximation sets. using our framework, we provide the first fptass for several np-hard problems in various fields of research such as knapsack models, logistics, operations management, economics, and mathematical finance. extensions of our framework via the use of the newly established computational rules are also discussed. © 2014 society for industrial and applied mathematics."
"as it stands today, the spectrum of methods, tools, and applications that populate the area of computational finance is literally vast. distinctively, it is this vast domain that differentiates today's financial decision makers from their counterparts of just a decade ago. couched within this landscape are a set of increasingly complex resource utilization decisions; decisions that are, today, impacted by a surprising growth in technology that now spans a more globally diverse production and engineering environment. collectively, firm financial managers, portfolio managers, and enterprise risk managers continue to exhort the computational finance community to formulate effective tools that more descriptively reconcile difficult problems in new product development, risk mitigation, and overall enterprise management. the computational finance community has responded to this call by offering refinements to classic computational methods while also introducing new ones. from continuous optimization to natural and evolutionary computing to time-series econometrics, this edition covers contemporary developments in computational finance. the book examines how interdisciplinary contributions from applied mathematics, statistics, and engineering can be adapted to a problem-solving approach in finance with an emphasis on vexing, but identifiable, real-world problems. © 2013 by nova science publishers, inc. all rights reserved."
"financial returns exhibit conditional heteroscedasticity, asymmetric responses of their volatility to negative and positive returns (leverage effects) and fat tails. the α-stable distribution is a natural candidate for capturing the tail-thickness of the conditional distribution of financial returns, while the garch-type models are very popular in depicting the conditional heteroscedasticity and leverage effects. however, practical implementation of α-stable distribution in finance applications has been limited by its estimation difficulties. the performance of the indirect inference approach using garch models with student's t distributed errors as auxiliary models is compared to the maximum likelihood approach for estimating garch-type models with symmetric α-stable innovations. it is shown that the expected efficiency gains of the maximum likelihood approach come at high computational costs compared to the indirect inference method. © 2013 elsevier b.v. all rights reserved."
"strategies for cost reduction in the wind offshore industry: the desire for cost reduction and optimisation of the design of offshore structures supporting wind turbines has to follow the requirement to identify, define and formulate all relevant boundary constraints driving the holistic cost of a design. this paper aims not to just summarize key cost drivers, but also shows examples with real cost saving potential at different stages (design, fabrication, construction, installation) during the development of a wind farm. in order to formulate an optimisation algorithm, the complex context the industry is operating in needs to be understood. however this may be a step to far. the complexity of unknown factors in an emerging and growing wind offshore industry will demonstrate that alongside to computational optimisation and academic problem formulation of individual sub tasks, what will be needed is human intuition, vision, innovation, engineering judgment and the masterminds which influence the outcome of their endeavour in a positive way. firstly, an overview of cost categories is given ranging from material cost to finance cost. secondly examples of risks and design obstacles are mapped, both in order to present the backdrop in front of which various strategies for delivery can be chosen.examples of structural optimisation taking into account some of the key cost drivers are presented at the heart of the paper, showing how different boundary constraints and design approaches will lead to different design solutions. further the influence of the choice of risk profiles and the impact on the design with a view of the different project phases is shown. consequentially the authors attempt to develop new strategies based thereon."
"computational intelligence based techniques have firmly established themselves as viable, alternate, mathematical tools for more than a decade. they have been extensively employed in many systems and application domains, among these signal processing, automatic control, industrial and consumer electronics, robotics, finance, manufacturing systems, electric power systems, and power electronics. image processing is also an extremely potent area which has attracted the attention of many researchers who are interested in the development of new computational intelligence-based techniques and their suitable applications, in both research problems and in real-world problems. part i of the book discusses several image preprocessing algorithms; part ii broadly covers image compression algorithms; part iii demonstrates how computational intelligence-based techniques can be effectively utilized for image analysis purposes; and part iv shows how pattern recognition, classification and clustering-based techniques can be developed for the purpose of image inferencing. the book offers a unified view of the modern computational intelligence techniques required to solve real-world problems and it is suitable as a reference for engineers, researchers and graduate students. © 2013 springer-verlag berlin heidelberg. all rights are reserved."
"modeling dynamical systems represents an important application class covering a wide range of disciplines including but not limited to biology, chemistry, finance, national security, and health care. such applications typically involve large-scale, irregular graph processing, which makes them difficult to scale due to the evolutionary nature of their workload, irregular communication and load imbalance. episimdemics is such an application simulating epidemic diffusion in extremely large and realistic social contact networks. it implements a graph-based system that captures dynamics among co-evolving entities. this paper presents an implementation of episimdemics in charm++ that enables future research by social, biological and computational scientists at unprecedented data and system scales. we present new methods for application-specific processing of graph data and demonstrate the effectiveness of these methods on a cray xe6, specifically ncsa's blue waters system. © 2014 ieee."
"a portfolio rebalancing model with self-finance strategy and consideration of v-shaped transaction cost is presented in this paper. our main contribution is that a new constraint is introduced to confirm that the rebalance necessity of the existing portfolio needs to be adjusted. the constraint is constructed by considering both the transaction amount and transaction cost without any additional supply to the investment amount. the v-shaped transaction cost function is used to calculate the transaction cost of the portfolio, and conditional value at risk (cvar) is used to measure the risk of the portfolios. computational tests on practical financial data show that the proposed model is effective and the rebalanced portfolio increases the expected return of the portfolio and reduces the cvar risk of the portfolio. © 2014 meihua wang et al."
"large-scale observational datasets are prevalent in many areas of research, including biomedical informatics, computational social science, and finance. however, our ability to use these data for decision-making lags behind our ability to collect and mine them. one reason for this is the lack of methods for inferring the causal impact of rare events. in cases such as the monitoring of continuous data streams from intensive care patients, social media, or finance, though, rare events may in fact be the most important ones - signaling critical changes in a patient's status or trading volume. while prior data mining approaches can identify or predict rare events, they cannot determine their impact, and probabilistic causal inference methods fail to handle inference with infrequent events. instead, we develop a new approach to finding the causal impact of rare events that leverages the large amount of data available to infer a model of a system's functioning and evaluates how rare events explain deviations from usual behavior. using simulated data, we evaluate the approach and compare it against others, demonstrating that it can accurately infer the effects of rare events."
"currently, the genetic algorithm (ga) technique has been used in finance-based scheduling to devise critical path method (cpm) schedules exhibiting cash flows of periodical finance needs below preset cash constraints. the chromosomes of the schedules that violate this condition are referred to as finance-infeasible chromosomes. infeasibility related to finance is peculiar to finance-based scheduling problems. in scheduling problems, chromosomes that are infeasible based on precedence relationships are typically penalized. this paper introduces a repair algorithm for the finance-infeasible chromosomes generated within the ga systems. the repair algorithm identifies the periods exhibiting finance needs that exceed the constrained cash, calculates the amounts of finance needs above the constraints, identifies the ongoing activities, selects randomly an activity for delaying its start time, determines the impact of the delay on the finance needs, and repeats the procedure until finance feasibility is attained. a 13-activity project was used to demonstrate the proposed repair algorithm. the performance of the repaired-chromosome ga system is evaluated through comparison against replaced-chromosome and penalized-chromosome ga systems using a fairly big project of 210 activities. finally, the results that were validated using the integer programming technique proved the superior performance of the repaired-chromosome ga in terms of the computational cost and quality of solutions. © 2013 american society of civil engineers."
"in many sequential decision-making problems we may want to manage risk by minimizing some measure of variability in costs in addition to minimizing a standard criterion. conditional value-at-risk (cvar) is a relatively new risk measure that addresses some of the shortcomings of the well-known variance-related risk measures, and because of its computational efficiencies has gained popularity in finance and operations research. in this paper, we consider the mean-cvar optimization problem in mdps. we first derive a formula for computing the gradient of this risk-sensitive objective function. we then devise policy gradient and actor-critic algorithms that each uses a specific method to estimate this gradient and updates the policy parameters in the descent direction. we establish the convergence of our algorithms to locally risk-sensitive optimal policies. finally, we demonstrate the usefulness of our algorithms in an optimal stopping problem."
"the efficient processing of large collections of patterns expressed as boolean expressions over event streams plays a central role in major data intensive applications ranging from user-centric processing and personalization to real-time data analysis. on the one hand, emerging user-centric applications, including computational advertising and selective information dissemination, demand determining and presenting to an end-user the relevant content as it is published. on the other hand, applications in real-time data analysis, including push-based multi-query optimization, computational finance and intrusion detection, demand meeting stringent subsecond processing requirements and providing high-frequency event processing. we achieve these event processing requirements by exploiting the shift towards multi-core architectures by proposing novel adaptive parallel compressed event matching algorithm (a-pcm) and online event stream re-ordering technique (osr) that unleash an unprecedented degree of parallelism amenable for highly parallel event processing. in our comprehensive evaluation, we demonstrate the efficiency of our proposed techniques. we show that the adaptive parallel compressed event matching algorithm can sustain an event rate of up to 233,863 events/second while state-of-the-art sequential event matching algorithms sustains only 36 events/second when processing up to five million boolean expressions. © 2014 ieee."
"stock analyst's report is among of several important information sources for making investment decisions, as it contains relevant information about stocks as well as recommendation where investors should buy or sell the stock together with entry and exit strategies. good analysts should often make trustworthy recommendations so that traders following them can make regularly profits from their advices. nevertheless, identifying good analysts is not a trivial task especially when processed manually. particularly, one has to collect and extract strategies from unstructured texts appearing in analyst reports, backtest such strategies with historical market data, and summarize backtested results by overall profits and losses. to address these problems, we propose a unified system which makes use of a combination of information integration and computational finance techniques to automate all these tasks. our system performs considerably well in extracting recommendations from various analysts' reports and provides new valuable information to traders. the system has been made available online as a mobile application for community use. © springer international publishing switzerland 2014."
"financial exploitation of forests comprises an important part of man activity. there are efforts being made to conserve the sustainable exploitation while simultaneously avoiding degradation of the environment. one tool used in these efforts is the modeling of tree features, such as total tree height, sawn-timber tree height, merchantable tree height, and total or sawn-timber tree volume, which yields an estimate of the forest in finance recoverable goods. sustainable forest management design must be supported by the adjustment of computational techniques. the purpose of this paper is to assess a reliable modeling approach for estimating individual tree heights for the maturity of trees for logging through determining the applicability of different types of neural network models and identifying a neural network procedure for accurate estimation of these variables. these models serve as an alternative to the traditional regression approach. all types of model estimations are evaluated and compared in this paper. back propagation artificial neural network (bpann), cascade correlation artificial neural network (ccann), and generalized regression neural network (grnn) models are developed to estimate individual tree heights for the logging of mature trees, such as sawn-timber height and merchantable height. the results reported in this research suggest that the selected bpann and ccann models are reliable and demonstrate their adequacy and potential for estimating sawn-timber and merchantable tree height. the results also illustrate that the ccann models are superior to the bpann and grnn models and lead to higher estimation accuracy. moreover, the nn models were found to be superior to the tested nonlinear regression models. © 2012 elsevier inc."
"modern compute systems continue to evolve towards increasingly complex, heterogeneous and distributed architectures. at the same time, functionality and performance are no longer the only aspects when developing applications for such systems, and additional concerns such as flexibility, power efficiency, resource usage, reliability and cost are becoming increasingly important. this does not only raise the question of how to efficiently develop applications for such systems, but also how to cope with dynamic changes in the application behaviour or the system environment. the epics project aims to address these aspects through exploring self-awareness and self-expression. self-awareness allows systems and applications to gather and maintain information about their current state and environment, and reason about their behaviour. self-expression enables systems to adapt their behaviour autonomously to changing conditions. innovations in epics are based on systematic integration of research in concepts and foundations, customisable hardware/software platforms and operating systems, and self-aware networking and middleware infrastructure. the developed technologies are validated in three application domains: computational finance, distributed smart cameras and interactive mobile media systems. © 2012 ieee."
"multivariate discrete response data can be found in diverse fields, including econometrics, finance, biometrics, and psychometrics. our contribution, through this study, is to introduce a new class of models for multivariate discrete data based on pair copula constructions(pccs) that has two major advantages. first, by deriving the conditions under which any multivariate discrete distribution can be decomposed as a pcc, we show that discrete pccs attain highly flexible dependence structures. second, the computational burden of evaluating the likelihood for an m-dimensional discrete pcc only grows quadratically with m. this compares favorably to existing models for which computing the likelihood either requires the evaluation of 2m terms or slow numerical integration methods. we demonstrate the high quality of inference function for margins and maximum likelihood estimates, both under a simulated setting and for an application to a longitudinal discrete dataset on headache severity. this article has online supplementary material. © 2012 american statistical association."
"social media contain a multitude of user opinions which can be used to predict realworld phenomena in many domains including politics, finance and health. most existing methods treat these problems as linear regression, learning to relate word frequencies and other simple features to a known response variable (e.g., voting intention polls or financial indicators). these techniques require very careful filtering of the input texts, as most social media posts are irrelevant to the task. in this paper, we present a novel approach which performs high quality filtering automatically, through modelling not just words but also users, framed as a bilinear model with a sparse regulariser. we also consider the problem of modelling groups of related output variables, using a structured multi-task regularisation method. our experiments on voting intention prediction demonstrate strong performance over large-scale input from twitter on two distinct case studies, outperforming competitive baselines. © 2013 association for computational linguistics."
"tracking expenses is a task performed in homes and businesses worldwide; for personal finances, the practice of organizing receipts for refunds or summarizing its contents for purposes such as budget planning and tax submission, has been recently aided by different services; these allow automatic collection of receipts either at store terminals or using a photo of the receipt submitted by the user, which can be later accessed using an online interface. given the importance of financial information and the inherent danger introduced by these services, we present in this article an architecture based on additive homomorphic cryptosystems and secret sharing schemes to store information securely while still allowing fast aggregation queries at an outsourced untrusted cloud server. the proposal was evaluated in terms of security, server load, amount of user interaction, computational load at the acquiring terminal and computational load at the untrusted server. © springer science+business media, llc 2012."
"the multi-level approach developed by giles (2008) can be used to estimate mean first exit times for stochastic differential equations, which are of interest in finance, physics and chemical kinetics. multi-level improves the computational expense of standard monte carlo in this setting by an order of magnitude. more precisely, for a target accuracy of tol, so that the root mean square error of the estimator is o(tol), the o(tol-4) cost of standard monte carlo can be reduced to o(tol-3 log(tol) 1/2) with a multi-level scheme. this result was established in higham, mao, roj, song, and yin (2013), and illustrated on some scalar examples. here, we briefly overview the algorithm and present some new computational results in higher dimensions. © 2012 ieee."
"optimization is an important tool in computational finance and business intelligence. multiple criteria mathematical program( mcmp), which is concerned with mathematical optimization problems involving more than one objective function to be optimized simultaneously, is one of the ways of utilizing optimization techniques. due to the existence of multiple objectives, mcmps are usually difficult to be optimized. in fact, for a nontrivial mcmp, there does not exist a single solution that optimizes all the objectives at the same time. in practice, many methods convert the original mcmp into a single-objective program and solve the obtained scalarized optimization problem. if the values of scalarization parameters, which measure the trade-offs between the conflicting objectives, are not chosen carefully, the converted single-objective optimization problem may be not solvable. therefore, to make sure mcmp always can be solved successfully, heuristic search and expert knowledge for deciding the value of scalarization parameters are always necessary, which is not an easy task and limits the applications of mcmp to some extend. in this paper, we take the multiple criteria linear program(mclp) for binary classification as the example and discuss how to modified the formulation of mclp directly to guarantee the solvability. in details, we propose adding a quadratic regularization term into the converted single-objective linear program. the new regularized formulation does not only overcomes some defects of the original scalarized problem in modeling, it also can be shown in theory that the finite optimal solutions always exist. to test the performance of the proposed method, we compare our algorithm with several state-of-the-art algorithms for binary classification on several different kinds of datasets. preliminary experimental results demonstrate the effectiveness of our regularization method. © 2013 the authors. published by elsevier b.v."
"abstract - in the past 20 years, computerization has driven explosive growth in the volume of financial markets and in the variety of traded financial instruments. increasingly sophisticated mathematical and statistical methods and rapidly expanding computational power to drive them have given rise to the field of computational finance. the wide applicability of these models, their computational intensity, and their real-time constraints require high-throughput parallel architectures. in this work, we have assembled a financial analytics workload for derivative pricing, an important area of computational finance. we characterize and compare our workload's performance on two modern, parallel architectures: the intel r xeon r processor 2680, and the recently announced intel r xeon phitm1 'knights corner' coprocessor. in addition to analysis of the peak performance of the workloads on each architecture, we also quantify the impact of several levels of compiler and algorithmic optimization. overall, we find that large caches on both architectures, out-of-order cores on intel r xeon r, and large compute and memory bandwidth on intel r xeon phitm deliver high level of performance on financial analytics. © 2012 ieee."
"in this chapter, we compare the development of the artificial agents in two popular kinds of agent-based computational economic models. one is the agent-based models guided by game experiments. the other is the agent-based financial markets.while the conversation between these two classes of agent-based models is rare, the two can be connected through the idea of the generalized reinforcement learning. this is because that the artificial agents in game experiments have been well developed into a hierarchical framework such that the cognitive capacity can be incrementally added to the artificial agents from a low-level one, such as zero-intelligence agents, to a high-level one, such as belief learning agents or level-k reasoning agents. however, this hierarchy has not been found in agent-based financial markets. therefore, bridging the two classes of agent-basedmodels through artificial agents can help build financial agents with different level of cognitive capacity.this step is crucial for the cognitive foundation of agent-based financialmarkets and is related tothe recent pilingup empirical studies of cognitive finance. © 2013 by world scientific publishing co. pte. ltd. all rights reserved."
"in this paper we perform a comparison study of alternating direction implicit (adi) and operator splitting (os) methods on multi-dimensional black-scholes option pricing models. the adi method is used extensively in mathematical finance for numerically solving multi-factor option pricing problems. however, numerical results from the adi scheme show oscillatory solution behaviors with nonsmooth payoffs or discontinuous derivatives at the exercise price with large time steps. in the adi scheme, there are source terms which include y-derivatives when we solve x-derivative involving equations. then, due to the nonsmooth payoffs, source terms contain abrupt changes which are not in the range of implicit discrete operators and this leads to difficulty in solving the problem. on the other hand, the os method does not contain the other variable's derivatives in the source terms. we provide computational results showing the performance of the methods for two-asset option pricing problems. the results show that the os method is very efficient and gives better accuracy and robustness than the adi method with large time steps. © 2013 elsevier ltd. all rights reserved."
"numerical methods based on time discretization and estimation of conditional expectations for solving backward stochastic differential equations (bsdes) have been the object of considerable research, particularly in view of the applications to finance. we introduce and implement a simple control variate technique to reduce the simulation error of the conditional expectation estimates in bsde methods. these modifications increase the accuracy of the existing algorithms without additional computational cost. © 2013 académie des sciences."
"this paper presents a new computational finance approach, combining a symbolic aggregate approximation (sax) technique together with an optimization kernel based on genetic algorithms (ga). the sax representation is used to describe the financial time series, so that, relevant patterns can be efficiently identified. the evolutionary optimization kernel is here used to identify the most relevant patterns and generate investment rules. the proposed approach was tested using real data from s&p500. the achieved results show that the proposed approach outperforms both b&h and other state-of-the-art solutions. © 2012 elsevier ltd. all rights reserved."
"petascale analytics is a hot research area both in academia and industry. it envisages processing massive amounts of data at extremely high rates to generate new scientific insights along with positive impact (for both users and providers) of industries such as e-commerce, telecom, finance, life sciences and so forth. we consider collaborative filtering (cf) and clustering algorithms that are key fundamental analytics kernels that help in achieving these aims. real-time cf and co-clustering on highly sparse massive datasets, while achieving a high prediction accuracy, is a computationally challenging problem. in this paper, we present a novel hierarchical design for soft real-time (less than 1 minute.) distributed co-clustering based collaborative filtering algorithm. our distributed algorithm has been optimized for multi-core cluster architectures. theoretical analysis of the time complexity of our algorithm proves the efficacy of our approach. using the netflix dataset (900m training ratings with replication) as well as the yahoo kdd cup 1 (4.6b training ratings with replication) datasets, we demonstrate the performance and scalability of our algorithm on a 4096-node multi-core cluster architecture. our distributed algorithm (implemented using openmp with mpi) demonstrates around 4x better performance (on blue gene/p) as compared to the best prior work, along with high accuracy (26 ± 4 rmse for yahoo kdd cup data and 0.87 ± 0.02 for netflix data). to the best of our knowledge, these are the best known performance results for collaborative filtering, at high prediction accuracy, for multi-core cluster architectures. © 2012 ieee."
"financial forecasting is an important area in computational finance. eddie 8 is an established genetic programming financial forecasting algorithm, which has successfully been applied to a number of international datasets. the purpose of this paper is to further increase the algorithm's predictive performance, by improving its data space representation. in order to achieve this, we use attribute construction to create new (high-level) attributes from the original (low-level) attributes. to examine the effectiveness of the above method, we test the extended eddie's predictive performance across 25 datasets and compare it to the performance of two previous eddie algorithms. results show that the introduction of attribute construction benefits the algorithm, allowing eddie to explore the use of new attributes to improve its predictive accuracy. © 2013 ieee."
"in this paper, we present a study of genetic-based stock selection models using the data of fundamentals of initial public offerings (ipos). the stock selection model intends to derive the relative quality of the ipos in order to obtain their relative rankings. top-ranked ipos can be selected to form a portfolio. in this study, we also employ genetic algorithms (ga) for optimization of model parameters and feature selection for input variables to the stock selection model. we will show that our proposed models deliver above-average first-day returns. based upon the promising results obtained, we expect our ga-based methodology to advance the research in soft computing for computational finance and provide an effective solution to stock selection for ipos in practice. © 2012 ieee."
"the monte carlo methods (mcms) are very convenient for parallel implementation because in many cases they can use powerful high performance computing (hpc) resources for achieving accurate results without losing their parallel efficiency. this advantage of mcms is used by the scientists for solving large-scale mathematical problems derived from the life science, finances, computational physics, computational chemistry, and many other fields. in this work we consider a monte carlo method for solving quantum-kinetic integral equations describing electron transport in semiconductors. the presented algorithm is a part of set of algorithms involved in set (simulation of electron transport) application which is developed by our team. the set application can be successfully used to support simulation of semiconductor devices at the nano-scale as well as other problems in computational electronics. here we study scalability of the presented a monte carlo algorithm using bulgarian hpc resources. numerical results for parallel efficiency and computational cost are also presented. in addition we discuss the coordinated use of heterogeneous hpc resources from one and the same application in order to achieve a good performance. © 2012 ieee."
"forecasting the term structure of interest rates plays a crucial role in portfolio management, household finance decisions, business investment planning, and policy formulation. this paper proposes the use of evolving fuzzy inference systems for interest rate forecasting in the us and brazilian markets. evolving models provide a high level of system adaptation and learns the system dynamic continuously, which is essential for uncertain environments as fixed income markets. besides the usefulness evaluation of evolving methods to forecast yields, this paper suggests the interest rate factors forecasting taking into account multi-input-multi-output (mimo) evolving systems, which reduces computational time complexity and provides more accurate forecasts. results based on mean squared forecast errors showed that mimo evolving methods perform better than traditional benchmark for short and long-term maturities, for both fixed income markets evaluated. © 2012 ieee."
"the quantlib library is a popular library used for many areas of computational finance. in this work, the parallel processing power of the gpu is used to accelerate quantlib financial applications. black-scholes, monte-carlo, bonds, and repo code paths in quantlib are accelerated using hand-written cuda and opencl codes specifically targeted for the gpu. additionally, hmpp and openacc versions of the applications were created to drive the automatic genera- tion of gpu code from sequential code. the results demon- strate a significant speedup for each code using each paral- lelization method. we were also able to increase the speedup of hmpp-generated code with auto-tuning. copyright 2013 acm."
"computational finance is one of the fields where machine learning and data mining have found in recent years a large application. neverthless, there are still many open issues regarding the predictability of the stock market, and the possibility to build an automatic intelligent trader able to make forecasts on stock prices, and to develop a profitable trading strategy. in this paper, we propose an automatic trading strategy based on support vector machines, which employs recall-precision curves in order to allow a buying action for the trader only when the confidence of the prediction is high. we present an extensive experimental evaluation which compares our trader with several classic competitors. © springer-verlag berlin heidelberg 2013."
"personal financial planning is the preparation of a series of ongoing target-oriented decisions concerning personal assets, incomes and expenses. an important task of personal financial planning is the generation of a financial schedule defining suitable times for realization of the expenses and incomes at a given point of time. we describe an integer programming model and algorithms for scheduling personal finances via the examination of a linear combination of the weighted objectives to be maximized. solving the integer programming problem provides an optimal financial schedule which reflects an individual's goals and preferences. we describe a worst-case scenario for a personal financial schedule with respect to possible variation of credit and interest rates. computational results demonstrate the largest problems that are solvable via the algorithm based on schedule enumeration which is needed for a stability analysis. © 2012 springer science+business media b.v."
"dynamic programming problems are common in economics, finance and natural resource management. however, exact solutions to these problems are exceptional. instead, solutions typically rely on numerical approximation techniques which vary in use, complexity and computational requirements. perturbation, projection and linear programming approaches are among the most useful of these numerical techniques. in this paper, we extend the parametric linear programming technique to include continuous-time problems with jump-diffusion processes, and compare it to projection and perturbation techniques for solving dynamic programming problems in terms of computational speed, accuracy, ease of use and scope. the comparisons are drawn from solutions to two fisheries management problems - a unidimensional model of optimal harvest and a multidimensional model for optimal marine reserve size. available computer code illustrates how each technique solves these problems and how they can be applied to other comparable problems in natural resource modelling. © 2012 elsevier ltd."
"this paper introduces a platform for online sensitivity analysis (sa) that is applicable in large scale real-time data acquisition (daq) systems. here, we use the term real-time in the context of a system that has to respond to externally generated input stimuli within a finite and specified period. complex industrial systems such as manufacturing, healthcare, transport, and finance require high-quality information on which to base timely responses to events occurring in their volatile environments. the motivation for the proposed eventtracker platform is the assumption that modern industrial systems are able to capture data in real-time and have the necessary technological flexibility to adjust to changing system requirements. the flexibility to adapt can only be assured if data is succinctly interpreted and translated into corrective actions in a timely manner. an important factor that facilitates data interpretation and information modeling is an appreciation of the affect system inputs have on each output at the time of occurrence. many existing sensitivity analysis methods appear to hamper efficient and timely analysis due to a reliance on historical data, or sluggishness in providing a timely solution that would be of use in real-time applications. this inefficiency is further compounded by computational limitations and the complexity of some existing models. in dealing with real-time event driven systems, the underpinning logic of the proposed method is based on the assumption that in the vast majority of cases changes in input variables will trigger events. every single or combination of events could subsequently result in a change to the system state. the proposed event tracking sensitivity analysis method describes variables and the system state as a collection of events. the higher the numeric occurrence of an input variable at the trigger level during an event monitoring interval, the greater is its impact on the final analysis of the system state. experiments were designed to compare the proposed event tracking sensitivity analysis method with a comparable method (that of entropy). an improvement of 10 percent in computational efficiency without loss in accuracy was observed. the comparison also showed that the time taken to perform the sensitivity analysis was 0.5 percent of that required when using the comparable entropy-based method. © 1989-2012 ieee."
"collecting, analyzing, and extracting valuable information from a large amount of data requires easily accessible, robust, computational and analytical tools. data mining and business analytics with r utilizes the open source software r for the analysis, exploration, and simplification of large high-dimensional data sets. as a result, readers are provided with the needed guidance to model and interpret complicated data and become adept at building powerful models for prediction and classification. highlighting both underlying concepts and practical computational skills, data mining and business analytics with r begins with coverage of standard linear regression and the importance of parsimony in statistical modeling. the book includes important topics such as penalty-based variable selection (lasso); logistic regression; regression and classification trees; clustering; principal components and partial least squares; and the analysis of text and network data. in addition, the book presents: a thorough discussion and extensive demonstration of the theory behind the most useful data mining tools. illustrations of how to use the outlined concepts in real-world situations. readily available additional data sets and related r code allowing readers to apply their own analyses to the discussed materials. numerous exercises to help readers with computing skills and deepen their understanding of the material. data mining and business analytics with r is an excellent graduate-level textbook for courses on data mining and business analytics. the book is also a valuable reference for practitioners who collect and analyze data in the fields of finance, operations management, marketing, and the information sciences. © 2013 john wiley & sons, inc."
"in this article, we study the numerical solutions of a class of complex partial differential equation (pde) systems with free boundary conditions. this problem arises naturally in pricing american options with regime-switching, which adds significant complexity in the pde systems due to regime coupling. developing efficient numerical schemes will have important applications in computational finance. we propose a new method to solve the pde systems by using a penalty method approach and an exponential time differencing scheme. first, the penalty method approach is applied to convert the free boundary value pde system to a system of pdes over a fixed rectangular region for the time and spatial variables. then, a new exponential time differncing crank-nicolson (etd-cn) method is used to solve the resulting pde system. this etd-cn scheme is shown to be second order convergent. we establish an upper bound condition for the time step size and prove that this etd-cn scheme satisfies a discrete version of the positivity constraint for american option values. the etd-cn scheme is compared numerically with a linearly implicit penalty method scheme and with a tree method. numerical results are reported to illustrate the convergence of the new scheme. © 2012 wiley periodicals, inc. numer methods partial differential eq 2013 copyright © 2012 wiley periodicals, inc."
"this paper shows two examples of how the analysis of option pricing problems can lead to computational methods efficiently implemented in parallel. these computational methods outperform ""general purpose"" methods (i.e., for example, monte carlo, finite differences methods). the gpu implementation of two numerical algorithms to price two specific derivatives (continuous barrier options and realized variance options) is presented. these algorithms are implemented in cuda subroutines ready to run on graphics processing units (gpus) and their performance is studied. the realization of these subroutines is motivated by the extensive use of the derivatives considered in the financial markets to hedge or to take risk and by the interest of financial institutions in the use of state of the art hardware and software to speed up the decision process. the performance of these algorithms is measured using the (cpu/gpu) speed up factor, that is using the ratio between the (wall clock) times required to execute the code on a cpu and on a gpu. the choice of the reference cpu and gpu used to evaluate the speed up factors presented is stated. the outstanding performance of the algorithms developed is due to the mathematical properties of the pricing formulae used and to the ad hoc software implementation. in the case of realized variance options when the computation is done in single precision the comparisons between cpu and gpu execution times gives speed up factors of the order of a few hundreds. for barrier options, the corresponding speed up factors are of about fifteen, twenty. the cuda subroutines to price barrier options and realized variance options can be downloaded from the website http://www.econ.univpm.it/recchioni/ finance/w13. a more general reference to the work in mathematical finance of some of the authors and of their coauthors is the website http://www.econ. univpm.it/recchioni/finance/. © springer science+business media, llc 2012."
"this work presents a systematic and cost-effective approach to identify potential sites for development wind farms over large areas (state, country etc.) using a geographical information systems based integrated approach. performance for identified sites is assessed using mesoscale numerical weather prediction model (order of 100.000 sq. km.) and is quantified by detailed project finance modelling results are presented for case of poland, where 38% of land area is found eligible for development. financial return and risk for identified sites is mapped to demonstrate the ease of portfolio development. microscale computational fluid dynamics wind simulations (order of few sq. km) are utilized for refinement of analysis. a method for optimization of wind farm layout is presented as a refinement for improved energy harness. it is shown using test case of an actual wind farm that cost of energy production can be reduced by €0.2cents/kwh."
"efficient computational solutions for scientific and engineering problems are a priority for many governments around the world, as they can offer major economic comparative advantages. financial computing problems are a prime example of such problems where even the slightest improvements in execution times and latency can generate large amounts of extra profits. however, financial computing has not benefited relatively greatly from early developments in high performance computing, as the latter aimed mainly at engineering and weapon design applications. besides, financial experts were initially focusing on developing mathematical models and computer simulations in order to comprehend the behavior of financial markets and develop risk-management tools. as this effort progressed, the complexity of financial computing applications grew up rapidly. hence, high performance computing turned out to be very important in the field of finance. many financial models do not have a practical closed-form solution in which case numerical methods are the only alternative. monte-carlo simulation is one of the most commonly used numerical methods, in financial modeling and scientific computing in general, with huge computation benefits in solving problems where closed-form solutions are impossible to derive. as the monte-carlo method relies on the average result of thousands of independent stochastic paths, massive parallelism can be harnessed to accelerate the computation. for this, high performance computers, increasingly with off-the-shelf accelerator hardware, are being proposed as an economic high performance implementation platform for monte-carlo-based simulations. field programmable gate arrays (fpgas) in particular have been recently proposed as a high performance and relatively low power acceleration platform for such applications.in light of the above, the project presented in this chapter develops novel fpga hardware architectures for monte-carlo simulations of different types of financial option pricing models, namely european, asian, and american options, the stochastic volatility model (garch model), and quasi-monte carlo simulation. these architectures have been implemented on an fpga-based supercomputer, called maxwell, developed at the university of edinburgh, which is one of the few openly available fpga parallel machines in the world. maxwell is a 32-cpu cluster augmented with 64 virtex-4 xilinx fpgas connected in a 2d torus. our hardware implementations all show significant computing efficiency compared to traditional software-based implementations, which in turn shows that reconfigurable computing technology can be an efficacious and efficient platform for high performance computing applications, particularly financial computing. © 2013 springer science+business media, llc. all rights are reserved."
"state-of-the-art financial computations based on realistic market models like the heston model require a high computational effort, since no closed-form solutions are available in general. due to the fact that the underlying asset behavior predictions are mainly based on number crunching operations, fpgas are promising target devices for this task. in this chapter, we give an overview about current problems and solutions in the finance and insurance domain and show how state-of-the-art market models and solution methods have increased the necessary computational power over time. for the reason of universality and robustness, we focus on monte carlo methods that require a huge amount of normally distributed random numbers. we summarize the state-of-the-art and present efficient hardware architectures to obtain these numbers, together with comprehensive quality investigations. build on these high-quality random number generators, we present an efficient fpga architecture for option pricing in the heston model, tailored to fpgas. for the problem pricing european barrier options in the heston model we show that a xilinx virtex-5 device can save up to 97% of energy, providing the same simulation throughput as a nvidia tesla 2050 gpu. © 2013 springer science+business media, llc. all rights are reserved."
"corporate sustainability can be defined as the translation of sustainable development concept to the business world. as so, it has a focus on the companies' performance, considering equally the environmental, social and economic aspects in the decision making processes. more than take into account the profit, the people and the planet, as intended by the term known as ""triple bottom line"", corporate sustainability also ascribes a central role to communication and the involvement of a plurality of legitimate actors (""stakeholders"") in decision-making, just as sustainable development principles do. beyond the recognized environmental, social and economic issues, the communication is also a challenge and a condition to achieve the sustainable pattern. this specific challenge includes conveying sustainable development values in public relations, marketing and all other forms of public or corporate communication, in a way to guarantee the engagement of the different stakeholders. according to the international finance corporation - ifc (2007), an important development in stakeholder engagement since the early 1990s has been the rapid growth in reporting on environmental performance. the ifc demonstrate that one of the benefits of good reporting is that sharing credible performance information provides a solid base for future dialogue. the value of stakeholder engagement to reporting is also recognized under the principle of inclusivity in the global reporting initiative's sustainability reporting guidelines. a review of the state of art realized by cepel (2007a) shows that the companies of electric sector all over the world are following that tendency and trying to improve stakeholder communication and engagement. these two aspects play an important role in the electric sector as it could be noted that it is much more mentioned the negative impacts to the environment and to the communities due to the construction and operation of hydropower plants, thermoelectric plants, transmission and distribution lines than the benefits that it brings to the society, as the improvement of the access to energy, and to regional and local environments. in brazil, eletrobras is also focused on those sustainability challenges, including the communication ones, in such a way that it has turned into an strategic issue, incorporated in the company strategic planning. the igs project (indicators for the sustainaility management) has the objective of developing a group of environmental indicators and a computational system for storage, edition, treatment, retrieve and visualization of sustainable information, and is being held by cepel, since 2007. the idea is to find the appropriate indicators that, considering the reality of the brazilian power sector, can assist the management of corporate sustainability and the long-term communication strategy, focusing on the social and environmental practices of the company and on dialogue with stakeholders. this paper describes all the process of research and definition of the selected environmental indicators and how they are helping the company to involve and respond to the different stakeholders demands, improving the engagement process."
"in this article, we consider the problem of detecting multiple changepoints in large datasets. our focus is on applications where the number of changepoints will increase as we collect more data: for example, in genetics as we analyze larger regions of the genome, or in finance as we observe time series over longer periods. we consider the common approach of detecting changepoints through minimizing a cost function over possible numbers and locations of changepoints. this includes several established procedures for detecting changing points, such as penalized likelihood and minimum description length. we introduce a new method for finding the minimum of such cost functions and hence the optimal number and location of changepoints that has a computational cost, which, under mild conditions, is linear in the number of observations. this compares favorably with existing methods for the same problem whose computational cost can be quadratic or even cubic. in simulation studies, we show that our new method can be orders of magnitude faster than these alternative exact methods. we also compare with the binary segmentation algorithm for identifying changepoints, showing that the exactness of our approach can lead to substantial improvements in the accuracy of the inferred segmentation of the data. this article has supplementary materials available online. © 2012 american statistical association."
"rapid development of natural language processing technologies has paved way for automatic sentiment analysis and emergence of robo-readers in computational finance. however, the technology is still in its nascent state. distilling sentiment information from unstructured sources has turned out to be a complicated and strongly domain-dependent problem. to emulate the human ability to recognize financial sentiments in natural language by using machines, we need to provide them with (i) necessary ontological knowledge on the relevant domain-concepts, and (ii) learning strategies that help the machines to combine this knowledge with the syntactic structures extracted from text. in this paper, we present a knowledge-driven tree kernel framework for sentence-level analysis of financial news sentiments. comparisons with linear kernels and classical lexicon-based systems suggest that significant performance gains can be achieved by incorporating information on financial concepts and their grammatical context. the framework is decomposable into learning, knowledge and syntactic structure components. contribution of each part is separately examined using a human-annotated phrase-bank with close to 5000 sentences collected across a number of financial news sources. the proposed sentiment analysis framework is flexible and can be applied also outside financial domain. to evaluate cross-domain performance, a further comparison of the algorithms is done with datasets from non-financial domains including movie reviews and general political discussions. © 2013 ieee."
"advances in hardware and software technology enable us to collect, store and distribute large quantities of data on a very large scale. automatically discovering and extracting hidden knowledge in the form of patterns from these large data volumes is known as data mining. data mining technology is not only a part of business intelligence, but is also used in many other application areas such as research, marketing and financial analytics. for example medical scientists can use patterns extracted from historic patient data in order to determine if a new patient is likely to respond positively to a particular treatment or not; marketing analysts can use extracted patterns from customer data for future advertisement campaigns; finance experts have an interest in patterns that forecast the development of certain stock market shares for investment recommendations. however, extracting knowledge in the form of patterns from massive data volumes imposes a number of computational challenges in terms of processing time, memory, bandwidth and power consumption. these challenges have led to the development of parallel and distributed data analysis approaches and the utilisation of grid and cloud computing. this chapter gives an overview of parallel and distributed computing approaches and how they can be used to scale up data mining to large datasets. © springer-verlag london 2013."
we use the multilevel monte carlo method to estimate option prices in computational finance and combine this method with two adaptive algorithms. in the first algorithm we consider time discretization and sample size as two separate dimensions and use dimension-adaptive refinement to optimize the error with respect to these dimensions in relation to the computational costs. the second algorithm uses locally adaptive timestepping and is constructed especially for non-lipschitz payoff functions whose weak and strong order of convergence is reduced when the euler-maruyama method is used to discretize the underlying sde. the numerical results show that for barrier and double barrier options the convergence order for smooth payoffs can be recovered in these cases. © 2013 springer-verlag berlin heidelberg.
"as highly related to the investors' earnings expectations and trading decision-making behavior, securities transaction tax (stt) has long been regarded as a typical regulatory mechanism exploited by policy makers. however, neither theoretical analysis nor empirical studies reach consensus about the role and policy effect of the securities transaction tax. within the framework of agent-based computational finance, this paper presents a new artificial stock market model with heterogeneous agents, which allows us to assess the impacts of varying stts on market behavior to come to robust conclusions. first we investigate the dynamics of benchmark market with no tax levied, and then market behaviors with different stts are thoroughly checked. the results show that a modest transactions tax does contribute to stabilize markets by reducing market volatility, but its negative effects on market efficiency cannot be ignored at the same time. the findings suggest that regulatory authorities should introduce stt discreetly to strike a balance between stability and efficiency. © 2013 the authors. published by elsevier b.v."
"this paper describes the vision of a european exploratory for economics and finance using an interdisciplinary consortium of economists, natural scientists, computer scientists and engineers, who will combine their expertise to address the enormous challenges of the 21st century. this academic public facility is intended for economic modelling, investigating all aspects of risk and stability, improving financial technology, and evaluating proposed regulatory and taxation changes. the european exploratory for economics and finance will be constituted as a network of infrastructure, observatories, data repositories, services and facilities and will foster the creation of a new cross-disciplinary research community of social scientists, complexity scientists and computing (ict) scientists to collaborate in investigating major issues in economics and finance. it is also considered a cradle for training and collaboration with the private sector to spur spin-offs and job creations in europe in the finance and economic sectors. the exploratory will allow social scientists and regulators as well as policy makers and the private sector to conduct realistic investigations with real economic, financial and social data. the exploratory will (i) continuously monitor and evaluate the status of the economies of countries in their various components, (ii) use, extend and develop a large variety of methods including data mining, process mining, computational and artificial intelligence and every other computer and complex science techniques coupled with economic theory and econometric, and (iii) provide the framework and infrastructure to perform what-if analysis, scenario evaluations and computational, laboratory, field and web experiments to inform decision makers and help develop innovative policy, market and regulation designs. © edp sciences, springer-verlag 2012."
"analytical workloads abound in application domains ranging from computational finance and risk analytics to engineering and manufacturing settings. in this paper we describe a platform for parallel r-based analytics on the cloud (p2rac). the goal of this platform is to allow an analyst to take a simulation or optimization job (both the code and associated data) that runs on their personal workstations and with minimum effort have them run on large-scale parallel cloud infrastructure. if this can be facilitated gracefully, an analyst with strong quantitative but perhaps more limited development skills can harness the computational power of the cloud to solve larger analytically problems in less time. p2rac is currently designed for executing parallel r scripts on the amazon elastic computing cloud infrastructure. preliminary results obtained from an experiment confirm the feasibility of the platform. © 2012 ieee."
"it has been observed that credit period has become a major concern for most of the retailers, because not only it has direct influence on inventory and finance but also on the demand of an item. unfortunately, the impact of credit period on demand has received very little attention in the literature, whereas in reality length of the credit period offered has positive impact on the demand rate. the impact of credit period on demand may be instant or delayed. the aim of this paper is to determine the retailer’s optimal replenishment and credit policy in eoq model under twolevels of trade credit policy when demand is influenced by credit period. these types of demand functions are observed in many consumer durables. results have been illustrated with the help of a numerical example. computational results provide some interesting policy implications. © the society for reliability engineering, quality and operations management (sreqom), india and the division of operation and maintenance, lulea university of technology, sweden 2012."
"big data analytics is a hot research area both in academia and industry. it envisages processing massive amounts of data at high rates to generate new insights leading to positive impact (for both users and providers) of industries such as ecommerce, telecom, finance, life sciences and so forth. we consider collaborative filtering (cf) and clustering algorithms that are key fundamental analytics kernels that help in achieving these aims. high throughput cf and co-clustering on highly sparse and massive datasets, along with a high prediction accuracy, is a computationally challenging problem. in this paper, we present a novel hierarchical design for soft real-time (less than 1 minute.) distributed co-clustering based collaborative filtering algorithm. we study both the online and offline variants of this algorithm. theoretical analysis of the time complexity of our algorithm proves the efficacy of our approach. further, we present the impact of load balancing based optimizations on multi-core cluster architectures. using the netflix dataset(900m training ratings with replication) as well as the yahoo kdd cup(2.3b training ratings with replication) datasets, we demonstrate the performance and scalability of our algorithm on a large multi-core cluster architecture. in offline mode, our distributed algorithm demonstrates around 4× better performance (on blue gene/p) as compared to the best prior work, along with high accuracy. in online mode, we demonstrated around 3× better performance compared to baseline mpi implementation. to the best of our knowledge, our algorithm provides the best known online and offline performance and scalability results with high accuracy on multi-core cluster architectures. © 2012 ieee."
"this paper presents the forward financial framework (f3), an application framework for describing and implementing forward looking financial computations on high performance, heterogeneous platforms. f3 allows the computational finance problem specification to be captured precisely yet succinctly, then automatically creates efficient implementations for heterogeneous platforms, utilising both multi-core cpus and fpgas. the automatic mapping of a high-level problem description to a low-level heterogeneous implementation is possible due to the domain-specific knowledge which is built in f3, along with a software architecture that allows for additional domain knowledge and rules to be added to the framework. currently the system is able to utilise domain-knowledge of the run-time characteristics of pricing tasks to partition pricing problems and allocate them to appropriate compute resources, and to exploit relationships between financial instruments to balance computation against communication. the versatility of the framework is demonstrated using a benchmark of option pricing problems, where f3 achieves comparable speed and energy efficiency to external manual implementations. further, the domain-knowledge guided partitioning scheme suggests a partitioning of subtasks that is 13% faster than the average, while exploiting domain dependencies to reduce redundant computations results in an average gain in efficiency of 27%. © 2013 ieee."
"the aim of this book is to promote interaction between engineering, finance and insurance, as these three domains have many models and methods of solution in common for solving real-life problems. the authors point out the strict inter-relations that exist among the diffusion models used in engineering, finance and insurance. in each of the three fields, the basic diffusion models are presented and their strong similarities are discussed. analytical, numerical and monte carlo simulation methods are explained with a view to applying them to obtain the solutions to the different problems presented in the book. advanced topics such as nonlinear problems, lévy processes and semi-markov models in interactions with the diffusion models are discussed, as well as possible future interactions among engineering, finance and insurance. contents 1. diffusion phenomena and models. 2. probabilistic models of diffusion processes. 3. solving partial differential equations of second order. 4. problems in finance. 5. basic pde in finance. 6. exotic and american options pricing theory. 7. hitting times for diffusion processes and stochastic models in insurance. 8. numerical methods. 9. advanced topics in engineering: nonlinear models. 10. lévy processes. 11. advanced topics in insurance: copula models and var techniques. 12. advanced topics in finance: semi-markov models. 13. monte carlo semi-markov simulation methods. about the authors jacques janssen is now honorary professor at the solvay business school (ulb) in brussels, belgium, having previously taught at euria (euro-institut d'actuariat, university of west brittany, brest, france) and télécom-bretagne (brest, france) as well as being a director of jacan insurance and finance services, a consultancy and training company. oronzio manca is professor of thermal sciences at seconda università degli studi di napoli in italy. he is currently associate editor of asme journal of heat transfer and journal of porous media and a member of the editorial advisory boards for the open thermodynamics journal, advances in mechanical engineering, the open fuels and energy science journal. raimondo manca is professor of mathematical methods applied to economics, finance and actuarial science at university of rome ""la sapienza"" in italy. he is associate editor for the journal methodology and computing in applied probability. his main research interests are multidimensional linear algebra, computational probability, application of stochastic processes to economics, finance and insurance and simulation models. © 2013 by john wiley & sons, inc."
"this paper explores novel, polynomial time, heuristic, approximate solutions to the np-hard problem of finding the optimal job schedule on identical machines which minimizes total weighted tardiness (twt). we map the twt problem to quadratic optimization and demonstrate that the hopfield neural network (hnn) can successfully solve it. furthermore, the solution can be significantly sped up by choosing the initial state of the hnn as the result of a known simple heuristic, we call this smart hopfield neural network (shnn). we also demonstrate, through extensive simulations, that by considering random perturbations to the largest weighted process first (lwpf) and shnn methods, we can introduce further improvements to the quality of the solution, we call the latter perturbed smart hopfieldneural network (pshnn). finally, we argue that due to parallelization techniques, such as the use of gpgpu, the additional cost of these improvements is small. numerical simulations demonstrate that pshnn outperforms hnn in over 99% of all randomly generated cases by an average of 3-7%, depending on the problem size. on a specific, large scale scheduling problem arising in computational finance at morgan stanley, one of the largest financial institutions in the world, pshnn produced a 5% improvement over the next best heuristic."
the paper introduces the use of several chosen maple software finance application tools - specific examples from the applications center of the canadian company maplesoft that has been developed maple since 1980. there are discussed the possibilities that maple environment offers. readers can also share author's experiences as can be seen in the maple environment usage presented in the maple case studies. the paper introduces also essential information about computational finance and the theory of portfolios of securities and presents how to use the maple tools for the computation of the efficient frontier and optimal utility of a portfolio of three securities from the prague stock exchange.
"rcpp is the glue that binds the power and versatility of r with the speed and efficiency of c++. with rcpp, the transfer of data between r and c++ is nearly seamless, and high-performance statistical computing is finally accessible to most r users. rcpp should be part of every statistician's toolbox. -- michael braun, mit sloan school of management ""seamless r and c++ integration with rcpp"" is simply a wonderful book. for anyone who uses c/c++ and r, it is an indispensable resource. the writing is outstanding. a huge bonus is the section on applications. this section covers the matrix packages armadillo and eigen and the gnu scientific library as well as rinside which enables you to use r inside c++. these applications are what most of us need to know to really do scientific programming with r and c++. i love this book. -- robert mcculloch, university of chicago booth school of business rcpp is now considered an essential package for anybody doing serious computational research using r. dirk's book is an excellent companion and takes the reader from a gentle introduction to more advanced applications via numerous examples and efficiency enhancing gems. the book is packed with all you might have ever wanted to know about rcpp, its cousins (rcpparmadillo, rcppeigen .etc.), modules, package development and sugar. overall, this book is a must-have on your shelf. -- sanjog misra, ucla anderson school of management the rcpp package represents a major leap forward for scientific computations with r. with very few lines of c++ code, one has r's data structures readily at hand for further computations in c++. hence, high-level numerical programming can be made in c++ almost as easily as in r, but often with a substantial speed gain. dirk is a crucial person in these developments, and his book takes the reader from the first fragile steps on to using the full rcpp machinery. a very recommended book! -- søren højsgaard, department of mathematical sciences, aalborg university, denmark ""seamless r and c ++ integration with rcpp"" provides the first comprehensive introduction to rcpp. rcpp has become the most widely-used language extension for r, and is deployed by over one-hundred different cran and bioconductor packages. rcpp permits users to pass scalars, vectors, matrices, list or entire r objects back and forth between r and c++ with ease. this brings the depth of the r analysis framework together with the power, speed, and efficiency of c++. dirk eddelbuettel has been a contributor to cran for over a decade and maintains around twenty packages. he is the debian/ubuntu maintainer for r and other quantitative software, edits the cran task views for finance and high-performance computing, is a co-founder of the annual r/finance conference, and an editor of the journal of statistical software. he holds a ph.d. in mathematical economics from ehess (paris), and works in chicago as a senior quantitative analyst. © the author 2013. all rights are reserved."
"economists can use computer algebra systems to manipulate symbolic models, derive numerical computations, and analyze empirical relationships among variables. maxima is an open-source multi-platform computer algebra system that rivals proprietary software. maxima’s symbolic and computational capabilities enable economists and financial analysts to develop a deeper understanding of models by allowing them to explore the implications of differences in parameter values, providing numerical solutions to problems that would be otherwise intractable, and by providing graphical representations that can guide analysis. this book provides a step-by-step tutorial for using this program to examine the economic relationships that form the core of microeconomics in a way that complements traditional modeling techniques. readers learn how to phrase the relevant analysis and how symbolic expressions, numerical computations, and graphical representations can be used to learn from microeconomic models. in particular, comparative statics analysis is facilitated. little has been published on maxima and its applications in economics and finance, and this volume will appeal to advanced undergraduates, graduate-level students studying microeconomics, academic researchers in economics and finance, economists, and financial analysts. © springer science+business media new york 2013."
"the design presented in this paper is a genetic algorithm (ga) written in vhsic hardware description language (vhdl) and intended for a hardware implementation which is used for real time applications. the hardware implementation exploits the reprogrammability of certain types of field-programmable gate arrays (fpgas) like those from xilinx. gas are a family of computational models inspired by evolution allowing the optimization of many problems related to digital solution, econometrics and finance. these algorithms encode a potential solution to a specific problem on a simple chromosome like data structure and apply recombination operators to these structures to preserve critical information. copyright 2012 acm."
"the aim of this article is to give a general introduction to the theory of interacting particle methods, and an overview of its applications to computational finance. we survey the main techniques and results on interacting particle systems and explain how they can be applied to the numerical solution of a variety of financial applications such as pricing complex path dependent european options, computing sensitivities, pricing american options or numerically solving partially observed control and estimation problems. © springer-verlag berlin heidelberg 2012."
"in recent times, graphics processing units (gpus) are considered not only for graphics, but also for speeding-up the execution of general purpose algorithms that can be expressed following a stream processing model. when gpus became programmable, the scientific community noticed that the enormous computational power behind these processors could be exploited to accelerate applications in many research fields. as a result, relevant works have been proposed in diverse areas such as computer vision, linear algebra, statistical analysis, physics and biological simulation, database analysis, computational finance and cryptography among others. the first trend on using gpus to speed-up the execution of general purpose algorithms was somehow tricky, and involved translating algorithms into graphics terms, storing data into textures and invoking rendering operations to perform algorithm computations. in the meantime, gpu architectures have moved to a unified rendering pipeline and have included new hardware resources that can benefit the execution of different kind of applications. programmability of gpus has improved and new frameworks like opencl are available to combine the power of multi-core cpus and gpus in heterogeneous systems. in this chapter a complete review on gpu computing history, techniques and principles are exposed, together with a deeper insight on the use of gpus for computer vision and image processing applications. finally, future trends on gpu and heterogeneous architectures programming are discussed. © 2011 by nova science publishers, inc. all rights reserved."
"this part of the book presents several applications which were motivated by the concept of bisociation, and to some extent exploited the notions of heterogeneous information networks, explicit contextualization and/or context crossing. the main goals of these applications are: to verify if the principles of heterogeneous information networks and bisociation, and their computational realization, can lead to new discoveries, to test the software platforms developed for bisociative knowledge discovery, and to find actual new discoveries in at least some application domains. most of the applications are in the area of biology, but in addition there are interesting digressions to finance, improvements of business processes, and music recommendations. © 2012 springer-verlag berlin heidelberg."
"currently, meta-heuristics including the genetic algorithms (ga) and simulated annealing (sa) have been used extensively to solve non-deterministic polynomial-time hard (np-hard) problems. continued efforts of researchers to upgrade the performance of the meta-heuristics in use resulted in the evolution of new ones. shuffled frog-leaping algorithm (sfla) is one of the recently introduced heuristics. the few applications of the sfla in the literature in different areas demonstrated the capacity of the sfla to provide high-quality solutions. the main objective of this paper is to further bring the sfla to the attention of researchers as a potential technique to solve the np-hard combinatorial problem of finance-based scheduling. the performance of the sfla is evaluated through benchmarking its results against those of the ga and sa. the traditional problem of generating infeasible solutions in scheduling problems is adequately tackled in the implementations of the ga, sa, and sfla. fairly large projects of 120 and 210 activities are used to compare the performance of the three meta-heuristics. finally, the obtained results indicate that the sfla improved the quality of solutions with a substantial reduction in the computational time. © 2012 american society of civil engineers."
"time-continuous markov jump processes are popular modeling tools in disciplines ranging from computational finance and operations research to human genetics and genomics. the data is often sampled at discrete points in time, and it can be useful to simulate sample paths between the datapoints. in this paper we firstly consider the problem of generating sample paths from a continuous-time markov chain conditioned on the endpoints using a new algorithm based on the idea of bisection. secondly we study the potentials of the bisection algorithm for variance reduction. in particular, examples are presented where the methods of stratification, importance sampling and quasi monte carlo are investigated. © springer-verlag berlin heidelberg 2012."
"we present a domain specific language embedded in haskell for specifying stochastic processes, called spl;. it is designed with the goal of matching the notation used in mathematical finance, where the price of a financial contract is specified using stochastic processes and distributions. spl; is declarative in the sense that it is agnostic of the choice of discretization and of the computational model. we provide an implementation of spl; that performs monte carlo simulation using gpgpu, and we present data indicating that this gives a 100x speedup compared to hand-written sequential c, and that the speedup scales linearly with the number of available cores. © 2012 acm."
"in recent years, interest-rate modeling has developed rapidly in terms of both practice and theory. the academic and practitioners' communities, however, have not always communicated as productively as would have been desirable. as a result, their research programs have often developed with little constructive interference. in this book, riccardo rebonato draws on his academic and professional experience, straddling both sides of the divide to bring together and build on what theory and trading have to offer. rebonato begins by presenting the conceptual foundations for the application of the libor market model to the pricing of interest-rate derivatives. next he treats in great detail the calibration of this model to market prices, asking how possible and advisable it is to enforce a simultaneous fitting to several market observables. he does so with an eye not only to mathematical feasibility but also to financial justification, while devoting special scrutiny to the implications of market incompleteness. much of the book concerns an original extension of the libor market model, devised to account for implied volatility smiles. this is done by introducing a stochastic-volatility, displaced-diffusion version of the model. the emphasis again is on the financial justification and on the computational feasibility of the proposed solution to the smile problem. this book is must reading for quantitative researchers in financial houses, sophisticated practitioners in the derivatives area, and students of finance. © 2002 by princeton university press. all rights reserved."
"for many discrete simulation optimization applications, it is often difficult to decide which ranking and selection (r&s) procedure to use. to efficiently compare r&s procedures, we present a three-layer performance evaluation process. we show that the two most popular performance formulations, namely the bayesian formulation and the indifference zone formulation, have a common representation analogous to convex risk measures used in mathematical finance. we then specify how a decision maker can impose a performance requirement on r&s procedures that is more adequate for her risk attitude than the indifference zone or the bayesian performance requirements. such a performance requirement partitions the space of r&s procedures into acceptable and nonacceptable procedures. the minimal computational budget required for a procedure to become acceptable introduces an easy-to-interpret preference order on the set of r&s policies. we demonstrate with a numerical example how the introduced framework can be used to guide the choice of selection procedure in practice. © 2012 acm."
"researchers in fields such as bioinformatics, cs, finance, and applied math have trouble managing the numerous code and data files generated by their computational experiments, comparing the results of trials executed with different parameters, and keeping up-to-date notes on what they learned from past successes and failures. we created a linux-based system called burrito that automates aspects of this tedious experiment organization and notetaking process, thus freeing researchers to focus on more substantive work. burrito automatically captures a researcher's computational activities and provides user interfaces to annotate the captured provenance with notes and then make queries such as, “which script versions and command-line parameters generated the output graph that this note refers to?. © 4th workshop on the theory and practice of provenance, tapp 2012. all rights reserved."
"this paper presents a new computational finance approach, combining a symbolic aggregate approximation (sax) technique together with an optimization kernel based on genetic algorithms (ga). the sax representation is used to describe the financial time series, so that, relevant patterns can be efficiently identified. the evolutionary optimization kernel is here used to identify the most relevant patterns and generate investment rules. the proposed approach was tested using real data from s&p500. the achieved results show that the proposed approach outperforms both b&h and other state-of-the-art solutions. © 2012 acm."
"monte carlo methods are a class of computational algorithms that rely on repeated random sampling to compute their results. monte carlo methods are often used in simulating complex systems. because of their reliance on repeated computation of random or pseudo-random numbers, these methods are most suited to calculation by a computer and tend to be used when it is infeasible or impossible to compute an exact result with a deterministic algorithm. in finance, monte carlo simulation method is used to calculate the value of companies, to evaluate economic investments and financial derivatives. on the other hand, grid computing applies heterogeneous computer resources of many geographically disperse computers in a network in order to solve a single problem that requires a great number of computer processing cycles or access to large amounts of data. in this paper, we have developed a simulation based on monte carlo method which is applied on grid computing in order to predict through complex calculations the future trends in stock prices. © 2012 world scientific publishing company."
"we aim at providing a comprehensive introduction to support vector machines and their applications in computational finance. based on the advances of the statistical learning theory, one of the first svm algorithms was proposed in mid 1990s. since then, they have drawn a lot of research interests both in theoretical and application domains and have became the state-of-the-art techniques in solving classification and regression problems. the reason for the success is not only because of their sound theoretical foundation but also their good generalization performance in many real applications. in this chapter, we address the theoretical, algorithmic and computational issues and try our best to make the article selfcontained. moreover, in the end of this chapter, a case study on default prediction is also presented. we discuss the issues when svm algorithms are applied to bankruptcy prognosis such as how to deal with the unbalanced dataset, how to tune the parameters to have a better performance and how to deal with large scale dataset. © springer-verlag berlin heidelberg 2012."
"this research addresses cross-training policies for a flexible assembly cell from the point of view of humanisation and finance. this policy is to determine which labours should be cross-trained and to assign which labours to which tasks. the first application of improved non-dominated sorting genetic algorithm (nsga-ii) to the bi-objective 0-1 integer programming model with an average satisfaction degree and an average salary function is presented. three performance metrics, including the number of non-dominated solutions, convergence and diversity are used to evaluate the nsga-ii. the optimal algorithm parameters, including the number of iteration, crossover rate and mutation rate are discussed and derived based on a series of experiments. and the experiment results gained by nsga-ii are compared to mopso. computational study shows that the algorithm nsga-ii is convergent and practical to this problem. a series of computational experiments are conducted to get some insights about how the effects of cross-training are influenced by the factors. the results indicate that with regard to average satisfaction degree, balanced preference structure is better than extreme, and with regard to average paid salary, non-uniform salary structure is better than uniform salary structure. those insights will provide the right direction for practitioners. © 2012 taylor & francis."
"this article presents a survey of low-discrepancy sequences and their applications to quasi-monte carlo methods for multidimensional numerical integration. quasi-monte carlo methods are deterministic versions of monte carlo methods which outperform monte carlo methods for many types of integrals and have thus been found enormously useful in computational finance. first a general background on quasi-monte carlo methods is given. then we describe principles for the construction of low-discrepancy sequences, with a special emphasis on the currently most powerful constructions based on the digital method and the theory of (t, s)-sequences. next, the important concepts of effective dimension and tractability are discussed. a synopsis of randomized quasi-monte carlo methods and their applications to computational finance is presented. a numerical example concludes the article. © springer-verlag berlin heidelberg 2012."
"any financial asset that is openly traded has a market price. except for extreme market conditions, market price may be more or less than a “fair” value. fair value is likely to be some complicated function of the current intrinsic value of tangible or intangible assets underlying the claim and our assessment of the characteristics of the underlying assets with respect to the expected rate of growth, future dividends, volatility, and other relevant market factors. some of these factors that affect the price can be measured at the time of a transaction with reasonably high accuracy. most factors, however, relate to expectations about the future and to subjective issues, such as current management, corporate policies and market environment, that could affect the future financial performance of the underlying assets. models are thus needed to describe the stochastic factors and environment, and their implementations inevitably require computational finance tools. © springer-verlag berlin heidelberg 2012."
"in the process of providing financial services, commercial banks assume various kinds of financial risks. the aim of the paper is to discuss the application of artificial intelligence methods to managing the financial risks. during recent years, the use of intelligent systems in the financial and economic industries have increased substantially, providing a new perspective to the agenda of finance and economics by their ability to handle large amounts of financial data and simulate complex models. generally, the commercial banking area can be considered as multi-agent systems with multiple interactions. banking management includes the activities of decision support systems, query and reporting, online analytical processing, statistical analysis, forecasting, and data mining. forecasting is essential for business. the advantages of artificial intelligence (ai) over traditional statistical forecasting methods are that ai do not have to fulfill any statistical assumptions and the ability to handle non-linearity, which are common in business. the most common applications of computational finance are within the area of investment banking and financial risk management, and currently employ learning methods such as support vector machines, bayesian approaches, logistic regression, artificial neural network, fuzzy logic and genetic algorithms, ant colony and particle swarm optimization. in this paper, we present a new concept for a formal description of the multi-agent systems with respect to the viewpoint of modeling, that is conditioned by the existence of nonlinear and fuzzy factor, respectively with entropy. the research was focused on the new approach of risk modeling and optimization based on structural transmutation of agent-based graph. © 2012 by nova science publishers, inc. all rights reserved."
"quasi-monte carlo (qmc) methods are important numerical tools in computational finance. path generation methods (pgms), such as brownian bridge and principal component analysis, play a crucial role in qmc methods. their effectiveness, however, is problem-dependent. this paper attempts to understand how a pgm interacts with the underlying function and affects the accuracy of qmc methods. to achieve this objective, we develop efficient methods to assess the impact of pgms. the first method is to exploit a quadratic approximation of the underlying function and to analyze the effective dimension and dimension distribution (which can be done analytically). the second method is to carry out a qmc error analysis on the quadratic approximation, establishing an explicit relationship between the qmc error and the pgm. equalities and bounds on the qmc errors are established, in which the effect of the pgm is separated from the effect of the point set (in a similar way to the koksmahlawka inequality). new measures for quantifying the accuracy of qmc methods combining with pgms are introduced. the usefulness of the proposed methods is demonstrated on two typical high-dimensional finance problems, namely, the pricing of mortgage-backed securities and asian options (with zero strike price). it is found that the success or failure of pgms that do not take into account the underlying functions (such as the standard method, brownian bridge and principal component analysis) strongly depends on the problem and the model parameters. on the other hand, the pgms that take into account the underlying function are robust and powerful. the investigation presents new insight on pgms and provides constructive guidance on the implementation and the design of new pgms and new qmc rules. © 2011 elsevier inc. all rights reserved."
"abstract this paper reviews the development of agent-based (computational) economics (ace) from an econometrics viewpoint. the review comprises three stages, characterizing the past, the present, and the future of this development. the first two stages can be interpreted as an attempt to build the econometric foundation of ace, and, through that, enrich its empirical content. the second stage may then invoke a reverse reflection on the possible agent-based foundation of econometrics. while ace modeling has been applied to different branches of economics, the one, and probably the only one, which is able to provide evidence of this three-stage development is finance or financial economics. we will, therefore, focus our review only on the literature of agent-based computational finance, or, more specifically, the agent-based modeling of financial markets. copyright © cambridge university press 2012."
"the june 2011 special issue of concurrency and computation: practice and experience offers papers from the workshop on high performance computational finance (whpcf) at supercomputing. the first part of the special issue focuses on the practice of financial modeling with accelerator platforms and covers topics ranging from pricing multiasset american and barrier options on gpus, estimating the market value-at-risk of large portfolios on highly parallel architectures, and pricing credit derivatives using fpgas. the second part of the special issue presents proof of concept high performance algorithms applied to financial applications. gilles pagés and benedikt wilbertz consider the difficult and ubiquitous problem of pricing american style and multiple exercise options. duy minh dang, christina christara and kenneth jackson consider the problem of american option pricing. the remaining two articles in the special issue both focus on the application of a novel algorithm to algorithmic trading."
"standard & poor' s (s & p) downgraded american government bonds from aaa to aa+ last year. the effects of the downgrade on financial markets have been studied in financial engineering, economics and computational finance, but not in agent-based simulation studies. in this paper, we investigate the effect of the rating system (e.g.: s & p) on asset price fluctuations in the artificial market, which is the agent-based simulation model of the financial market. the rating information is defined as a discrete version of the fundamental value. four strategies : the noise trader, the fundamentalist, the trend predictor, and the contrarian trader, were assumed in previous studies of the artificial market, plus we assume a new agent called “rating user” which uses the rating value, defined as the discrete value of the fundamental value of an asset. we investigate if the rating user makes the artificial market unstable. first, the simulation results show that kurtosis of an asset price return in the market, without fundamentalists, is higher than without rating users. this suggests the usage of rating information makes the artificial market unstable. the simulation outcomes also suggest volatility continuity of asset price return is stronger in the market without fundamentalists than without rating users. second, we investigate how two parameters, the update interval and rating length, which control the rating value, makes the market stable. the simulation outcomes show that both standard deviation of asset price return and kurtosis of asset price return becomes smaller as the update interval increases. the standard deviation gets larger and kurtosis of that gets larger with the increasing length of rating. these results imply that the rating information should be updated at short intervals and the length of rating should be moderate to make the artificial market stable. keywords: raing-information, financial market, artificial market, agent-based simulation. © 2012, the japanese society for artificial intelligence. all rights reserved."
"in a spoken dialog system that can handle natural conversation between a human and a machine, spoken language understanding (slu) is a crucial component aiming at capturing the key semantic components of utterances. building a robust slu system is a challenging task due to variability in the usage of language, need for labeled data, and requirements to expand to new domains (movies, travel, finance, etc.). in this paper, we survey recent research on bootstrapping or improving slu systems by using information mined or extracted from web search query logs, which include (natural language) queries entered by users as well as the links (web sites) they click on. we focus on learning methods that help unveiling hidden information in search query logs via implicit crowd-sourcing. © 2012 association for computational linguistics"
"α-stable distributions are utilized as models for heavy-tailed noise in many areas of statistics, finance and signal processing engineering. however, in general, neither univariate nor multivariate α-stable models admit closed form densities which can be evaluated pointwise. this complicates the inferential procedure. as a result, α-stable models are practically limited to the univariate setting under the bayesian paradigm, and to bivariate models under the classical framework. a novel bayesian approach to modelling univariate and multivariate α-stable distributions is introduced, based on recent advances in ""likelihood-free"" inference. the performance of this procedure is evaluated in 1, 2 and 3 dimensions, and through an analysis of real daily currency exchange rate data. the proposed approach provides a feasible inferential methodology at a moderate computational cost. © 2010 elsevier b.v. all rights reserved."
"this paper describes a new technique that can be used in financial mathematics for a wide range of situations where the calculation of complicated integrals is required. the numerical schemes proposed here are deterministic in nature but their proof relies on known results from probability theory regarding the weak convergence of probability measures. we adapt those results to unbounded payoffs under certain mild assumptions that are satisfied in finance. because our approximation schemes avoid repeated simulations and provide computational savings, they can potentially be used when calculating simultaneously the price of several derivatives contingent on the same underlying. we show how to apply the new methods to calculate the price of spread options and american call options on a stock paying a known dividend. the method proves useful for calculations related to the log-weibull model proposed recently for empirical asset pricing. © 2012 copyright taylor and francis group, llc."
"agent-based computational finance (acf), which develops and reinforces the experimental economics, has a repid big development for over the 2 decades. although its research methods is almost perfect, there still some challenges on its ideological foundation and research paradigm. we discussed the development of acf from the angle of financial research needs, the categories of models and answered these representative challenges. from these answers we can learn acf's ideological foundation and research paradigm very well, and show some advantages of acf to traditional methods in the financial markets' anomalous, market microstructure, behavioral finance and trading mechanism design etc."
"we consider a geometric lévy market model. since these markets are generally incomplete, we cannot find a unique martingale measure. there are many ways to handle this problem. in this paper, we choose the completion technique, firstly introduced in j.m. corcuera, d. nualart and w. schoutens (completion of a lévy market by power-jump assets, journal of computational finance, 7, 1-49, 2004), which employs special artificial assets called power-jump assets. the price processes of power-jump assets are based on an orthogonalized family of teugel martingales. by using the gram-schmidt process and obtaining the coefficients we express the price process of the power-jump assets in terms of teugel martingales. afterwards, we derive pricing formulas for european call options by using two methods: the martingale pricing approach and the characteristic formula approach which is performed via the fast fourier transform (fft). throughout the pricing and application to real market price data, jump sizes are assumed to have a particular distribution."
"provides an accessible foundation to bayesian analysis using real world models this book aims to present an introduction to bayesian modelling and computation, by considering real case studies drawn from diverse fields spanning ecology, health, genetics and finance. each chapter comprises a description of the problem, the corresponding model, the computational method, results and inferences as well as the issues that arise in the implementation of these approaches. case studies in bayesian statistical modelling and analysis: illustrates how to do bayesian analysis in a clear and concise manner using real-world problems. each chapter focuses on a real-world problem and describes the way in which the problem may be analysed using bayesian methods. features approaches that can be used in a wide area of application, such as, health, the environment, genetics, information science, medicine, biology, industry and remote sensing. case studies in bayesian statistical modelling and analysis is aimed at statisticians, researchers and practitioners who have some expertise in statistical modelling and analysis, and some understanding of the basics of bayesian statistics, but little experience in its application. graduate students of statistics and biostatistics will also find this book beneficial. © 2013 john wiley & sons, ltd. all rights reserved."
"bayesian analysis of complex models based on stochastic processes has in recent years become a growing area. this book provides a unified treatment of bayesian analysis of models based on stochastic processes, covering the main classes of stochastic processing including modeling, computational, inference, forecasting, decision making and important applied models. key features: explores bayesian analysis of models based on stochastic processes, providing a unified treatment. provides a thorough introduction for research students. computational tools to deal with complex problems are illustrated along with real life case studies looks at inference, prediction and decision making. researchers, graduate and advanced undergraduate students interested in stochastic processes in fields such as statistics, operations research (or), engineering, finance, economics, computer science and bayesian analysis will benefit from reading this book. with numerous applications included, practitioners of or, stochastic modelling and applied statistics will also find this book useful. © 2012 john wiley & sons, ltd. all rights reserved."
"the world of finance faces the computational performance challenge of massively expanding data volumes, extreme response time requirements, and compute-intensive complex (risk) analyses. simultaneously, new international regulatory rules require considerably more transparency and external auditability of financial institutions, including their software systems. to top it off, increased product variety and cus-tomisation necessitates shorter software development cycles and higher development productivity. in this paper, we report about hiperfit, a recently etablished strategic research center at the university of copenhagen that attacks this triple challenge of increased performance, transparency and productivity in the financial sector by a novel integration of financial mathematics, domain-specific language technology, parallel functional programming, and emerging massively parallel hardware. hiperfit seeks to contribute to effective high-performance modelling by domain specialists, and to functional programming on highly parallel computer architectures in particular, by pursuing a research trajectory informed by the application domain of finance, but without limiting its research scope, generality, or applicablity, to finance. research in hiperfit draws on and aims at producing new research in its different scientific fields, and it fosters synergies between them to deliver showcases of modern language technology and advanced functional methods with the potential for disruptive impact on an area of increasing societal importance. © springer-verlag berlin heidelberg 2012."
"this paper aims at providing a critical assessment of the new farmerian economics, i.e., farmer's attempt to provide a fresh micro-foundation of the general theory grounded on modern search and business cycle theories with the goal of offering a rationale for finance-induced recessions. specifically, i develop a model that summarizes the main arguments of the suggested approach by showing that a special importance has to be paid to the search mechanism, the choice of units and animal spirits modelling. thereafter, referring to recent computational experiments, i discuss some possible empirical implications of the resulting framework. finally, i put forward the lines for new theoretical and empirical developments by sketching the policy implications towards which the new farmerian economics might lead."
"as global financial innovation opens innumerable risks and opportunities, a global view of the asset allocation brings advantages in risk diversification for investments. we propose a novel framework for asset selection under global diversification principles using genetic network programming. simulations using the stocks, bonds and currencies from relevant financial markets in usa, europe and asia show that the proposed framework is effective and offers competitive advantages against the conventional methods in finance and computational fields. © 2011 institute of electrical engineers of japan."
"this outstanding collection of articles includes papers presented at the fields institute, toronto, as part of the thematic program in quantitative finance that took place in the first six months of the year 2010. the scope of the volume in very broad, including papers on foundational issues in mathematical finance, papers on computational finance, and papers on derivatives and risk management. many of the articles contain path-breaking insights that are relevant to the developing new order of post-crisis financial risk management. © 2013 by world scientific publishing co. pte. ltd. all rights reserved."
"jacket is a software platform developed at accelereyes, which allows users to execute matrix laboratory (matlab) m-code on cuda-capable graphics processing units (gpus). matlab by the mathworks is a standard platform for technical computing and graphics in science, engineering, and finance. the combination of a simple matrix language, interactive prompt, automatic memory management, and on-the-fly compilation make matlab well suited to rapid prototyping of algorithms and exploring data. matlab's one drawback is performance, and jacket alleviates this by seamless offloading of computations to the gpu. jacket provides users access to a set of libraries, functions, and tools that facilitate numerical computation on the gpu including multi-gpu support built on matlab's parallel computing toolbox and distributed computing server. jacket has been designed for programmers who have large data-parallel tasks but who are not low level programmers accustomed to dealing with gpu-specific constructs. once data is marked as ""gpu"" data using these functions, jacket provides native gpu implementations of a large set of the standard matlab functions to operate on that data. jacket achieves transparency by defining a new set of classes dubbed ""g"" objects, where each element of this set corresponds to a base class of the matlab standard interface-single, uint16, ones, etc. map to gsingle, guint16, gones, etc. the jacket architecture uses object-oriented programming to handle references to data. jacket includes a graphics toolbox that provides a simple method of displaying computational results on the gpu without bringing those results back to the host. © 2012 elsevier inc. all rights reserved."
"the efficient processing of large collections of patterns (boolean expressions, xpath queries, or continuous sql queries) over data streams plays a central role in major data intensive applications ranging from user-centric processing and personalization to real-time data analysis. on the one hand, emerging user-centric applications, including computational advertising and selective information dissemination, demand determining and presenting to an end-user only the most relevant content that is both user-consumable and suitable for limited screen real estate of target (mobile) devices. we achieve these user-centric requirements through novel high-dimensional indexing structures and (parallel) algorithms. on the other hand, applications in real-time data analysis, including computational finance and intrusion detection, demand meeting stringent subsecond processing requirements and providing high-frequency and low-latency event processing over data streams. we achieve real-time data analysis requirements by leveraging reconfigurable hardware - fpgas - to sustain line-rate processing by exploiting unprecedented degrees of parallelism and potential for pipelining, only available through custom-built, application-specific, and low-level logic design. finally, we conduct a comprehensive evaluation to demonstrate the superiority of our proposed techniques in comparison with state-of-the-art algorithms designed for event processing. © 2012 acm."
"stochastic simulation and applications in finance with matlab programs explains the fundamentals of monte carlo simulation techniques, their use in the numerical resolution of stochastic differential equations and their current applications in finance. building on an integrated approach, it provides a pedagogical treatment of the need-to-know materials in risk management and financial engineering. the book takes readers through the basic concepts, covering the most recent research and problems in the area, including: the quadratic re-sampling technique, the least squared method, the dynamic programming and stratified state aggregation technique to price american options, the extreme value simulation technique to price exotic options and the retrieval of volatility method to estimate greeks. the authors also present modern term structure of interest rate models and pricing swaptions with the bgm market model, and give a full explanation of corporate securities valuation and credit risk based on the structural approach of merton. case studies on financial guarantees illustrate how to implement the simulation techniques in pricing and hedging. the book also includes an accompanying cd-rom which provides matlab programs for the practical examples and case studies, which will give the reader confidence in using and adapting specific ways to solve problems involving stochastic processes in finance. ""this book provides a very useful set of tools for those who are interested in the simulation method of asset pricing and its implementation with matlab. it is pitched at just the right level for anyone who seeks to learn about this fascinating area of finance. the collection of specific topics thoughtfully selected by the authors, such as credit risk, loan guarantee and value-at-risk, is an additional nice feature, making it a great source of reference for researchers and practitioners. the book is a valuable contribution to the fast growing area of quantitative finance."" -tan wang, sauder school of business, ubc. ""this book is a good companion to text books on theory, so if you want to get straight to the meat of implementing the classical quantitative finance models here's the answer."" -paul wilmott, wilmott.com. ""this powerful book is a comprehensive guide for monte carlo methods in finance. every quant knows that one of the biggest issues in finance is to well understand the mathematical framework in order to translate it in programming code. look at the chapter on quasi monte carlo or the paragraph on variance reduction techniques and you will see that huu tue huynh, van son lai and issouf soumaré have done a very good job in order to provide a bridge between the complex mathematics used in finance and the programming implementation. because it adopts both theoretical and practical point of views with a lot of applications, because it treats about some sophisticated financial problems (like brownian bridges, jump processes, exotic options pricing or longstaff-schwartz methods) and because it is easy to understand, this handbook is valuable for academics, students and financial engineers who want to learn the computational aspects of simulations in finance."" -thierry roncalli, head of investment products and strategies, sgam alternative investments & professor of finance, university of evry. © 2008 john wiley & sons ltd, the atrium, southern gate, chichester, west sussex po19 8sq, england."
"this paper presents the regime-switching recurrent reinforcement learning (rsrrl) model and describes its application to investment problems. the rsrrl is a regime-switching extension of the recurrent reinforcement learning (rrl) algorithm. the basic rrl model was proposed by moody and wu (proceedings of the ieee/iafe 1997 on computational intelligence for financial engineering (cifer). ieee, new york, pp 300-307 1997) and presented as a methodology to solve stochastic control problems in finance. we argue that the rrl is unable to capture all the intricacies of financial time series, and propose the rsrrl as a more suitable algorithm for such type of data. this paper gives a description of two variants of the rsrrl, namely a threshold version and a smooth transition version, and compares their performance to the basic rrl model in automated trading and portfolio management applications. we use volatility as an indicator/transition variable for switching between regimes. the out-of-sample results are generally in favour of the rsrrl models, thereby supporting the regime-switching approach, but some doubts exist regarding the robustness of the proposed models, especially in the presence of transaction costs. © 2011 springer-verlag."
"articial neural networks (anns) are being successfully applied for a wide range of problem domains in diverse areas including engineering, physics, finance, medicine, and others related to purposes of prediction, classification, or control. this extensive success can be attributed to many factors: (1) power of modeling: neural networks (nns) are very sophisticated techniques capable of modeling extremely complex functions. a priori knowledge of the system is not needed for constructing the ann because the ann will learn its internal representation from the input/output data of its environment and response. (2) ease of use: nns learn by example. the user of nns gathers representative data and then invokes training algorithms to automatically learn the structure of the data. although the user does need to have some heuristic knowledge of how to select and prepare data, how to select an appropriate nn, and how to interpret the results, the level of user knowledge needed to successfully apply nns is much lower than to use some more traditional nonlinear statistical methods. (3) high computational speed: the ann is an inherently parallel architecture. the result comes from the collective behavior of a large number of simple parallel processing units. therefore, once trained, ann can calculate results from a given input very quickly. because of this feature, anns have a greater potential to be used for the online control system than conventional modeling methods. © 2012 by taylor & francis group, llc."
"the handbook of computational statistics - concepts and methods (second edition) is a revision of the first edition published in 2004, and contains additional comments and updated information on the existing chapters, as well as three new chapters addressing recent work in the field of computational statistics. this new edition is divided into 4 parts in the same way as the first edition. it begins with ""how computational statistics became the backbone of modern data science"" (ch.1): an overview of the field of computational statistics, how it emerged as a separate discipline, and how its own development mirrored that of hardware and software, including a discussion of current active research. the second part (chs. 2 - 15) presents several topics in the supporting field of statistical computing. emphasis is placed on the need for fast and accurate numerical algorithms, and some of the basic methodologies for transformation, database handling, high-dimensional data and graphics treatment are discussed. the third part (chs. 16 - 33) focuses on statistical methodology. special attention is given to smoothing, iterative procedures, simulation and visualization of multivariate data. lastly, a set of selected applications (chs. 34 - 38) like bioinformatics, medical imaging, finance, econometrics and network intrusion detection highlight the usefulness of computational statistics in real-world applications. © springer-verlag berlin heidelberg 2012. all rights reserved."
"during the recent decades, option pricing became an important topic in computational finance. the main issue is to obtain a model of option prices that reflects price movements observed in the real world. in this paper we address option pricing using an evolving fuzzy system model and brazilian interest rate options data. evolving models are particularly appropriate because they gradually develops the model structure and parameters from a stream of data. therefore, evolving fuzzy models provide a higher level of system adaptation and learns the system dynamics continuously, an essential attribute in pricing options estimation. in particular, we emphasize the use of the evolving participatory learning methods. the participatory evolving models considered in this paper are compared against the traditional black's closed-form formula, artificial neural networks structures, and alternative evolving fuzzy system approaches reported in the literature. actual daily data used in the experiments cover the period from january 2003 to june 2008. we measure forecast performance of all models and report the statistical tests done for the competing forecast models. the results show that the participatory evolving fuzzy system modeling approach is effective to estimate prices of fixed income options. © 2011 springer-verlag."
"agent-based computational economics (ace) has received increased attention and importance over recent years. some researchers have attempted to develop an agent-based model of the stock market to investigate the behavior of investors and provide decision support for innovation of trading mechanisms. however, challenges remain regarding the design and implementation of such a model, due to the complexity of investors, financial information, policies, and so on. this paper will describe a novel architecture to model the stock market by utilizing stock agent, finance agent and investor agent. each type of investor agent has a different investment strategy and learning method. a prototype system for supporting stock market simulation and evolution is also presented to demonstrate the practicality and feasibility of the proposed intelligent agent-based artificial stock market system architecture. © 2012 elsevier ltd. all rights reserved."
"matlab is a versatile software package used in many areas of applied mathematics, including computational finance. it is a programming language with a large number of functions for monte carlo simulation useful in financial analysis. the design of matlab allows for flexible data entry, including easy access of financial data from web resources. the graphical capabilities of matlab facilitate exploratory analysis, and the wide range of mathematical and statistical functionality provides the financial data analyst with a powerful tool. this article illustrates some of the basic capabilities ofmatlab, with an emphasis on financial applications. © springer-verlag berlin heidelberg 2012."
"forecasting volatility is an essential step in many financial decision makings. garch family of models has been extensively used in finance and economics, particularly for estimating volatility. the motivation of this study is to enhance the ability of garch models in forecasting the return volatility. we propose two hybrid models based on egarch and artificial neural networks to forecast the volatility of s&p 500 index. the estimates of volatility obtained by an egarch model are fed forward to a neural network. the input to the first hybrid model is complemented by historical values of other explanatory variables. the second hybrid model takes as inputs both series of the simulated data and explanatory variables. the forecasts obtained by each of those hybrid models have been compared with those of egarch model in terms of closeness to the realized volatility. the computational results demonstrate that the second hybrid model provides better volatility forecasts. © 2011 elsevier ltd. all rights reserved."
"this paper presents a new computational scheme for an asymptotic expansion method of an arbitrary order. the asymptotic expansion method in finance initiated by kunitomo and takahashi (1992), yoshida (1992b) and takahashi (1995, 1999) is a widely applicable methodology for an analytic approximation of expectation of a certain functional of diffusion processes. hence, not only academic researchers but also many practitioners have used the methodology for a variety of financial issues such as pricing or hedging complex derivatives under high-dimensional underlying stochastic environments. in practical applications of the expansion, a crucial step is calculation of conditional expectations for a certain kind of wiener functionals. takahashi (1995, 1999) and takahashi and takehara (2007) provided explicit formulas for those conditional expectations necessary for the asymptotic expansion up to the third order. this paper presents the new method for computing an arbitrary-order expansion in a general diffusion-type stochastic environment, which is powerful especially for high-order expansions: we develops a new calculation algorithm for computing coefficients of the expansion through solving a system of ordinary differential equations that is equivalent to computing the conditional expectations directly. to demonstrate its effectiveness, the paper gives numerical examples of the approximation for a λ-sabr model up to the fifth order. © 2012 world scientific publishing company."
"reflected brownian motion has been played an important role in economics, finance, queueing and many other fields. in this paper, we present the explicit spectral representation for the hitting time density of the reflected brownian motion with two-sided barriers, and give some detailed analysis on the computational issues. numerical analysis reveals that the spectral representation is more appealing than the method of numerical laplace inversion. two applications are included at the end of the paper. © 2011 springer science+business media, llc."
"the main assumptions for the statistical process control models is the independent and identically distributed (i.i.d.) observations, but in practice the observed data from industrial processes or finance are serially correlated or have trending. the exponentially weighted moving average (ewma) control chart can detect small shifts in the process mean more quickly than the shewhart control chart. in this paper, we derive an explicit formula for the characteristic of ewma control chart for trend stationary exponential ar(1) processes. we compare the results for average run length (arl) obtained from the explicit formula with values obtained from the integral equation. the new results are simple, easy to programming, which make it attractive to be used in practice by performers. our results show that the explicit expressions reduce considerably the computational time used to evaluate the arl, comparable with the integral equation approach. © 2012 newswood limited. all rights reserved."
"parallel programming using the current state-of-the-art in software engineering techniques is hard. expertise in parallel programming is necessary to deliver good performance in applications; however, it is very common that domain experts lack the requisite expertise in parallel programming. in order to drive the computer science research toward effectively using the available parallel hardware platforms, it is very important to make parallel programming systematical and productive. we believe that the key to designing parallel programs in a systematical way is software architecture, and the key to improve the productivity of developing parallel programs is software frameworks. the basis of both is design patterns and a pattern language. we illustrate how we can use design patterns to architect a wide variety of real applications, including image recognition, speech recognition, optical ?ow computation, video background subtraction, compressed sensing mri, computational finance, video games, and machine translation. by exploring software architectures of our applications, we achieved 10x-140x speedups in each of the applications. we illustrate how we can develop parallel programs productively using application frameworks and programming frameworks. we achieve 50%-100% of the performance while using four times fewer lines of code compared to hand-optimized code. © 2011 springer science+business media, llc."
"complex systems modeling and simulation approaches are being adopted in a growing number of sectors, including finance, economics, biology, astronomy, and many more. technologies ranging from distributed computing to specialized hardware are explored and developed to address the computational requirements arising in complex systems simulations. the aim of this book is to present a representative overview of contemporary large-scale computing technologies in the context of complex systems simulations applications. the intention is to identify new research directions in this field and to provide a communications platform facilitating an exchange of concepts, ideas and needs between the scientists and technologist and complex system modelers. on the application side, the book focuses on modeling and simulation of natural and man-made complex systems. on the computing technology side, emphasis is placed on the distributed computing approaches, but supercomputing and other novel technologies are also considered. © 2012 john wiley & sons, inc."
"many recent survival studies propose modeling data with a cure fraction, i. e., data in which part of the population is not susceptible to the event of interest. this event may occur more than once for the same individual (recurrent event). we then have a scenario of recurrent event data in the presence of a cure fraction, which may appear in various areas such as oncology, finance, industries, among others. this paper proposes a multiple time scale survival model to analyze recurrent events using a cure fraction. the objective is analyzing the efficiency of certain interventions so that the studied event will not happen again in terms of covariates and censoring. all estimates were obtained using a sampling-based approach, which allows information to be input beforehand with lower computational effort. simulations were done based on a clinical scenario in order to observe some frequentist properties of the estimation procedure in the presence of small and moderate sample sizes. an application of a well-known set of real mammary tumor data is provided. © 2011 sociedad de estadística e investigación operativa."
"the problem of identification of statistically significant patterns in a sequence of data has been applied to many domains such as intrusion detection systems, financial models, web-click records, automated monitoring systems, computational biology, cryptology, and text analysis. an observed pattern of events is deemed to be statistically significant if it is unlikely to have occurred due to randomness or chance alone. we use the chi-square statistic as a quantitative measure of statistical significance. given a string of characters generated from a memoryless bernoulli model, the problem is to identify the substring for which the empirical distribution of single letters deviates the most from the distribution expected from the generative bernoulli model. this deviation is captured using the chi-square measure. the most significant substring (mss) of a string is thus defined as the substring having the highest chi-square value. till date, to the best of our knowledge, there does not exist any algorithm to find the mss in better than o(n2) time, where n denotes the length of the string. in this paper, we propose an algorithm to find the most significant substring, whose running time is o(n3/2) with high probability. we also study some variants of this problem such as finding the top-t set, finding all substrings having chi-square greater than a fixed threshold and finding the mss among substrings greater than a given length. we experimentally demonstrate the asymptotic behavior of the mss on varying the string size and alphabet size. we also describe some applications of our algorithm on cryptology and real world data from finance and sports. finally, we compare our technique with the existing heuristics for finding the mss."
"we introduce a new discriminant analysis method (empirical discriminant analysis or eda) for binary classification in machine learning. given a dataset of feature vectors, this method defines an empirical feature map transforming the training and test data into new data with components having gaussian empirical distributions. this map is an empirical version of the gaussian copula used in probability and mathematical finance. the purpose is to form a feature mapped dataset as close as possible to gaussian, after which standard quadratic discriminants can be used for classification. we discuss this method in general, and apply it to some datasets in computational biology. © 2011 ieee."
"both private and public organizations are considering the implementation of video surveillance technology for the purposes of general law enforcement and public safety programs. in several situations, such solutions, often characterized by high-speed network connections, plenty of storage capacity, and a high computational power, may be suitable in protecting public safety, detection or deterring, as well as assisting in the investigating of criminal activity. in this scenario, privacy protection, lawful evidence enforcement (through incontrovertible documentary proof), and content confidentiality are the most challenging security topics relating to several information society sectors (finance, homeland security, healthcare, etc.) that require interdisciplinary input from legal experts, technicians, privacy advocates, as well as security consultants. starting from these ideas and concepts, this chapter aims at presenting an innovative network-based digital video surveillance solution that meets all the aforementioned security and privacy requirements ensuring that the recorded data will be only accessible to a subset of authorities, trusting each other under precisely defined policies, agreements, and circumstances. this would aid the surveillance activities, when needed, without disrupting the privacy of individuals. © 2012, igi global."
"the selection of assets in which to invest money is of critical importance in the finance industry, and is rendered very treacherous because of the inherent market fluctuations, and the connections with the economy, and major world events. because of the high dimensionality of the problem of selecting an optimal portfolio (in the financial sense of, a portfolio outperforming other portfolios), and the large amount of data available, intelligent systems (e.g. artificial intelligence techniques, machine learning approaches) are a natural approach to tackle this problem, from a computational standpoint. numerous techniques have been developed to combine the values of return, risk, and other characteristics of an asset. however, the majority of techniques that have been used to construct a portfolio, tend to ignore dependencies among the characteristics of an asset. moreover, most of the techniques assume that all available data are precise, which is not the case since, for instance, the expected return of an asset is a prediction of future behavior. to address these drawbacks, it was proposed in magoc, modave, ceberio, and kreinovich (2009) to use non-additive (or fuzzy) methods. fuzzy methods outperformed other techniques, at least in the case of the shanghai market, where full disclosure of information is assumed. in this paper, we give an intuition why fuzzy approach performs very well in this particular finance problem. © 2010 elsevier ltd. all rights reserved."
"this book uses the em (expectation maximization) algorithm to simultaneously estimate the missing data and unknown parameter(s) associated with a data set. the parameters describe the component distributions of the mixture; the distributions may be continuous or discrete. the editors provide a complete account of the applications, mathematical structure and statistical analysis of finite mixture distributions along with mcmc computational methods, together with a range of detailed discussions covering the applications of the methods and features chapters from the leading experts on the subject. the applications are drawn from scientific discipline, including biostatistics, computer science, ecology and finance. this area of statistics is important to a range of disciplines, and its methodology attracts interest from researchers in the fields in which it can be applied. © 2011 john wiley & sons, ltd. all rights reserved."
"we formulate a portfolio planning model that is based on second-order stochastic dominance as the choice criterion. this model is an enhanced version of the multi-objective model proposed by roman et al. [math. progr. ser. b, 2006, 108, 541-569]; the model compares the scaled values of the different objectives, representing tails at different confidence levels of the resulting distribution. the proposed model can be formulated as a risk minimization model where the objective function is a convex risk measure; we characterize this risk measure and the resulting optimization problem. moreover, our formulation offers a natural generalization of the ssd-constrained model of dentcheva and ruszczyń ski [j. bank. finance, 2006, 30, 433-451]. a cutting plane-based solution method for the proposed model is outlined. we present a computational study showing: (a) the effectiveness of the solution methods and (b) the improved modeling capabilities: the resulting portfolios have superior return distributions. © 2011 taylor & francis."
"during the recent decades, option pricing became an important topic in computational finance. the main issue is to obtain a model of option prices that reflects price movements observed in the real world. in this paper we address option pricing using an evolving fuzzy system model and brazilian interest rate options pricing data. evolving models are particularly appropriate since it gradually develops the model structure and its parameters from a stream of data. therefore, evolving fuzzy models provide a higher level of system adaptation and learns the system dynamics continuously, an essential attribute in pricing option estimation. in particular, we emphasize the use of the evolving participatory learning method. the model suggested in this paper is compared against the traditional black closed-form formula, artificial neural networks structures and alternative evolving fuzzy system approaches. actual daily data used in the experiments cover the period from january 2003 to june 2008. we measure forecast performance of all models based on summary measures of forecast accuracy and statistical tests for competing models. the results show that the evolving fuzzy system model is effective especially for out-of-the-money options. © 2011 ieee."
"ipo underpricing is always the focus of finance theory and practice. according to behavioral finance, this paper explores the inherent process of ipo underpricing from microscopic investor behavior and market environment by constructing an agent-based computational model. the conclusions show that the uppermost causation for severe ipo underpricing is the presence of crazy investors and hot market environment rather than the beliefs divergence. so it is necessary to strengthen the information disclosure and investors' education. © 2011 ieee."
"stock portfolio selection is a classic problem in finance, and it involves deciding how to allocate an institution's or an individual's wealth to a number of stocks, with certain investment objectives (return and risk). in this paper, we adopt the classical markowitz mean-variance model and consider an additional common realistic constraint, namely, the cardinality constraint. thus, stock portfolio optimization becomes a mixed-integer quadratic programming problem and it is difficult to be solved by exact optimization algorithms. chemical reaction optimization (cro), which mimics the molecular interactions in a chemical reaction process, is a population-based metaheuristic method. two different types of cro, named canonical cro and super molecule-based cro (s-cro), are proposed to solve the stock portfolio selection problem. we test both canonical cro and s-cro on a benchmark and compare their performance under two criteria: markowitz efficient frontier (pareto frontier) and sharpe ratio. computational experiments suggest that s-cro is promising in handling the stock portfolio optimization problem."
"the ability to conceptualize an economic problem verbally, to formulate it as a mathematical model, and then represent the mathematics in software so that the model can be solved on a computer is a crucial skill for economists.computational economicscontains well-known models--and some brand-new ones--designed to help students move from verbal to mathematical to computational representations in economic modeling. the authors' focus, however, is not just on solving the models, but also on developing the ability to modify them to reflect one's interest and point of view. the result is a book that enables students to be creative in developing models that are relevant to the economic problems of their times.unlike other computational economics textbooks, this book is organized around economic topics, among them macroeconomics, microeconomics, and finance. the authors employ various software systems--including matlab, mathematica, gams, the nonlinear programming solver in excel, and the database systems in access--to enable students to use the most advantageous system. the book progresses from relatively simple models to more complex ones, and includes appendices on the ins and outs of running each program.the book is intended for use by advanced undergraduates and professional economists and even, as a first exposure to computational economics, by graduate students.organized by economic topicsprogresses from simple to more complex modelsincludes instructions on numerous software systemsencourages customization and creativity. © 2006 by princeton university press. all rights reserved."
"convex multiobjective programming problems and multiplicative programming problems have important applications in areas such as finance, economics, bond portfolio optimization, engineering, and other fields. this paper presents a quite easy algorithm for generating a number of efficient outcome solutions for convex multiobjective programming problems. as an application, we propose an outer approximation algorithm in the outcome space for solving the multiplicative convex program. the computational results are provided on several test problems. © 2011 le quang thuy et al."
"a contour integral method recently proposed by w eideman [ima j. numer. anal., 30 (2010), pp. 334-350] for integrating semidiscrete advection-diffusion pdes is improved and extended for application to some of the important equations of mathematical finance. using estimates for the numerical range of the spatial operator, optimal contour parameters are derived theoretically and tested numerically. an improvement on the existing method is the use of krylov methods for the shifted linear systems, the solution of which represents the major computational cost of the algorithm. a parallel implementation is also considered. test examples presented are the black-scholes pde in one space dimension and the heston pde in two dimensions, for both vanilla and barrier options. in the heston case efficiency is compared to adi splitting schemes, and experiments show that the contour integral method is superior for the range of medium to high accuracy requirements. © 2011 society for industrial and applied mathematics."
"domain-specific modeling languages (dsmls) have been recognized as a viable solution for reducing the gap between domain abstractions and computational expression within specific domains. in several domains and contexts, dsmls have been applied successfully to various areas (e.g., finance, combat simulation, and image manipulation) and have shown improvements to productivity and quality. however, development of a new dsml is not an easy task for either computer scientists or end-users because designing and implementing a dsml requires profound knowledge of the domain and deep experience in modeling language development. to address the challenges of dsml development, this doctoral symposium abstract outlines a new approach for building dsmls that represents a demonstration-based technique for specifying the details of a new modeling language. the approach provides an environment for describing and generating the abstract and concrete syntax of a dsml. initial work on describing the semantics of a new dsml is also a focus of the work. the research represents an investigation into a technique that allows end-users to sketch (or demonstrate) a domain model with free-form shapes. the goal of the proposed research is to develop the underlying science and tool support to enable end-users to assist in designing a dsml for their domain, while minimizing the typical mundane tasks of dsml development involving many accidental complexities."
"this paper presents a theoretical model with micro-foundations that captures some important features of pakistan's economy which have emerged in sixty-four years of its history. a comparison of pakistan's economic performance during different regimes shows that macroeconomic fundamentals tend to show an improvement during the autocratic regimes as compared with those prevailing during democratic regimes. in particular, periods of autocratic regimes are typically characterised by low inflation, robust growth and low level of bureaucratic corruption due to better governance. in contrast, the economic performance during the democratic regimes has been observed to worsen with weak governance and high levels of corruption, high inflation due partly to reliance on seigniorage to finance public spending, and lacklustre growth. using annual data from 1950 to 2011, computational modelling is carried out by applying markov-regime switching technique with maximum-likelihood procedures. the estimation results based on empirical modelling setup are supportive of the above stylised-facts and also confirm the implications of the theoretical model. © the pakistan development review."
"this book contains selected papers presented at the 14th international conference on computing in economics and finance (cef 2008), organized by the society of computational economics as well as some additional invited papers. a main topic in this volume is the issue of market design and resulting market dynamics. the economic crisis of 2007–2009 has once again highlighted the importance of a proper design of market protocols and institutional details for economic dynamics and macroeconomics. in particular, it became clear that the failure of many traditional models to capture behavioral details of agents’ decision making, contagion effects, spillovers between markets and effects to the macroeconomy made it difficult to understand the mechanisms driving the economic meltdown. also apart from the treatment of economic crises it has been recognized in several important areas of economic policy, like regulation of energy markets, that a proper study of implications of different institutional setups is crucial for an understanding of the evolution of markets as well as policy effects. © 2011, springer-verlag berlin heidelberg."
"a computational method is described for option replication. in particular, a procedure is provided for computing the projection basis that corresponds to a positive basis of m. application of this procedure in order to compute maximal submarkets that replicate any option is demonstrated. specifically, we provide a computational study for the replication of options in security markets with a finite number of states and a finite number of primitive assets with payoffs given by linearly independent vectors of m. the theoretical background of this work follows the results in polyrakis and xanthos [maximal submarkets that replicate any option, ann. finance, doi: 10.1007/s10436-009-0143-9]. our goal is to make option replication computationally tractable and hence more viable as a financial tool. © 2011 taylor & francis."
"a rapid development of time series models and methods addressing volatility in computational finance and econometrics are recently reported in the financial literature. this paper considers doubly stochastic volatility models with garch errors. general properties for process mean, variance and kurtosis are derived as these results can be used in model identification. © 2011 elsevier ltd. all rights reserved."
"financial business prediction has lately raised a great interest due to the recent world crisis events. in spite of the many advanced shallow computational methods that have extensively been proposed, most algorithms have not yet attained a desirable level of applicability. all show a good performance for a given financial setup but fail in general to create better and reliable models. the main focus of this paper is to present a deep learning model with strong ability to generate high level feature representations for accurate financial prediction. the proposed deep belief network (dbn) approach tested in a real dataset of french companies compares favorably to shallow architectures such as support vector machines (svm) and single restricted boltzmann machine (rbm). we show that the underlying financial model with deep machine technology has a strong potential thus empowering the finance industry. © 2011 springer-verlag."
"in this paper we survey the primary research, both theoretical and applied, in the area of robust optimization (ro). our focus is on the computational attractiveness of ro approaches, as well as the modeling power and broad applicability of the methodology. in addition to surveying prominent theoretical results of ro, we also present some recent results linking ro to adaptable models for multistage decision-making problems. finally, we highlight applications of ro across a wide spectrum of domains, including finance, statistics, learning, and various areas of engineering. © 2011 society for industrial and applied mathematics."
"interest in the skew-normal and related families of distributions has grown enormously over recent years, as theory has advanced, challenges of data have grown, and computational tools have made substantial progress. this comprehensive treatment, blending theory and practice, will be the standard resource for statisticians and applied researchers. assuming only basic knowledge of (non-measure-theoretic) probability and statistical inference, the book is accessible to the wide range of researchers who use statistical modelling techniques. guiding readers through the main concepts and results, it covers both the probability and the statistics sides of the subject, in the univariate and multivariate settings. the theoretical development is complemented by numerous illustrations and applications to a range of fields including quantitative finance, medical statistics, environmental risk studies, and industrial and business efficiency. the author’s freely available r package sn, available from cran, equips readers to put the methods into action with their own data. © adelchi azzalini and antonella capitanio 2014."
"the curse of dimensionality limits the accuracy of the quasi-monte carlo (qmc) method in high-dimensional problems. imai and tan (proceedings of the 2002 winter simulation conference, 2002; monte carlo and quasi-monte carlo methods 2002, pp 275-292, springer, berlin, 2004; j comput finance 10(2):129-155, 2007) have proposed a dimension reduction technique, named linear transformation (lt), aiming to improve the efficiency of the qmc method. we investigate this approach in detail and make it more convenient. we implement a faster qr decomposition that considerably reduces the computational burden. the efficacy of our algorithm is illustrated by considering two high-dimensional option pricing problems: asian basket options in the black-scholes (bs) model and asian options in the cox-ingersoll-ross (cir) model. we employ a qmc generator only for the components selected by the lt construction and use latin hypercube sampling (lhs) for all the others. finally, we compare our results to those obtained by different random number generators and standard algorithms; subsequently, we benchmark our computational times against those presented in imai and tan (j comput finance 10(2):129-155, 2007). © 2009 springer-verlag."
"with the widespread availability of business big data and the recent advancement in text and web mining, tremendous opportunities exist for computational and finance researchers to advance research relating to smart market and money. this t&c department includes three article on smart market and money from distinguished experts in information systems and business. each article presents unique perspectives, advanced computational methods, and selected results and examples. © 2006 ieee."
"with the latest developments in the area of advanced computer architectures, we are already seeing large scale machines at petascale level and we are faced with the exascale computing challenge. all these require scalability at system, algorithmic and mathematical model level. in particular, efficient scalable algorithms are required to bridge the performance gap. in this paper, examples of various approaches of designing scalable algorithms for such advanced architectures will be given. we will briefly present our approach to monte carlo scalable algorithms for linear algebra and explain how these approaches are extended to the field of computational finance. implementation examples will be presented using linear algebra problems and problems from computational finance. furthermore, the corresponding properties of these algorithms will be outlined and discussed. © 2011 published by elsevier ltd."
"this research addresses a new cross-training programming method for an assembly cell from the point of view of humanization and finance. the cross-training method is about how to decide which labors should be cross-trained on which tasks, and a multiobjective 0-1 integer programming model is proposed. the first objective seeks to maximize average satisfaction degree (asd), and the second objective seeks to minimize average paid salary (aps). a series of computational experiments are proceeded to get some insights. the results indicate that with regards to asd, balanced preference structure is better than extreme, and with regards to aps, nonuniform salary structure is better than uniform. those insights will provide the right direction for practioners."
"in this paper we propose and analyze a number of two-scale discretization schemes for integrodifferential equations arising in finance. it is shown theoretically and numerically that the number of degrees of freedom of the two-scale discretization is significantly smaller than that of the standard one-scale finite element approach while at the same time preserving the accuracy of the one-scale discretization. the main idea of these algorithms is to use a coarse grid to approximate the low frequencies and then to use a fine grid to correct the relatively high frequencies. as a result, both the computational time and the storage can be reduced considerably. a combination of wavelet and lagrangian finite element basis functions is applied to further reduce the complexity arising from the non-locality of the integrodifferential operators. © 2011 rocky mountain mathematics consortium."
"graph pattern matching involves finding exact or approximate matches for a query subgraph in a larger graph. it has been studied extensively and has strong applications in domains such as computer vision, computational biology, social networks, security and finance. the problem of exact graph pattern matching is often described in terms of subgraph isomorphism which is np-complete. the exponential growth in streaming data from online social networks, news and video streams and the continual need for situational awareness motivates a solution for finding patterns in streaming updates. this is also the prime driver for the real-time analytics market. development of incremental algorithms for graph pattern matching on streaming inputs to a continually evolving graph is a nascent area of research. some of the challenges associated with this problem are the same as found in continuous query (cq) evaluation on streaming databases. this paper reviews some of the representative work from the exhaustively researched field of cq systems and identifies important semantics, constraints and architectural features that are also appropriate for hpc systems performing real-time graph analytics. for each of these features we present a brief discussion of the challenge encountered in the database realm, the approach to the solution and state their relevance in a high-performance, streaming graph processing framework. © 2011 acm."
the proceedings contain 6 papers. the topics discussed include: domain specific languages and the acceleration of computational finance; exascale computing challenges & their application to a production datacenter; algorithmic complexity in the heston model: an implementation view; finding the right level of abstraction for minimizing operational expenditure; autotuning for high performance computing; finrc: challenges and opportunities for high-performance reconfigurable computing (hprc) in computational finance; federal market information technology in the post flash crash era: roles for supercomputing; dsl programmable engine for high frequency trading acceleration; and practical experiences on the gridification of financial applications.
"the availability of high-frequency data on transactions, quotes, and order flow in electronic order-driven markets has revolutionized data processing and statistical modeling techniques in finance and brought up new theoretical and computational challenges. market dynamics at the transaction level cannot be characterized solely in terms the dynamics of a single price, and one must also take into account the interaction between buy and sell orders of different types by modeling the order flow at the bid price, ask price, and possibly other levels of the limit order book. © 2011 ieee."
"this paper describes a new computational finance approach. this approach combines pattern recognition techniques with an evolutionary computation kernel applied to financial markets time series in order to optimize trading strategies. moreover, for pattern matching a template-based approach is used in order to describe the desired trading patterns. the parameters for the pattern templates, as well as, for the decision making rules are optimized using a genetic algorithm kernel. the approach was tested considering actual data series and presents a robust profitable trading strategy which clearly beats the market, s&p 500 index, reducing the investment risk significantly. © 2011 ieee."
"the workshop on high-performance computing applied to finance (hpcf) focuses on the computational issues in the solution by advanced architectures of financial problems, particularly concerning the evaluation of financial instruments, asset and liability portfolio management, measuring and monitoring of risks and assessment of solvency requirements. © 2011 springer-verlag berlin heidelberg."
"using the method of agent-based computational finance, this paper designs ten experiments to examine the impacts of the index futures market, typical investment strategies, and different trading mechanisms on the volatility of the chinese stock market, taking into account the behavior of investors. we have the following results. first, the volatility of the stock market decreases with the index future market and cross-market arbitrageurs. second, different investment strategies have different effects on stock market volatility. in many cases, both market-imitating and stop-loss strategies can increase stock market volatility. third, the mechanism of price limits for the index futures market can help to stabilize the fluctuation of the stock market. © 2011 world scientific publishing company."
"we are interested in forecasting bankruptcies in a probabilistic way. specifically, we compare the classification performance of several statistical and machine-learning techniques, namely discriminant analysis (altman’s z-score), logistic regression, least-squares support vector machines and different instances of gaussian processes (gp’s)—that is gp classifiers, bayesian fisher discriminant and warped gps. our contribution to the field of computational finance is to introduce gps as a competitive probabilistic framework for bankruptcy prediction. data from the repository of information of the us federal deposit insurance corporation is used to test the predictions. © 2011, springer-verlag berlin heidelberg."
"a comprehensive overview of monte carlo simulation that explores the latest topics, techniques, and real-world applications. more and more of today's numerical problems found in engineering and finance are solved through monte carlo methods. the heightened popularity of these methods and their continuing development makes it important for researchers to have a comprehensive understanding of the monte carlo approach. handbook of monte carlo methods provides the theory, algorithms, and applications that helps provide a thorough understanding of the emerging dynamics of this rapidly-growing field. the authors begin with a discussion of fundamentals such as how to generate random numbers on a computer. subsequent chapters discuss key monte carlo topics and methods, including: • random variable and stochastic process generation • markov chain monte carlo, featuring key algorithms such as the metropolis-hastings method, the gibbs sampler, and hit-and-run • discrete-event simulation • techniques for the statistical analysis of simulation data including the delta method, steady-state estimation, and kernel density estimation • variance reduction, including importance sampling, latin hypercube sampling, and conditional monte carlo • estimation of derivatives and sensitivity analysis • advanced topics including cross-entropy, rare events, kernel density estimation, quasi monte carlo, particle systems, and randomized optimization. the presented theoretical concepts are illustrated with worked examples that use matlab®, a related web site houses the matlab® code, allowing readers to work hands-on with the material and also features the author's own lecture notes on monte carlo methods. detailed appendices provide background material on probability theory, stochastic processes, and mathematical statistics as well as the key optimization concepts and techniques that are relevant to monte carlo simulation. handbook of monte carlo methods is an excellent reference for applied statisticians and practitioners working in the fields of engineering and finance who use or would like to learn how to use monte carlo in their research. it is also a suitable supplement for courses on monte carlo methods and computational statistics at the upper-undergraduate and graduate levels. © 2011 by john wiley & sons, inc. all rights reserved."
"explicit finite difference method is widely used in finance for pricing many kinds of options. its regular computational pattern makes it an ideal candidate for acceleration using reconfigurable hardware. however, because the corresponding hardware designs must be optimised both for the specific option and for the target platform, it is challenging and time consuming to develop designs efficiently and productively. this paper presents a unifying framework for describing and automatically implementing financial explicit finite difference procedures in reconfigurable hardware, allowing parallelised and pipelined implementations to be created from high-level mathematical expressions. the proposed framework is demonstrated using three option pricing problems. our results show that an implementation from our framework targeting a virtex-6 device at 310mhz is more than 24 times faster than a software implementation fully optimised by the intel compiler on a four-core xeron cpu at 2.66ghz. in addition, the latency of the fpga solvers is up to 90 times lower than the corresponding software solvers. © 2011 ieee."
"starting from first principles, this book covers all of the foundational material needed to develop a clear understanding of the mathematica language, with a practical emphasis on solving problems. concrete examples throughout the text demonstrate how mathematica language, can be used to solve problems in science, engineering, economics/finance, computational linguistics, geoscience, bioinformatics, and a range of other fields.; the book will appeal to students, researchers and programmers wishing to further their understanding of mathematica language. designed to suit users of any ability, it assumes no formal knowledge of programming so it is ideal for self-study. over 290 exercises are provided to challenge the reader’s understanding of the material covered and these provide ample opportunity to practice using the language. mathematica language notebooks containing examples, programs and solutions to exercises are available from www.cambridge.org/wellin. © paul wellin 2013 and 1962, 1964 by new directions publishing corp, 2012 artists rights sociery (ars), new york / adagp, paris / succession marcel duchamp."
"we consider the problem of estimating the covariance matrix of a high-dimensional random vector in the scarce data setting, where the number of samples is less than or comparable to the dimension. the sample covariance matrix is a poor choice in this setting, and a variety of structural assumptions have been considered in the literature: covariance selection models with sparse precision matrices, low-rank models (pca and factor analysis), sparse plus low-rank, and even multi-scale structures. we consider another type of structure, which plays an important role in several applications, where the random vectors can be indexed over a low-dimensional manifold, and the covariance matrix has smoothness and monotonicity properties over the manifold. these assumptions appear in applications as diverse as modeling the noise covariance in sensor-array networks, and in interest-rate modeling in computational finance. we describe how these assumptions can be enforced in a convex optimization framework using semidefinite programming (sdp) and first order proximal gradient methods, and motivate expected sample complexity requirements. we apply our approach in the interest rate modeling setting. © 2011 ieee."
"this book describes computational finance tools. it covers fundamental numerical analysis and computational techniques, such as option pricing, and gives special attention to simulation and optimization. many chapters are organized as case studies around portfolio insurance and risk estimation problems. in particular, several chapters explain optimization heuristics and how to use them for portfolio selection and in calibration of estimation and option pricing models. such practical examples allow readers to learn the steps for solving specific problems and apply these steps to others. at the same time, the applications are relevant enough to make the book a useful reference. matlab and r sample code is provided in the text and can be downloaded from the book's website. shows ways to build and implement tools that help test ideas focuses on the application of heuristics; standard methods receive limited attention presents as separate chapters problems from portfolio optimization, estimation of econometric models, and calibration of option pricing models. © 2011 elsevier inc. all rights reserved."
"in this paper, a simple algorithm to improve the computational accuracy of the analytical approximation for the value of american put options and their optimal exercise boundary proposed by zhu (int. j. theor. appl. finance 9(7):1141-1177, 2006) is presented. in the current approach, zhu's simple approximation formula is used as an initial guess for the optimal exercise boundary of american put options. the determination of an improved optimal exercise boundary is then achieved by setting a null value of the theta of option on the optimal exercise boundary. numerical test results show that the improvement in accuracy is indeed significant in determining the optimal exercise boundary. © 2010 korean society for computational and applied mathematics."
"this book describes computational finance tools. it covers fundamental numerical analysis and computational techniques, such as option pricing, and gives special attention to simulation and optimization. many chapters are organized as case studies around portfolio insurance and risk estimation problems.  in particular, several chapters explain optimization heuristics and how to use them for portfolio selection and in calibration of estimation and option pricing models. such practical examples allow readers to learn the steps for solving specific problems and apply these steps to others. at the same time, the applications are relevant enough to make the book a useful reference. matlab and r sample code is provided in the text and can be downloaded from the book's website. © 2011 elsevier inc. all rights reserved."
"the last twenty years have witnessed tremendous advances in the mathematical, statistical, and computational tools available to applied macroeconomists. this rapidly evolving field has redefined how researchers test models and validate theories. yet until now there has been no textbook that unites the latest methods and bridges the divide between theoretical and applied work.fabio canova brings together dynamic equilibrium theory, data analysis, and advanced econometric and computational methods to provide the first comprehensive set of techniques for use by academic economists as well as professional macroeconomists in banking and finance, industry, and government. this graduate-level textbook is for readers knowledgeable in modern macroeconomic theory, econometrics, and computational programming using rats, matlab, or gauss. inevitably a modern treatment of such a complex topic requires a quantitative perspective, a solid dynamic theory background, and the development of empirical and numerical methods--which is where canova's book differs from typical graduate textbooks in macroeconomics and econometrics. rather than list a series of estimators and their properties, canova starts from a class of dsge models, finds an approximate linear representation for the decision rules, and describes methods needed to estimate their parameters, examining their fit to the data. the book is complete with numerous examples and exercises.today's economic analysts need a strong foundation in both theory and application.methods for applied macroeconomic researchoffers the essential tools for the next generation of macroeconomists. © 2007 by princeton university press. all rights reserved."
"domain-specific modeling languages (dsmls) have been widely used in several domains (e.g., finance, combat simulation, and image manipulation) because they aid to improve productivity and quality by reducing the gap between domain abstractions and computational expression within specific domains. however, dsmls are developed when they are absolutely necessary because dsmls engineered by iterating complex and mundane language creation tasks and dsml development requires domain knowledge and language development expertise. to tackle the challenges of dsml development, this poster abstract outlines a new approach for specifying and generating the abstract and concrete syntax of a dsml based on user demonstration. the goal of the proposed research is to develop the underlying science and tool support to enable end-users to assist in designing a dsml for their domain, while minimizing the typical mundane tasks of dsml development involving many accidental complexities."
"this paper compares multifractal models for assets returns from four aspects of economic perspective: ability of reproducing stylized facts, support of economic theory, accuracy of forecasting and explaining, mathematical elegancy and computational parsimony. all the multifractal models, according to their hypotheses and logic, can be classified into three categories, the representatives of which are mrw, msm and semf respectively. comparing with the models in classical finance, the multifractal models have better forecasting performance and more excellent ability to reproduce stylized facts. mrw has a little preponderance in reproducing stylized facts but fewer literature and empirical estimated method than msm. msm has more preponderance in dealing with multi-frequency asset returns than mrw. semf is better than mrw and msm in four aspects of economic perspective but has the least literature and empirical estimated method than msm or mrw. the semf and its extend models have extraordinary advantage and glamour for economists and are worth further research. © 2011 ieee."
"second-order stochastic dominance (ssd) is widely recognised as an important decision criterion in portfolio selection. unfortunately, stochastic dominance models are known to be very demanding from a computational point of view. in this paper we consider two classes of models which use ssd as a choice criterion. the first, proposed by dentcheva and ruszczyński (j bank finance 30:433-451, 2006), uses a ssd constraint, which can be expressed as integrated chance constraints (iccs). the second, proposed by roman et al. (math program, ser b 108:541-569, 2006) uses ssd through a multi-objective formulation with cvar objectives. cutting plane representations and algorithms were proposed by klein haneveld and van der vlerk (comput manage sci 3:245-269, 2006) for iccs, and by künzi-bay and mayer (comput manage sci 3:3-27, 2006) for cvar minimization. these concepts are taken into consideration to propose representations and solution methods for the above class of ssd based models. we describe a cutting plane based solution algorithm and outline implementation details. a computational study is presented, which demonstrates the effectiveness and the scale-up properties of the solution algorithm, as applied to the ssd model of roman et al. (math program, ser b 108:541-569, 2006). © 2009 springer and mathematical programming society."
behavioral finance researchers can apply computational methods to large-scale social media data to better understand and predict markets. © 2011 ieee.
"random number generation is a key component of many forms of simulation, and fast parallel generation is particularly important for the naturally parallel monte carlo simulations that are used extensively in computational finance and many areas of computational science and engineering. this chapter discusses the parallelization of three very popular random number generators. in each case, the random number sequence that is generated is identical to that produced on a cpu by the standard sequential algorithm. the key to the parallelization is that each cuda thread block generates a particular block of numbers within the original sequence, and to do this step, it needs an efficient skip-ahead algorithm to jump to the start of its block. although there is much in common in the underlying mathematical formulation of these three generators, there are also very significant differences owing to differences in the size of the state information required by each generator. the intel random number generators are contained in the vector statistical library (vsl). this library is not multithreaded, but is thread safe and contains all the necessary skip-ahead functions to advance the generators' states. the way in which consideration of the number of registers required, the details of data dependency in advancing the state, and the desire for memory coalescence in storing the output lead to different implementations in the three cases is of most importance. © 2011 copyright © 2011 nvidia corporation and wen-mei w. hwu published by elsevier inc. all rights reserved.."
"computational intelligence techniques are very useful tools for solving problems that involve understanding, modeling, and analyzing large data sets. one of the numerous fields where computational intelligence has found an extremely important role is finance. more precisely, the problem of selecting an investment portfolio to guarantee a given return, at a minimal risk, have been solved using intelligent techniques such as support-vector machines, neural networks, rule-based expert systems, and genetic algorithms. even though these methods provide good and usually fast approximation of the best investment strategy, they suffer some common drawbacks including the neglect of the dependence among criteria characterizing investment assets (i.e. return, risk, etc.), the ignorance of the interdependence among assets, and the assumption that all available data are precise and certain. to face these weaknesses, we suggest the use of utility-based multi-criteria decision making setting and fuzzy integration over intervals."
"computational and applied methods in science and engineering is an emerging and promising discipline in shaping future research and development activities in both academia and industry, in fields ranging from engineering, science, finance, and economics, to arts and humanities. new challenges arise in the modeling of complex systems, sophisticated algorithms, advanced scientific and engineering computing and associated (multidisciplinary) problem-solving environments. because the solution of large and complex problems must cope with tight timing schedules, powerful algorithms and computational techniques are inevitable. this book covers new research activities and development in the field of applied sciences, engineering and technology. © 2010 by nova science publishers, inc. all rights reserved."
"we present a dynamic programming based solution to a probabilistic reach-avoid problem for a controlled discrete time stochastic hybrid system. we address two distinct interpretations of the reach-avoid problem via stochastic optimal control. in the first case, a sum-multiplicative cost function is introduced along with a corresponding dynamic recursion which quantifies the probability of hitting a target set at some point during a finite time horizon, while avoiding an unsafe set during each time step preceding the target hitting time. in the second case, we introduce a multiplicative cost function and a dynamic recursion which quantifies the probability of hitting a target set at the terminal time, while avoiding an unsafe set during the preceding time steps. in each case, optimal reach while avoid control policies are derived as the solution to an optimal control problem via dynamic programming. computational examples motivated by two practical problems in the management of fisheries and finance are provided. © 2010 elsevier ltd. all rights reserved."
"contrary to the growing use of portfolios and in spite of its rich literature, there are some problems and unanswered questions. the aim of this work is to be a useful instrument for helping finance practitioners and researchers with the portfolio selection problem. this study first reviews modern portfolio theory's literature and describes the investment selection process. second, this paper specifically aims at applying efficient optimization methods for solving the portfolio selection problem. hence, the genetic algorithm and the particle swarm optimization (pso) approaches to resolve the portfolio selection problem with the objective of simultaneous risk minimization/return maximization are applied. computational analyses are provided so as to investigate the performance of the algorithms and the input data. therefore, at one step, four different portfolios are selected from tehran stock exchange market, 50 top companies (annual and monthly-based portfolios). finally, this paper applies the selected portfolios to the stock market data of a 10-month period proceeding portfolios constructions. the results indicate that, the genetic annual-based portfolio has the best performance in contrast to its other counterparts; it outperforms the average return of market portfolio in a relatively short period. moreover, pso annual-based portfolio has a relatively good performance and could achieve the market portfolio's return during the test period. findings show that using annual data is more efficient than the monthly data. on the whole, the results show that the evolutionary methods of this paper with annual data, can consistently handle the practical portfolio selection problem. © 2010 ieee."
"the finite state markov-chain approximation methods developed by tauchen (1986) and tauchen and hussey (1991) are widely used in economics, finance and econometrics to solve functional equations in which state variables follow autoregressive processes. for highly persistent processes, the methods require a large number of discrete values for the state variables to produce close approximations which leads to an undesirable reduction in computational speed, especially in a multivariate case. this paper proposes an alternative method of discretizing multivariate autoregressive processes. this method can be treated as an extension of rouwenhorst's (1995) method which, according to our finding, outperforms the existing methods in the scalar case for highly persistent processes. the new method works well as an approximation that is much more robust to the number of discrete values for a wide range of the parameter space. © 2010 elsevier b.v."
"inequality constraints occur in many different fields of application, e.g., in structural mechanics, flow processes in porous media or mathematical finance. in this paper, we make use of the mathematical structure of these conditions in order to obtain an abstract computational framework for problems with inequality conditions. the constraints are enforced locally by means of lagrange multipliers which are defined with respect to dual basis functions. the reformulation of the inequality conditions in terms of nonlinear complementarity functions leads to a system of semismooth nonlinear equations that is solved by a generalized version of newton's method for semismooth problems. by this, both nonlinearities in the pde model and inequality constraints are treated within a single newton iteration which converges locally superlinear. the scheme can efficiently be implemented in terms of an active set strategy with local static condensation of the non-essential variables. numerical examples from different fields of application illustrate the generality and the robustness of the method. © 2010 wiley-vch verlag gmbh & co. kgaa, weinheim."
"we develop a nonparametric regression-based goodness-of-fit test for multifactor continuous-time markov models using the conditional characteristic function, which often has a convenient closed form or can be approximated accurately for many popular continuous-time markov models in economics and finance. an omnibus test fully utilizes the information in the joint conditional distribution of the underlying processes and hence has power against a vast class of continuous-time alternatives in the multifactor framework. a class of easy-to-interpret diagnostic procedures is also proposed to gauge possible sources of model misspecification. all the proposed test statistics have a convenient asymptotic n(0, 1) distribution under correct model specification, and all asymptotic results allow for some data-dependent bandwidth. simulations show that in finite samples, our tests have reasonable size, thanks to the dimension reduction in nonparametric regression, and good power against a variety of alternatives, including misspecifications in the joint dynamics, but the dynamics of each individual component is correctly specified. this feature is not attainable by some existing tests. a parametric bootstrap improves the finite-sample performance of proposed tests but with a higher computational cost. copyright © cambridge university press 2009."
"efficient portfolio design is a real challenge in the area of computational finance. optimisation based on markowitz (1959) two-objective mean-variance approach is computationally expensive for real financial world. practical portfolio design introduces further complexity as it requires the optimisation of multiple return and risk measures. some of these measures are non-linear and non-convex. three well known multi-objective evolutionary algorithms, i.e., pareto envelope-based selection algorithm, micro-genetic algorithm and multi-objective particle swarm optimisation are chosen and applied for solving the bi-objective portfolio optimisation problem which simultaneously maximise the return and minimise the associated risk. performance comparison is obtained by carrying out using practical data. the results demonstrate that mopso outperforms the existing two methods for the considered test cases. © 2010 inderscience enterprises ltd."
"financial institutions could achieve a two- orders-of-magnitude optimization by utilizing parallel computing for their applications. if programmers can meet the software challenges, parallelization could dramatically reshape computational finance for years to come. © 2010 ieee."
"effects of civil engineering construction projects realisation depends on a lot of different factors. the numerous factors are of tangible nature. they include time, finance, material and hardware resources. there are also important intangible aspects which also pertain to a wider, non-technical context of building investments. the factors of both kinds can influence both short-time and long-time building investment process effects considerably. proper scheduling of construction activities is therefore very important for these effects. unfortunately, available scheduling approaches do not usually take multiple aspects into account. they do not include influence of intangible aspects too. an approach is therefore proposed in the paper which makes a thorough, multi-attribute and intangibility-aware analysis of available schedules possible. the approach consists of two phases. the first one delivers a set of schedule alternatives for a project under consideration. any suitable method can be utilised for generation of set components. the second phase deals with multi-attribute decision analysis of set components. a set of easy evaluation methods is applied with this regard. the methods differ in forms of delivered outcomes. a dedicated minimalistic common data structure facilitates parallel utilisation of the methods. the structure allows testing input data for consistency and facilitates group decision making support. multiple consistent results justify analysis conclusions with regard to identification of the most suitable schedule alternative(s). they also make it possible to gain general knowledge which pertains to effects and suitability of application of different approaches for generation of schedule alternatives. the approach is flexible enough to adapt to changing requirements which result from development in civil engineering industry. another advantage comes from its easy computational implementation. it seems therefore to be interesting supplement to single tangible criteria only-aware approaches for optimal schedule identification. © vilnius gediminas technical university, 2010."
"with the evolution of graphics processing units (gpus) into powerful and cost-efficient computing architectures, their range of application has expanded tremendously, especially in the area of computational finance. current research in the area, however, is limited both in terms of the type of options priced and the complexity of stock price models. this paper presents algorithms, based on the fourier space time-stepping (fst) method, for pricing single- and multi-asset european and american options with stock prices following exponential lévy processes on a gpu. furthermore, the single-asset pricing algorithm is parallelized to attain greater efficiency. © 2010 elsevier b.v. all rights reserved."
"statistical learning refers to statistical aspects of automated extraction of regularities (structure) in datasets. it is a broad area which includes neural networks, regression-trees, nonparametric statistics and sieve approximation, boosting, mixtures of models, computational complexity, computational statistics, and nonlinear models in general. although statistical learning theory and econometrics are closely related, much of the development in each of the areas is seemingly proceeding independently. this special issue brings together these two areas, and is intended to stimulate new applications and appreciation in economics, finance, and marketing. this special volume contains ten innovative articles covering a broad range of relevant topics. © taylor & francis group, llc."
"this paper uses the agent-based modelling approach to construct an artificial stock market with the continuous double auction mechanism. in this market, several psychological effects described in behavioral finance, such as anchoring effect, mental accounting and disposition effect, are introduced into the decision process of agent to illustrate the impact of agent's psychologic factors. through the computational simulation, fat tail of the returns distribution and inefficiency of the market were shown in the experiment, implying that this model could be used to do further research about the emergence of market. © 2010 ieee."
"many examples of complex systems are provided by applications in finance and economics areas. some of intrinsic features of such systems lie with the fact that their parts are interacting in a non-trivial dynamic manner and they can be subject to stochastic forces and jumps. the mathematical models for such systems are often based on stochastic differential equations and efficient computational tools are required to solve them. here, on an example from the credit risk analysis of multiple correlated firms, we develop a fast monte-carlo type procedure for the analysis of complex systems such as those occurring in the financial market. our procedure is developed by combining the fast monte-carlo method for one-dimensional jump-diffusion processes and the generation of correlated multidimensional variates. as we demonstrate on the evaluation of first passage time density functions in credit risk analysis, this allows us to analyze efficiently multivariate and correlated jump-diffusion processes. © 2010 published by elsevier ltd."
"financial risk that a firm will be unable to meet its financial obligations. this risk is primarily a function of the relative amount of debt that the firm uses to finance its assets. a higher proportion of debt increases the likelihood that at some point the firm will be unable to make the required interest and principal payments. early warning and controlling financial risks effectively can provide a safe and steady operating environment. this paper believe that through analyzing the financial situation, preparing the cash-flow budget, establishing the financial risk index system and computational model to warn early the financial risk. on the other hand, through establishing effective capital structure, selecting correct fund-raising methods and keep the assets highly liquid to control the financial risk effectively. © 2010 ieee."
"there is a need for improved performance in financial computing, both to improve the quality of the results produced by existing methods, such as incorporating more realistic models of risk, and to increase the scope of applications, such as modelling corporation-wide exposure to risk without applying simplifying abstractions. however, increased performance comes at significant cost, both in terms of the capital costs, and also in terms of power consumption. field-programmable gate arrays (fpgas) offer one way of providing greater performance than cpus while also reducing power consumption, and are particularly well-suited for computationally intensive tasks such as monte-carlo simulation. however, existing programming models for fpgas require too much specialist knowledge and programmer effort, making it infeasible to use them in most financial applications. this paper gives an overview of contessa, a high-level language for describing monte-carlo simulations, with an automated compilation route to pipelined high-performance hardware. by providing a push-button high-level route to fpga-accelerated performance, languages such as contessa provide one way in which the benefits of fpgas can be exploited in computational finance. © 2010 ieee."
"we develop a penalized kernel smoothing method for the problem of selecting non-zero elements of the conditional precision matrix, known as conditional covariance selection. this problem has a key role in many modern applications such as finance and computational biology. however, it has not been properly addressed. our estimator is derived under minimal assumptions on the underlying probability distribution and works well in the high-dimensional setting. the efficiency of the algorithm is demonstrated on both simulation studies and the analysis of the stock market. copyright 2010 by the author(s)/owner(s)."
"compared to analytical modelling, simulation has sometimes a greater expressive and computational power, especially for a behavioural study of an activity, system, organisation and other topics from the social sciences, which an analytic study cannot adequately achieve. in this paper simulation is discussed as a support for teaching and knowledge transfer: a model can be built and used to dynamically show and explain a particular phenomenon through direct experiments, though contributing to a ""maieutical"" way of learning by the students (learning by doing). also team work is triggered by using simulation models as a educational tool. in particular, three simulation paradigms are described, along with their potential applications and points of strength and weakness. management and finance are the focus of the work, but the same considerations may be extended to other social disciplines and sciences. note: although the article is the result of a joint research project, the paragraphs are divided among the authors as follows: paragraphs 1 and 5 are jointly written and equally divided among all the authors; paragraph 2 is by nicola miglietta; paragraph 3 is by anna maria bruno and marco remondino; paragraph 4 is by marco remondino."
"in many prediction problems, including those that arise in computer security and computational finance, the process generating the data is best modelled as an adversary with whom the predictor competes. even decision problems that are not inherently adversarial can be usefully modeled in this way, since the assumptions are sufficiently weak that effective prediction strategies for adversarial settings are very widely applicable. © 2010 springer-verlag."
"mathematical models involving ordinary differential equation (odes) arise in many diverse applications, such as fluid flows, physics-based animation, mechanical systems, mathematical finances, or chemical reaction. the realistic simulation of these applications depends on fast methods for the numerical solution of odes as well as adequate parallel computation schemes exploiting the potential parallelism in an optimal way. due to the advent of multicore technology, parallel resources are now widely available in form of multicore processors or clusters. it is required to revisit parallel computation schemes of ode solvers for the use on these multicore platforms. the objective of this article is a survey and classification of computational techniques for the numerical integration of ode systems. the emphasis lies on a computational model which captures the specifics of ode codes as well as a hierarchical architectural model of multicore systems. © 2010 the authors and ios press. all rights reserved."
"a conic integer program is an integer programming problem with conic constraints. many problems in finance, engineering, statistical learning, and probabilistic optimization are modeled using conic constraints. here we study mixed-integer sets defined by second-order conic constraints. we introduce general-purpose cuts for conic mixed-integer programming based on polyhedral conic substructures of second-order conic sets. these cuts can be readily incorporated in branch-and-bound algorithms that solve either second-order conic programming or linear programming relaxations of conic integer programs at the nodes of the branch-and-bound tree. central to our approach is a reformulation of the second-order conic constraints with polyhedral second-order conic constraints in a higher dimensional space. in this representation the cuts we develop are linear, even though they are nonlinear in the original space of variables. this feature leads to a computationally efficient implementation of nonlinear cuts for conic mixed-integer programming. the reformulation also allows the use of polyhedral methods for conic integer programming. we report computational results on solving unstructured second-order conic mixed-integer problems as well as mean-variance capital budgeting problems and least-squares estimation problems with binary inputs. our computational experiments show that conic mixed-integer rounding cuts are very effective in reducing the integrality gap of continuous relaxations of conic mixed-integer programs and, hence, improving their solvability. © 2008 springer-verlag."
"precedence-preserving crossover and mutation operators for scheduling problems with activities' start times encoding are proposed and employed in this paper. the objective is to tackle the incapability of the genetic algorithms (gas) operators to preserve the precedence relationships among activities and generate feasible solutions in scheduling problems. the proposed operators employ an embedded precedence-preserving algorithm that determines the activities' forward free float and backward free float and utilize them in randomly selected backward and forward paths, respectively. the proposed operators were evaluated using finance-based scheduling problems for large-scale projects of 120 repetitive activities. moreover, the proposed operators were validated by comparing the results with the optimum results of a resource-constrained scheduling problem reported in the literature. the results exhibited the robustness of the proposed operators to reduce the computational costs. in addition, the results demonstrated the high potential and effectiveness of the proposed operators to capture the optimal solutions of the problems considered. © 2010 asce."
"fletcher and gardner have created a comprehensive resource that will be of interest not only to those working in the field of finance, but also to those using numerical methods in other fields such as engineering, physics, and actuarial mathematics. by showing how to combine the high-level elegance, accessibility, and flexibility of python, with the low-level computational efficiency of c++, in the context of interesting financial modeling problems, they have provided an implementation template which will be useful to others seeking to jointly optimize the use of computational and human resources. they document all the necessary technical details required in order to make external numerical libraries available from within python, and they contribute a useful library of their own, which will significantly reduce the start-up costs involved in building financial models. this book is a must read for all those with a need to apply numerical methods in the valuation of financial claims. © 2009 john wiley & sons ltd."
"option pricing is an important problem in computational finance due to the fast-growing market and increasing complexity of options. for option pricing, a model is required to describe the price process of the underlying asset. the garch model is one of the prominent option pricing models since it can model stochastic volatility of the underlying asset. to derive expected profit based on the garch model, tree-based simulations are one of the commonly used approaches. treebased garch option pricing is computing intensive since the tree grows exponentially, and it requires enormous floating point arithmetic operations. in this paper, we present the first work on accelerating the tree-based garch option pricing on gpus with cuda. as the conventional tree data structure is not memory access friendly to gpus, we propose a new family of tree data structures which position concurrently accessed nodes in contiguous and aligned memory locations. moreover, to reduce memory bandwidth requirement, we apply fusion optimization, which combines two threads into one to keep data with temporal locality in register files. our results show 50× speedup compared to a multithreaded program on a 4-core cpu. © 2010 ieee."
"global financial development have opened innumerable risks and opportunities for investments. a global view of the portfolio allocation through diversification brings advantages for the risk allocation in investments. in this paper, an asset allocation framework under the return, risk and liquidity considerations is proposed for short term investment using genetic relation algorithm. simulations using the stocks, bonds and currencies from relevant financial markets in usa, europe and asia show that the proposed framework is effective and robust. the efficacy of the proposed method is compared against the relevant constructs in finance and computational fields."
"a useful class of problems can be expressed as dependency graphs that are continuously evaluated while ""immersed"" in a real time data stream that updates values of input nodes. a number of computational finance problems contain such structure. a refactoring approach is explored for the migration of legacy applications to exploit shared memory parallel architectures. copyright 2010 is held by the author(s)."
"time-series analysis is central to many problems in signal processing, including acoustics, image processing, vision, tracking, information retrieval, and finance, to name a few [1], [2]. because of the wide base of application areas, having a common description of the models is useful in trans ferring ideas between the various communities. graphical models provide a compact way to represent such models and thereby rapidly transfer ideas. we will discuss briefly how classical timeseries models such as kalman filters and hidden markov models (hmms) can be represented as graphical models and critically how this representation differs from other common graphical representations such as state-transition and block diagrams. we will use this framework to show how one may easily envisage novel models and gain insight into their computational implementation. © 2010 ieee."
"connections are important: in studying nature, technology, commerce and the social sciences, it often makes sense to focus on the pattern of interactions between individual components. furthermore, improvements in computing power have made it possible to gather, store and analyze large data sets across many disciplines, and it is apparent that universal features may exist across seemingly disparate application areas.network science is the emerging field concerned with the study of large, realistic networks. this interdisciplinary endeavor, focusing on the patterns of interactions that arise between individual components of natural and engineered systems, has been applied to data sets from activities as diverse as high-throughput biological experiments, online trading information, smart-meter utility supplies, and pervasive telecommunications and surveillance technologies. this unique text/reference provides a fascinating insight into the state of the art in network science, highlighting the commonality across very different areas of application and the ways in which each area can be advanced by injecting ideas and techniques from another. the book includes contributions from an international selection of experts, providing viewpoints from a broad range of disciplines. it emphasizes networks that arise in nature - such as food webs, protein interactions, gene expression, and neural connections - and in technology - such as finance, airline transport, urban development and global trade. topics and features: begins with a clear overview chapter to introduce this interdisciplinary field discusses the classic network science of fixed connectivity structures, including empirical studies, mathematical models and computational algorithms examines time-dependent processes that take place over networks, covering topics such as synchronization, and message passing algorithms investigates time-evolving networks, such as the world wide web and shifts in topological properties (connectivity, spectrum, percolation) explores applications of complex networks in the physical and engineering sciences, looking ahead to new developments in the field researchers and professionals from disciplines as varied as computer science, mathematics, engineering, physics, chemistry, biology, ecology, neuroscience, epidemiology, and the social sciences will all benefit from this topical and broad overview of current activities and grand challenges in the unfolding field of network science. dr. ernesto estrada is a professor in the department of mathematics and statistics, and the department of physics, at the university of strathclyde, glasgow, scotland. dr. maria fox is a professor and head of the computer and information sciences department at the university of strathclyde. dr. des higham is a professor in the department of mathematics and statistics at the university of strathclyde. dr. gian-luca oppo is a professor and chair of computational and nonlinear physics in the department of physics at the university of strathclyde. © springer-verlag london limited 2010."
"optimal tax design attempts to resolve a well-known trade-off: namely, that high taxes are bad insofar as they discourage people from working, but good to the degree that, by redistributing wealth, they help insure people against productivity shocks. until recently, however, economic research on this question either ignored people's uncertainty about their future productivities or imposed strong and unrealistic functional form restrictions on taxes. in response to these problems, the new dynamic public finance was developed to study the design of optimal taxes given only minimal restrictions on the set of possible tax instruments, and on the nature of shocks affecting people in the economy. in this book, narayana kocherlakota surveys and discusses this exciting new approach to public finance. an important book for advanced phd courses in public finance and macroeconomics, the new dynamic public finance provides a formal connection between the problem of dynamic optimal taxation and dynamic principal-agent contracting theory. this connection means that the properties of solutions to principal-agent problems can be used to determine the properties of optimal tax systems. the book shows that such optimal tax systems necessarily involve asset income taxes, which may depend in sophisticated ways on current and past labor incomes. it also addresses the implications of this new approach for qualitative properties of optimal monetary policy, optimal government debt policy, and optimal bequest taxes. in addition, the book describes computational methods for approximate calculation of optimal taxes, and discusses possible paths for future research. © 2010 by princeton university press. all rights reserved."
"portfolio selection problems in investments are most studied in modern finance because of their computational intractability. the basic topic of modern portfolio theory is the way in which investors can construct a diversified portfolio of financial securities so as to achieve improved tradeoffs between risk and return. in this paper, a heuristic algorithm using particle swarm optimization (pso) is applied to the problem. pso realizes the search algorithm by combining a local search method through self-experience with global search method through neighboring experience, attempting to balance the exploration trade-off which achieves the efficiency and accuracy of an optimization. a newly obtained effect is proposed in this paper by adding the mutation operator of genetic algorithms (ga) to unravel the stagnation and control the velocity. we applied our adaptation and implementation of the pso search strategy to the portfolio selection problem. results on typical applications demonstrate that the velocity information and mutation operator play pivotal roles in searching for the best solution, and that our method is a viable approach for the portfolio selection problem."
"this paper describes a novel hybrid intelligent system approach to inversion of nondestructive pavement deflection data and back-calculation of nonlinear stress-dependent pavement layer moduli. particle swarm optimization (pso), a population-based stochastic optimization technique inspired by social behavior of bird flocking or fish schooling, is fast emerging as an innovative and powerful computational metaphor for solving complex problems in design, optimization, control, management, business, and finance. back-calculation of pavement layer moduli is an ill-posed inverse engineering problem which involves searching for the optimal combination of pavement layer stiffness solutions in an unsmooth, multimodal, complex search space. pso is especially considered a robust and efficient approach for global optimization of multimodal functions. the hybrid back-calculation system described in this paper integrates finite element modeling, neural networks, and pso in an efficient manner to mitigate the limitations and take advantages of the strengths to produce a system that is more effective and powerful than those which could be built with single technique. this is the first time the pso approach is applied to real-time nondestructive evaluation of pavement systems. © 2010 asce."
"option pricing is one of the challenging problems of computational finance. nature-inspired algorithms have gained prominence in real world optimization problems such as in mobile ad hoc networks. the option pricing problem fits very well into this category of problems due to the ad hoc nature of the market. particle swarm optimization (pso) is one of the novel global search algorithms based on swarm intelligence. we first show that pso could be effectively used for the option pricing problem. the results are compared with standard classical black-scholes-merton model for simple european options. in this study, we developed two algorithms for option pricing using particle swarm optimization (pso). the first algorithm we developed is synchronous option pricing algorithm using pso (spso), and the second is parallel synchronous option pricing algorithm. the pricing results of these algorithms are close when compared with classical black-scholes-merton model for simple european options. we test our parallel synchronous pso algorithm in three architectures: shared memory machine using openmp, distributed memory machine using mpi and on a homogeneous multicore architecture running mpi and openmp (hybrid model). the results show that the hybrid model handles the load well as we increase the number of particles in simulation while maintaining equivalent accuracy. © 2010 ieee."
"the portfolio optimization problem is modeled as a mean-risk bicriteria optimization problem where the expected return is maximized and some (scalar) risk measure is minimized. in the original markowitz model the risk is measured by the variance while several polyhedral risk measures have been introduced leading to linear programming (lp) computable portfolio optimization models in the case of discrete random variables represented by their realizations under specified scenarios. among them, the second order quantile risk measures, recently, become popular in finance and banking. the simplest such measure, now commonly called the conditional value at risk (cvar) or tail var, represents the mean shortfall at a specified confidence level. recently, the second order quantile risk measures have been introduced and become popular in finance and banking. the corresponding portfolio optimization models can be solved with general purpose lp solvers. however, in the case of more advanced simulation models employed for scenario generation one may get several thousands of scenarios. this may lead to the lp model with huge number of variables and constraints thus decreasing the computational efficiency of the model since the number of constraints (matrix rows) is usually proportional to the number of scenarios. while the number of variables (matrix columns) is proportional to the total of the number of scenarios and the number of instruments. we show that the computational efficiency can be then dramatically improved with an alternative model taking advantages of the lp duality. in the introduced models the number of structural constraints (matrix rows) is proportional to the number of instruments thus not affecting seriously the simplex method efficiency by the number of scenarios. © 2010 ieee."
"in this paper, through the prism of refining covariance matrices, we study the mean-variance efficiency and diversification contradiction that the high concentration in a few securities in portfolio selection implicates the incompatibility of efficiency and diversification which are two corner stones of modern finance. typical treatments include refining covariance matrices and imposing portfolio constraints. unfortunately, the existing studies usually suffer from simplified models, small-scale computation, and crude representation of efficient frontiers. to make thing worse, inequality constraints of portfolio optimization such as restrictions of short sales knock out the possibility of closed-form type of optima, so computational methodology based on empirical data can be the only way to study the contradiction. we sample 13, 26, 52, 125, 326, and 687 chinese stocks from 2003-2005 and 2006-2008 periods, deploy factor models and equal correlation coefficient models to refine covariance matrices, and utilize parametric quadratic programming to obtain precise and complete efficient frontiers of portfolio optimization. we find that contrary to existing beliefs, the refinements can not alleviate the contradiction. our systematic sampling, large-scale computation, and precise representations of efficient frontiers are original in the area in china, which alone make the paper unique. the research can help both individual and institutional investors balance efficiency and diversification and consequently serve portfolio theory and finance industry of our motherland, especially in the turmoil world-wide financial crises. moreover, our methodology is based on the latest computational extensions of portfolio optimization and can be deployed to stock markets worldwide to draw more comprehensive conclusions. © 2010 ieee."
"considered by many eminent scientists as one of the most important fields shaping the face of future advancements in science and technology, computational optimization has applications in almost every aspect of modern life including industry, finance, medicine and biology. the computational requirements posed by current optimization techniques represent an overwhelming challenge for available computational resources to deal with realistic applications, which are commonly becoming of large-scale-size nature. hence, advancing the state of the art in optimization techniques becomes a crucial matter when facing limited computational resources. in this commentary, we generally address the different issues facing the advancement of computational optimization practice. these issues include algorithm design, mathematical modeling, application areas of particular interest, open questions, dissemination of information, and approaches to encourage independent research in the computational optimization field. in particular, we focus our discussion on algorithm development and algorithm classes where the weaknesses and strengths of the existing algorithms are pointed out. also, we shed light on a promising class of algorithms to be considered as a framework for future algorithm development in computational optimization. © 2010 nova science publishers, inc."
"the use of interval mathematics to solve non-linear problems is an attractive alternative to traditional real-number techniques. it was demonstrated in a previous paper [stradi, b., haven, e., 2005. optimal investment strategy via interval arithmetic. international journal of theoretical and applied finance 8(2), 185-205] that interval arithmetic in the form of the interval-newton generalized bisection (in/gb) method is effective in solving highly non-linear problems. in this paper we solve a rational expectations models with the help of the in/gb method. this method is capable of (i) rapidly eliminating no solution sections of the multidimensional space and (ii) concentrate computational efforts on those areas of multidimensional space where there may be a solution. © 2009 elsevier b.v. all rights reserved."
"introduction in recent years stock-flow consistent keynesian models have received increasing attention. typically applying simulation of difference/differential equations, this strand of literature focus on long run equilibria. although unsatisfactory from a keynesian point of view, dos santos and zezza (2008: 472) argue that short run dynamics, e.g. in the form of unsatisfied expectations, is too complex to handle within formal models of “complete” monetary economies. agent-based models, however, pose an alternative to mathematical equations, while retaining the ability to model complete monetary economies. when an almost stock-flow consistent approach is chosen here, it is not due to limitations in the method applied, but a positive choice of diverging from the usual accounting principles of stockflow consistent modelling, in order to capture what is regarded as essential dynamics in the interaction between real and financial spheres. the model is almost stock-flow consistent - following the path of e.g. godley and lavoie (2007) and dos santos (2006), but choosing the micro-convention of equity revaluations rather than following the macroeconomic convention (patterson 1990). this option is crucial since it undermines the long period equilibria of stock-flow consistent keynesian models, and allows finance a more decisive role. in the following we shall employ a three-step analysis; first we shall present the macrofoundation of keynes in the terminology of stock-flow consistent modelling, next we shall discuss his microfoundation and finally we shall attempt to unify the two in an agent-based computational model. experiments are performed on some of the behavioural parameters of the model relating stock and flow magnitudes. © 2010 selection and editorial matter, stefano zambelli; individual chapters, the contributors."
"large-scale stochastic models are relevant in many different fields such as computational biology, finance, social sciences, communication, and traffic networks. in order to both efficiently simulate and analyze such models and to understand the essential properties of the system, it is desirable to have model reduction techniques that much reduce the dimensionality of the model while at the same time preserving the system's essential dynamical properties. in this paper, a general model reduction technique for the class of discrete space and time hidden markov models is presented, thereby also including the more special class of discrete markov chains. the method is illustrated on some model applications. © 2010 society for industrial and applied mathematics."
"in standard voting procedures, random audits are one method for increasing election integrity. in the case of cryptographic (or end-to-end) election verification, random challenges are often used to establish that the tally was computed correctly. in both cases, a source of randomness is required. in two recent binding cryptographic elections, this randomness was drawn from stock market data. this approach allows anyone with access to financial data to verify the challenges were generated correctly and, assuming market fluctuations are unpredictable to some degree, the challenges were generated at the correct time. however the degree to which these fluctuations are unpredictable is not known to be sufficient for generating a fair and unpredictable challenge. in this paper, we use tools from computational finance to provide an estimate of the amount of entropy in the closing price of a stock. we estimate that for each of the 30 stocks in the dow jones industrial average, the entropy is between 6 and 9 bits per trading day. we then propose a straight-forward protocol for regularly publishing verifiable 128-bit random seeds with entropy harvested over time from stock prices. these “beacons” can be used as challenges directly, or as a seed to a deterministic pseudorandom generator for creating larger challenges. © evt/wote 2010 - 2010 electronic voting technology workshop/workshop on trustworthy elections. all rights reserved."
"linear regression is widely-used in finance. while the standard method to obtain parameter estimates, least squares, has very appealing theoretical and numerical properties, obtained estimates are often unstable in the presence of extreme observations which are rather common in financial time series. one approach to deal with such extreme observations is the application of robust or resistant estimators, like least quantile of squares estimators. unfortunately, for many such alternative approaches, the estimation is much more difficult than in the least squares case, as the objective function is not convex and often has many local optima. we apply different heuristic methods like differential evolution, particle swarm and threshold accepting to obtain parameter estimates. particular emphasis is put on the convergence properties of these techniques for fixed computational resources, and the techniques' sensitivity for different parameter settings. © 2010 springer-verlag berlin heidelberg."
"computational finance covers a wide and still growing array of topics and methods within quantitative economics. the core focus has long been on efficient methods, models and algorithms for numerically demanding problems. the advent of new computational methods, together with the advances in available hardware, has pushed the boundaries of this field outwards. not only can the complexity of investigated problems be increased, one can even approach problems that defy traditional analytical examination all together. one major contributor of such methods is natural computing. © 2010 springer-verlag berlin heidelberg."
"the portfolio optimisation problem is modelled as a mean-risk bicriteria optimisation problem where the expected return is maximised and some (scalar) risk measure is minimised. in the original markowitz model the risk is measured by the variance while several polyhedral risk measures have been introduced leading to linear programming (lp) computable portfolio optimisation models in the case of discrete random variables represented by their realisations under specified scenarios. recently, the second order quantile risk measures have been introduced and become popular in finance and banking. the simplest such measure, now commonly called the conditional value at risk (cvar) or tail var, represents the mean shortfall at a specified confidence level. the corresponding portfolio optimisation models can be solved with general purpose lp solvers. however, in the case of more advanced simulation models employed for scenario generation one may get several thousands of scenarios. this may lead to the lp model with a huge number of variables and constraints, thus decreasing the computational efficiency of the model. we show that the computational efficiency can be then dramatically improved with an alternative model taking advantages of the lp duality. moreover, similar reformulation can be applied to more complex quantile risk measures like gini's mean difference as well as to the mean absolute deviation."
"the proceedings contain 49 papers. the topics discussed include: constraint-directed search in computational finance and economics; constraints, graphs, algebra, logic, and complexity; testing expressibility is hard; applying constraint programming to identification and assignment of service professionals; computing the density of states of boolean formulas; towards parallel non serial dynamic programming for solving hard weighted csp; making adaptive an interval constraint propagation algorithm exploiting monotonicity; checking-up on branch-and-check; spatial, temporal, and hybrid decompositions for large-scale vehicle routing with time windows; propagating the bin packing constraint using linear programming; sweeping with continuous domains; a new hybrid tractable class of soft constraint problems; a propagator for maximum weight string alignment with arbitrary pairwise dependencies; and generating special-purpose stateless propagators for arbitrary constraints."
"the sharpe-lintner-mossin (sharpe 1964; lintner 1965; mossin 1966) capital asset pricing model (capm) plays a central role in modern finance theory. it is founded on the paradigm of homogeneous beliefs and a rational representative agent. however, froma theoretical perspective this paradigmhas been criticized on a number of grounds, in particular concerning its extreme assumptions about homogeneous beliefs, information about the economic environment, and the computational ability on the part of the rational representative economic agent. the impact of heterogeneous beliefs among investors on the market equilibrium price has been an important focus in the capm literature. a number of models with investorswho have heterogeneous beliefs have been previously studied. 1 a common finding in this strand of research is that heterogeneous beliefs can affect aggregate market returns. however, the question remains as to how exactly does heterogeneity affect themarket risk of risky assets? in much of this earlier work, the heterogeneous beliefs reflect either differences of opinion among the investors2 or differences in information upon which investors are trying to learn by using some bayesian updating rule.3 heterogeneity has been investigated in the context of either capm-like mean-variancemodels (for instance, lintner 1969; miller 1977;williams 1977; and mayshar 1982) or arrow-debreu contingent claims models (as in varian 1985;abel 1989; 2002; and calvet et al. 2004). © 2010 springer-verlag berlin heidelberg."
"the uses of simulation are virtually endless. in this article we consider only a few. the first is the use of simulation to take a data set and generate pseudo data points which are essentially stochastic combinations of points in the data set. this simdat application is model free. the second is the use of simulation to obtain data-based estimates of a stochastic process model. in order to estimate model parameters, we simulate assuming parameter values pseudo data points. using goodness-of-fit measures we can then adjust the parameter values to bring the resulting pseudo data into maximal concordance with the actual data. this simest algorithm is highly a model dependent. since the time of poisson, stochatic processes are modeled in the forward (temporal) direction. fisher's maximum likelihood is a temporally backwards technique of parameter estimation. thus, for example, if we are looking at a data set of the times of detection of 'secondary' tumors, we need to consider all the possible pathways for genesis of these tumors, and there are an infinite number of these. a 'secondary tumor' may have been generated from a primary over a continuum of times. it might have been generated from a secondary tumor over a continuum of times. it might actually be a new primary, having been generated from a systemic failure of the immune system. in this paper, we give an example of the forwards simest estimation procedure. simest has been successfully used to generate pseudo data based on an assumption of parameter values and then compared with clinical data via pearson's goodness of fit. in the area of computational finance, we briefly consider the work which, using simulation-based data analysis, challenges the efficient market hypothesis and looks at data-based portfolio alternatives. © 2009 john wiley & sons, inc."
"in mathematics and computer science, an optimization problem is the problem of finding the best solution from all feasible solutions. computational optimization is crucial in many fields of science and technology as well as in finance, business and medicine. this new book presents state-of-the-art research in the field. © 2010 by nova science publishers, inc. all rights reserved."
"optimal tax design attempts to resolve a well-known trade-off: namely, that high taxes are bad insofar as they discourage people from working, but good to the degree that, by redistributing wealth, they help insure people against productivity shocks. until recently, however, economic research on this question either ignored people's uncertainty about their future productivities or imposed strong and unrealistic functional form restrictions on taxes. in response to these problems, the new dynamic public finance was developed to study the design of optimal taxes given only minimal restrictions on the set of possible tax instruments, and on the nature of shocks affecting people in the economy. in this book, narayana kocherlakota surveys and discusses this exciting new approach to public finance.an important book for advanced phd courses in public finance and macroeconomics,the new dynamic public financeprovides a formal connection between the problem of dynamic optimal taxation and dynamic principal-agent contracting theory. this connection means that the properties of solutions to principal-agent problems can be used to determine the properties of optimal tax systems. the book shows that such optimal tax systems necessarily involve asset income taxes, which may depend in sophisticated ways on current and past labor incomes. it also addresses the implications of this new approach for qualitative properties of optimal monetary policy, optimal government debt policy, and optimal bequest taxes. in addition, the book describes computational methods for approximate calculation of optimal taxes, and discusses possible paths for future research. © 2010 by princeton university press. all rights reserved."
"discrete choice models are commonly used by applied statisticians in numerous fields, such as marketing, economics, finance, and operations research. when agents in discrete choice models are assumed to have differing preferences, exact inference is often intractable. markov chain monte carlo techniques make approximate inference possible, but the computational cost is prohibitive on the large datasets now becoming routinely available. variational methods provide a deterministic alternative for approximation of the posterior distribution. we derive variational procedures for empirical bayes and fully bayesian inference in the mixed multinomial logit model of discrete choice. the algorithms require only that we solve a sequence of unconstrained optimization problems, which are shown to be convex. one version of the procedures relies on a new approximation to the variational objective function, based on the multivariate delta method. extensive simulations, along with an analysis of real-world data, demonstrate that variational methods achieve accuracy competitive with markov chain monte carlo at a small fraction of the computational cost. thus, variational methods permit inference on datasets that otherwise cannot be analyzed without possibly adverse simplifications of the underlying discrete choice model. appendices c through f are available as online supplemental materials. © 2010 american statistical association."
"""constructive computation in stochastic models with applications: the rg-factorizations"" provides a unified, constructive and algorithmic framework for numerical computation of many practical stochastic systems. it summarizes recent important advances in computational study of stochastic models from several crucial directions, such as stationary computation, transient solution, asymptotic analysis, reward processes, decision processes, sensitivity analysis as well as game theory. graduate students, researchers and practicing engineers in the field of operations research, management sciences, applied probability, computer networks, manufacturing systems, transportation systems, insurance and finance, risk management and biological sciences will find this book valuable. dr. quan-lin li is an associate professor at the department of industrial engineering of tsinghua university, china. © tsinghua university press, beijing and springer-verlag berlin heidelberg 2010. all rights are reserved."
"the martingale difference restriction is an outcome of many theoretical analyses in economics and finance. a large body of econometric literature deals with tests of that restriction. we provide new tests based on radial basis function (rbf) neural networks. our work is based on the test design of blake and kapetanios (2000, 2003a, 2003b). however, unlike that work we provide a formal theoretical justification for the validity of these tests and present some new general theoretical results. these results take advantage of the link between the algorithms of blake and kapetanios (2000, 2003a, 2003b) and boosting. we carry out a monte carlo study of the properties of the new tests and find that they have very good power performance. a simplified implementation of boosting is found to have desirable properties and small computational cost. an empirical application to the s&p 500 constituents illustrates the usefulness of our new test. © 2010 cambridge university press."
"this book provides a hands-on practical guide to using the most suitable models for analysis of statistical data sets using eviews - an interactive windows-based computer software program for sophisticated data analysis, regression, and forecasting - to define and test statistical hypotheses. rich in examples and with an emphasis on how to develop acceptable statistical models, time series data analysis using eviews is a perfect complement to theoretical books presenting statistical or econometric models for time series data. the procedures introduced are easily extendible to cross-section data sets. the author: provides step-by-step directions on how to apply eviews software to time series data analysis offers guidance on how to develop and evaluate alternative empirical models, permitting the most appropriate to be selected without the need for computational formulae examines a variety of times series models, including continuous growth, discontinuous growth, seemingly causal, regression, arch, and garch as well as a general form of nonlinear time series and nonparametric models gives over 250 illustrative examples and notes based on the author's own empirical findings, allowing the advantages and limitations of each model to be understood describes the theory behind the models in comprehensive appendices provides supplementary information and data sets an essential tool for advanced undergraduate and graduate students taking finance or econometrics courses. statistics, life sciences, and social science students, as well as applied researchers, will also find this book an invaluable resource. © 2009 john wiley & sons (asia) pte ltd. all rights reserved."
"we use the study method of computational finance to explore the formation and evolution of asset prices from the standpoint of the evolution of investor individual's heterogeneous behavior through building an agent-based artificial financial market. in our model, agent will consider fundamental information and price tendency simultaneously at each period to form expectation that based on personal characters, such as mood, memory length, adjustment and extrapolation speed. the weight that he relies on both fundamental and technical analysis varies over time, which is the best prediction to current market state from empirical rule-set that has been updated through learning from the market situation with genetic algorithm (ga) and individual's trading experience with generation function (gf). the adaptive updating of the weight represents the evolution of agent's behavior. the model captures the two prime behaviors of agent and the trade-off between them, which realized by agent's adaptively personal learning. simulation testing shows that even considering agent's variation of behavior in the market, the market fraction also has to be composed of the proportions of confident fundamentalists, chartists and adaptively rational agents as empirical evidence suggests, which will cause the so-called ""stylized facts"" in financial time series, under a market maker scenario. the impact of the market fraction varies on asset pricing also has been examined. ©2010 ieee."
"this paper presents an application of the recently developed method for simultaneous dimension reduction and metastability analysis of high-dimensional time series in the context of computational finance. further extensions are included to combine state-specific principal component analysis (pca) and state-specific regressive trend models to handle the high-dimensional, nonstationary data. the identification of market phases allows one to control the involved phase-specific risk for futures portfolios. the numerical optimization strategy for futures portfolios based on tikhonovtype regularization is presented. the application of proposed strategies to online detection of the market phases is exemplified first on the simulated data and then on historical futures prices for oil and wheat from 2005-2008. numerical tests demonstrate the comparison of the presented methods with existing approaches. © 2010 society for industrial and applied mathematics."
"banks and investment funds are increasingly basing their competitiveness on the quality of their quantitative technology, including programming techniques, analytical methods, and applications such as financial forecasting, option pricing, and risk management, which are all essential elements of the field of computational finance. © 2010 ieee."
"temporal causal modeling has been a highly active research area in the last few decades. temporal or time series data arises in a wide array of application domains ranging from medicine to finance. deciphering the causal relationships between the various time series can be critical in understanding and consequently, enhancing the efficacy of the underlying processes in these domains. grouped graphical modeling methods such as granger methods provide an efficient alternative for finding out such dependencies. a key parameter which affects the performance of these methods is the maximum lag. the maximum lag specifies the extent to which one has to look into the past to predict the future. a smaller than required value of the lag will result in missing important dependencies while an excessively large value of the lag will increase the computational complexity alongwith the addition of noisy dependencies. in this paper, we propose a novel approach for estimating this key parameter efficiently. one of the primary advantages of this approach is that it can, in a principled manner, incorporate prior knowledge of dependencies that are known to exist between certain pairs of time series out of the entire set and use this information to estimate the lag for the entire set. this ability to extrapolate the lag from a known subset to the entire set, in order to get better estimates of the overall lag efficiently, makes such an approach attractive in practice. © 2010 ieee."
"at the nanoscale, no circuit parameters are truly deterministic; most quantities of practical interest present themselves as probability distributions. thus, monte carlo techniques comprise the strategy of choice for statistical circuit analysis. there are many challenges in applying these techniques efficiently: circuit size, nonlinearity, simulation time, and required accuracy often conspire to make monte carlo analysis expensive and slow. are wethe integrated circuit communityalone in facing such problems? as it turns out, the answer is no. problems in computational finance share many of these characteristics: high dimensionality, profound nonlinearity, stringent accuracy requirements, and expensive sample evaluation. we perform a detailed experimental study of how one celebrated technique from that domainquasi-monte carlo (qmc) simulationcan be adapted effectively for fast statistical circuit analysis. in contrast to traditional pseudorandom monte carlo sampling, qmc uses a (shorter) sequence of deterministically chosen sample points. we perform rigorous comparisons with both monte carlo and latin hypercube sampling across a set of digital and analog circuits, in 90 and 45 nm technologies, varying in size from 30 to 400 devices. we consistently see superior performance from qmc, giving 2 × to 8× speedup over conventional monte carlo for roughly 1% accuracy levels. we present rigorous theoretical arguments that support and explain this superior performance of qmc. the arguments also reveal insights regarding the (low) latent dimensionality of these circuit problems; for example, we observe that over half of the variance in our test circuits is from unidimensional behavior. this analysis provides quantitative support for recent enthusiasm in dimensionality reduction of circuit problems. © 2006 ieee."
"the massively parallel computing using graphical processing unit (gpu), which based on tens of thousands of parallel threats within hundreds of gpu's streaming processors, has gained broad popularity and attracted researchers in a wide range of application areas from finance, computer aided engineering, computational fluid dynamics, game physics, numerics, science, medical imaging, life science, and so on, including molecular biology and bioinformatics. meanwhile, markov clustering algorithm (mcl) has become one of the most effective and highly cited methods to detect and analyze the communities/clusters within an interaction network dataset on many real world problems such us social, technological, or biological networks including protein-protein interaction networks. however, as the dataset become bigger and bigger, the computation time of mcl algorithm become slower and slower. hence, gpu computing is an interesting and challenging alternative to attempt to improve the mcl performance. in this poster paper we introduce our improvement of mcl performance based on ellpack-r sparse dataset format using gpu computing with the compute unified device architecture tool (cuda) from nvidia (called cuda-mcl). as the results show the significant improvement in cuda-mcl performance and with the low-cost and widely available gpu devices in the market today, this cuda-mcl implementation is allowing large-scale parallel computation on off-the-shelf desktop machines. moreover the gpu computing approaches potentially may contribute to significantly change the way bioinformaticians and biologists compute and interact with their data. © 2010 ieee."
"from the constitution of computation, the main subjects of computer science and technology have been summarized. meanwhile, the reasons why the developments of software technologies have pushed the computational science be widely used were analyzed. multiple computing models were discussed base on different computer architectures. finally, computation as a methodology, two important finance problems, effective financial supervision and high-speed financial product pricing, were solved by distributed and parallel computing. © 2010 ieee."
"auto-pipe, an application development environment for streaming applications executing on architecturally diverse computing platforms, supports the flexible mapping and automatic delivery of application components between computational resources. auto-pipe's focus is on enabling designers to develop high-performance applications that run correctly despite the above limitations. an important component of this development environment is the emphasis placed on performance assessment and evaluation. the major purpose for deploying applications on diverse systems is to exploit the achievable performance gains. in addition to the computational finance application, auto-pipe has been used to implement applications ranging from cryptography to astrophysics. auto-pipe currently supports applications deployed on chip multiprocessors and fpgas. in addition, a block library and incorporation of analog computation is under development."
"so far, approach of agent-based computational finance modeling approach is one of most pop means of financial markets complex adaptive system modeling. but this modeling approach considers agent as a static model units, it can not reflect the emergence process in-depth, and how the agent build the financial markets complex adaptive system emergence phenomenon by gather, internal models and building blocks. this article attempts to look at agent properties of adaptation and evolutionary from a novel perspective. it extends and expands the concept of agent, and introduces concept of the mechanism. it considers emergent structure pattern of agents as dynamic mechanisms when agents are evolving and adapting ceaselessly. finally, it proposes the universal theoretical framework of mechanism-based computing finance modeling method. and the theoretical framework is applied to the stock market modeling. © 2010 ieee."
proper asset allocations are vital for property-casualty insurers to be competitive and solvent. theories of finance offer little practical guidance in constructing such asset allocations however. this research integrates simulation models with a newly developed evolutionary algorithm for the multi-period asset allocation problem of a property-casualty insurer. we first construct a simulation model to simulate operations of a property-casualty insurer. then we develop multi-phase evolution strategies (mpes) to be used with the simulation model to search for promising asset allocations for the insurer. a thorough experiment is conducted to evaluate the performance of our simulation optimization approach. computational results show that mpes is an effective search algorithm. it dominates the grid search method by a significant margin. the re-allocation strategy resulting from mpes outperforms re-balancing strategies significantly. this research further demonstrates that the simulation optimization approach can be used to study economic issues related to multi-period asset allocation problems in practical settings. © 2010 elsevier b.v. all rights reserved.
"scientific computing often requires the availability of a massive number of computers for performing large scale experiments. traditionally, these needs have been addressed by using high-performance computing solutions and installed facilities such as clusters and super computers, which are difficult to setup, maintain, and operate. cloud computing provides scientists with a completely new model of utilizing the computing infrastructure. compute resources, storage resources, as well as applications, can be dynamically provisioned (and integrated within the existing infrastructure) on a pay per use basis. these resources can be released when they are no more needed. such services are often offered within the context of a service level agreement (sla), which ensure the desired quality of service (qos). aneka, an enterprise cloud computing solution, harnesses the power of compute resources by relying on private and public clouds and delivers to users the desired qos. its flexible and service based infrastructure supports multiple programming paradigms that make aneka address a variety of different scenarios: from finance applications to computational science. as examples of scientific computing in the cloud, we present a preliminary case study on using aneka for the classification of gene expression data and the execution of fmri brain imaging workflow. © 2009 ieee."
"there is a lot of interest in parallel computing methods for nonlinear optimization, especially for large scale applications in areas such as computational finance, scientific computing and bioinformatics. in this paper, we show how gpgpu (general purpose computation on graphics processing units), a state-of-the-art and highly affordable parallel computing technology, is used to efficiently solve the quasi-newton optimization problem for applications that involve expensive objective functions. we discuss the very significant performance gain (up to about 20 times faster than the serial algorithm) with gpgpu on very large datasets in a financial application."
"in this paper we consider a location-optimization problem where the classical uncapacitated facility location model is recast in a stochastic environment with several risk factors that make demand at each customer site probabilistic and correlated with demands at the other customer sites. our primary contribution is to introduce a new solution methodology that adopts the mean-variance approach, borrowed from the finance literature, to optimize the ""value-at-risk"" (var) measure in a location problem. specifically, the objective of locating the facilities is to maximize the lower limit of future earnings based on a stated confidence level. we derive a nonlinear integer program whose solution gives the optimal locations for the p facilities under the new objective. we design a branch-and-bound algorithm that utilizes a second-order cone program (socp) solver as a subroutine. we also provide computational results that show excellent solution times on small to medium sized problems. © 2007 elsevier ltd. all rights reserved."
"we illustrate the correspondence between uncertainty sets in robust optimization and some popular risk measures in finance and show how robust optimization can be used to generalize the concepts of these risk measures. we also show that by using properly defined uncertainty sets in robust optimization models, one can construct coherent risk measures and address the issue of the computational tractability of the resulting formulations. our results have implications for efficient portfolio optimization under different measures of risk. © 2009 informs."
"fuzzy statistics have been developing for decades and though many contributions have gone into the expansion of theorems, most practitioners in the field of finance who usually use statistical methods actually seldom apply fuzzy set theory. one of the more likely reasons is that many operation rules of fuzzy statistics are still in progress. among them, the statistical measures of mean, variance, and standard deviation of fuzzy numbers are the most practically used in descriptive and inferential statistics. although they have been investigated before, previous studies on fuzzy variance and standard deviation are either defective or too rough to follow. this work therefore step-by-step develops their definitions, computational algorithms, propositions, and proofs. the deficiency of fuzzy variance is mended by substituting the requisite equality-constraint operation for standard fuzzy arithmetic. the derivation of membership functions completely depicts the shapes of the fuzzy measures and is not just an approximation. finally a numerical example of portfolio selection illustrates the calculation process and their use. © springer-verlag 2009."
"this chapter provides a methodology to solve dynamic portfolio strategies considering realistic assumptions regarding the return distribution. first, it analyzes the empirical behavior of some equities, suggesting how to approximate a historical return series with a factor model that accounts for most of the variability and proposing a methodology to generate realistic return scenarios. then it examines the profitability of some reward-risk strategies based on a forecasted evolution of the returns. since several studies in behavioral finance have shown that most investors in the market are neither risk averters nor risk lovers, the chapter discusses the use of portfolio strategies based on the maximization of performance measures consistent with these investors' preferences. it first argues the computational complexity of reward-risk portfolio selection problems and then compares the optimal sample paths of the future wealth obtained by performing reward-risk portfolio optimization on simulated data. © 2010 elsevier ltd. all rights reserved."
"it is now recognized that long memory and structural change can easily be confused because the statistical properties of times series of lengths typical of financial and econometric series are similar for both models. the implications of long memory models or short memory with breaks for the pricing of financial assets (e.g. options) have been studied by other authorities who have concluded the pricing is significantly different between the two classes of models. option pricing is also of interest to regulatory authorities. thus it is of interest both from a finance theory and regulatory point of view whether the data are generated by a true long memory process or by something else. we propose a new approach aimed at distinguishing between long memory and structural change. the approach, which utilizes the computational efficient methods based upon atheoretical regression trees (art), uses a null hypothesis of the data being a fractionally integrated series. it establishes through simulation the bivariate distribution of the fractional integration parameter, d, with regime length for simulated fractionally integrated series of given nominal d value and series length. confidence intervals can be established empirically as contours on the simulated data. this bivariate distribution is then compared with the data for the time series under test. we apply these methods to the realized volatility series of 16 stocks in the dow jones industrial average in the 10 year period from the beginning of 1994 to the end 2003. we show that in these series the value of the fractional integration d parameter is not constant with time. given that a constant d value is fundamental to the definition of a fractionally integrated process. © modsim 2009.all rights reserved."
"time series data are a ubiquitous data type appearing in many domains such as statistics, finance, multimedia, etc. similarity search and measurement on time series data are typically different from on other data types since time series data have the associations among adjacent dimensions. accordingly, the classic euclidean distance metric is not an accurate similarity measure for time series. therefore, dynamic time warping (dtw) has become a better choice for similarity measurement on time series in various applications regardless of its high computational cost. to speed up the calculation, many research works attempt to speed up dtw calculation using indexing method, which always has a tradeoff between indexing efficiency and i/o cost. in this paper, we propose a novel method to balance this tradeoff under indexed sequential access using sequentially indexed structure (sis), an approach to time series indexing with low computational cost and small overheads on i/o. finally, we conduct experiments to demonstrate our superiority in speed performance over the best existing method. © springer-verlag berlin heidelberg 2009."
"the authors develop a new monte carlo-based method for pricing path-dependent options under the variance gamma (vg) model. the gamma bridge sampling method proposed by avramidis et al. (avramidis, a. n., p. l'ecuyer, p. a. tremblay. 2003. efficient simulation of gamma and variance-gamma processes. proc. 2003 winter simulation conf. ieee press, piscataway, nj, 319-326) and ribeiro and webber (ribeiro, c., n. webber. 2004. valuing path-dependent options in the variance-gamma model by monte carlo with a gamma bridge. j. computational finance 7(2) 81-100) is generalized to a multivariate (dirichlet) construction, bridging ""simultaneously"" over all time partition points of the trajectory of a gamma process. the generation of the increments of the gamma process, given its value at the terminal point, is interpreted as a dirichlet partition of the unit interval. the increments are generated in a decreasing stochastic order and, under the kingman limit, have a known distribution. thus, simulation of a trajectory from the gamma process requires generating only a small number of uniforms, avoiding the expensive simulation of beta variates via numerical probability integral inversion. the proposed method is then applied in simulating the trajectory of a vg process using its difference-of-gammas representation. it has been implemented in both plain monte carlo and quasi-monte carlo environments. it is tested in pricing lookback, barrier, and asian options and is shown to provide consistent efficiency gains, compared to the sequential method and the difference-of-gammas bridge sampling proposed by avramidis and l'ecuyer (avramidis, a. n., p. l'ecuyer. 2006. efficient monte carlo and quasi-monte carlo option pricing under the variance gamma model. management sci. 52(12) 1930-1944)."
"a computational grid is a geographically disperssed heterogeneous computing facility owned by dissimilar organizations with diverse usage policies. as a result, guaranteeing grid resources availability as well as pricing them raises a number of challenging issues varying from security to management of the grid resources. in this chapter we design and develop a grid resources pricing model using a fuzzy real option approach and show that finance models can be effectively used to price grid resources. © 2010, igi global."
"applications of copula models have been increasing in number in recent years. this class of models provides a modular parameterization of joint distributions: the specification of the marginal distributions is parameterized separately from the dependence structure of the joint, a convenient way of encoding a model for domains such as finance. some recent advances on how to specify copulas for arbitrary dimensions have been proposed, by means of mixtures of decomposable graphical models. this paper introduces a bayesian approach for dealing with mixtures of copulas which, due to the lack of prior conjugacy, raise computational challenges. we motivate and present families of markov chain monte carlo (mcmc) proposals that exploit the particular structure of mixtures of copulas. different algorithms are evaluated according to their mixing properties, and an application in financial forecasting with missing data illustrates the usefulness of the methodology. © 2009 by the authors."
"we propose a computationally efficient and effective novel neural network for predicting the next-day's closing price of us stocks in different sectors: technology, energy and finance. in this paper we used a computationally efficient functional link artificial neural network (flann) in making stock price prediction. we modeled the trend in stock price movement as a dynamic system and apply flann to predict the stock price behavior. in addition to historical pricing data, we considered other financial indicators such as the industrial indices and technical indicators, for better accuracy. we showed its superior performance by comparing with a multilayer perceptron (mlp)-based model through several experiments based on different performance metrics, namely, computational complexity, root mean square error, average percentage error and hit rate. © 2009 ieee."
"option pricing is one of the challenging problems in finance. finding the best time to exercise an option is a even more challenging problem, especially since the price of the underlying assets change rapidly. in this work, we study complex path dependent options by exploiting and extending a novel idea that we proposed earlier using a nature inspired meta-heuristic algorithm. ant colony optimization (aco). aco has been used extensively in combinatorial optimization problems and recently in dynamic applications such as mobile ad-hoc networks where the objective is find a shortest path. however, in finance, especially in option pricing, we look for best time to exercise an option. specifically, we use ants to decide on the best time to exercise so that the holder of the option contract will get the maximum benefit from his/her investment.our algorithm and implementation suggests a better way to price options than traditional techniques such as monte carlo simulation or binomial lattice algorithm. our pricing results compare very well with other techniques and at the same timethe computational cost is reduced to a large extent. © 2009 ieee."
"computational intelligence techniques are very useful tools for solving problems that involve understanding, modeling, and analysis of large data sets. one of the numerous fields where computational intelligence has found an extremely important role is finance. more precisely, optimization issues of one's financial investments, to guarantee a given return, at a minimal risk, have been solved using intelligent techniques such as genetic algorithm, rule-based expert system, neural network, and support-vector machine. even though these methods provide good and usually fast approximation of the best investment strategy, they suffer some common drawbacks including the neglect of the dependence among among criteria characterizing investment assets (i.e. return, risk, etc.), and the assumption that all available data are precise and certain. to face these weaknesses, we propose a novel approach involving utility-based multi-criteria decision making setting and fuzzy integration over intervals. © 2009 springer-verlag berlin heidelberg."
"we review the basic principles of quasi-monte carlo (qmc) methods, the randomizations that turn them into variance-reduction techniques, the integration error and variance bounds obtained in terms of qmc point set discrepancy and variation of the integrand, and the main classes of point set constructions: lattice rules, digital nets, and permutations in different bases. qmc methods are designed to estimate s-dimensional integrals, for moderate or large (perhaps infinite) values of s. in principle, any stochastic simulation whose purpose is to estimate an integral fits this framework, but the methods work better for certain types of integrals than others (e.g., if the integrand can be well approximated by a sum of low-dimensional smooth functions). such qmc-friendly integrals are encountered frequently in computational finance and risk analysis. we summarize the theory, give examples, and provide computational results that illustrate the efficiency improvement achieved. this article is targeted mainly for those who already know monte carlo methods and their application in finance, and want an update of the state of the art on quasi-monte carlo methods. © the author(s) 2009."
"from the perspective of chinese commercial banks' loan pricing, combined with credit rationing theory, using the method of agent-based computational finance (acf), to do the bank loan's simulation experiment both on ″one cutting″ type of interest rate pricing and a comprehensive pricing model. from the comparison, we found that the different interest rates pricing model impact smes loan. from the experiment, when the loan interest rate raise to a certain extent, the earning of bank will reduce accompany by the increasing of interest rate, to some extent, reacting the credit rationing phenomenon of smes. from the experiment to prove that the comprehensive loan interest rate pricing method slow down their financing difficulties, and they also don't reduce bank's earning. so the commercial banks should choose a reasonable interest rate pricing model, according to the loan amount, credit grade, asset-liability ratio, the way of security, and other indicators, according to the risk of small, low cost and the specific circumstances of the borrower companies, to implement different interest rate."
"many mathematical calculations in the field of computational finance consume a lot of time and resources for processing. some of the short rate models used in quantitative finance which have been taken into consideration in this paper have been optimized for performance within a cluster computing environment. the back-end cluster has been seamlessly integrated with an easy-to-use front-end which can be used by finance professionals who are not aware of the details of the computational and database cluster. furthermore, many techniques that have been utilized to improve the efficiency of the models have also been described. this paper also describes the generalization of a high performance computing cluster designed for one-factor short rate models and how it can be used easily to be further extended for other mathematical models in quantitative finance. the ultimate objective is to come up with a generalized framework for quantitative finance. © 2009 ieee."
"options are important instruments in modern finance. in this paper, we investigate reinforcement learning (rl) methods in particular, least-squares policy iteration (lspi)for the problem of learning exercise policies for american options. we develop finite-time bounds on the performance of the policy obtained with lspi and compare lspi and the fitted q-iteration algorithm (fqi) with the longsta-schwartz method (lsm), the standard least-squares monte carlo algorithm from the finance community. our empirical results show that the exercise policies discovered by lspi and fqi gain larger payo fis than those discovered by lsm, on both real and synthetic data. furthermore, we find that for all methods the policies learned from real data generally gain similar payo fis to the policies learned from simulated data. our work shows that solution methods developed in machine learning can advance the state-of-the-art in an important and challenging application area, while demonstrating that computational finance remains a promising area for future applications of machine learning methods.© 2009 by the authors."
"a differential item functioning analysis is performed on a cohort of e-learning students undertaking a unit in computational finance. the motivation for this analysis is to identify differential item functioning based on attributes of the student cohort that are unobserved. the authors find evidence that a model containing two distinct latent classes of students is preferred, and identify those examination items that display the greatest level of differential item functioning. on reviewing the attributes of the students in each of the latent classes, and the items and categories that mostly distinguish those classes, the authors conclude that the bias associated with the differential item functioning is related to the a priori background knowledge that students bring to the unit. based on this analysis, they recommend changes in unit instruction and examination design so as to remove this bias. © 2009, igi global."
"this paper describes in detail the cognitive distortions studied by behavioural finance (bf) and transposes these concepts in the knowledge management (km) context, with the proposal of formally embedding them into cognitive agents, for computational modeling. orthodox-economic theory fails in representing the decisional process of individuals in a realistic way, especially regarding the non-rational component of their behavior. by moving beyond those approaches, which assume a completely rational behavior, bf explores the cognitive distortions that could lead to sub-optimal decisions and behaviors. the study of systematic cognitive errors brings to an improvement of km processes as well, where irrational human behavior affect the efficiency of collaborative groups through possible misinterpretations of data, and implicit knowledge in general. the authors propose a formal modeling technique to represent the cognitive distortions through computational agents, with the aim to create realistic simulations of organizations and social systems in general."
"this chapter introduces a new approach to genetic programming (gp), based on gmdh-based technique, which integrates a gp-based adaptive search of tree structures, and a local parameter tuning mechanism employing statistical search. the gp is supplemented with a local hill climbing search, using a parameter tuning procedure. more precisely, we integrate the structural search of traditional gp with a multiple regression analysis method and establish our adaptive program called .stroganoff' (i.e. structured representation on genetic algorithms for nonlinear function fitting). the fitness evaluation is based on aminimumdescription length (mdl) criterion, which effectively controls the tree growth in gp. its effectiveness is demonstrated by solving several system identification (numerical) problems and comparinf the performance of stroganoff with traditional gp and another standard technique. the effectiveness of this numerical approach to gp is demonstrated by successful application to computational finances. © 2009 springer-verlag berlin heidelberg."
"modelling and predicting long-range dependent time series data can find important and practical applications in many areas such as telecommunications and finance. in this paper, we consider fractional autoregressive integrated moving average (farima) processes which provide a unified approach to characterising both short-range and long-range dependence. we compare two linear prediction methods for predicting observations of farima processes, namely the innovations algorithm and kalman filter, from the computational complexity and prediction performance point of view. we also study the problem of prediction with expert advice for farima and propose a simple but effective way to improve the prediction performance. alongside the main experts (farima models) we propose to use some naive methods (such as least-squares regression) in order to improve the performance of the system. experiments on publicly available datasets show that this construction can lead to great improvements of the prediction system.we also compare our approach with a traditional method of model selection for the farima model, namely akaike information criterion. © springer-verlag 2009."
"the increasing availability of data in our current, information overloaded society has led to the need for valid tools for its modelling and analysis. data mining and applied statistical methods are the appropriate tools to extract knowledge from such data. this book provides an accessible introduction to data mining methods in a consistent and application oriented statistical framework, using case studies drawn from real industry projects and highlighting the use of data mining methods in a variety of business applications. introduces data mining methods and applications. covers classical and bayesian multivariate statistical methodology as well as machine learning and computational data mining methods. includes many recent developments such as association and sequence rules, graphical markov models, lifetime value modelling, credit risk, operational risk and web mining. features detailed case studies based on applied projects within industry. incorporates discussion of data mining software, with case studies analysed using r. is accessible to anyone with a basic knowledge of statistics or data analysis. includes an extensive bibliography and pointers to further reading within the text. applied data mining for business and industry, 2nd edition is aimed at advanced undergraduate and graduate students of data mining, applied statistics, database management, computer science and economics. the case studies will provide guidance to professionals working in industry on projects involving large volumes of data, such as customer relationship management, web design, risk management, marketing, economics and finance. © 2009 john wiley & sons, ltd."
"once a real options framework has been defined for a problem or a project, the true goal is to value that option. the use of software-based models allows the analyst to apply a consistent, well-tested, and replicable set of models. it reduces computational errors and allows the user to focus more on the process and problem at hand rather than on building potentially complex and mathematically intractable models. this chapter provides a good starting point with an introduction to the real options super lattice solver software. for more details on using the software, consult the user manual, whereas for more technical, theoretical, and practical details of real options analysis, consult real options analysis: tools and techniques, 2nd ed. (wiley finance; mun 2005). the materials covered in this chapter assume that the reader is suf?ciently well versed in the basics of real options analytics. © 2010 by taylor and francis group, llc."
"behavioural finance (bf) is an approach for studying finance and economics, based on the interactions among cognitive sciences and decision-making models. orthodox-economic theory fails in representing the decisional process of individuals in a realistic way, especially regarding the non-rational component of their behavior. by moving beyond those approaches, which assume a completely rational behavior, bf explores the main cognitive distortions that could lead to sub-optimal decisions and behaviors. traditional finance considers non-rational behaviors like anomalies, but the effects observed in the real world indicate that new modellng efforts are required for the emotional components. this paper analyzes the most frequent behavioural distortions (biases, heuristics and framing effects) in terms of bf, and proposes their use within computational models, employed as tools for better understanding the aggregate and complex effects of emotionally distorted behaviors, as opposed to pure rational ones, when dealing with financial topics. © 2009 ieee."
"from an importance sampling viewpoint, broadie and glasserman [m. broadie, p. glasserman, a stochastic mesh method for pricing high-dimensional american options, journal of computational finance 7 (4) (2004) 35-72] proposed a stochastic mesh method to price american options. in this paper, we revisit the method from a conditioning viewpoint, and derive some new weights. © 2009 elsevier b.v. all rights reserved."
"the problem of growing computational complexity in the finance industry demands manageable, high-speed and real-time solutions in solving complex mathematical problems such as option pricing. in current option trading scenarios, determining a fair price for options ""any time"" and ""anywhere"" has become vital yet difficult computational problem. in this study, we have designed, implemented, and deployed an architecture for pricing options on-line using a hand-held device that is j2me-based mobile computing-enabled and is assisted by web mining tools. in our architecture, the client is a midp user interface, and the back end servlet runs on a standalone server bound to a known port address. in addition, the server uses table-mining techniques to mine real-time data from reliable web sources upon the mobile trader's directive. the server performs all computations required for pricing options since mobile devices have limited battery power, low bandwidth, and low memory. we have parallelized and implemented various computational techniques such as binomial lattice and finite differencing. to the best of our knowledge, this is one of the first studies that facilitate the mobile-enabled-trader to compute the price of an option in ubiquitous fashion. this architecture aims at providing the trader with various computational techniques to avail (to provide results from approximate to accurate results) while on-the-go and to make important and effective trading decisions using the results that will ensure higher returns on investments in options. © 2008 springer science+business media, llc."
"agent-based artificial financial markets are an area of increasing interest in computational finance. recent work by lebaron and yamamoto proposes an order-driven market model based on evolutionary algorithm based artificial agents. in this paper, we present a mechanism for incorporating elements of swarm intelligence into this model, and find that our model produces market price behavior that, in some ways, is closer to that of real financial markets. © 2009 ieee."
"several artificial intelligence (ai)-related conferences have organized in greece to emphasize its increasing importance for different fields and applications. the ai-related conferences that have been organized in the country include the european conference on artificial intelligence (ecai 08), the european chapter of the association for computational linguistics (eacl 09), user modeling (um 07), and the ieee international conference on tools with artificial intelligence (ictai 07). significant efforts are being made to encourage cooperation greek ai laboratories and industrial sites. a network of research sites has been established in academic institutions and the industry to encourage people to start thinking of possible cooperative ventures. a survey has also revealed that greece's ai professionals are developing a wide range of applications in fields, such as entertainment, business, commerce, finance, e-government, health and biomedicine, and education."
option pricing is one of the challenging areas of computational finance. in this paper an attempt is made to apply particle swarm optimization (pso) for pricing options. pso is one of the novel global search algorithm based on swarm intelligence. it is shown that pso could be effectively used for single variate option pricing problem. the results are compared with standard classical black- scholes model for simple european options. with the current understanding from these initial experiments we suggest various avenues for further exploration. copyright © 2009 acm.
"as many data-driven fields, finance is rich in problems requiring high computational power and intelligent systems techniques. in particular, the problem of selecting an optimal financial portfolio can be conveniently represented as a constrained optimization problem or a decisionmaking problem. the aim of this paper is to show how to express the optimal portfolio selection problem from a decision-theoretic perspective and show how to address this problem using fuzzy measures and fuzzy integrals. © 2010 wiley periodicals, inc."
"this new volume introduces readers to the current topics of industrial and applied mathematics in china, with applications to material science, information science, mathematical finance and engineering. the authors utilize mathematics for the solution of problems. the purposes of the volume are to promote research in applied mathematics and computational science; further the application of mathematics to new methods and techniques useful in industry and science; and provide for the exchange of information between the mathematical, industrial, and scientific communities. © 2009 by."
"we propose an extension of the transform approach to option pricing introduced in duffie, pan and singleton (econometrica 68(6) (2000) 1343-1376) and in carr and madan (journal of computational finance 2(4) (1999) 61-73). we term this extension the ""coherent state transform"" approach, it applies when the markov generator of the factor process can be decomposed as a linear combination of generators of a lie symmetry group. then the family of group invariant coherent states determine the transform to price derivatives. we exemplify this procedure deriving a coherent state transform for affine jump-diffusion processes with positive state space. it improves the traditional fft because inversion of the latter requires integration over an unbounded domain, while inversion of the coherent state transform requires integration over unit ball. we explicitly perform the pricing exercise for some contracts like the plain vanilla options on (credit) risky bonds and on the spread option. © 2009 world scientific publishing company."
"in mathematical finance, pricing a path-dependent financial derivative, such as a continuously monitored asian option, requires the computation of e[g(b(·))], the expectation of a payoff functional, g, of a brownian motion, b(t). the expectation problem is an infinite dimensional integration which has been studied in [1], [5], [7], [8], and [10]. a straightforward way to approximate such an expectation is to take the average of the functional over n sample paths, b1, . . .,bn. the brownian paths may be simulated by the karhunen-lóeve expansion truncated at d terms, b̂d . the cost of functional evaluation for each sampled brownian path is assumed to be o(d). the whole computational cost of an approximate expectation is then o(n), where n =nd. the (randomized) worst-case error is investigated as a function of both n and d for payoff functionals that arise from hilbert spaces defined in terms of a kernel and coordinate weights. the optimal relationship between n and d given fixed n is studied and the corresponding worst-case error as a function of n is derived. © springer-verlag berlin heidelberg 2009."
"data streams arise in several domains. for instance, in computational finance, several statistical applications revolve around the real-time discovery of associations between a very large number of co-evolving data feeds representing asset prices. the problem we tackle in this paper consists of learning a linear regression function from multivariate input and output streaming data in an incremental fashion while also performing dimensionality reduction and variable selection. when input and output streams are high-dimensional and correlated, it is plausible to assume the existence of hidden factors that explain a large proportion of the covariance between them. the methods we propose build on recursive partial least squares (pls) regression. the hidden factors are dynamically inferred and tracked over time and, within each factor, the most important streams are recursively identified by means of sparse matrix decompositions. moreover, the recursive regression model is able to adapt to sudden changes in the data generating mechanism and also identifies the number of latent factors. extensive simulation results illustrate how the methods perform and compare with alternative penalized regression models for streaming data. we also apply the algorithm to solve a multivariate version of the enhanced index tracking problem in computational finance. © 2010 wiley periodicals, inc."
"in this paper, through an computational approach, we study mean-variance efficiency and diversification contradiction that the high concentration in a few securities in portfolio selection implicates the incompatibility of efficiency and diversification which are two corner stones of modern finance. typical treatments include imposing portfolio constraints. unfortunately, the existing studies usually suffer from simplified models, small-scale computation, and crude representation of efficient frontiers which are the set of nondominated solutions in the terminology of multiple objective optimization. to make thing worse, inequality constraints of portfolio optimization such as upper bounds for portfolio weights knock out the possibility of closed-form type of optima, so computational methodology based on empirical data can be the only way to study the contradiction. we sample 13, 26, 52, 125, and 326 chinese stocks from 2003-2005 and 2006-2008 periods, jointly construct portfolio constraints of upper bounds, market values, p/e ratios, turnover ratios, and industries, and utilize parametric quadratic programming to obtain precise and complete efficient frontiers of portfolio optimization. we find that upper bounds can be effective in alleviating the contradiction, while market values, p/e ratios, turnover ratios, and industries have much dampened influence when applied separately or joints (excluding upper bounds). the conclusion is supported by all the samples of both time periods. our research can help both individual and institutional investors balance efficiency and diversification and consequently serve portfolio theory and finance industry of our motherland, especially in the turmoil world-wide financial crises. moreover, our methodology is based on the latest computational extensions of portfolio optimization and can be deployed to stock markets worldwide to draw more comprehensive conclusions. ©2009 ieee."
the proceedings contain 16 papers. the topics discussed include: asia federation report international symposium on grid computing 2007; e-infrastructure for taiwan and international collaboration; e-science for high energy physics in korea; hep grid computing in the uk: moving towards the lhc era; medigrid - grid computing for medicine and life sciences; porting biological applications in grid: an experience within the euchinagrid framework; ress: a resource selection service for the open science grid; a model for the storage resource manager; the session based fault tolerance algorithm of platform ego web service gateway; experiences with using unicore in production grid infrastructures deisa and d-grid; dashboard for the lhc experiments; monitoring the availability of grid services using sam and gridview; and managing parallel and distributed monte carlo simulations for computational finance in a grid environment.
"analyses of serially-sampled data often begin with the assumption that the observations represent discrete samples from a latent continuous-time stochastic process. the continuous-time markov chain (ctmc) is one such generative model whose popularity extends to a variety of disciplines ranging from computational finance to human genetics and genomics. a common theme among these diverse applications is the need to simulate sample paths of a ctmc conditional on realized data that is discretely observed. here we present a general solution to this sampling problem when the ctmc is defined on a discrete and finite state space. specifically, we consider the generation of sample paths, including intermediate states and times of transition, from a ctmc whose beginning and ending states are known across a time interval of length t. we first unify the literature through a discussion of the three predominant approaches: (1) modified rejection sampling, (2) direct sampling, and (3) uniformization. we then give analytical results for the complexity and efficiency of each method in terms of the instantaneous transition rate matrix q of the ctmc, its beginning and ending states, and the length of sampling time t. in doing so, we show that no method dominates the others across all model specifications, and we give explicit proof of which method prevails for any given q, t, and endpoints. finally, we introduce and compare three applications of ctmcs to demonstrate the pitfalls of choosing an inefficient sampler. © institute of mathematical statistics, 2009."
"quasi-monte carlo method (qmc) is an efficient technique for numerical integration. qmc provides a lower convergence rate, o(lnd n/ n), than the standard monte carlo (mc), o(1/√n), where n is the number of simulations and d the nominal problem dimension. however, some studies in the literature have claimed that the qmc performs better than the mc method for d < 20/30 because of its dependence on d. caflisch et al. (j comput finance 1(1):27-46, 1997) have proposed to extend the qmc superiority by anova considerations. to this aim, we consider the asian basket option pricing problem, where d is much higher than 30, by qmc simulation. we investigate the applicability of several path-generation constructions that have been proposed to overtake the dimensional drawback. we employ the principal component analysis, the linear transformation, the kronecker product approximation and test their performance both in terms of computational cost and accuracy. finally, we compare the results with those obtained by the standard mc. © springer-verlag 2009."
"agent-based modeling (abm) is a computational technique employed to simulate the behavior of autonomous units called agents. an agent is a virtual entity which interacts with its environment and other agents and is capable of making decisions and taking actions based on these interactions. relatively simple rules are utilized to govern individual interactions; however the cumulative behavior of all agents may be complex and may sometimes reveal interesting patterns. agent-based modeling, though first introduced in the field of artificial intelligence, has gained acceptance in a many diverse fields such as finance engineering, bio-film modeling, social computation, and ecology. the systems studied in these fields are inherently complex involving many interacting components. these systems are modeled by representing the various system components as agents and having simple rules governing the behavior of agents. these rules can be changed to match the emergent behavior of the model with the behavior of the actual system. however, validating such a model is not straight forward as it is difficult to objectively determine similarity between the patterns in the model output and data. we propose a universal paradigm to facilitate such comparisons by extracting features from patterns in the data and the model output and use these features to train and validate the model. the merit of this methodology will be adopted to train an abm simulating the movement of cattle grazing in a pasture using data from gps units installed on individual cows."
"forecasting is an important activity in finance. traditionally, forecasting has been done with in-depth knowledge in finance and the market. advances in computational intelligence have created opportunities that were never there before. computational finance techniques, machine learning in particular, can dramatically enhance our ability to forecast. they can help us to forecast ahead of our competitors and pick out scarce opportunities. this paper explains some of the opportunities offered by computational intelligence and some of the achievements so far. it also explains the underlying technologies and explores the research horizon. © 2009 higher education press and springer-verlag gmbh."
"in the past decade, quasi-monte carlo (qmc) method has become an important numerical tool in computational finance. this is driven, in part, by the sophistication of the models and, in part, by the complexity of the derivative securities. in this paper, we consider an enhanced qmc method recently proposed by imai and tan (2009). this method is known as the generalized linear transformation (glt) and it increases the effciency of qmc via dimension reduction. glt can be used to simulate general stochastic processes and hence has a much wider range of applications. by assuming that the dynamics of the underlying asset price follows an exponen-tial meixner lévyprocessandbyresortingtosome exotic options including average options and lookback options, we demonstrate the effectiveness and robustness of glt and it substantially outperforms the standard applications of qmc and monte carlo methods. © international association of engineers."
"this paper concerns itself with applications of pair-copulas in finance, and bridges the gap between theory and application. we provide a broad view of the problem of modeling multivariate financial log-returns using pair-copulas, gathering together for this purpose theoretical and computational results from the literature on canonical vines. from the practitioner's viewpoint, the paper shows the advantages of modeling through pair-copulas and makes clear that it is possible to implement this methodology on a daily basis. all the necessary steps (model selection, estimation, validation, simulations, and applications) are discussed at a level easily understood by all data analysts. © 2010 swiss society for financial market research."
"forward start options are examined in heston's (review of financial studies 6 (1993) 327-343) stochastic volatility model with the cir (econometrica 53 (1985) 385-408) stochastic interest rates. the instantaneous volatility and the instantaneous short rate are assumed to be correlated with the dynamics of stock return. the main result is an analytic formula for the price of a forward start european call option. it is derived using the probabilistic approach combined with the fourier inversion technique, as developed in carr and madan (journal of computational finance 2 (1999) 61-73). © 2009 world scientific publishing company."
"option pricing is one of the fundamental problems in finance. this chapter proposes a novel idea for pricing options using a nature inspired meta-heuristic algorithm called ant colony optimization (aco). aco has been used in many np-hard combinatorial optimization problems and most recently in self-organized environments in dynamic networks such as ad hoc and sensor networks. the dynamic changes in financial asset prices poses greater challenges to exercise the option at the right time. the dynamic nature of the option pricing problem lends itself very easily in using the aco technique to the solution of computing option prices. aco is as intuitive as other techniques such as binomial lattice approach. aco searches the computational space eliminating areas that may not provide a profitable solution. the computational cost, therefore, tends to decrease during the execution of the algorithm. there has been no study reported in the literature on the use of aco for pricing financial derivatives. we first study the suitability of aco in finance and confirm that aco could be applied to financial derivatives. we propose two aco based algorithms to apply to derivative pricing problems in computational finance. the first algorithm, named sub-optimal path generation is an exploitation technique. the second algorithm named the dynamic iterative algorithm captures market conditions by using an exploration and exploitation technique. we analyze the advantages and disadvantages of both the algorithms. with both the algorithms we are able to compute the option values and we find that the sub-optimal path generation algorithm outperforms the binomial lattice method. the dynamic iterative algorithm can be used on any random graph and the uncertainties in the market can be captured easily but it is slower when compared to the sub-optimal path generation algorithm. © 2009 springer-verlag berlin heidelberg."
"this paper provides the construction of a powerful and efficient computational method, that translates polyrakis algorithm [i.a. polyrakis, minimal lattice-subspaces, trans. am. math. soc. 351 (1999) 4183-4203, theorem 3.19] for the calculation of lattice-subspaces and vector sublattices in rn. in the theory of finance, lattice-subspaces have been extensively used in order to provide a characterization of market structures in which the cost-minimizing portfolio is price-independent. specifically, we apply our computational method in order to solve a cost minimization problem that ensures the minimum-cost insured portfolio. © 2009 elsevier inc. all rights reserved."
"regulation can play an important role in effectively managing systemic risk while providing accountability to all affected governments. imf points out weak governance structures as one of the main causes for financial/economical crisis. however, research in this area is still limited. one of the reasons is the inherent complexity of the public sector governance notion. in this research, the regulatory governance of the financial sector is conceived as a complex system, in which governance is perceived as a phenomenon resulting from the interactions among all the actors that influence or are influenced by regulatory activities within the financial sector. an agent-based simulation was developed to analyze and evaluate the emergent behaviors from the governance in the brazilian finance sector under different macroeconomics variables and different attitudes, perceptions and desires of economic and political actors. the agent-based model is combined with an econometric model, which is intended to characterize the macroeconomic environment. the regulatory environment is modeled by computational agents using bdi (beliefs-desires-intentions) architecture. the agents have beliefs about their environment and desires they want to satisfy, thus leading them to create intentions to act. the agents' behavior was modeled using fuzzy rules built by means of content analysis of newspapers and in-depth interviews with experts from the financial area. computational experiments demonstrate the potential of the agent-based model simulation in the study of complex environments involving regulatory governance. © 2009 elsevier ltd. all rights reserved."
"as the study of agent-based computational economics and finance grows, so does the need for appropriate techniques for the modeling of complex dynamic systems and the intelligence of the constructive agent. these methods are important where the classic equilibrium analytics fail to provide sufficiently satisfactory understanding. in particular, one area of computational intelligence, approximate dynamic programming, holds much promise for applications in this field and demonstrate the capacity for artificial higher order neural networks to add value in the social sciences and business. this chapter provides an overview of this area, introduces the relevant agent-based computational modeling systems, and suggests practical methods for their incorporation into the current research. a novel application of honn to adp specifically for the purpose of studying agent-based financial systems is presented. © 2009, igi global."
"recent advances in mathematical finance established linkages among several key concepts related to coherence, distorted risk measures, and information theory. the purpose of this paper is to extend these theoretical results for empirical applications in computational finance. first, we use a concentrated (dual) entropy approach to derive a computational algorithm for estimating the parameters of a distorted probability model associated with a coherent risk measure for a given sample of observed data. second, we derive the asymptotic sampling properties of the estimated model parameters, which may be used to conduct classical hypothesis tests or to form other statistical inferences based on the estimated coherent risk measure. third, we note that researchers may also require an estimate of the net loss distribution, and we propose an information theoretic procedure for jointly estimating the net loss probability model and the distorted probability distribution associated with a particular coherent risk measure."
"credit scoring has been regarded as a critical topic and studied extensively in the finance field. many artificial intelligence techniques have been used to solve credit scoring. the paper is to build a classification model based on a decision tree by learning historical data. clustering algorithm and genetic algorithm are combined to further improve the accuracy of this credit scoring model. the clustering algorithm aims at removing noise data, while the genetic algorithm is used to reduce the redundancy attribute of data. the computational results on the two real world benchmark data sets show that the presented hybrid model is efficient. © 2008 ieee."
"as a result of the increased availability of higher precision spatiotemporal datasets, coupled with the realization that most real-world human systems are complex, a new field of computational modeling is emerging in which the goal is to develop minimal models of collective human behavior which are consistent with the observed real-world dynamics in a wide range of systems. for example, in the field of finance, the fluctuations across a wide range of markets are known to exhibit certain generic stylized facts such as a non-gaussian 'fat-tailed' distribution of price returns. in this paper, we illustrate how such minimal models can be constructed by bridging the gap between two existing, but incomplete, market models: a model in which a population of virtual traders make decisions based on common global information but lack local information from their social network, and a model in which the traders form a dynamically evolving social network but lack any decision-making based on global information. we show that a combination of these two models - in other words, a population of virtual traders with access to both global and local information - produces results for the price return distribution which are closer to the reported stylized facts. going further, we believe that this type of model can be applied across a wide range of systems in which collective human activity is observed. © 2008 springer-verlag berlin heidelberg."
"with the evolution of graphics processing units (gpus) into powerful and cost-efficient computing architectures, their range of application has expanded tremendously, especially in the area of computational finance. current research in the area, however, is limited in terms of options priced and complexity of stock price models. this paper presents algorithms, based on the fourier space time-stepping (fst) method, for pricing single and multi-asset european and american options with lévy underliers on a gpu. furthermore, the single-asset pricing algorithm is parallelized to attain greater efficiency. ©2008 ieee."
"computational finance is an important application area for high-performance computing today. large computational resources are used for a variety of operations related to securities and asset portfolios. for online operations, the focus has been both on reducing latency and improving the quality of the algorithms. this focus on latency has forced a predominance of univariate analysis simply from a feasibility perspective. in this paper, we demonstrate that current supercomputers, and in particular the blue gene family of supercomputers, enables the move to online multivariate analysis of entire markets. we use a simple but representative example of multivariate analysis, namely the computation of the correlation matrix, to explore that space. we show how the computation can be parallelized and run as an online real-time operation at the scale of thousands of securities and millions of events per second. © 2008 ieee."
"computing option prices is a challenging problem. finding the best time to exercise an option is a even more challenging problem. one has to be watchful for the price changes in the market place and act at the right time. that is. prices need to be policed. this paper proposes a novel idea for pricing options using a nature inspired meta-heuristic algorithm, ant colony optimization (aco). aco has been used extensively in combinatorial optimization problems and recently in dynamic applications such as mobile ad-hoc networks. specifically, we adapt the general aco algorithm to apply to a totally different application, computational finance, in the current study. we police the prices using ants to decide on the best time to exercise so that the holder of the option contract will get the maximum benefit out of his/her investment. our algorithm and implementation suggests a better-way to price options than traditional numerical techniques such as binomial lattice algorithm. from our results we conclude that reactive ants may be best suited for long-dated options whose performance can still be improved. copyright © 2008 acm."
"in the realm of computational finance, the performance of the optimal portfolio largely depends upon its composition and its ability to accurately predict the market movements. recent empirical studies have shown that the underlying assumption of normality of asset returns for risk modeling is seriously flawed, in view of their asymmetric and fat-tailed behavior. this problem is further aggravated when we delve into the functioning of the financial market and realize that the market parameters have highly nonlinear kind of inter-dependence amongst themselves. any investment portfolio that does not account for these factors and their mutual relationship, will tend to under-perform. this work is a novel attempt, which aims at developing a framework which solves all of these problems in an integrated fashion, without overlooking any of them or pre-assigning lesser importance to any of these issues. the contemporary techniques often neglect one of them, resulting in an incomplete and sometimes even a misleading picture of the market scenario. in this work, copula theory effectively captures the non-linear inter-dependence. the scenarios are generated from a non-elliptical multivariate distribution constructed by a students t-copula assuming marginal distributions as gaussian in the center and evt distributed in the tail. for gauging the market risk we have used cvar (conditional value-at-risk) as the risk measure. the efficient frontier thus resulted by minimizing the cvar and maximizing the returns, gives a clear insight into how does the composition of the optimal portfolio changes with respect to change in cvar of the portfolio. our aim is to prove that much more reliable conclusions will certainly be drawn if a more realistic representation of data can be done using the concept of copulas. © 2008 ieee."
"this brief survey gives an introduction on agent-based computational finance (abcf), focusing on features of heterogeneity and interaction among agents. in contrast to traditional deductive asset pricing theory with strictly defined representative investors, abcf is characteristic of multi heterogeneous agents, making their own trading decisions in a virtual market respectively and interacting with each other evolutionally. among a vast array of potential abcf models, santa fe artificial stock model (sf-asm) and heterogeneous agent model (ham) are supposed to be the most prominent and prevalent. we give a simple conclusion and future directions for abcf. © 2008 crown copyright."
"support vector regression (svr) is an established non-linear regression technique that has been applied successfully to a variety of predictive problems arising in computational finance, such as forecasting asset returns and volatilities. in real-time applications with streaming data two major issues that need particular care are the inefficiency of batch-mode learning, and the arduous task of training the learning machine in presence of non-stationary behavior. we tackle these issues in the context of algorithmic trading, where sequential decisions need to be made quickly as new data points arrive, and where the data generating process may change continuously with time. we propose a master algorithm that evolves a pool of on-line svr experts and learns to trade by dynamically weighting the experts' opinions. we report on risk-adjusted returns generated by the hybrid algorithm for two large exchange-traded funds, the ishare s&p 500 and dow jones eurostoxx 50. © 2008 springer-verlag."
"motivated by the compensatory property of ea and pso, where the latter can enhance solutions generated from the evolutionary operations by exploiting their individual memory and social knowledge of the swarm, this paper examines the implementation of pso as a local optimizer for fine tuning in evolutionary search. the proposed approach is evaluated on applications from the field of computational finance, namely portfolio optimization and time series forecasting. exploiting the structural similarity between these two problems and the non-linear fractional knapsack problem, an instance of the latter is generalized and implemented as the preliminary test platform for the proposed ea-pso hybrid model. the experimental results demonstrate the positive effects of this memetic synergy and reveal general design guidelines for the implementation of pso as a local optimizer. algorithmic performance improvements are similarly evident when extending to the real-world optimization problems under the appropriate integration of pso with ea. © 2008 elsevier ltd. all rights reserved."
"we have been organizing the workshop on computational finance and business intelligence (cfbi) at international conference on computational science (iccs) since 2003. this workshop at iccs, baton rouge, louisiana, u.s.a., may 25-27, 2009 focuses on computational science aspects of asset and derivatives pricing, financial risk management, and related topics to business intelligence. it will include but not limited to modeling, numeric computation, algorithmic and complexity issues in arbitrage, asset pricing, future and option pricing, risk management, credit assessment, interest rate determination, insurance, foreign exchange rate forecasting, online auction, cooperative game theory, general equilibrium, information pricing, network band witch pricing, rational expectation, repeated games, etc. © 2009 springer berlin heidelberg."
the world of finance is an exciting and challenging environment. recent years have seen an explosion in the application of computational intelligence methodologies in finance. in this article we provide an overview of some of these applications concentrating on those employing an evolutionary computation approach © 2006 non ieee.
"fpga based implementations of two classes of pseudo random number(prn) generator, intended for use in monte carlo methods for finance, are presented. fpga implementations potentially offer reduced cost and improved performance compared to general purpose processor (gpp) systems such as pcs or mainframes. the first class of prn generator, which includes the mersenne twister, uses generalised feedback shift registers (gf-srs). the second class is based on multiplication of fixed precision integers (with overflow). in both cases we compare a high quality generator and a generator with minimal resource usage. comparisons of fpga resource usage, data throughput and the quality of the generated series are given with a view to applications in high performance computing (hpc) for computational finance. the two classes of generator are shown to be complementary in their use of fpga resources. © 2008 ieee."
"in this paper, we present a new formulation for constructing an n-dimensional ellipsoid by generalizing the computation of the minimum volume covering ellipsoid. the proposed ellipsoid construction is associated with a user-defined parameter β [0,1), and formulated as a convex optimization based on the cvar minimization technique proposed by rockafellar and uryasev (j. bank. finance 26: 1443-1471, 2002). an interior point algorithm for the solution is developed by modifying the drn algorithm of sun and freund (oper. res. 52(5):690-706, 2004) for the minimum volume covering ellipsoid. by exploiting the solution structure, the associated parametric computation can be performed in an efficient manner. also, the maximization of the normal likelihood function can be characterized in the context of the proposed ellipsoid construction, and the likelihood maximization can be generalized with parameter β. motivated by this fact, the new ellipsoid construction is examined through a multiclass discrimination problem. numerical results are given, showing the nice computational efficiency of the interior point algorithm and the capability of the proposed generalization. © 2007 springer science+business media, llc."
"the computational burden of numerical barrier option pricing is significant, even prohibitive, for some parameterizations - especially for more realistic models of underlying asset behavior, such as jump diffusions. we extend a binomial jump diffusion pricing algorithm into a trinomial setting and demonstrate how an adaptive mesh may fit into the model. our result is a barrier option pricing method that employs fewer computational resources, reducing run times substantially. we demonstrate that this extension allows the pricing of options that were previously computationally infeasible and examine the parameterizations in which use of the adaptive mesh is most beneficial. © 2008 the southern finance association and the southwestern finance association."
"with the reformation of project investment and finance system, multi-regional investment project, which is of great investment, long construction cycle, complex influence factors and so on, needs distribution investment among benefited regions. in view of investment-distribution decision problems, this paper introduces some concepts such as optimal expectation level, tolerating limitation, relative deviation value and relative deviation tolerating level and so on, sets up an interactive investment-distribution decision model and gets the solution by the construction of assistant model. the model takes into consider of not only the impact of investment projects on every region but also the economic development degree of regions. furthermore, it combines preference information of decision maker with distribution result, and owns easy interactive process and low computational complexity. a numerical analysis confirms that the investment-distribution results are easily accepted by all regions. © 2008 ieee."
"the paper describes the development of the software tool transformation of algorithms in c++ (tac++) for automatic differentiation (ad) of c(++) codes by source-to-source translation. we have transferred to tac++ a subset of the algorithms from its well-established fortran equivalent, transformation of algorithms in fortran (taf). tac++ features forward and reverse as well as scalar and vector modes of ad. efficient higher order derivative code is generated by multiple application of tac++. high performance of the generated derivate code is demonstrated for five examples from application fields covering remote sensing, computer vision, computational finance, and aeronautics. for instance, the run time of the adjoints for simultaneous evaluation of the function and its gradient is between 1.9 and 3.9 times slower than that of the respective function codes. options for further enhancement are discussed. © 2008 springer-verlag berlin heidelberg."
"this paper introduces the distribution of a stochastic control algorithm which is applied to gas storage valuation, and presents its experimental performances on two pc clusters and an ibm blue gene/l supercomputer. this research is part of a french national project which gathers people from the academic world (computer scientists, mathematicians, ...) as well as people from the industry of energy and finance in order to provide concrete answers on the use of computational clusters, grids and supercomputers applied to problems of financial mathematics. the designed distribution allows to run gas storage valuation models which require considerable amounts of computational power and memory space while achieving both speedup and size-up: it has been successfully implemented and experimented on pc clusters (up to 144 processors) and on a blue gene supercomputer (up to 1024 processors). finally, our distributed algorithm allows to use more computing resources in order to maintain constant the execution time while increasing the calculation accuracy. ©2008 ieee."
"numerical simulations in computational physics, biology, and finance, often require the use of high quality and efficient parallel random number generators. we design and optimize several parallel pseudo random number generators on the cell broadband engine, with minimal correlation between the parallel streams: the linear congruential generator (lcg) with 64-bit prime addend and the mersenne twister (mt) algorithm. as compared with current intel and amd microprocessors, our cell/b.e. lcg and mt implementations achieve a speedup of 33 and 29, respectively. we also explore two normalization techniques, gaussian averaging method and box mueller polar/cartesian, that transform uniform random numbers to a gaussian distribution. using these fast generators we develop a parallel implementation of value at risk, a commonly used model for risk assessment in financial markets. to our knowledge we have designed and implemented the fastest parallel pseudo random number generators on the cell/b.e.. © 2008 ieee."
"monte carlo methods are used extensively in computational finance to estimate the price of financial derivative options. we review the use of quasi-monte carlo methods to obtain the same accuracy at a much lower computational cost, and focus on three key ingredients: the generation of sobol' and lattice points, reduction of effective dimension using the principal component analysis approach at full potential, and randomization by shifting or digital shifting to give an unbiased estimator with a confidence interval. our aim is to provide a starting point for finance practitioners new to quasi-monte carlo methods. © austral. mathematical soc. 2008."
"use of grid resources has been free so far and a trend is developing to charge the users. the challenges that characterize a grid resource pricing model include the dynamic ability of the model to provide a high satisfaction guarantee measured as quality of service (qos) - from users perspectives, profitability constraints - from the grid operator perspectives, and the ability to orchestrate grid resources for their availability on-demand. in this study, we design, develop, and simulate a grid resources pricing model that balances these constraints. we employ financial option theory and treat the grid resources as real assets to capture the realistic value of the grid compute commodities. we then price the grid resources by solving the finance model. we discuss the results on pricing of compute cycles based on the actual data of grid usage pattern obtained from the westgrid and the sharcnet. we extend and generalize our study to any computational grid. © 2008 ieee."
"options are important financial instruments, whose prices are usually determined by computational methods. computational finance is a compelling application area for reinforcement learning research, where hard sequential decision making problems abound and have great practical significance. in this paper, we investigate reinforcement learning methods, in particular, least squares policy iteration (lspi), for the problem of learning an exercise policy for american options. we also investigate a method by tsitsiklis and van roy, referred to as fqi. we compare lspi and fqi with lsm, the standard least squares monte carlo method from the finance community. we evaluate their performance on both real and synthetic data. the results show that the exercise policies discovered by lspi and fqi gain larger payoffs than those discovered by lsm, on both real and synthetic data. our work shows that solution methods developed in reinforcement learning can advance the state of the art in an important and challenging application area, and demonstrates furthermore that computational finance remains an under-explored area for deployment of reinforcement learning methods. © 2008 springer berlin heidelberg."
"a method of detecting changes or anomalies in periodic information-carrying signals or any other sets of data using kullback-leibler divergence is described. theoretical reasons for using this information-theoretic approach are briefly outlined and followed by its detailed application on disturbance/anomaly detection in wireless signals. even though the concept is illustrated in a communications centric framework, it is more generally applicable in areas such as computational neuroscience, mathematical finance and others where it is important to statistically detect unexpected signal distortions. the results obtained show that the proposed approach is robust, highly effective, and has a low implementation complexity."
"message oriented middleware (mom) is a key technology in financial market data delivery. in this context we study the advanced message queuing protocol (amqp), an emerging open standard for mom communication. we design a basic suite of benchmarks for amqp's direct. fanout, and topic exchange types. we then evaluate these benchmarks with apache qpid, an open source implementation ofamqp. in order to observe how amqp performs in a real-life scenario, we also perform evaluations with a simulated stock exchange application. all our evaluations are perfonned over infiniband as well as i gigabit ethernet networks. our results indicate that in order to achieve the high scalability requirements demanded by high performance computational finance applications, we need to use modern communication protocols, like rdma. which place less processing load on the host. we also .find that the centralized architecture ofamqp presents a considerable bottleneck asfar as scalability is concerned .© 2008 ieee."
"the unlimited growth of supply chain finance data will inevitably lead to a situation in which it is increasingly difficult to access the desired information. supply chain finance is often necessary to analyze large data sets, maintained over geographically bank, supplier and customer distributed sites by using cooperative context-aware distributed data mining (ddm) systems. the study of the existing approaches shows that no single solution fulfills all requirements identified for the cooperative context-aware ddm systems. one of the basic obstacles is the lack of context-aware and supporting some of the computational resources - such as data and information bases, computational models, compute power to execute these models, specialized data mining algorithms - required to develop a new compound is not available locally, but accessible via the global computing network infrastructure. this paper proposes a multi-agent-based architecture for supply chain finance cooperative distributed data mining systems. the use of multi-agent-systems (mas) creates a framework which allows the inter-operation of a vast set of heterogeneous solutions to carry out the complex supply chain finance context-aware distributed data mining tasks, many data mining tasks to connect heterogeneous resources, as data sources, processing nodes and end user applications. it considers a data warehousing that supports context-aware olap queries, ensuring the interoperability of all data sources, and then focuses on distributed clustering algorithms and some potential applications in multi-agent-based problem solving scenarios. finally, we outline the implementation of a prototype for shoes manufacturing. © 2008 ieee."
"abstract: in this chapter, we present an overview of the recent developments of vector quantization and functional quantization and their applications as a numerical method in finance, with an emphasis on the quadratic case. quantization is a way to approximate a random vector or a stochastic process, viewed as a hilbert-valued random variable, using a nearest neighbor projection on a finite codebook. we make a review of cubature formulas to approximate expectation, an conditional expectation, including the introduction of a quantization-based richardson-romberg extrapolation method. the optimal quadratic quantization of the brownian motion is presented in full detail. a special emphasis is made on the computational aspects and the numerical applications, in particular, the pricing of different kinds of options in various fields (swing options on gas and options in a heston stochastic volatility model). © 2009 elsevier b.v. all rights reserved."
"computational finance relies heavily on the use of monte carlo simulation techniques. however, monte carlo simulation is computationally very demanding. we demonstrate the use of architecturally diverse systems to accelerate the performance of these simulations, exploiting both graphics processing units and field-programmable gate arrays. performance results include a speedup of 74 × relative to an 8 core multiprocessor system (180× relative to a single processor core). © 2008 ieee."
"we compute an analytical expression for the moment generating function of the joint random vector consisting of a spot price and its discretely monitored average for a large class of square-root price dynamics. this result, combined with the fourier transform pricing method proposed by carr and madan [carr, p., madan d., 1999. option valuation using the fast fourier transform. journal of computational finance 2(4), summer, 61-73] allows us to derive a closed-form formula for the fair value of discretely monitored asian-style options. our analysis encompasses the case of commodity price dynamics displaying mean reversion and jointly fitting a quoted futures curve and the seasonal structure of spot price volatility. four tests are conducted to assess the relative performance of the pricing procedure stemming from our formulae. empirical results based on natural gas data from nymex and corn data from cbot show a remarkable improvement over the main alternative techniques developed for pricing asian-style options within the market standard framework of geometric brownian motion. © 2007 elsevier b.v. all rights reserved."
"this book highlights recent developments in mathematical control theory and its applications to finance. it presents a collection of original contributions by distinguished scholars, addressing a large spectrum of problems and techniques. control theory provides a large set of theoretical and computational tools with applications in a wide range of fields, ranging from ""pure"" areas of mathematics up to applied sciences like finance. stochastic optimal control is a well established and important tool of mathematical finance. other branches of control theory have found comparatively less applications to financial problems, but the exchange of ideas and methods has intensified in recent years. this volume should contribute to establish bridges between these separate fields. the diversity of topics covered as well as the large array of techniques and ideas brought in to obtain the results make this volume a valuable resource for advanced students and researchers. © 2008 springer-verlag berlin heidelberg."
"in [t. coleman, c. he, y. li, calibrating volatility function bounds for an uncertain volatility model, journal of computational finance (2006) (submitted for publication)], an entropy minimization formulation has been proposed to calibrate an uncertain volatility option pricing model (uvm) from market bid and ask prices. to avoid potential infeasibility due to numerical error, a quadratic penalty function approach is applied. in this paper, we show that the solution to the quadratic penalty problem can be obtained by minimizing an objective function which can be evaluated via solving a hamilton-jacobian-bellman (hjb) equation. we prove that the implicit finite difference solution of this hjb equation converges to its viscosity solution. in addition, we provide computational examples illustrating accuracy of calibration. © 2008."
"recent years have witnessed a growing importance of quantitative methods in both financial research and industry. this development requires the use of advanced techniques on a theoretical and applied level, especially when it comes to the quantification of risk and the valuation of modern financial products. applied quantitative finance (2nd edition) provides a comprehensive and state-of-the-art treatment of cutting-edge topics and methods. it provides solutions to and presents theoretical developments in many practical problems such as risk management, pricing of credit derivatives, quantification of volatility and copula modelling. the synthesis of theory and practice supported by computational tools is reflected in the selection of topics as well as in a finely tuned balance of scientific contributions on practical implementation and theoretical concepts. this linkage between theory and practice offers theoreticians insights into considerations of applicability and, vice versa, provides practitioners comfortable access to new techniques in quantitative finance. themes that are dominant in current research and which are presented in this book include among others the valuation of collaterized debt obligations (cdos), the high-frequency analysis of market liquidity, the pricing of bermuda options and realized volatility. all quantlets for the calculation of the given examples are downloadable from the springer web pages. © 2009 springer berlin heidelberg."
"rapid developments of time series models and methods addressing volatility in computational finance and econometrics have been recently reported in the financial literature. the non-linear volatility theory either extends and complements existing time series methodology by introducing more general structures or provides an alternative framework (see abraham and thavaneswaran [b. abraham, a. thavaneswaran, a nonlinear time series model and estimation of missing observations, ann. inst. statist. math. 43 (1991) 493-504] and granger [c.w.j. granger, overview of non-linear time series specification in economics, berkeley nsf-symposia, 1998]). in this work, we consider gaussian first-order linear autoregressive models with time varying volatility. general properties for process mean, variance and kurtosis are derived; examples illustrate the wide range of properties that can appear under the autoregressive assumptions. the results can be used in identifying some volatility models. the kurtosis of the classical rca model of nicholls and quinn [d.f. nicholls, b.g. quinn, random coefficient autoregressive models: an introduction, in: lecture notes in statistics, vol. 11, springer, new york, 1982] is shown to be a special case. © 2008 elsevier ltd. all rights reserved."
"the proceedings contain 32 papers. the special focus in this conference is on scientific computing, computer arithmetic and verified numerical computations. the topics include: on the accuracy of the solution of linear problems on the cell processor; solving overdetermined systems in ?p quasi-norms; staggered correction computations with enhanced accuracy and extremely wide exponent range; definition of the arithmetic operations and comparison relations for an interval arithmetic standard; towards the possibility of objective interval uncertainty in physics; capabilities of constraint programming in safe global optimization; estimating variance under interval and fuzzy uncertainty; block floating point interval alu for digital signal processing; computational aspects of the implementation of disk inversions; higher order methods for the inclusion of multiple zeros of polynomials; an interval newton method based on the bernstein form for bounding the zeros of polynomial systems; solving and certifying the solution of a linear system; application of order-preserving functions to the modeling of computational mechanics problems with uncertainty; from interval arithmetic to interval constraints; a note on a verified automatic integration algorithm; verified solution and propagation of uncertainty in physiological models; verified factorization methods for smp/g/1 queueing systems and their interplay in an integrated problem-solving environment; efficient parallel solvers for large dense systems of linear interval equations; interval methods for solving underdetermined nonlinear equations systems; using preference constraints to solve multi-criteria decision making problems; a hybrid algorithm for global optimization problems; global optimization and singular nonlinear programs; the extrapolated taylor model; solving decidability problems with interval arithmetic; dynamics with a range of choice; applications of fuzzy measures and intervals in finance; detection and reduction of overestimation in guaranteed simulations of hamiltonian systems; robust and optimal control of uncertain dynamical systems with state-dependent switchings using interval arithmetic; tolerable solution set for interval linear systems with constraints on coefficients and on nonnegative interval linear systems and their solution."
"although there are several publications on similar subjects, this book mainly focuses on pricing of options and bridges the gap between mathematical finance and numerical methodologies. the author collects the key contributions of several monographs and selected literature, values and displays their importance, and composes them here to create a work which has its own characteristics in content and style. this invaluable book provides working matlab codes not only to implement the algorithms presented in the text, but also to help readers code their own pricing algorithms in their preferred programming languages. availability of the codes under an internet site is also offered by the author. not only does this book serve as a textbook in related undergraduate or graduate courses, but it can also be used by those who wish to implement or learn pricing algorithms by themselves. the basic methods of option pricing are presented in a self-contained and unified manner, and will hopefully help readers improve their mathematical and computational backgrounds for more advanced topics. © 2009 by imperial college press. all rights reserved."
"in this article, we develop a computational method for an algorithmic process first posed by polyrakis in 1996 in order to check whether a finite collection of linearly independent positive functions in c [a, b] forms a lattice-subspace. lattice-subspaces are closely related to a cost minimization problem in the theory of finance that ensures the minimum-cost insured portfolio and this connection is further investigated here. finally, we propose a computational method in order to solve the minimization problem and to calculate the minimum-cost insured portfolio. all of the numerical work is performed using the matlab high-level language. © 2007 elsevier inc. all rights reserved."
"computational intelligence approaches, such as artificial neural networks and fuzzy systems, have become popular tools in approximating complicated nonlinear systems and time series forecasting. in finance applications, there is evidence that these computational intelligence models are able to provide a more accurate forecast given their capacity for capturing nonlinearities and other stylized facts of financial time series. thus, this paper investigates the hypothesis that the mathematical models of multilayer perception, radial basis function neural networks (nn), and the takagi-sugeno (ts) fuzzy systems are able to provide a more accurate out-of-sample forecast than the traditional autoregressive moving average (arma) and arma generalized autoregressive conditional heteroskedasticity (arma-garch) models. using a series of brazilian exchange rate (r$/us$) returns with 15 minutes, 60 minutes, 120 minutes, daily and weekly basis, the one-step-ahead forecast performance is compared. the results indicate that forecast performance is strongly related to the series'frequency, possibly due to nonlinearities effects. besides, the forecasting evaluation shows that nn models perform better than the arma and arma-garch ones. in the trade strategy based on forecasts, nn models achieved higher returns when compared to a buy-and-hold strategy and to the other models considered in this study."
"this study presents a simple but powerful approximation approach that is both accurate and computationally efficient for valuing basket options on multiple assets with mean-reverting prices. it accomplishes this by solving technical problems involved in reducing the dimensionality of basket options. the approach is readily applicable to multi-factor situations where traditional techniques do not work and contributes to the fields of option pricing, computational finance, and energy industry risk management. numerical examples, including applications to the energy commodity market, illustrate the computational efficiency and accuracy of the approach when compared with results from monte carlo (mc) simulations and extant methods in the literature. © 2006 elsevier ltd. all rights reserved."
"quasi-monte carlo (qmc) methods have been playing an important role for high-dimensional problems in computational finance. several techniques, such as the brownian bridge (bb) and the principal component analysis, are often used in qmc as possible ways to improve the performance of qmc. this paper proposes a new bb construction, which enjoys some interesting properties that appear useful in qmc methods. the basic idea is to choose the new step of a brownian path in a certain criterion such that it maximizes the variance explained by the new variable while holding all previously chosen steps fixed. it turns out that using this new construction, the first few variables are more ""important"" (in the sense of explained variance) than those in the ordinary bb construction, while the cost of the generation is still linear in dimension. we present empirical studies of the proposed algorithm for pricing high-dimensional asian options and american options, and demonstrate the usefulness of the new bb. © 2007 elsevier inc. all rights reserved."
"finance market is a complex adaptive system. agent-based computational finance modeling general methodology and problems relevant to the modeling of finance market from the basic views of complex adaptive systems theory were researched. in the approach of agent-based computational finance modeling, the finance market complex adaptive system was considered as emergent dynamic mechanism networks in which agents mechanism of system interacted ceaselessly with resource mechanism of environment and else agents mechanism, one another assemble and level upon levels construct. and this general methodology is applied to modeling and simulation of stock market, and the purpose is to make agent-based computational finance modeling and simulation become a perfect theory of modeling and simulation, which can instruct the research for finance market complex adaptive systems."
"evaluation of default probability (dp) is one of the keys to finance risk management in modern commercial banks. traditional dp models are easy to be used in practice for their good interpretation and computational cheapness, but easy to produce model bias. on the other hand, modern artificial intelligence models often have a higher prediction accuracy, but drawbacks of lack of explanation capacity, high computational cost and easy to be over fitted. in this article, we introduce a new effective method for evaluation of dp based on generalized additive models. real data analysis and comparison with some other exist methods indicate that these models perform well on discriminant accuracy as well as interpretation and computational effect. from the view of practice, some limits of the models are discussed on behalf of the further theoretical and practical studies."
"recent research in mathematical methods for finance suggests that time series for financial data should be studied with non-stationary models and with structural changes that include both jumps and heteroskedasticity (with jumps in variance). it has been recognized that discriminating between variations caused by the continuous motion of brownian shocks and the genuine discontinuities in the path of the process constitutes a challenge for existing computational procedures. this issue is addressed here, using the product partition model (ppm), for performing such discrimination and the estimation of process jump parameters. computational implementation aspects of ppm applied to the identification of change points in data sequences are discussed. in particular, we analyze the use of a gibbs sampling scheme to compute the estimates and show that there is no significant impact of such use on the quality of the results. the influence of the size of the data sequence on the estimates is also of interest, as well as the efficiency of the ppm to correctly identify atypical observations occurring in close instants of time. extensive monte carlo simulations attest to the effectiveness of the gibbs sampling implementation. an illustrative financial time series example is also presented. © 2006 elsevier ltd. all rights reserved."
"the proceedings contain 32 papers. the special focus in this conference is on scientific computing, computer arithmetic and verified numerical computations. the topics include: on the accuracy of the solution of linear problems on the cell processor; solving overdetermined systems in ℓp quasi-norms; staggered correction computations with enhanced accuracy and extremely wide exponent range; definition of the arithmetic operations and comparison relations for an interval arithmetic standard; towards the possibility of objective interval uncertainty in physics; capabilities of constraint programming in safe global optimization; estimating variance under interval and fuzzy uncertainty; block floating point interval alu for digital signal processing; computational aspects of the implementation of disk inversions; higher order methods for the inclusion of multiple zeros of polynomials; an interval newton method based on the bernstein form for bounding the zeros of polynomial systems; solving and certifying the solution of a linear system; application of order-preserving functions to the modeling of computational mechanics problems with uncertainty; from interval arithmetic to interval constraints; a note on a verified automatic integration algorithm; verified solution and propagation of uncertainty in physiological models; verified factorization methods for smp/g/1 queueing systems and their interplay in an integrated problem-solving environment; efficient parallel solvers for large dense systems of linear interval equations; interval methods for solving underdetermined nonlinear equations systems; using preference constraints to solve multi-criteria decision making problems; a hybrid algorithm for global optimization problems; global optimization and singular nonlinear programs; the extrapolated taylor model; solving decidability problems with interval arithmetic; dynamics with a range of choice; applications of fuzzy measures and intervals in finance; detection and reduction of overestimation in guaranteed simulations of hamiltonian systems; robust and optimal control of uncertain dynamical systems with state-dependent switchings using interval arithmetic; tolerable solution set for interval linear systems with constraints on coefficients and on nonnegative interval linear systems and their solution."
"the proceedings contain 32 papers. the special focus in this conference is on scientific computing, computer arithmetic and verified numerical computations. the topics include: on the accuracy of the solution of linear problems on the cell processor; solving overdetermined systems in ?p quasi-norms; staggered correction computations with enhanced accuracy and extremely wide exponent range; definition of the arithmetic operations and comparison relations for an interval arithmetic standard; towards the possibility of objective interval uncertainty in physics; capabilities of constraint programming in safe global optimization; estimating variance under interval and fuzzy uncertainty; block floating point interval alu for digital signal processing; computational aspects of the implementation of disk inversions; higher order methods for the inclusion of multiple zeros of polynomials; an interval newton method based on the bernstein form for bounding the zeros of polynomial systems; solving and certifying the solution of a linear system; application of order-preserving functions to the modeling of computational mechanics problems with uncertainty; from interval arithmetic to interval constraints; a note on a verified automatic integration algorithm; verified solution and propagation of uncertainty in physiological models; verified factorization methods for smp/g/1 queueing systems and their interplay in an integrated problem-solving environment; efficient parallel solvers for large dense systems of linear interval equations; interval methods for solving underdetermined nonlinear equations systems; using preference constraints to solve multi-criteria decision making problems; a hybrid algorithm for global optimization problems; global optimization and singular nonlinear programs; the extrapolated taylor model; solving decidability problems with interval arithmetic; dynamics with a range of choice; applications of fuzzy measures and intervals in finance; detection and reduction of overestimation in guaranteed simulations of hamiltonian systems; robust and optimal control of uncertain dynamical systems with state-dependent switchings using interval arithmetic; tolerable solution set for interval linear systems with constraints on coefficients and on nonnegative interval linear systems and their solution."
"in this paper, we present an efficient fingerprint classification algorithm which is an essential component in many critical security application systems e.g. systems in the e-government and e-finance domains. fingerprint identification is one of the most important security requirements in homeland security systems such as personnel screening and anti-money laundering. the problem of fingerprint identification involves searching (matching) the fingerprint of a person against each of the fingerprints of all registered persons. to enhance performance and reliability, a common approach is to reduce the search space by firstly classifying the fingerprints and then performing the search in the respective class. jain et al. proposed a fingerprint classification algorithm based on a two-stage classifier, which uses a k-nearest neighbor classifier in its first stage. the fingerprint classification algorithm is based on the fingercode representation which is an encoding of fingerprints that has been demonstrated to be an effective fingerprint biometric scheme because of its ability to capture both local and global details in a fingerprint image. we enhance this approach by improving the efficiency of the k-nearest neighbor classifier for fingercode-based fingerprint classification. our research firstly investigates the various fast search algorithms in vector quantization (vq) and the potential application in fingerprint classification, and then proposes two efficient algorithms based on the pyramid-based search algorithms in vq. experimental results on db1 of fvc 2004 demonstrate that our algorithms can outperform the full search algorithm and the original pyramid-based search algorithms in terms of computational efficiency without sacrificing accuracy. copyright © 2008 the institute of electronics, information and communication engineers."
"the field of automated sentiment analysis has emerged in recent years as an exciting challenge to the computational linguistics community. research in the field investigates how emotion, bias, mood or affect is expressed in language and how this can be recognised and represented automatically. to date, the most successful applications have been in the classification of product reviews and editorials. this paper aims to open a discussion about alternative evaluation methodologies for sentiment analysis systems that broadens the scope of this new field to encompass existing work in other domains such as psychology and to exploit existing resources in diverse domains such as finance or medicine. we outline some interesting avenues for research which investigate the impact of affective text content on the human psyche and on external factors such as stock markets."
"environment: in the model economies, individuals live for an uncertain but finite number of periods. they work between age 25 and 65 and retire afterwards. the first component of their labor market productivity is ability which is permanent and also determines the age-productivity profile. their labor productivity is also hit by idiosyncratic shocks which follow an ability-dependent markov process. the survival probabilities are also functions of ability. markets are assumed to be incomplete, that is agents can only use savings in a risk-free bond to insure against productivity and survival shocks. insurance markets are neither available for earnings uncertainty nor for mortality risk. the production side is modelled in a standard way by a representative firm who operates a cobb-douglas production technology with no aggregate uncertainty. agents' saving decision and labor supply decision will determine the aggregate supply of capital and labor, respectively. there is constant population growth and no productivity growth. fiscal policy is modelled in a great detail. the government is running balanced budget period-by-period and it has to finance government expenditure which is a constant fraction of output over time. it raises revenue from proportional capital, labor and consumption taxation. in addition to this, there is a realistic pay-as-you-go social security system financed by payroll taxes. similarly to the u.s. social security system, pensions depend on average lifetime earnings in a progressive way.11in the pure life-cycle model, the government also collects and redistributes accidental bequests. the dynastic model is imposing a family (dynasty) structure on this environment. households are formed by a parent who is between 55 and 90 years old and 1.52 (because of population growth) offsprings who are between 20 and 55. parents and offsprings discount their own next period utility and their offspring's next period utility at the same rate. moreover, there are no bargaining, commitment or informational frictions within the household. these properties imply that within the household, pooled resources are allocated efficiently in terms of consumption, savings and labor supply. consequently, transitional income shocks are perfectly insured within the household. obviously, there is still considerable income uncertainty at the household level, which is not insurable. this household structure provides some insurance against income shocks but, at the same time, it introduces new dimensions of uncertainty. first, households have different structures: there are complete households, other ones without living parents, and there are also households where offsprings are not alive. finally, although ability is assumed to be correlated across different generations within the same dynasty, still there is considerable uncertainty regarding the earning potential of future offsprings. this model is quite complex as in the benchmark dynastic setup, there are nine state variables:•(potential) age of the offsprings (the age of the parent is determined by the age of the offsprings).•type of the household (parent-children, only children, only parent).•ability levels for both parent and children.•idiosyncratic productivity for both parent and children.•household's asset level.•up-to-date life-time earnings for both parent and children. (they are needed for the determination of future or current pensions payments.). although, apart from the asset level and life-time earnings, the above variables are naturally discrete or approximated by a discrete process, this model imposes a considerable computational burden. on the other hand, it has attractive features which can probably go far beyond the current application: it utilizes the fact that, in the presence of altruism, there are important insurance opportunities across generations within a household. these intergenerational links serve as a source of insurance and can serve as a compensation instrument when policies have different impact across generations. second, this model introduces some new sources of uncertainty individuals face in the real world into the incomplete market heterogenous agent framework. as i will discuss later, these features of the model may raise further empirical or quantitative questions and could be very appealing for further applications. results: the main results of the paper are quantitative in nature. first of all, in the benchmark model, the reform which has the highest long-run welfare gain is the one where all income taxes are eliminated and replaced by consumption taxes.22in table 2, the authors also consider a reform where social security and capital income taxation are eliminated together at the cost of higher consumption taxes. this case seems to have an even higher welfare gain, but the short-run dynamics and political support for this reform is not analyzed. in this case, there is a significant capital accumulation effect (41.5%) which is not accompanied by a significant change in labor supply, hence wages increase significantly. the considerable reduction in the (pre-tax) interest rate and the modest increase in the effective labor tax rate implies that, eventually, the after tax interest rate remains constant while after tax wages increase. all these changes imply that (in consumption equivalent terms) we see a 5% increase in aggregate (equal weighted average) welfare. when transition dynamics is taken into account, the welfare gains are more modest but still positive, they are around 1% on average.33the paper does not provide the exact numbers, but they can be guessed from figs. 4.12 and 4.13. probably more importantly, 75.4% of the population would be in favor of this reform. the tax reform which replace capital income taxation with higher labor income taxes has a more modest long-run welfare gain (3.5%) and would not gain public support in this environment. we would draw significantly different conclusion about the desirability of the tax reforms if we used the pure life-cycle framework. first of all, from the long-run perspective, the elimination of all income taxes is again the reform which results in the highest welfare gains. however, this reform is detrimental from the welfare point of view when the transition is taken into account. according to fig. 4.14, the average welfare loss maybe of a magnitude of 10-12% in consumption equivalent terms. hence, not surprisingly, this plan would gain only 9% support in the population. on the other hand, the elimination of capital taxes accompanied by a rise in labor taxes, is reducing welfare in the long run (by 0.4%), but would gain a popular support (63.5%) and would have a significant (around 3.5%) average welfare gains from the initial population's point of view (see fig. 4.6) in the short run. © 2007 elsevier b.v. all rights reserved."
"there is widespread acceptance that much of the developed world faces a potential pensions and welfare crisis as a result of declining birth rates and an ageing population. however, there is considerable uncertainty about the specifics of demographic forecasting and this has significant implications for public finances. uncertain demographics and fiscal sustainability addresses the economic consequences of uncertainty and, with particular reference to european economies, explores the impact of demographic risks on public finances, including pension systems, health care and old-age care expenditures. covering a spectrum of theoretical and empirical approaches, different types of computational models are used to demonstrate not only the magnitudes of the uncertainties involved but also how these can be addressed through policy initiatives. the book is divided into four parts covering demographic, measurement, policy and methodological issues. each part is followed by a discussion essay that draws out key elements and identifies common themes. © cambridge university press 2008 and cambridge university press, 2009."
"this paper describes a real-valued quantum-inspired evolutionary algorithm (qiea), a new computational approach which bears similarity with estimation of distribution algorithms (edas). the study assesses the performance of the qiea on a series of benchmark problems and compares the results with those from a canonical genetic algorithm. furthermore, we apply qiea to a finance problem, namely non-linear principal component analysis of implied volatilities. the results from the algorithm are shown to be robust and they suggest potential for useful application of the qiea to high-dimensional optimization problems in finance. © 2008 springer-verlag berlin heidelberg."
"this book is structured around the main theories and models used by practitioners to engineer finance and investment tools. the methods developed and implemented in the text are organized as chapters which cover the core areas. each chapter is largely self-contained, thus the practitioner or student can conveniently focus on a defined tool and have immediate access to an implemented solution. those engaged in the design and evaluation of new products will find the quick access to a wide range of robust core methods invaluable in constructing bespoke implementations. with the pervasiveness and rapid pace of advancement in web based technologies, it is now a given that any commercially useful computational tool is able to make an effective and efficient use of web based platforms. java is the language of choice for developing highly efficient web based applications. all of the methods in this book are written in java and use is made of the fully optimised java collections for data manipulation. for practitioners and students alike who are still working with legacy c++ or visual basic implementations, this book will serve as an excellent reference for translating or porting their applications into a web centric environment. phil barker followed an academic career for many years , holding a lectureship in computer science at heriot-watt university, edinburgh where he led undergraduate degree and postgraduate masters courses in computer science and accountancy. he has a considerable depth of commercial experience having directed research, technical and operations functions within technology based companies serving blue chips in finance, investment and banking. he is currently ceo of bwa technologies ltd a company specializing in the development of investment and risk management software. © springer-verlag london limited 2007."
"text is not unadulterated fact. a text can make you laugh or cry but can it also make you short sell your stocks in company a and buy up options in company b? research in the domain of finance strongly suggests that it can. studies have shown that both the informational and affective aspects of news text affect the markets in profound ways, impacting on volumes of trades, stock prices, volatility and even future firm earnings. this paper aims to explore a computable metric of positive or negative polarity in financial news text which is consistent with human judgments and can be used in a quantitative analysis of news sentiment impact on financial markets. results from a preliminary evaluation are presented and discussed. © 2007 association for computational linguistics."
"monte carlo simulation of weak approximations of stochastic differential equations constitutes an intensive computational task. in applications such as finance, for instance, to achieve ""real time"" execution, as often required, one needs highly efficient implementations of the multi-point distributed random number generator underlying the simulations. in this paper, a fast and flexible dedicated hardware solution on a field programmable gate array is presented. a comparative performance analysis between a software-only and the proposed hardware solution demonstrates that the hardware solution is bottleneck-free, retains the flexibility of the software solution and significantly increases the computational efficiency. moreover, simulations in applications such as economics, insurance, physics, population dynamics, epidemiology, structural mechanics, chemistry and biotechnology can benefit from the obtained speedups. © 2007 imacs."
"we present the solution that hypo real estate bank international (hypo) implemented to perform monte carlo simulations of its commercial real estate credit risk. the solution uses risk integrated's proprietary software, the specialized finance system, which is supported by another risk integrated technology, the enterprise spreadsheet platform. the platform embeds individual master spreadsheets within a high-performance, server-based, computational-engine architecture; thus, it enables them to be accessed enterprise-wide. the major benefit of this approach is the near-elimination of spreadsheet risk in hypo's banking system. the solution allows domain experts to have the flexible programming power that spreadsheets provide; however, it does not sacrifice the reliability and auditability expected from traditional business applications. © 2008 informs."
"genetic programming (gp) is an automated computational programming methodology, inspired by the workings of natural evolution techniques. it has been applied to solve complex problems in multiple domains including finance. this paper reviewed its applications in financial markets. financial time series prediction and technical trading are the most attractive things in financial markets as driven by the profits. this paper analyzes gp's potential utility in these two areas and reviews its applications cross different financial markets. the future research directions of gp in financial markets have been highlighted."
"the proceedings contain 32 papers. the special focus in this conference is on scientific computing, computer arithmetic and verified numerical computations. the topics include: on the accuracy of the solution of linear problems on the cell processor; solving overdetermined systems in ?p quasi-norms; staggered correction computations with enhanced accuracy and extremely wide exponent range; definition of the arithmetic operations and comparison relations for an interval arithmetic standard; towards the possibility of objective interval uncertainty in physics; capabilities of constraint programming in safe global optimization; estimating variance under interval and fuzzy uncertainty; block floating point interval alu for digital signal processing; computational aspects of the implementation of disk inversions; higher order methods for the inclusion of multiple zeros of polynomials; an interval newton method based on the bernstein form for bounding the zeros of polynomial systems; solving and certifying the solution of a linear system; application of order-preserving functions to the modeling of computational mechanics problems with uncertainty; from interval arithmetic to interval constraints; a note on a verified automatic integration algorithm; verified solution and propagation of uncertainty in physiological models; verified factorization methods for smp/g/1 queueing systems and their interplay in an integrated problem-solving environment; efficient parallel solvers for large dense systems of linear interval equations; interval methods for solving underdetermined nonlinear equations systems; using preference constraints to solve multi-criteria decision making problems; a hybrid algorithm for global optimization problems; global optimization and singular nonlinear programs; the extrapolated taylor model; solving decidability problems with interval arithmetic; dynamics with a range of choice; applications of fuzzy measures and intervals in finance; detection and reduction of overestimation in guaranteed simulations of hamiltonian systems; robust and optimal control of uncertain dynamical systems with state-dependent switchings using interval arithmetic; tolerable solution set for interval linear systems with constraints on coefficients and on nonnegative interval linear systems and their solution."
"computational intelligence (ci), as an alternative to statistical and econometric approaches, has been applied to a wide range of economics and finance problems in recent years, for example to price forecasting and market efficiency. this book contains research ranging from applications in financial markets and business administration to various economics problems. not only are empirical studies utilizing various ci algorithms presented, but so also are theoretical models based on computational methods. in addition to direct applications of computational intelligence, readers can also observe how these methods are combined with conventional analytical methods such as statistical and econometric models to yield preferred results. chen, wang, and kuo have grouped the 12 contributions following their introductory chapter into applications of fuzzy logic, neural networks (including self-organizing maps and support vector machines), and evolutionary computation. all chapters were selected either by invitation or based on a careful selection and extension of best papers from the international workshop on computational intelligence in economics and finance in 2005. overall, the book offers researchers an excellent overview of current advances and applications of computational intelligence techniques to economics and finance problems. © springer-verlag berlin heidelberg 2007. all rights are reserved."
"we consider an n-dimensional square root process and we obtain a formula involving series expansions for the associated transition probability density. the process mentioned previously can be used to model forward rates, future prices, forward prices and, as a consequence, can be used to price derivatives on these underlyings. the formula that we propose for the transition probability density has been obtained using appropriately a perturbative expansion in the correlation coefficients of the square root process, the fourier transform and the method of characteristics to solve first-order hyperbolic partial differential equations. the computational effort needed to evaluate this formula is polynomial with respect to the dimension n of the space spanned by the square root process when the order where all the series involved in the transition probability density formula are truncated is fixed. this strategy gives an accuracy that some numerical tests show approximately constant for a wide range of values of n. some examples of prices of financial derivatives whose evaluation involves integrals in two, twenty and one hundred dimensions (i.e. n = 2, 20, 100), that is derivatives on two, twenty and one hundred assets, where accurate results can be obtained are shown. an experiment shows that the formula derived here for the transition probability density is well suited for parallel computing. this feature makes the formula computationally very attractive to price derivatives of the libor market such as caplets or swaptions since the use of parallel computing and the formula makes it possible to evaluate derivatives on several tens of underlyings in negligible times. the website http://www.econ.univpm.it/recchioni/finance/w1 contains an interactive tool that helps with the understanding of this paper and a portable software library that makes it possible to the user to exploit the formula derived in this paper to evaluate the transition probability densities of its own models and the prices of the associated financial derivatives. © 2007 elsevier ltd. all rights reserved."
"in this paper we describe parallelization of interior-point method (ipm) aimed at achieving high scalability on large-scale chip-multiprocessors (cmps). ipm is an important computational technique used to solve optimization problems in many areas of science, engineering and finance. ipm spends most of its computation time in a few sparse linear algebra kernels. while each of these kernels contains a large amount of parallelism, sparse irregular datasets seen in many optimization problems make parallelism difficult to exploit. as a result, most researchers have shown only a relatively low scalability of 4x-12x on medium to large scale parallel machines. this paper proposes and evaluates several algorithmic and hardware features to improve ipm parallel performance on large-scale cmps. through detailed simulations, we demonstrate how exploring multiple levels of parallelism with hardware support for low overhead task queues and parallel reduction enables ipm to achieve up to 48x parallel speedup on a 64-core cmp. (c) 2007 acm."
"quasi-monte carlo (qmc) methods have been successfully used to compute high-dimensional integrals arising in many applications, especially in finance. to understand the success and the potential limitation of qmc, this paper focuses on quality measures of point sets in high dimensions. we introduce the order-ℓ, superposition and truncation discrepancies, which measure the quality of selected projections of a point set on lower-dimensional spaces. these measures are more informative than the classical ones. we study their relationships with the integration errors and study the tractability issues. we present efficient algorithms to compute these discrepancies and perform computational investigations to compare the performance of the sobol' nets with that of the sets of latin hypercube sampling and random points. numerical results show that in high dimensions the superiority of the sobol' nets mainly derives from the one-dimensional projections and the projections associated with the earlier dimensions; for order-2 and higher-order projections all these point sets have similar behavior (on the average). in weighted cases with fast decaying weights, the sobol' nets have a better performance than the other two point sets. the investigation enables us to better understand the properties of qmc and throws new light on when and why qmc can have a better (or no better) performance than monte carlo for multivariate integration in high dimensions. © 2007 elsevier b.v. all rights reserved."
"in this paper we estimate and analyze the errors associated with the use of the discrete (fast) fourier transformation for the numerical calculation of convolutions. we suggest and compare methods to reduce these errors without loosing the computational efficiency of the calculation scheme. a typical field of application of our findings is the calculation of aggregate loss distributions for, e.g., losses from insurance cases or operational risk losses in the finance industry. © 2008, institute of mathematics, nas of belarus. all rights reserved."
"currently, there is a renewed interest in the use of optimal experimentation (adaptive control) in economics. example are found in [amman, h. m. & kendrick, d. a. (1999). should macroeconomic policy makers consider parameter covariances. computational economics 14, 263-267; amman, h. m. & kendrick, d. a. (2003). mitigation of the lucas critique with stochastic control methods. journal of economic dynamics and control 27, 2035-2057; cosimano, t. f. optimal experimentation and the perturbation method in the neighborhood of the augmented linear regulator problem. journal of economics, dynamics and control (in press); cosimano, t. f., & gapen, m. t. (2005b). recursive methods of dynamic linear economics and optimal experimentation using the perturbation method, working paper. notre dame, indiana, usa: department of finance, university of notre dame; cosimano, t. f., & gapen, m. t. (2005a). program notes for optimal experimentation and the perturbation method in the neighborhood of the augmented linear regulator problem, working paper. notre dame, indiana, usa: department of finance, university of notre dame; cosimano, t. f., & gapen, m. t. (2006). an algorithm for approximating optimal experimentation problems using the perturbation method, working paper. notre dame, indiana, usa: department of finance, university of notre dame; tesfaselassie, m. f., schaling, e., & eijffinger, s. (2007). learning about the term structure and optimal rules for inflation targeting, working paper. tilburg, the netherlands: tilburg university; tucci, m. p. (1997). adaptive control in the presence of time-varying parameters. journal of economic dynamics and control, 22, 39-47; wieland, v. (2000a). learning by doing and the value of optimal experimentation. journal of economic dynamics and control, 24, 501-543; wieland, v., (2000b). monetary policy, parameter uncertainty and optimal learning. journal of monetary economics, 46, 199-228]. in this paper we present the beck & wieland model [beck, g., & wieland, v. (2002). learning and control in a changing economic environment. journal of economic dynamics and control, 26, 1359-1378] and the methodology to solve this model with time-varying parameters using the various control methods described in [kendrick, d. a. (1981). stochastic control for economic models (1st ed.), new york, ny, usa: mcgraw-hill book company; kendrick, d. a. (2002). stochastic control for economic models (2nd ed.) available at url: http://www.eco.utexas.edu/faculty/kendrick]. furthermore, we also provide numerical results using the dualpc software [amman, h. m., & kendrick, d. a. (1999). the duali/dualpc software for optimal control models: user's guide. working paper, austin, tx 78712, usa: center for applied research in economics, university of texas] and show first evidence that optimal experimentation or dual control may produce better results than expected optimal feedback. © 2008 elsevier ltd. all rights reserved."
"the computer unified device architecture (cuda) developed by nvidia provides financial and other multivariate analysis for gpu to addresses the challenges faced in parallel computing. nvidia released a cuda software developers kit in 2007 to support development of algorithms using c language to run both gpu and cpu functions on the company's processors. cuda is used in commercial applications related to oil and gas exploration, computational finance, and other computational modeling projects. nvidia is also working to make cuda more open by providing code downloads and by producing a tool set built under the open64 open research compiler. cuda parallelization does a better job at linear scaling than people handcoding for multicore, and enable load balancing across the two types of processors."
"we use panel data to estimate nonlinear euler equations for preferences that are nonseparable in consumption and leisure. this approach departs from existing panel data studies that investigate linearizations and/or separable preferences. intuitively plausible estimates are obtained only when excluding nonassetholders from the sample, which indicates the importance of asset market participation. for market participants, estimated parameter values are intuitively appealing, but differ from existing estimates. they also differ from parameter values commonly used in computational experiments. these findings have implications for the extensive literature in macroeconomics and finance that studies models of intertemporal decision-making, and they confirm the importance of market incompleteness. © 2006 elsevier b.v. all rights reserved."
"over the last few decades, fuzzy logic has been shown as a powerful methodology for dealing with imprecision and nonlinearity efficiently. applications can be found in a wide context ranging from medicine to finance, from human factors to consumer products, from vehicle control to computational linguistics, and so on (wang 1997; dubois and prade 2000; passino and yurkovich 1998; jang et al. 1997; sugeno 1985; pedrycz 1993). however, one of the shortcomings of fuzzy logic is the lack of systematic design. to circumvent this problem, fuzzy logic is usually combined with neural networks (nns) by virtue of the learning capability of nns. nns are networks of highly interconnected neural computing elements that have the ability of responding to input stimuli and learning to adapt to the environment. both fuzzy systems and nns are dynamic and parallel processing systems that estimate input-output functions (mitra and hayashi 2000). the merits of both fuzzy and neural systems can be integrated in fuzzy neural networks (fnns) (lee and lee 1974, 1975; pal and mitra 1999; zanchettin and ludermir 2003). therefore, the integration of fuzzy and neural systems leads to a symbiotic relationship in which fuzzy systems provide a powerful framework for expert knowledge representation, while nns provide learning capabilities. © 2008 springer-verlag berlin heidelberg."
"credit derivatives have enjoyed explosive growth in the last decade, particularly synthetic collateralized debt obligations (synthetic cdos). this modern book describes the state-of-the-art in quantitative and computational modeling of cdos. beginning with an overview of the structured finance landscape, readers are introduced to the basic modeling concepts necessary to model and value simple credit derivatives. the modeling, valuation and risk management of synthetic cdos are described and a detailed picture of the behavior of these complex instruments is built up. the final chapters introduce more advanced topics such as portfolio management of synthetic cdos and hedging techniques. detailing the latest models and techniques, this is essential reading for quantitative analysts, traders and risk managers working in investment banks, hedge funds and other financial institutions, and for graduates intending to enter the industry. it is also ideal for academics who need be informed with the best current practice in the credit derivatives industry. © c. c. mounfield 2009."
"in many applications, differential equation models require geometric integration, i.e., the application of structure-preserving integration schemes. in computational finance, for example, the numerical simulation of extended libor market models used to value structured interest rate derivatives has to preserve positivity or boundedness of the underlying stochastic processes used to model mean-reverting volatility or forward rates. this paper discusses how stochastic integration schemes can be constructed in order to maintain these properties of the analytical solution. milstein-type methods prove to be the method-of-choice with respect to both efficiency and preservation of structural properties, as they turn out to dominate the increments of brownian motions. these theoretical results are confirmed by numerical tests. © 2006 imacs."
"information technology has been proved to be a strategic weapon in the business armory for the creation and sustention of competitive advantage, especially, when it is aligned with the needs of the internal and external environment. solutions are provided from the operational level up to strategic planning and are capable to support every choice in the strategy portfolio, from cost to quality and flexibility. it systems in the manufacturing and operational level were analyzed extensively in literature: erp systems, computer aided design/computer aided manufacturing (cad/cam), and so forth. according to wong, bo, bodnovich, and selvi (1997), 53.5% of the reviewed literature in artificial intelligence refers to applications in production and operations management. nevertheless, the second most important area for advanced it applications is that of finance (25.4%). this research will be focused on the common set of the two previously mentioned areas: production management and the necessary financial tools. production and operation management requires specific financial tools in order to accomplish the functions of production planning, costing, investment appraisal, and so forth. computational intelligence in those financial functions is mostly needed for the production operation department and for the production operation strategy. specifically, the weight will be put on information technology automation of financial functions adopted by production departments: forecasting production needs, production planning and control, profit volume analysis, cost analysis, investment appraisal analysis, and so forth. an attempt will be made to classify the various quantitative and qualitative techniques in relation to various fi nancial aspects. specifically, advances of neural networks, expert systems, advanced statistical analysis and operational research methods, and various hybrid techniques will be presented in relation to financial models applied in production. financial applications will be analyzed according to their modules and their outputs in a strategic alignment concept. finally, a strategic alignment model will be derived for the adoption of financial applications in businesses. © 2008, igi global."
"the first cutting-edge guide to using the sas® system for the analysis of econometric data applied econometrics using the sas® system is the first book of its kind to treat the analysis of basic econometric data using sas®, one of the most commonly used software tools among today's statisticians in business and industry. this book thoroughly examines econometric methods and discusses how data collected in economic studies can easily be analyzed using the sas® system. in addition to addressing the computational aspects of econometric data analysis, the author provides a statistical foundation by introducing the underlying theory behind each method before delving into the related sas® routines. the book begins with a basic introduction to econometrics and the relationship between classical regression analysis models and econometric models. subsequent chapters balance essential concepts with sas® tools and cover key topics such as: regression analysis using proc iml and proc reg hypothesis testing instrumental variables analysis, with a discussion of measurement errors, the assumptions incorporated into the analysis, and specification tests heteroscedasticity, including gls and fgls estimation, group-wise heteroscedasticity, and garch models panel data analysis discrete choice models, along with coverage of binary choice models and poisson regression duration analysis models assuming only a working knowledge of sas®, this book is a one-stop reference for using the software to analyze econometric data. additional features include complete sas® code, proc iml routines plus a tutorial on proc iml, and an appendix with additional programs and data sets. applied econometrics using the sas® system serves as a relevant and valuable reference for practitioners in the fields of business, economics, and finance. in addition, most students of econometrics are taught using gauss and stata, yet sas® is the standard in the working world; therefore, this book is an ideal supplement for upper-undergraduate and graduate courses in statistics, economics, and other social sciences since it prepares readers for real-world careers. © 2009 john wiley & sons, inc.."
"let us suppose that the dynamics of the stock prices and of their stochastic variance is described by the heston model, that is by a system of two stochastic differential equations with a suitable initial condition. the aim of this paper is to estimate the parameters of the heston model and one component of the initial condition, that is the initial stochastic variance, from the knowledge of the stock and option prices observed at discrete times. the option prices considered refer to an european call on the stock whose prices are described by the heston model. the method proposed to solve this problem is based on a filtering technique to construct a likelihood function and on the maximization of the likelihood function obtained. the estimated parameters and initial value component are characterized as being a maximizer of the likelihood function subject to some constraints. the solution of the filtering problem, used to construct the likelihood function, is based on an integral representation of the fundamental solution of the fokker-planck equation associated to the heston model, on the use of the wavelet expansions presented in (fatone et al. in high performance algorithms based on a new wavelet expansion for time dependent acoustic obstacle scattering. commun. computat. phys. (2007), research developments in acoustics, vol. 2, pp. 39-69. transworld research network, kerala (2005), new wavelet bases made of piecewise polynomial functions: approximation theory, quadrature rules and applications to kernel sparsification and image compression. siam j. sci. comput. (submitted)) to approximate the integral kernel appearing in the representation formula of the fundamental solution, on a simple truncation procedure to exploit the sparsifying properties of the wavelet expansions and on the use of the fast fourier transform (fft). the use of these techniques generates a very efficient and fully parallelizable numerical procedure to solve the filtering problem, this last fact makes possible to evaluate very efficiently the likelihood function and its gradient. as a byproduct of the solution of the filtering problem we have developed a stochastic variance tracking technique that gives very good results in numerical experiments. the maximum likelihood problem used in the estimation procedure is a low dimensional constrained optimization problem, its solution with ad hoc techniques is justified by the computational cost of evaluating the likelihood function and its gradient. we use parallel computing and a variable metric steepest ascent method to solve the maximum likelihood problem. some numerical examples of the estimation problem using synthetic and real data, that is data relative to an index of the milano stock exchange (s&pmib30), obtained with a parallel implementation of the previous numerical method are presented. very impressive speed up factors are obtained in the numerical examples using the parallel implementation of the numerical method proposed. the website: http://www.econ.univpm.it/pacelli/mariani/finance/ w1 contains animations and some auxiliary material that helps the understanding of this paper and makes available to the interested users the computer programs used to produce the numerical experience presented. © 2007 springer-verlag."
"natural computing can be broadly defined as the development of computer programs and computational algorithms using metaphorical inspiration from systems and phenomena that occur in the natural world. the inspiration for natural computing methodologies typically stem from real-world phenomena which exist in high-dimensional, noisy and uncertain, dynamic environments. these are characteristics which fit well with the nature of financial markets. prima facie, this makes natural computing methods interesting for financial modelling applications. another feature of natural environments is the phenomenon of emergence, or the activities of multiple individual agents combining to create their own environment. this book contains fourteen chapters which illustrate the cutting-edge of natural computing and agent-based modelling in modern computational finance. a range of methods are employed including, differential evolution, genetic algorithms, evolution strategies, quantum-inspired evolutionary algorithms, bacterial foraging algorithms, genetic programming, agent-based modelling and hybrid approaches including fuzzy-evolutionary algorithms, radial-basis function networks with kalman filters, and a multi-layer perceptron-wavelet hybrid. a complementary range of applications are addressed including fund allocation, asset pricing, market prediction, market trading, bankruptcy prediction, and the agent based modelling of payment card and financial markets. the book is divided into three sections each corresponding to a distinct grouping of chapters. the first section deals with optimisation applications of natural computing in finance, the second section explores the use of natural computing methodologies for model induction and the final section illustrates a range of agent-based applications in finance. © 2008 springer-verlag berlin heidelberg."
"this book is very easy to read and one can gain a quick snapshot of computational issues arising in financial mathematics. researchers or students of the mathematical sciences with an interest in finance will find this book a very helpful and gentle guide to the world of financial engineering. siam review (46, 2004). the third edition is thoroughly revised and significantly extended. the largest addition is a new section on analytic methods with main focus on interpolation approach and quadratic approximation. new sections and subsections are among others devoted to risk-neutrality, early-exercise curves, multidimensional black-scholes models, the integral representation of options and the derivation of the black-scholes equation. new figures, more exercises, more background material make this guide to the world of financial engineering a real must-to-have for everyone working in fe. © springer-verlag berlin heidelberg 2006."
"the valuation of many financial derivatives leads to high-dimensional integrals. the constructions of robust or universal good lattice rules for financial applications are both important and challenging. an important common feature of the integrands in computational finance is that they can often be well approximated by a sum of low-dimensional functions, i.e., functions that depend on only a small number of variables (usually just two variables). for numerical integration of such functions the quality of the low-order (i.e., low-dimensional) projections of the node set is crucial. in this paper we propose methods to construct good lattice points with ""optimal"" low-order projections. the quality of a point set is measured by a new measure called elementary order- discrepancy, which measures the quality of all order- projections and is more informative than usual measures. two constructions, namely the korobov and the component-by-component constructions, are studied such that the low-order projections are optimized. numerical experiments demonstrate that even in high dimensions it is possible to construct new good lattice points with order-2 projections that are better than those of the sobol' points and random points and with higher-order projections that are no worse (while the sobol' points lost the advantage over random points in order-2 projections on the average). the new lattice rules have the potential to improve upon the accuracy for favorable functions, while doing no harm for unfavorable ones. their applications for pricing path-dependent options and american options (based on the least-square monte carlo method) are studied and their high efficiency is demonstrated. a nice surprise revealed is the robustness property of such lattice rules: the good projection property and the suitability for a large range of problems. the potential possibility and limitations of good lattice points in achieving good quality of moderateand high-order projections is investigated. the reason why classical lattice rules may not be efficient for high-dimensional finance problems is also discussed. © 2007 society for industrial and applied mathematics."
"the quantum finance pricing formulas for coupon bond options and swaptions derived by baaquie [phys. rev. e 75, 016703 (2006)] are reviewed. we empirically study the swaption market and propose an efficient computational procedure for analyzing the data. empirical results of the swaption price, volatility, and swaption correlation are compared with the predictions of quantum finance. the quantum finance model generates the market swaption price to over 90% accuracy. © 2007 the american physical society."
a major challenge in computational finance is the pricing of options that depend on a large number of risk factors. prominent examples are basket or index options where dozens or even hundreds of stocks constitute the underlying asset and determine the dimensionality of the corresponding degenerate parabolic equation. the objective of this article is to show how an efficient discretization can be achieved by hierarchical approximation as well as asymptotic expansions of the underlying continuous problem. the relation to a number of state-of-the-art methods is highlighted. © 2007 society for industrial and applied mathematics.
"genetic complementary learning (gcl) is a biological brain-inspired learning system based on human pattern recognition, and genes selection process. it is a confluence of the hippocampal complementary learning and the evolutionary genetic algorithm. with genetic algorithm providing the possibility of optimal solution, and complementary learning providing the efficient pattern recognition, gcl may offer superior performance. in contrast to other computational finance tools such as neural network and statistical methods, gcl provides greater interpretability and it does not rely on the assumption of the underlying data distribution. it is an evolving and autonomous system that avoids the time-consuming process of manual rule construction or modeling. this is highly favorable especially in financial world where data is ever changing, and requires frequent update. the feasibility of gcl as stock market predictor, and bank failure early warning system is investigated. the experimental results show that gcl is a competent computational finance tools for stock market prediction and bank failure early warning system. © 2007 blackwell publishing, inc."
"we apply exploratory data analysis to some of the basic models of neoclassical computational finance. these include the portfolio selection algorithm of markowitz, the capital market line of sharpe, and the option pricing model of black-scholes-merton. we demonstrate that the markowitzian assumption of positive correlation of expected return and volatility is not supported by the data. the notion that an index fund based on market cap weighting is optimal is also shown to be inconsistent with market data. it is noted that the option pricing model of black-scholes-merton is not supported by market history. the simugram™, an empirical data-based paradigm for portfolio selection, is discussed. it is observed that some of the basic contemporary strategies of neoclassical computational finance may be seriously flawed and might profitably be replaced by data-based rules. we conclude that several nobel prizes in economics have been awarded for nonsense. © 2006 m.e. sharpe, inc."
this chapter surveys research on agent-based models used in finance. it will concentrate on models where the use of computational tools is critical for the process of crafting models which give insights into the importance and dynamics of investor heterogeneity in many financial settings. © 2006 elsevier b.v. all rights reserved.
"intelligent information technology (iit) that focuses on web intelligence (wi) has proved to be effective for intelligent information processing and solving the complex real world problems. the applications of wi includes e-finance, e-science, and e-service, and it represents a significant benefit in it development. enormous number of new data records that are generated every second are required to be summarized and synthesized to support problem solving and decision making in organizations, such as business, science, government, and university organizations. the main feature of the iit is the ability to combine artificial intelligence (ai), computational intelligence, wi, and intelligent agents in the design and implementation of intelligent web-based information systems. the requirements supported by wi technology include information empowerment, knowledge sharing, virtual social communities, service enrichment, and practical wisdom development."
"many large scale autonomous systems based on a large number of interacting agents in a structured physical environment have emerged in diverse areas such as biology, ecology or finance. inspired by the desire to better understand and make the best out of such systems, we model them in order to gain insight, predict the future and control it partially if not fully. in this paper, we present a stochastic approach to modelling such systems based on g-networks. we propose two methods which deal with cases where complete or incomplete world knowledge is available. we use strategic military planning in urban scenarios as an example to demonstrate our approach. our results suggest that this approach tackles the problem of modelling autonomous systems at low computational cost. apart from offering numerical estimates of various outcomes, the approach helps us identify the parameters or characteristics that have the greatest impact on the system most and allows us to compare alternative strategies."
"computational requirements for solving models of financial derivatives, for example, the option pricing problems, are huge and demand efficient algorithms and high performance computing capabilities. this demand has been rekindled by the recent developments in the mobile technology making wireless trading a possibility. in this paper, we focus on the development of a monte-carlo algorithm on a modern multicore chip architecture, cyclops-64 (c64) under development at ibm as the experimental platform for our study in pricing options. the timing results on c64 show that various sets of simulations could be done in a real-time fashion while yielding high performance/price improvement over traditional microprocessors for finance applications. © springer-verlag 2006."
"this chapter reviews fundamental concepts and results in the area of online algorithms. we first address classical online problems and then study various applications of current interest. online algorithms represent a theoretical framework for studying problems in interactive computing. they model, in particular, that the input in an interactive system does not arrive as a batch but as a sequence of input portions and that the system must react in response to each incoming portion. moreover, they take into account that at any point in time future inputis unknown. as the name suggests, online algorithms consider the algorithmic aspects of interactive systems: we wish to design strategies that always compute good output and keep a given system in good state. no assumptions are made about the input stream. the input can even be generated by an adversary that creates new input portions based on the system's reactions to previous ones. we seek algorithms that have a provably good performance. formally, an online algorithm receives a sequence of requests σ = σ(1), . . . , σ(m). these requests must be served in the order of occurrence. when serving request σ(t), an online algorithm does not know requests σ(t) with t> t. serving requests incurs cost and the goal is to minimize the total cost paid on the entire request sequence. this process can be viewed as a request answer game. an adversary generates requests and an online algorithm has to serve them one at a time. the performance of online algorithms is usually evaluated using competitive analysis [65]. here an online algorithm alg is compared to an optimal offline algorithm opt that knows the entire request sequence σ in advance and can serve it with minimum cost. given a sequence σ, let alg(σ) and opt(σ) denote the costs incurred by alg and opt, respectively. algorithm alg is called c-competitive if there exists a constant b such that alg(σ) ≤ c ' opt(σ) + b, for all sequences σ. the constant b must be independent of the input σ. we note that competitive analysis is a strong worst-case performance measure.over the past 15 years online algorithms have received tremendous research interest. online problems have been studied in many application areas including resource management in operating systems, data structuring, scheduling, networks, and computational finance. in the following sections we first survey fundamental results. we address the paging problem, selforganizing lists, the k-server problem as well as metrical task systems. then we review a number of new results in application areas of current interest. we focus on algorithmic problems in large networks and competitive auctions. finally we present refinements of competitive analysis and conclude with some remarks. © 2006 springer-verlag berlin heidelberg."
"utility based indifference pricing and hedging are now considered to be an economically natural method for valuing contingent claims in incomplete markets. however, acceptance of these concepts by the wide financial community has been hampered by the computational and conceptual difficulty of the approach. this paper focuses on the problem of computing indifference prices for derivative securities in a class of incomplete stochastic volatility models general enough to include important examples. a rigorous development is presented based on identifying the natural martingales in the model, leading to a nonlinear feynman-kac representation for the indifference price of contingent claims on volatility. to illustrate the power of this representation, closed form solutions are given for the indifference price of a variance swap in the standard heston model and in a new ""reciprocal heston"" model. these are the first known explicit formulas for the indifference price for a class of derivatives that is important to the finance industry. © 2007 taylor & francis."
"this paper is an editorial guide for the second special issue on computational intelligence in economics and finance, which is a continuation of the special issue of information sciences, vol. 170, no. 1. this second issue appears as a part of the outcome from the 3rd international workshop on computational intelligence in economics and finance, which was held in cary, north carolina, september 26-30, 2003. this paper offers some main highlights of this event, with a particular emphasis on some of the observed progress made in this research field, and a brief introduction to the papers included in this special issue. © 2006 elsevier inc. all rights reserved."
"finance is a fast growing field in business and is among the fastest growing in scientific computing, helping to sustain economies that include those of new york city and of the united states. the dynamics of finance have enticed computer scientists, engineers, mathematicians, and physicists. this has helped in the growth of interdisciplinary fields that involve computational finance, financial computing, financial engineering, mathematical finance, and quantitative finance. while most of these interdisciplinary programs are introduced to graduate students at universities, few of them are introduced to the undergraduate students. the frequent model that includes a computer science minor and a financial major requires a finance student to be in a general computer science minor that is open to all students who satisfy the minimum requirements for the minor. this interdisciplinary model does not serve sufficiently the needs of industry and of society. the study introduces an interdisciplinary major/minor curriculum model that seamlessly integrates computer science into finance through free elective credits. the model is that of financial computing that is both discipline and industry oriented in the university. the paper of the study evaluates the financial computing model, indicating how it conforms to the needs of financial firms in industry and of society and that of the international basel ii capital accord. this study will benefit educators and researchers in integrating a special and timely curriculum model helpful to the financial services industry."
"multivariate time series (mts) data are widely available in different fields including medicine, finance, bioinformatics, science and engineering. modelling mts data accurately is important for many decision making activities. one area that has been largely overlooked so far is the particular type of time series where the data set consists of a large number of variables but with a small number of observations. in this paper we describe the development of a novel computational method based on natural computation and sparse matrices that bypasses the size restrictions of traditional statistical mts methods, makes no distribution assumptions, and also locates the associated parameters. extensive results are presented, where the proposed method is compared with both traditional statistical and heuristic search techniques and evaluated on a number of criteria. the results have implications for a wide range of applications involving the learning of short mts models. © springer science+business media, inc. 2006."
"in this article we develop a computational method for an algorithmic process first posed by abramovich-aliprantis-polyrakis in 1994 in order to check whether a finite collection of linearly independent positive vectors in rm forms a lattice-subspace. lattice-subspaces are closely related to a cost minimization problem in the theory of finance that ensures the minimum-cost insured portfolio and this connection is further investigated here. finally, we propose a computational method in order to solve the minimization problem and to calculate the minimum-cost insured portfolio. all of the numerical work is performed using the matlab high-level language. © 2006 elsevier inc. all rights reserved."
"a conic integer program is an integer programming problem with conic constraints. conic integer programming has important applications in finance, engineering, statistical learning, and probabilistic integer programming. here we study mixed-integer sets defined by second-order conic constraints. we describe general-purpose conic mixed-integer rounding cuts based on polyhedral conic substructures of second-order conic sets. these cuts can be readily incorporated in branch-and-bound algorithms that solve continuous conic programming relaxations at the nodes of the search tree. our preliminary computational experiments with the new cuts show that they are quite effective in reducing the integrality gap of continuous relaxations of conic mixed-integer programs. © springer-verlag berlin heidelberg 2007."
"this book presents techniques for valuing derivative securities at a level suitable for practitioners, students in doctoral programs in economics and finance, and those in masters-level programs in financial mathematics and computational finance. it provides the necessary mathematical tools from analysis, probability theory, the theory of stochastic processes, and stochastic calculus, making extensive use of examples. it also covers pricing theory, with emphasis on martingale methods. the chapters are organized around the assumptions made about the dynamics of underlying price processes. readers begin with simple, discrete-time models that require little mathematical sophistication, proceed to the basic black-scholes theory, and then advance to continuous-time models with multiple risk sources. the second edition takes account of the major developments in the field since 2000. new topics include the use of simulation to price american-style derivatives, a new one-step approach to pricing options by inverting characteristic functions, and models that allow jumps in volatility and markov-driven changes in regime. the new chapter on interest-rate derivatives includes extensive coverage of the libor market model and an introduction to the modeling of credit risk. as a supplement to the text, the book contains an accompanying cd-rom with user-friendly fortran, c++, and vba program components. © 2007 by world scientific publishing co. pte. ltd. all rights reserved."
"there has been a major push into the field of ""financial engineering"" in the last few years, although the field has been growing in both research and education for over two decades. initially, the field was described as the design (engineering) of financial products (i.e., derivatives), but now it has a much broader interpretation, including the application of advanced tools (i.e., mathematical programming or stochastic processes) to the analysis of financial instruments with the goal of increasing profits and/or decreasing risk. as this field has grown, so have the employment opportunities for engineers in the financial sector, including jobs as analysts with investment banks, hedge funds, brokers, and insurance companies. this has resulted in a number of schools offering graduate programs in financial engineering (or similar areas, such as computational finance, mathematical finance, quantitative finance, and analytical finance). inevitably, and in our experience, demand is growing at the undergraduate level for such a major. if offered, it would seem natural that the degree would come from an industrial engineering, engineering management, operations research or systems engineering department. we examine how schools are addressing this issue and also ponder the requirements for such a degree and how it may or may not impact the ""traditional"" offering of engineering economy. © american society for engineering education, 2007."
this paper derives sharp estimates of the error arising from explicit and implicit approximations of the constant-coefficient 1d convection-diffusion equation with dirac initial data. the error analysis is based on fourier analysis and asymptotic approximation of the integrals resulting from the inverse fourier transform. this research is motivated by applications in computational finance and the desire to prove convergence of approximations to adjoint partial differential equations.
"intelligent finance represents a new direction recently emerging from the confluence of several distinct disciplines in financial market analysis, investing and trading, removing any historical or artificial barrier between them. it is conceived as the science, technology and art of the comprehensive, predictive, dynamic and strategic analysis of global financial markets, towards a unification and integration of academic finance and professional finance. as a comprehensive approach, it is a quest for absolute positive and non-trivial returns in investing and trading by exploiting complete information about financial markets from all general perspectives, drawing ideas, theories, models and techniques from many related academic disciplines, such as macroeconomics, microeconomics, academic finance, financial mathematics, econophysics, behavioural finance and computational finance, and from professional schools of thought, such as macrowave investing, trend following, fundamental analysis, technical analysis, mind analysis, active speculation, etc. in terms of risk management, intelligent finance is expected to minimize the very last risk-the incompleteness of an investing or trading method or system. the theoretical framework of intelligent finance consists of four major components: financial information fusion, multilevel stochastic dynamic process models, active portfolio and total risk management, and financial strategic analysis. we first provide the background from which intelligent finance has recently emerged as a new direction in finance research and industry, and then provide a brief theoretical review of the predictability of financial markets since bachelier. after these background discussions, we clarify the major research directions of intelligent finance. © 2006 taylor & francis."
"this chapter surveys work on dynamic heterogeneous agent models (hams) in economics and finance. emphasis is given to simple models that, at least to some extent, are tractable by analytic methods in combination with computational tools. most of these models are behavioral models with boundedly rational agents using different heuristics or rule of thumb strategies that may not be perfect, but perform reasonably well. typically these models are highly nonlinear, e.g. due to evolutionary switching between strategies, and exhibit a wide range of dynamical behavior ranging from a unique stable steady state to complex, chaotic dynamics. aggregation of simple interactions at the micro level may generate sophisticated structure at the macro level. simple hams can explain important observed stylized facts in financial time series, such as excess volatility, high trading volume, temporary bubbles and trend following, sudden crashes and mean reversion, clustered volatility and fat tails in the returns distribution. © 2006 elsevier b.v. all rights reserved."
"investment opportunities that are deferrable over a finite period of time are finite-lived american exchange options. although such investment opportunities can be valued using the carr [carr, p., 1988, the valuation of sequential exchange opportunities, journal of finance 43:5, 1235-1256.] model, the derivation of this model is not very accurate when correctly formulated. furthermore, past applications (e.g., [taudes, a. 1998, software growth options, journal of management information systems 15: 1, 165-185.]) have implemented the model without correcting for an important typo in the carr paper. while such investment opportunities are more accurately valued using the carr [carr, p. 1995, the valuation of american exchange options with application to real options, in: l.trigeorgis, ed, real options in capital investment: models, strategies and applications (praeger, westport, connecticut, london).] model, this model suffers from the problem known as ""non-uniform convergence"". this paper proposes a modified approach for estimating the carr [carr, p., 1988, the valuation of sequential exchange opportunities, journal of finance 43:5, 1235-1256.] model that emits more accurate output values with a minimal addition of mathematical and computational cost. the paper then demonstrates the superiority of this modified model for three real investment opportunities. © 2006 elsevier inc. all rights reserved."
"a fast and accurate method for pricing early exercise options in computational finance is presented in this paper. the main idea is to reformulate the well-known risk-neutral valuation formula by recognizing that it is a convolution. this novel pricing method, which we name the 'conv method for short, is applicable to a wide variety of payoffs and only requires the knowledge of the characteristic function of the model. as such the method is applicable within exponentially lévy models, including the exponentially affine jump-diffusion models. for an m-times exercisable bermudan option, the overall complexity is o(mn log(n)) with n grid points used to discretize the price of the underlying asset. it is also shown that american options can be very efficiently computed by combining richardson extrapolation to the conv method. © springer-verlag berdelberg 2007."
"this paper compares neurolinear feature extraction technique to two other feature extraction techniques. the author has developed all three methods recently, but in this paper, neurolinear method has been improved even further. the study is performed in respect to performance, accuracy and suitability for visualisation. experimental study is performed on a variety of datasets from numerous domains: chemistry, biology, finance etc. in total, 19 real and two artificial datasets were used. comparison is performed in terms of computational efficiency and accuracy. results show that neurolinear method achieves best accuracy while best directions method is not so accurate but often much faster."
"a balanced introduction to the theoretical foundations and real-world applications of mathematical finance the ever-growing use of derivative products makes it essential for financial industry practitioners to have a solid understanding of derivative pricing. to cope with the growing complexity, narrowing margins, and shortening life-cycle of the individual derivative product, an efficient, yet modular, implementation of the pricing algorithms is necessary. mathematical finance is the first book to harmonize the theory, modeling, and implementation of today's most prevalent pricing models under one convenient cover. building a bridge from academia to practice, this self-contained text applies theoretical concepts to real-world examples and introduces state-of-the-art, object-oriented programming techniques that equip the reader with the conceptual and illustrative tools needed to understand and develop successful derivative pricing models. utilizing almost twenty years of academic and industry experience, the author discusses the mathematical concepts that are the foundation of commonly used derivative pricing models, and insightful motivation and interpretation sections for each concept are presented to further illustrate the relationship between theory and practice. in-depth coverage of the common characteristics found amongst successful pricing models are provided in addition to key techniques and tips for the construction of these models. the opportunity to interactively explore the book's principal ideas and methodologies is made possible via a related web site that features interactive java experiments and exercises. while a high standard of mathematical precision is retained, mathematical finance emphasizes practical motivations, interpretations, and results and is an excellent textbook for students in mathematical finance, computational finance, and derivative pricing courses at the upper undergraduate or beginning graduate level. it also serves as a valuable reference for professionals in the banking, insurance, and asset management industries. © 2007 john wiley & sons, inc. all rights reserved."
"this paper presents a numerical method to price european options on realized variance. a european realized variance option is an option where payoff depends on the time of maturity, on the observed variance of the log-returns of the stock prices in a preassigned sequence of time values ti, i = 0, 1,⋯, n. the realized variance is the variance observed in the sample of the log-returns considered, so that the value at maturity of the realized variance option depends on the discrete sample of the log-returns of the stock prices observed at the preassigned dates ti, i = 0, 1,⋯, n. the method proposed to approximate the price of these options is based on the idea of approximating the discrete sum that gives the realized variance with an integral, using as model of the dynamics of the log-return of the stock price the heston stochastic volatility model. in this way the price of a realized variance option is approximated with the price of an integrated stochastic variance option where payoff depends on the time of maturity and on the integrated stochastic variance. the integrated stochastic variance option is priced with the method of discounted expectations. we derive an integral representation formula for the price of this last kind of options. this integral formula reduces to a one dimensional fourier integral in the case of the most commonly traded options that have a simple payoff function. the method has been validated on some test problems. the numerical experiments show that the approach suggested in this paper gives satisfactory approximations of the prices of the realized variance options (relative error 10-2, 10 -3). this approach also allows substantial savings of computational time when compared with the monte carlo method used to evaluate with approximately the same accuracy. the website http://www.econ.univpm.it/ recchioni/finance/w4 contains auxiliary material that can help in the understanding of this paper and makes available to the interested users the codes that implement the numerical method proposed here to price realized variance options. the use of these codes on a computing grid has been made user friendly developing a dedicated application using the software symphony (that is, a service oriented architecture (soam) software of platform computing toronto, canada). the website mentioned above makes this symphony application available to the users."
"genetic programming (gp) is an automated computational programming methodology, inspired by the workings of natural evolution techniques. it has been applied to solve complex problems in multiple domains including finance. this paper illustrates the application of an adaptive form of gp, where the probability of crossover and mutation is adapted dynamically during the gp run, to the important real-world problem of options pricing. the tests are carried out using market option price data and the results illustrate that the new method yields better results than are obtained from gp with fixed crossover and mutation rates. the developed method has potential for implementation across a range of dynamic problem environments. copyright 2007 acm."
"the world that we live in is filled with large scale agent systems, from diverse fields such as biology, ecology or finance. inspired by the desire to better understand and make the best out of these systems, we propose an approach which builds stochastic mathematical models, in particular g-networks models, that allow the efficient representation of systems of agents and offer the possibility to analyze their behavior using mathematics. this work complements our previous results on the discrete event simulation of adversarial tactical scenarios. we aim to provide insights into systems in terms of their performance and behavior, to identify the parameters which strongly influence them, and to evaluate how well individual goals can be achieved. with our approach, one can compare the effects of alternatives and chose the best one available. we model routine activities as well as situations such as: changing plans (e.g. destination or target), splitting forces to carry out alternative plans, or even changing on adversary group. behaviors such as competition and collaboration are included. we demonstrate our approach with some urban military planning scenarios and analyze the results. this work can be used to model the system at different abstraction levels, in terms of the number of agents and the size of the geographical location. in doing so, we greatly reduce computational complexity and save time and resources. we conclude the paper with potential extensions of the model, for example the arrival of reinforcements, the impact of released chemicals and so on."
"problems in computational finance share many of the characteristics that challenge us in statistical circuit analysis: high dimensionality, profound nonlinearity, stringent accuracy requirements, and expensive sample simulation. we offer a detailed experimental study of how one celebrated technique from this domain - quasi-monte carlo (qmc) analysis - can be used for fast statistical circuit analysis. in contrast with traditional pseudo-random monte carlo sampling, qmc substitutes a (shorter) sequence of deterministically chosen sample points. across a set of digital and analog circuits, in 90nm and 45nm technologies, varying in size from 30 to 400 devices, we obtain speedups in parametric yield estimation from 2x to 50x. © 2007 ieee."
"this paper uses agent-based computational finance as a method to link micro level investor behaviour and macro level stock market dynamics. empirical data from an online investment survey on individual investors' decision-making and social interaction is used to formalize the trading and interaction rules of the agents of the artificial stock market simstockexchange. multiple simulation runs are performed with this artificial stock market, which generated macro level results, like stock market prices and returns over time. these outcomes are subsequently compared to empirical macro level data from real stock markets, and qualitative as well as quantitative agreement between the simulated asset returns distributions and the real stock markets' asset returns distributions are found. © title proceedings of the 4th conference of the european social simulation association, essa 2007. all rights reserved."
"e-service intelligence is a new research field that deals with fundamental roles, social impacts and practical applications of various intelligent technologies on the internet based e-service applications that are provided by e-government, e-business, e-commerce, e-market, e-finance, and e-learning systems, to name a few. this chapter offers a thorough introduction and systematic overview of the new field e-service intelligence mainly based on computational intelligence techniques. it covers the state-of-the-art of the research and development in various aspects including both theorems and applications of e-service intelligence. moreover, it demonstrates how adaptations of existing computational intelligent technologies benefit from the development of e-service applications in online customer decision, personalized services, web mining, online searching/data retrieval, and various web-based support systems. © springer-verlag berlin heidelberg 2007."
"computational intelligence (ci) encompasses approaches primarily based on artificial neural networks, fuzzy logic rules, evolutionary algorithms, support vector machines and also approaches that combine two or more techniques (hybrid). these methods have been applied to solve many complex and diverse problems. recent years have seen many new developments in ci techniques and, consequently, this has led to many applications in a variety of areas including engineering, finance, social and biomedical. in particular, ci techniques are increasingly being used in biomedical and human movement areas because of the complexity of the biological systems. the main objective of this chapter is to provide a brief description of the major computational intelligence techniques for pattern recognition and modelling tasks that often appear in biomedical, health and human movement research. © 2006, idea group inc."
"agent-based computational finance is attracting significant interest in many disciplines. but most artificial stock markets (asm) was built in a complex manner and not suited for studying the short-term dynamics of stock market. this activated us to build a new model different from literatures in three main parts of designing an artificial stock market: economic environment, agent's behavior and price formation. the results of simulation show that our model can reproduce the stylized facts of short-term dynamics. © 2007 ieee."
"a self-contained, contemporary treatment of the analysis of long-range dependent data. long-memory time series: theory and methods provides an overview of the theory and methods developed to deal with long-range dependent data and describes the applications of these methodologies to real-life time series. systematically organized, it begins with the foundational essentials, proceeds to the analysis of methodological aspects (estimation methods, asymptotic theory, heteroskedastic models, transformations, bayesian methods, and prediction), and then extends these techniques to more complex data structures. to facilitate understanding, the book: assumes a basic knowledge of calculus and linear algebra and explains the more advanced statistical and mathematical concepts. features numerous examples that accelerate understanding and illustrate various consequences of the theoretical results. proves all theoretical results (theorems, lemmas, corollaries, etc.) or refers readers to resources with further demonstration. includes detailed analyses of computational aspects related to the implementation of the methodologies described, including algorithm efficiency, arithmetic complexity, cpu times, and more. includes proposed problems at the end of each chapter to help readers solidify their understanding and practice their skills. a valuable real-world reference for researchers and practitioners in time series analysis, economerics, finance, and related fields, this book is also excellent for a beginning graduate-level course in long-memory processes or as a supplemental textbook for those studying advanced statistics, mathematics, economics, finance, engineering, or physics. a companion web site is available for readers to access the s-plus and r data sets used within the text. © 2007 by john wiley & sons, inc. all rights reserved."
"a fast and accurate method for pricing early exercise and certain exotic options in computational finance is presented. the method is based on a quadrature technique and relies heavily on fourier transformations. the main idea is to reformulate the well-known risk-neutral valuation formula by recognizing that it is a convolution. the resulting convolution is dealt with numerically by using the fast fourier transform. this novel pricing method, which we dub the convolution method, is applicable to a wide variety of payoffs and requires only the knowledge of the characteristic function of the model. as such, the method is applicable within many regular affine models, among w;hich is the class of exponential lévy models. for an m-times exercisable bermudan option, the overall complexity is o(mn log2(n)), with n grid points used to discretize the price of the underlying asset. american options are priced efficiently by applying richardson extrapolation to the prices of bermudan options. © 2008 society for industrial and applied mathematics."
"large deviations analysis for light-tailed systems provides an asymptotic description of the optimal importance sampler in the scaling of the law of large numbers. as we will show by means of a simple example related to computational finance, such asymptotic description can be interpreted in different ways suggesting several importance sampling algorithms, some of them state-dependent. in turn, the performance of the suggested algorithms can be substantially different."
"a rapid development of time series models and methods addressing nonlinearity in computational finance and econometrics are recently reported in the financial literature. the non-linear theory either extends and complements existing time series methodology by introducing more general structures or provides an alternative framework (see [1,2]). this article considers moment properties as well as the kurtosis of various types of volatility sign models, including the sign rca models and sign garch models. the kurtosis of the classical rca model of nicholls and quinn [3] is shown to be a special case of the sign rca model. © 2006 elsevier ltd. all rights reserved."
"this paper proposes a simplified risky discount bond pricing model based on longstaff and schwartz (1995). the advantage of this model is that it yields a closed form solution for probability of default. also, a practical feature with our model is that computing durations and other risk management tools become computationally less expensive, while the appealing properties in the ls model are preserved. the numerical comparisons show that the differences in credit spreads between this model and longstaff and schwartz are within a few basis points for fairly general parameter values. moreover, the computational time is shown remarkably reduced by the simplified model. sensitivity analysis of credit spread with respect to different parameter values is presented. © 2007 world scientific publishing co. and center for pacific basin business, economics and finance research."
"medicine, engineering, and finance are traditional fields for the application of fuzzy logic (fl); however, in social sciences, the utilization of fl can be intensely explored as a valuable analysis instrument. joining fuzzy with multiple criteria decision making (mcdm), the natural complex and uncertain criteria of social problems can be adequately explored. a suitable uncertainty analysis is the focal point in the social field. in this direction, an intelligent computational method for election forecasting is proposed as a practical result of this research. the main objective is to present a more flexible methodology for political assessment comparing to traditional methods. © 2004 elsevier b.v. all rights reserved."
"in this paper we study an inverse problem for a parabolic partial differential equation. the parabolic partial differential equation considered is the fokker planck equation associated to a system of stochastic differential equations and the inverse problem studied consists in finding from suitable data the values of the parameters that appear in the coefficients of this fokker planck equation. the data used in the reconstruction of the parameters are observations made at discrete times of the stochastic process solution of the system of stochastic differential equations. that is, the data of the inverse problem are a ""sample"" taken at discrete times of some of the components of the random vector solution of the stochastic differential equations and not, as usual, observations made on the solution of the parabolic equation. the choice of the system of stochastic differential equations and of the data used in the inverse problem are motivated by applications in mathematical finance. the stochastic differential equations presented can be used to model the dynamics of the log-returns of the index of some classes of hedge funds, such as, for example, the so called ""long short equity"" hedge funds and of some auxiliary variables. the solution of the inverse problem proposed is obtained through the solution of a filtering and of an estimation problem. the solution of these last two problems is based on the knowledge of the joint probability density function of the state variables of the model conditioned to the observations made and to the initial condition. this joint probability density function is solution of an initial value problem for the kushner equation that in the circumstances considered here can be written as a sequence of initial value problems for the fokker planck equation associated to the system of stochastic differential equations with appropriate initial conditions. an integral representation formula for this probability density function is derived and used to develop a numerical procedure to solve the estimation problem using the maximum likelihood method. the kushner equation provides the relation between the data and the fokker planck equation used to solve the inverse problem considered. the computational method proposed has been tested on synthetic data and the results obtained are presented. some auxiliary material useful to understand this paper including some animations and some numerical experiments can be found in the website http://www.econ.univpm.it/ recchioni/finance/w5. a more general reference to the work in mathematical finance of the authors and of their coauthors is the website http://www.econ.univpm.it/recchioni/finance. © de gruyter 2007."
"the world that we live in is filled with large scale agent systems, from diverse fields such as biology, ecology or finance. inspired by the desire to better understand and make the best out of these systems, we propose to build stochastic mathematical models, in particular g-networks models. with our approach, we aim to provide insights into systems in terms of their performance and behavior, to identify the parameters which strongly influence them, and to evaluate how well individual goals can be achieved. through comparing the effects of alternatives, we hope to offer the users the possibility of choosing an option that address their requirements best. we have demonstrated our approach in the context of urban military planning and analyzed the obtained results. the results are validated against those obtained from a simulator (gelenbe et al. in simulating the navigation and control of autonomous agents, pp 183-189, 2004a; in enabling simulation with augmented reality, pp 290-310, 2004b) that was developed in our group and the observed discrepancies are discussed. the results suggest that the proposed approach has tackled one of the classical problems in modeling multi-agent systems and is able to predict the systems' performance at low computational cost. in addition to offering the numerical estimates of the outcome, these results help us identify which characteristics most impact the system. we conclude the paper with potential extensions of the model. © springer-verlag 2006."
"recent years have seen many new developments in computational intelligence (ci) techniques and, consequently, this has led to an exponential increase in the number of applications in a variety of areas, including: engineering, finance, social and biomedical. in particular, ci techniques are increasingly being used in biomedical and human movement areas because of the complexity of the biological systems as well as the limitations of the existing quantitative techniques in modelling. computational intelligence for movement sciences: neural networks and other emerging techniques contains information regarding state-of-the-art research outcomes and cutting-edge technology from leading scientists and researchers working on various aspects of the human movement. readers of this book will gain an insight into this field as well as access to pertinent information, which they will be able to use for continuing research in this area. this book is a part of the computational intelligence and its applications series. © 2006 by idea group inc. all rights reserved."
"an agent-based computational (abc) modeling approach is an interactive model that includes irrational and rational investors, allows expansion on existing work, and provides the survival for irrational-investor survival in artificial stock markets (asm). the abc develops a behavioral finance model in five steps by building the conceptual model, designing the artificial stock market's architecture, coding and running the asm program, examining the asm data, and presenting new behavioral finance theories. a bsv model strategy builds sim-asm, in which the exchange sets he tradable assets, market price, discovered using a double-sided auction mechanism. the anc bsv and dssw models are combined to find the survival market dynamics. abc approach uses data of individual wealth fluctuations and calculates and compares rates of return on various investor categories."
"the internet has opened up new opportunities to educators especially in terms of embracing constructivist approaches to learning. the objective of this paper is to describe how an interactive market approach to distant learning is designed and implemented in the masters of computational finance (mscf) program at carnegie mellon university. this approach provides a subtle blend of both behaviorist and radical constructivist principles applied to the learning environment. this resulted in a new solution to a problem facing educators, which is how to design and implement experiential settings that engage students to learn effectively."
"intermediate probability is the natural extension of the author's fundamental probability. it details several highly important topics, from standard ones such as order statistics, multivariate normal, and convergence concepts, to more advanced ones which are usually not addressed at this mathematical level, or have never previously appeared in textbook form. the author adopts a computational approach throughout, allowing the reader to directly implement the methods, thus greatly enhancing the learning experience and clearly illustrating the applicability, strengths, and weaknesses of the theory. the book: places great emphasis on the numeric computation of convolutions of random variables, via numeric integration, inversion theorems, fast fourier transforms, saddlepoint approximations, and simulation. provides introductory material to required mathematical topics such as complex numbers, laplace and fourier transforms, matrix algebra, confluent hypergeometric functions, digamma functions, and bessel functions.  presents full derivation and numerous computational methods of the stable paretian and the singly and doubly non-central distributions. a whole chapter is dedicated to mean-variance mixtures, nig, gig, generalized hyperbolic and numerous related distributions. a whole chapter is dedicated to nesting, generalizing, and asymmetric extensions of popular distributions, as have become popular in empirical finance and other applications. provides all essential programming code in matlab and r. the user-friendly style of writing and attention to detail means that self-study is easily possible, making the book ideal for senior undergraduate and graduate students of mathematics, statistics, econometrics, finance, insurance, and computer science, as well as researchers and professional statisticians working in these fields. © 2007 john wiley & sons, ltd."
"this article introduces some relevant research works on computational intelligence applied to finance and economics. the objective is to offer an appropriate context and a starting point for those who are new to computational intelligence in finance and economics and to give an overview of the most recent works. a classification with five different main areas is presented. those areas are related with different applications of the most modern computational intelligence techniques showing a new perspective for approaching finance and economics problems. each research area is described with several works and applications. finally, a review of the research works selected for this special issue is given. © 2007 blackwell publishing, inc."
"discretisation methods to simulate stochastic differential equations belong to the main tools in mathematical finance. for itô processes, there exist several euler- or runge-kutta-like methods which are analogues of well-known approximation schemes in the nonstochastic case. in the multidimensional case, there appear several difficulties, caused by the mixed second order derivatives. these mixed terms (or more precisely their differences) correspond to special random variables called lévy stochastic area terms. in the present paper, we compare three approximation methods for such random variables with respect to computational complexity and the so-called effective dimension. © 2007 elsevier inc. all rights reserved."
closed-form approximations for the density and cumulative distribution function of the doubly noncentral t distribution are developed based on saddlepoint methods. they exhibit remarkable accuracy throughout the entire support of the distribution and are vastly superior to existing approximations. an application in finance is considered which capitalizes on the enormous increase in computational speed. © 2007 elsevier b.v. all rights reserved.
"this paper examines stock prices forecasting and trading strategies' development with means of computational intelligence (ci), addressing the issue of an artificial neural network (ann) topology dependency. simulations reveal optimal network settings. optimality of discovered ann topologies' is explained through their links with the arma processes, thus presenting identified structures as nonlinear generalizations of such processes. optimal settings examination demonstrates the weak relationships between statistical and economic criteria. the research demonstrates that fine-tuning ann settings is an important stage in the computational model set-up for results' improvement and mechanism understanding. genetic algorithm (ga) is proposed to be used for model discovery, making technical decisions less arbitrary and adding additional explanatory power to the analysis of economic systems with ci. the paper is a step towards the econometric foundation of ci in finance. the choice of evaluation criteria combining statistical and economic qualities is viewed as essential for an adequate analysis of economic systems. © springer-verlag berlin heidelberg 2006."
"this paper presents a new and high performance solution method for multistage stochastic convex programming. stochastic programming is a quantitative tool developed in the field of optimization to cope with the problem of decision-making under uncertainty. among others, stochastic programming has found many applications in finance, such as asset-liability and bond-portfolio management. however, many stochastic programming applications still remain computationally intractable because of their overwhelming dimensionality. in this paper we propose a new decomposition algorithm for multistage stochastic programming with a convex objective and stochastic recourse matrices, based on the path-following interior point method combined with the homogeneous self-dual embedding technique. our preliminary numerical experiments show that this approach is very promising in many ways for solving generic multistage stochastic programming, including its superiority in terms of numerical efficiency, as well as the flexibility in testing and analyzing the model. © springer-verlag berlin 2005."
"random matrix theory is now a big subject with applications in many disciplines of science, engineering and finance. this article is a survey specifically oriented towards the needs and interests of a numerical analyst. this survey includes some original material not found anywhere else. we include the important mathematics which is a very modern development, as well as the computational software that is transforming the theory into useful practice. © cambridge university press, 2005."
"we consider optimization problems for minimizing conditional value-at-risk (cvar) from a computational point of view, with an emphasis on financial applications. as a general solution approach, we suggest to reformulate these cvar optimization problems as two-stage recourse problems of stochastic programming. specializing the l-shaped method leads to a new algorithm for minimizing conditional value-at-risk. we implemented the algorithm as the solver cvarmin. for illustrating the performance of this algorithm, we present some comparative computational results with two kinds of test problems. firstly, we consider portfolio optimization problems with 5 random variables. such problems involving conditional value at risk play an important role in financial risk management. therefore, besides testing the performance of the proposed algorithm, we also present computational results of interest in finance. secondly, with the explicit aim of testing algorithm performance, we also present comparative computational results with randomly generated test problems involving 50 random variables. in all our tests, the experimental solver, based on the new approach, outperformed by at least one order of magnitude all general-purpose solvers, with an accuracy of solution being in the same range as that with the lp solvers. © springer-verlag berlin/heidelberg 2006."
"in the field of mathematical finance, the concern with the applications of lévy processes has been growing for the last several years. the models are aimed at incorporating stylized empirical facts. in the classical black-scholes option pricing model, the log return of asset is assumed to follow the normal distribution. however, compared to the normal distribution, the empirical density of log returns typically has more mass near the origin, less in the flanks, and more in the tail. empirical works also suggest the discontinuity of the sample path of price processes. to account for these features, several models based on lévy processes have been propounded in the literature. prominent examples include the poisson jump model (merton (1976)), the variance gamma process (madan and seneta (1990), madan et al. (1998)), the normal inverse gaussian process (barndorff-nielsen (1998)), and the cgmy process (carr et al. (2002)). although relevant estimators of the parameters are required for applications of these models, the estimation of lévy processes is challenging. since the law of lévy processes is entirely specified by the infinitely divisible distribution, it suffices to estimate the parameters of corresponding infinitely divisible distributions. however, they are parametrized in terms of the characteristic function and in general we cannot obtain the closed form expression of density function. thus, to obtain the maximum likelihood estimate, one must rely on the numerical method such as fourier inversion, which is computationally demanding. since the closed form expression of the characteristic function is known, several estimation methods based on the empirical characteristic function can be applied to the estimation of lévy processes. the basic idea of this approach is to match the theoretical characteristic function with its empirical counterpart. this type of approach was originally proposed by press (1972) for the estimation of stable distributions. press (1972) introduced minimum distance estimation and minimum rth-mean distance estimation. feurverger and mcdunnough (1981) proposed generalized method of moments (gmm) type estimator. carrasco and florens (2000) extended the gmm procedure to the case of a continuum of moment conditions (cgmm). kunitomo and owada (2004) recently introduced the maximum empirical likelihood estimator (mele). this paper has two aims. firstly, we propose an efficient and computationally convenient estimation method. we use the framework of sueishi (2005), which discusses the estimation of stable distributions. we rely on the quasi-likelihood approach. we construct the quasi-likelihood function via the characteristic function. the estimator is characterized as the root of the quasi-likelihood equation. secondly we compare the finite sample performances of recently proposed estimation methods. although various estimation methods have been proposed, there have been little study on the comparison of the estimation performance of each method. we employ the cgmm and the mele as our competitors. a monte carlo study is implemented for two well-known lévy processes: the variance gamma process and the normal inverse gaussian process. we conduct simulations using different sample sizes and settings. simulation results show that the qle outperforms other estimators in many situations, though computational burden is much smaller than the other estimators. especially, in the case of the normal inverse gaussian process, the qle performs on par with the maximum likelihood estimator in moderate sample size."
"the paper presents a state dependent multinomial model of intertemporal changes in the term structure of interest rates. the model is a one-factor interest-rate model within the markov family models for short-term interest rate and it extends the ho and lee [j. finance xli (5) (1986) 1001] binomial model. we derive the theoretical basis of the multinomial model, suggest a computational framework to evaluate the model's parameters and investigate the suitability of the model for the italian market. © 2004 published by elsevier b.v."
"bsv (barberis, huang and vishny [1998]) model is one of the most important explanative models in the area of behavioral finance. the existing bsv model is a model of how behavioral investors form beliefs, which produces both overreaction and mean-reversion for a wide range of parameter values. however, bsv model does not address the market equilibrium problems, which is critical for the asset pricing theory. thus the following 2 questions are meaningful for the existing bsv model to be extended to an equilibrium model in which there are both bsv investors and rational investors in a market of risky asset: will bsv model maintain its explanative power if rational investors are introduced? who will lose money to their counterparts in trading of risky assets, the behavioral investors or the rational investors? in this paper, we present an agent-based computational model of the dynamic game between bsv investors and rational investors. the results of our model answer the above two questions. first, the introduction of rational investors to the market of risky asset will not eliminate the anomalies of overreaction and mean-reversion. we believe that it is because the unpredictability of bsv agents' behavior adds the total risk of risky asset, which in turn prevents rational investors from taking unlimited position to arbitrage. at that time, the risky asset is mispriced. second, we do not find any evidence that the behavioral investors will lose money to their counterparts although their cognitive bias causes them to form a false kind of expectation on the future price of the risky asset. on the contrary, some weak evidence on that behavioral investors are less likely to bankrupt is reported. we contribute it to that bsv investors will be overcautious when the market price is going down continuously due to their cognitive bias, and then decrease the probability of bankruptcy. while at that time, rational investors will try to correct the market price distorted by bsv investors and thus take an additional arbitrage position, which may increase their returns, but will increase the probability of bankruptcy."
"random matrix theory is now a big subjest with applications in many fields of science, finance and engineering. this talk surveys some of the important mathematics that is a very modem development as well as the computational software that is transforming theory into useful practice."
"many of the concepts in theoretical and empirical finance developed over the past decades - including the classical portfolio theory, the black-scholes-merton option pricing model and the riskmetrics variance-covariance approach to value at risk (var) - rest upon the assumption that asset returns follow a normal distribution. however, it has been long known that asset returns are not normally distributed. rather, the empirical observations exhibit fat tails. this heavy tailed or leptokurtic character of the distribution of price changes has been repeatedly observed in various markets and may be quantitatively measured by the kurtosis in excess of 3, a value obtained for the normal distribution (bouchaud and potters, 2000; carr et al., 2002; guillaume et al., 1997; mantegna and stanley, 1995; rachev, 2003; weron, 2004). it is often argued that financial asset returns are the cumulative outcome of a vast number of pieces of information and individual decisions arriving almost continuously in time (mcculloch, 1996; rachev and mittnik, 2000). as such, since the pioneering work of louis bachelier in 1900, they have been modeled by the gaussian distribution. the strongest statistical argument for it is based on the central limit theorem, which states that the sum of a large number of independent, identically distributed variables from a finite-variance distribution will tend to be normally distributed. however, as we have already mentioned, financial asset returns usually have heavier tails. in response to the empirical evidence mandelbrot (1963) and fama (1965) proposed the stable distribution as an alternative model. although there are other heavy-tailed alternatives to the gaussian law - like student's t, hyperbolic, normal inverse gaussian, or truncated stable - there is at least one good reason for modeling financial variables using stable distributions. namely, they are supported by the generalized central limit theorem, which states that stable laws are the only possible limit distributions for properly normalized and centered sums of independent, identically distributed random variables. since stable distributions can accommodate the fat tails and asymmetry, they often give a very good fit to empirical data. in particular, they are valuable models for data sets covering extreme events, like market crashes or natural catastrophes. even though they are not universal, they are a useful tool in the hands of an analyst working in finance or insurance. hence, we devote this chapter to a thorough presentation of the computational aspects related to stable laws. in section 1.2 we review the analytical concepts and basic characteristics. in the following two sections we discuss practical simulation and estimation approaches. finally, in section 1.5 we present financial applications of stable laws. © springer-verlag berlin heidelberg 2005."
"analysis of variance (anova) is now often applied to functions defined on the unit cube, where it serves as a tool for the exploratory analysis of functions. the mean dimension of a function, defined as a natural weighted combination of its anova mean squares, provides one measure of how hard or easy it is to integrate the function by quasi-monte carlo sampling. this article presents some new identities relating the mean dimension, and some analogously defined higher moments, to the variance importance measures of i. m. sobol. as a result, we are able to measure the mean dimension of certain functions arising in computational finance. we produce an unbiased and nonnegative estimate of the variance contribution of the highest-order interaction that avoids the cancellation problems of previous estimates. in an application to extreme value theory, we find that, among other things, the minimum of d independent u[0,1] random variables has a mean dimension of 2(d + 1)/(d + 3). © 2006 american statistical association."
"in recent years portfolio optimization and construction methodologies have become an increasingly critical ingredient of asset and fund management, while at the same time portfolio risk assessment has become an essential ingredient in risk management, and this trend will only accelerate in the coming years. unfortunately there is a large gap between the limited treatment of portfolio construction methods that are presented in most university courses with relatively little hands-on experience and limited computing tools, and the rich and varied aspects of portfolio construction that are used in practice in the finance industry. current practice demands the use of modern methods of portfolio construction that go well beyond the classical markowitz mean-variance optimality theory and require the use of powerful scalable numerical optimization methods. this book fills the gap between current university instruction and current industry practice by providing a comprehensive computationally-oriented treatment of modern portfolio optimization and construction methods. the computational aspect of the book is based on extensive use of s-plus®, the s+nuopt™ optimization module, the s-plus robust library and the s+bayes™ library, along with about 100 s-plus scripts and some crsp® sample data sets of stock returns. a special time-limited version of the s-plus software is available to purchasers of this book. ""for money managers and investment professionals in the field, optimization is truly a can of worms rather left un-opened, until now! here lies a thorough explanation of almost all possibilities one can think of for portfolio optimization, complete with error estimation techniques and explanation of when non-normality plays a part. a highly recommended and practical handbook for the consummate professional and student alike!"" steven p. greiner, ph.d. chief large cap quant & fundamental research manager harris investment management ""the authors take a huge step in the long struggle to establish applied post-modern portfolio theory. the optimization and statistical techniques generalize the normal linear model to include robustness, non-normality, and semi-conjugate bayesian analysis via mcmc. the techniques are very clearly demonstrated by the extensive use and tight integration of s-plus software. their book should be an enormous help to students and practitioners trying to move beyond traditional modern portfolio theory."" peter knez cio, global head of fixed income barclays global investors. © 2005 springer science+business media, llc. all rights reserved."
"a principal challenge in modern computational finance is efficient portfolio design - portfolio optimization followed by decision-making. optimization based on even the widely used markowitz two-objective mean-variance approach becomes computationally challenging for real-life portfolios. practical portfolio design introduces further complexity as it requires the optimization of multiple return and risk measures subject to a variety of risk and regulatory constraints. further, some of these measures may be nonlinear and nonconvex, presenting a daunting challenge to conventional optimization approaches. we introduce a powerful hybrid multiobjective optimization approach that combines evolutionary computation with linear programming to simultaneously maximize these return measures, minimize these risk measures, and identify the efficient frontier of portfolios that satisfy all constraints. we also present a novel interactive graphical decision-making method that allows the decision-maker to quickly down-select to a small subset of efficient portfolios. the approach has been tested on real-life portfolios with hundreds to thousands of assets, and is currently being used for investment decision-making in industry. © 2005 ieee."
"computational intelligence has a long history of applications to business - expert systems have been used for decision support in management, neural networks and fuzzy logic have been used in process control, a variety of techniques have been used in forecasting, and data mining has become a core component of customer relationship management in marketing. while there is literature on this field, it is spread over many disciplines and in many different publications, making it difficult to find the pertinent information in one source. business applications and computational intelligence addresses the need for a compact overview of the diversity of applications in a number of business disciplines, and consists of chapters written by leading international researchers. chapters cover most fields of business, including: marketing, data mining, e-commerce, production and operations, finance, decision-making, and general management. business applications and computational intelligence provides a comprehensive review of research into computational intelligence applications in business, creating a powerful guide for both newcomers and experienced researchers. © 2006 by idea group inc. all rights reserved."
"a book named business applications and computational intelligence has presented the practical applications of the technique of computational intelligence in business operations. computational intelligence is the modern term for artificial intelligence, which is the machine embodiment of the mechanisms that influence intelligent behavior. computational intelligence can be used to adapt data to provide information about queries. several computational intelligence techniques can be effectively used solve the problems of the complexity of business databases. the techniques can also be used for other areas such as the control of complex processes in manufacturing or in distribution networks. a range of application areas in the field of business such as marketing, data mining, e-commerce, production and operations, finance, decision-making, and general management have been discussed in the book."
"the computational intelligence and its applications series publishes academic work based either on one of the computational intelligence techniques or on applications of these techniques to a field of human endeavor. the books may be single author monographs, multiple author editions, conference proceedings, handbooks, or edited volumes by leading researchers with chapters contributed by researchers in the field. computational economics: a perspective from computational intelligence is a part of this series. computational economics: a perspective from computational intelligence provides models of various economic and financial issues while using computational intelligence as a foundation. the scope of this volume comprises finance, economics, management, organizational theory and public policies. it explains the ongoing and novel research in this field, and displays the power of these computational methods in coping with difficult problems with methods from traditional perspectives. by encouraging the discussion of different views, this book serves as an introductory and inspiring volume that helps to flourish studies in computational economics. © 2006 by idea group inc. all rights reserved."
"we consider a multidimensional diffusion process (xt α)0≤t≤t whose dynamics depends on a parameter α. our first purpose is to write as an expectation the sensitivity ∇α j(α) for the expected cost j(α) = double-struk e sign (f(xtα)), in order to evaluate it using monte carlo simulations. this issue arises, for example, from stochastic control problems (where the controller is parameterized, which reduces the control problem to a parametric optimization one) or from model misspecifications in finance. previous evaluations of ∇ ∇(α) using simulations were limited to smooth cost functions f or to diffusion coefficients not depending on a (see yang and kushner, siam j. control optim., 29 (1991), pp. 1216-1249). in this paper, we cover the general case, deriving three new approaches to evaluate ∇αj(α), which we call the malliavin calculus approach, the adjoint approach, and the martingale approach. to accomplish this, we leverage itô calculus, malliavin calculus, and martingale arguments. in the second part of this work, we provide discretization procedures to simulate the relevant random variables; then we analyze their respective errors. this analysis proves that the discretization error is essentially linear with respect to the time step. this result, which was already known in some specific situations, appears to be true in this much wider context. finally, we provide numerical experiments in random mechanics and finance and compare the different methods in terms of variance, complexity, computational time, and time discretization error. © 2005 society for industrial and applied mathematics."
"in this chapter, i consider a design framework of a computational experiment in finance. the examination of statistics used for economic forecasts evaluation and profitability of investment decisions, based on those forecasts, reveals only weak relationships between them. the ""degree of improvement over efficient prediction"" combined with directional accuracy are proposed in an estimation technique, as an alternative to the conventional least squares. rejecting a claim that the accuracy of the forecast does not depend upon which error-criteria are used, profitability of networks trained with l6 loss function appeared to be statistically significant and stable. the best economic performances are realized for a 1-year investment horizon with longer training not leading to enhanced accuracy. an improvement in profitability is achieved for models optimized with genetic algorithm. computational intelligence is advocated for searching optimal relationships among economic agents' risk attitude, loss function minimization in the learning process, and the profitability of trading decisions. © 2006, idea group inc."
"as a result of the structure and content transformation of an evolving society, many large scale autonomous systems emerged in diverse areas such as biology, ecology or finance. inspired by the desire to better understand and make the best out of these systems, we propose an approach which builds stochastic mathematical models, in particular g-networks models, that allow the efficient representation of systems of agents and offer the possibility to analyze their behavior using mathematics. this approach is capable of modelling the system at different abstraction levels, both in terms of the number of agents and the size of the geographical location. we demonstrate our approach with some urban military planning scenarios and the results suggest that this approach has tackled the problem in modelling autonomous systems at low computational cost. apart from offering the numerical estimates of the outcome, the approach helps us identify the characteristics that impact the system most and allows us to compare alternative strategies. © springer-verlag berlin heidelberg 2006."
"the issues related to msc program in computational economics offered by the erasmus school of economics at erasmus university rotterdam in the netherlands are discussed. the program serves to meet the increasing demand from the society for experts with computational skills and experience in a quantitative domain such as finance, marketing, or logistics. it also contributes to the development of computational intelligence (ci) and computational economics (ce) fields. the computational economics program has a strong orientation toward research and training the students in analyzing complex economic problems from the finance, marketing, and logistics domains using modeling tools on ci."
"in this paper, we study the option pricing problem, one of the prominent and challenging problems in computational finance. using the padé approximation, we have developed a second order l0 stable discrete parallel algorithm for experimentation on advanced architectures. we have implemented the sequential version of this algorithm and evaluated the european options. numerical results are compared with those obtained using other commonly used numerical methods and shown that the new algorithm is robust and efficient than the traditional schemes. also, using explicit forward time centered space (ftcs) on the reduced black-scholes partial differerential equation, we report pricing of european options. we have done our experiments on a shared memory multiprocessor machine using openmp and report a maximum speedup of 3.43 with 16 threads. © 2006 inderscience enterprises ltd."
"several recently developed chaotic forecasting methods give better results than the random walk forecasts. however they do not take into account specific regularities of stock returns reported in empirical finance literature, such as the calendar effects. in this paper, we present a method for filtering the day-of-the-week and the holiday effect in a time series. our main objective is twofold. on the one hand we study how the underlying dynamics of the nasdaq composite, and tse 300 composite returns series can be influenced by the presence of calendar effects. on the other hand we adapt our method to chaotic forecasting. its computational advantages lead to significant improvements of forecasts. © world scientific publishing company."
"obtaining the theoretical fair value of an option based on the factors affecting its price is a process called option pricing and commonly known approaches are the black-scholes formula and the binomial pricing model. however, these parametric models are generally dependent on the assumptions of continuous-time finance theory and presumed complex and rigid statistical formulations. nonparametric and computational methods of option pricing, on the other hand, are able to accurately model the pricing formula from historical data but suffer from poor interpretability due to their opaque architectures. generally, there is no guarantee that the prices derived from these model-free approaches conform to rational pricing. this paper proposes a novel brain-inspired nonparametric model for pricing american-style option on currency futures based on a dynamically evolving semantic memory model named gensofnn-tvr(s). logical reasoning rules governing the pricing decisions can be extracted from the proposed model. subsequently, the gensofnn-tvr(s) based option pricing model is implemented in a mis-priced option arbitrage trading system named genso-opats, and simulation results demonstrated an encouraging rate of return on investment. ©2005 ieee."
"3-d nonlinear diffusion equations occur in various fields such as heat transfer, fluid dynamics, astrophysics, and finance. the finite volume method for linear 3-d diffusion equations on unstructured tetrahedral grids was modified to create a finite volume method for 3-d nonlinear diffusion equations on unstructured tetrahedron meshes. flux conservation was used to get the computational solutions. so the scheme is second-order and flux conservative. the cell-center approximation reduces the number of computations. the lagrangian multiplier method makes the method more suitable for unstructured grids with the splitting method used to reduce computational costs. the newton-bicgstab method is used to solve the equations derived from the finite volume scheme. the numerical results show that the method is fast and effective."
"we present an overview of the literature relating to computational intelligence (also commonly called artificial intelligence) and business applications, particularly the journal-based literature. the modern investigation into artificial intelligence started with alan turing who asked in 1948 if it would be possible for ""machinery to show intelligent behaviour."" the computational intelligence discipline is primarily concerned with understanding the mechanisms underlying intelligent behavior, and consequently embodying these mechanisms in machines. the term ""artificial intelligence"" first appeared in print in 1955. as this overview shows, the 50 years of research since then have produced a wide range of techniques, many of which have important implications for many business functions, including finance, economics, production, operations, marketing, and management. however, gaining access to the literature can prove difficult for both the computational intelligence researcher and the business practitioner, as the material is contained in numerous journals and discipline areas. the chapter provides access to the vast and scattered literature by citing reviews of the main computational intelligence techniques, including expert systems, artificial neural networks, fuzzy systems, rough sets, evolutionary algorithms, and multi-agent systems. © 2006, idea group inc."
"this paper uses bond prices to investigate how the creditworthiness of argentina, brazil, mexico and venezuela is influenced by global, regional and country-specific factors. each country's distance-to-default is estimated monthly for 1994-2001, by fitting the structural model of cathcart and el-jahel [cathcart, l., el-jahel, l., 2003. semi-analytical pricing of defaultable bonds in a signalling jump-default model. the journal of computational finance 6, 91-108] with a kalman filter to brady bonds. a small set of variables is able to explain up to 80% of the variance of the estimated distance-to-default for each country. surprisingly, country-specific variables account for only about 8% of the explained variance; the largest part of the variance (45%) is explained by regional factors, which relate to joint stock-market returns, volatility and market sentiment; global conditions, related mainly to us stock-market returns, explain another 25% of the variance. of the 20% variance which remains unexplained, more than half is due to another common (but unidentified) factor. the conclusion is that the creditworthiness of these four emerging markets is driven mainly by a common set of factors, which are related closely to stock markets in the region and the united states. © 2006 elsevier ltd. all rights reserved."
"the problem of growing computational complexity in finance industry demands manageable, high-speed and real-time solutions in solving complex mathematical problems such as option pricing. in option trading scenario, determining a fair price for options “any time” and “any-where” has become vital yet difficult computational problem. in this study, we have designed, implemented, and deployed architecture for pricing options on-line using a hand-held device that is j2me-based mobile computing-enabled and is assisted by web mining tools. in our architecture, the client is a midp user interface, and the back end servlet runs on a standalone server bound to a known port address. in addition, the server uses table-mining techniques to mine real-time data from reliable web sources upon the mobile trader’s directive. the server performs all computations required for pricing options since mobile devices have limited battery power, low bandwidth, and low memory. to the best of our knowledge, this is one of the first studies that facilitate the mobile-enabled-trader to compute the price of an option in ubiquitous fashion. this architecture aims at providing the trader with various computational techniques to avail (to provide results from approximate to accurate results) while on-the-go and to make important and effective trading decisions using the results that will ensure higher returns on investments in option trading. © springer-verlag berlin heidelberg 2006."
"agent-based computational finance (i.e. financial simulations of stock markets using artificial intelligence) has provoked an increasing amount of interest these past ten years. based on models dealing with many heterogeneous interacting agents, this field provides an interesting experimental method to investigate the world of finance viewed as an evolving complex system. it also allows for a perfectly controlled study of various cognitive mechanisms in the emergence of financial dynamics. agent-based computational finance (i.e. financial simulations of stock markets using artificial intelligence) has provoked an increasing amount of interest these past ten years. based on models dealing with many heterogeneous interacting agents, this field provides an interesting experimental method to investigate the world of finance viewed as an evolving complex system. it also allows for a perfectly controlled study of various cognitive mechanisms in the emergence of financial dynamics. © 2009 lavoisier. tous droits réservés pour tous pays."
"background: alzheimer's disease (ad) is characterized by multiple cognitive deficits and affects functional competency to perform daily activities (adl). as this may contribute to the patient's overall disability, it is important to identify factors that compromise competency. objective: the relationship between different cognitive domains and functional activities in ad was studied. methods: the functional competency of 73 japanese ad patients, most with mild dementia, was assessed using a 27-item relative/carer-rating scale covering 7 adl: managing finances, using transportation, taking precautions, self-care, housekeeping, communication and taking medicine. cognitive assessment used 16 neuropsychological tests from the japanese version of the wais-r and cognistat, covering 9 cognitive domains: orientation, attention, episodic memory, semantic memory, language, visuoperceptual and construction abilities, computational ability, abstract thinking, and psychomotor speed. results: multiple regression analysis by the stepwise method indicated that functional competency could, for the most part, be predicted from test scores for orientation, abstract thinking and psychomotor speed. discussion: the results of this study suggest that impairment of these three cognitive domains plays an important role in the functional deterioration of ad. © 2005 international psychogeriatric association."
"a state-of-the-art introduction to the powerful mathematical and statistical tools used in the field of finance. the use of mathematical models and numerical techniques is a practice employed by a growing number of applied mathematicians working on applications in finance. reflecting this development, numerical methods in finance and economics: a matlab?-based introduction, second edition bridges the gap between financial theory and computational practice while showing readers how to utilize matlab?--the powerful numerical computing environment--for financial applications. the author provides an essential foundation in finance and numerical analysis in addition to background material for students from both engineering and economics perspectives. a wide range of topics is covered, including standard numerical analysis methods, monte carlo methods to simulate systems affected by significant uncertainty, and optimization methods to find an optimal set of decisions. among this book's most outstanding features is the integration of matlab?, which helps students and practitioners solve relevant problems in finance, such as portfolio management and derivatives pricing. this tutorial is useful in connecting theory with practice in the application of classical numerical methods and advanced methods, while illustrating underlying algorithmic concepts in concrete terms. newly featured in the second edition: in-depth treatment of monte carlo methods with due attention paid to variance reduction strategies. new appendix on ampl in order to better illustrate the optimization models in chapters 11 and 12. new chapter on binomial and trinomial lattices. additional treatment of partial differential equations with two space dimensions. expanded treatment within the chapter on financial theory to provide a more thorough background for engineers not familiar with finance. new coverage of advanced optimization methods and applications later in the text. numerical methods in finance and economics: a matlab?-based introduction, second edition presents basic treatments and more specialized literature, and it also uses algebraic languages, such as ampl, to connect the pencil-and-paper statement of an optimization model with its solution by a software library. offering computational practice in both financial engineering and economics fields, this book equips practitioners with the necessary techniques to measure and manage risk. © 2006 by john wiley & sons, inc. all rights reserved."
"valuing high-dimensional options has many important applications in finance but when the true distributions are unknown or complex, numerical approximations must be used. approximation methods based on monte-carlo simulation show a steep trade-off between estimation accuracy and computational efficiency. this article presents an alternative semi-analytic approximation method for pricing options on the maximum or minimum of multiple assets with unknown distributions. computational efficiency is shown to improve significantly without sacrificing estimation accuracy. the method is illustrated with applications to options on underlying assets with mean-reverting prices, time-dependent correlations, and stochastic volatility. © springer-verlag 2005."
"many inverse problems for differential equations can be formulated as optimal control problems. it is well known that inverse problems often need to be regularized to obtain good approximations. this work presents a systematic method to regularize and to establish error estimates for approximations to some control problems in high dimension, based on symplectic approximation of the hamiltonian system for the control problem. in particular the work derives error estimates and constructs regularizations for numerical approximations to optimally controlled ordinary differential equations in ℝd, with non smooth control. though optimal controls in general become non smooth, viscosity solutions to the corresponding hamilton-jacobi-bellman equation provide good theoretical foundation, but poor computational efficiency in high dimensions. the computational method here uses the adjoint variable and works efficiently also for high dimensional problems with d ≫ 1. controls can be discontinuous due to a lack of regularity in the hamiltonian or due to colliding backward paths, i.e. shocks. the error analysis, for both these cases, is based on consistency with the hamilton-jacobi-bellman equation, in the viscosity solution sense, and a discrete pontryagin principle: the bi-characteristic hamiltonian ode system is solved with a c2 approximate hamiltonian. the error analysis leads to estimates useful also in high dimensions since the bounds depend on the lipschitz norms of the hamiltonian and the gradient of the value function but not on d explicitly. applications to inverse implied volatility estimation, in mathematical finance, and to a topology optimization problem are presented. an advantage with the pontryagin based method is that the newton method can be applied to efficiently solve the discrete nonlinear hamiltonian system, with a sparse jacobian that can be calculated explicitly. © edp sciences, smai 2006."
"this is an editorial guide for the special issue on computational intelligence (ci) in economics and finance. a historical introduction to the background is given. this research paradigm is traced back to herbert simon, who, as a founder of artificial intelligence, pioneered the applications of ai to economics. the move from the classical ai to ci indicates a continuation of the legacy of herbert simon. computational intelligence has proved to be a constructive foundation for economics. in responding to what herbert simon referred as procedural rationality, our study of bounded rationality has been enriched by bringing autonomous agents into the economic analysis. © 2003 elsevier inc. all rights reserved."
"recent advances on computational intelligence (ci) methods have been published employing novel tools to solve problems in the world of finances, management and marketing research. different ci techniques have been applied in data mining to obtain a model for commercial preferences of consumers and tendencies of consumption. this paper discusses and proposes an approach to model (by applying some fuzzy methods) a target selection from large databases for direct marketing. we state a methodology, via an algorithm, which pre-processes a database, by using properties of the hotelling transformation. the processed database is used for the identification and modelling of the representative clients and preferences with a fuzzy clustering approach. copyright © 2006 inderscience enterprises ltd."
the american put is one of the oldest problems in mathematical finance. we review the development of the relevant literature over the last 40 years. today the mainstream computational problems have been solved satisfactorily and the target of research is shifting towards the development of further insights into the value of timing investment decisions. © 2005 elsevier b.v. all rights reserved.
"the book introduces the key ideas behind practical nonlinear optimization. computational finance-an increasingly popular area of mathematics degree programmes-is combined here with the study of an important class of numerical techniques. the financial content of the book is designed to be relevant and interesting to specialists. however, this material-which occupies about one-third of the text-is also sufficiently accessible to allow the book to be used on optimization courses of a more general nature. the essentials of most currently popular algorithms are described and their performance is demonstrated on a range of optimization problems arising in financial mathematics. theoretical convergence properties of methods are stated and formal proofs are provided in enough cases to be instructive rather than overwhelming. practical behaviour of methods is illustrated by computational examples and discussions of efficiency, accuracy and computational costs. supporting software for the examples and exercises is available (but the text does not require the reader to use or understand these particular codes). the author has been active in optimization for over thirty years in algorithm development and application and in teaching and research supervision. audience the book is aimed at lecturers and students (undergraduate and postgraduate) in mathematics, computational finance and related subjects. it is also useful for researchers and practitioners who need a good introduction to nonlinear optimization. © 2005 kluwer academic publishers. all rights reserved."
"because they incorporate both time- and event-driven dynamics, stochastic hybrid systems (shs) have become ubiquitous in a variety of fields, from mathematical finance to biological processes to communication networks to engineering. comprehensively integrating numerous cutting-edge studies, stochastic hybrid systems presents a captivating treatment of some of the most ambitious types of dynamic systems. cohesively edited by leading experts in the field, the book introduces the theoretical basics, computational methods, and applications of shs. it first discusses the underlying principles behind shs and the main design limitations of shs. building on these fundamentals, the authoritative contributors present methods for computer calculations that apply shs analysis and synthesis techniques in practice. the book concludes with examples of systems encountered in a wide range of application areas, including molecular biology, communication networks, and air traffic management. it also explains how to resolve practical problems associated with these systems. stochastic hybrid systems achieves an ideal balance between a theoretical treatment of shs and practical considerations. the book skillfully explores the interaction of physical processes with computerized equipment in an uncertain environment, enabling a better understanding of sophisticated as well as everyday devices and processes. © 2007 by taylor and francis group, llc."
"bsv (barberis, shleifer and vishny [journal of financial economics 49 (1998) 307-343]) model is one of the three major behavioral finance models. the existing bsv model is about how behavioral investors form beliefs, and is able to produce both overre-action and mean-reversion for a wide range of parameter values. however, the assumption that all investors in the market must all be bsv investors is a little strict and remains controversial. in this paper, we present an agent-based computational model of the dynamic game between bsv investors and rational investors. time series from the artificial stock market are analyzed and two interesting findings are reported. first, the introduction of rational investors will not eliminate the anomalies of overreaction and mean-reversion. second, no evidence is found that the bsv investors will lose money to their counterparts although their cognitive bias makes them form a false kind of expectation. on the contrary, some weak evidence is reported that bsv investors are less likely to bankrupt. © world scientific publishing company."
"we propose statistical tests for deciding between two alternatives for diffusion processes observed continuously over a finite time interval. our tests emphasize the large deviation aspects, or equivalently, the asymptotic behavior of probabilities of type i and type ii errors and the rate at which these probabilities go to zero as the observation time increases. we obtain these rates using direct methods of calculation. we provide specific computational examples for diffusion processes commonly used in finance and show that the error probabilities for these cases go to zero exponentially fast. applications in finance and economics are discussed. © 2005 elsevier inc. all rights reserved."
"in recent years the complexity of numerical computations in computational financial applications has been increased enormously. monte carlo algorithm is one of main tools in computational finance. in this paper, we show a parallel monte carlo algorithm for financial derivatives pricing. we show that the parallel monte carlo algorithm1 has good speed-up feature by extensive experiments. © 2005 ieee."
"multi-point distributed random variables whose moments match those of a gaussian random variable up to a certain order play an important role in monte carlo simulations of weak approximations of stochastic differential equations. in applications such as finance, where ""real time"" execution is required, there is a strong need for highly efficient implementations. in this paper a fast and flexible dedicated hardware solution on a field programmable gate array (fpga) is presented. a comparative performance analysis between a software-only and the proposed hardware solution demonstrates that the fpga solution is bottleneck-free, retains the flexibility of the software solution and significantly increases the computational efficiency."
"optimization models play an increasingly important role in financial decisions. this is the first textbook devoted to explaining how recent advances in optimization models, methods and software can be applied to solve problems in computational finance more efficiently and accurately. chapters discussing the theory and efficient solution methods for all major classes of optimization problems alternate with chapters illustrating their use in modeling problems of mathematical finance. the reader is guided through topics such as volatility estimation, portfolio optimization problems and constructing an index fund, using techniques such as nonlinear optimization models, quadratic programming formulations and integer programming models respectively. the book is based on master's courses in financial engineering and comes with worked examples, exercises and case studies. it will be welcomed by applied mathematicians, operational researchers and others who work in mathematical and computational finance and who are seeking a text for self-learning or for use with courses. © gerard cornuejols and reha tütüncü 2007."
"cyberworlds are being formed in cyberspaces as computational spaces. now cyberspaces are rapidly expanding on the web either intentionally or spontaneously, with or without design. widespread and intensive local activities are melting each other on the web globally to create cyberworlds. the major key players of cyberworlds include e-finance that trades a gdp-equivalent a day and e-manufacturing that is transforming industrial production into web shopping of product components and assembly factories. lacking proper theory and design, cyberworlds have continued to grow chaotic and are now out of human understanding and control. this research first presents a generic theoretical framework and design based on algebraic topology, and also provides an axiomatic approach to theorize the potentials of cyberworlds. copyright © 2005 the institute of electronics, information and communication engineers."
"in this paper we review the central concepts of the open grid service environment, which represents an abstract service stack and was introduced to design workflow-based problem solving environments. so far, it has been used to model large-scale computational finance problems as abstract workflows with meta-components and instantiate such workfiows with different components based on semantic matching. in this paper we continue by presenting two further examples from the field of computational finance, which substantiate the need for grid technologies and take a closer look at the implementational issues and apply the concepts of enterprise service busses for parallel process orchestration. the cca (common component architecture) and its convergence to web service specifications is the key technology for integrating heterogeneous components, with which one has to deal with in the financial sector. © springer-verlag berlin heidelberg 2005."
"quasi-monte carlo (qmc) methods have begun to displace ordinary monte carlo (mc) methods in many practical problems. it is natural and obvious to combine qmc methods with traditional variance reduction techniques used in mc sampling, such as control variates. there can, however, be some surprises. the optimal control variate coefficient for qmc methods is not in general the same as for mc. using the mc formula for the control variate coefficient can worsen the performance of qmc methods. a good control variate in qmc is not necessarily one that correlates with the target integrand. instead, certain high frequency parts or derivatives of the control variate should correlate with the corresponding quantities of the target. we present strategies for applying control variate coefficients with qmc and illustrate the method on a 16-dimensional integral from computational finance. we also include a survey of qmc aimed at a statistical readership. © institute of mathematical statistics, 2005."
currently there exists little support to specify and control the execution of a large number of experiments required for performance oriented development of scientific applications. we have developed a new directive based language called zen for the specification of wide sets of synthetic experiments in the context of large scale parameter and performance studies of parallel applications. we report experimental results for the performance analysis of an ocean simulation application and for the parameter study of a computational finance code. © 2005 inderscience enterprises ltd.
"multivariate stochastic processes with poisson marginals are of interest in insurance and finance; they can be used to model the joint behaviour of several claim arrival processes, for example. we discuss various methods for the construction of such models, with particular emphasis on the use of copulas. an important class of multivariate counting processes with poisson marginals arises if the events of a background poisson process with constant intensity are moved forward in time by a random amount and possibly deleted; here we think of the events of the background process as triggering later claims in different categories. we discuss structural aspects of these models, their dependence properties together with stochastic order aspects, and also some related computational issues. various actuarial applications are indicated. © 2005, astin bulletin. all rights reserved."
"this volume brings together twelve papers by many of the most prominent applied general equilibrium modelers honoring herbert scarf, the father of equilibrium computation in economics. it deals with new developments in applied general equilibrium, a field which has broadened greatly since the 1980s. the contributors discuss some traditional as well as some newer topics in the field, including non-convexities in economy-wide models, tax policy, developmental modeling and energy modeling. the book also covers a range of new approaches, conceptual issues and computational algorithms, such as calibration and new areas of application such as macroeconomics of real business cycles and finance. an introductory chapter written by the editors maps out issues and scenarios for the future evolution of applied general equilibrium. © cambridge university press 2005."
"this paper considers a design framework of a computational experiment in finance. the examination of relationships between statistics used for economic forecasts evaluation and profitability of investment decisions reveals that only the 'degree of improvement over efficient prediction' shows robust links with profitability. if profits are not observable, this measure is proposed as an evaluation criterion for an economic prediction. combined with directional accuracy, it could be used in an estimation technique for economic behavior, as an alternative to conventional least squares. model discovery and performance surface optimization with genetic algorithm demonstrate profitability improvement with an inconclusive effect on statistical criteria. © springer-verlag berlin heidelberg 2005."
"drawn from the us national science foundation's symposium on next generation of data mining and cyber-enabled discovery for innovation (ngdm 07), next generation of data mining explores emerging technologies and applications in data mining as well as potential challenges faced by the field. gathering perspectives from top experts across different disciplines, the book debates upcoming challenges and outlines computational methods. the contributors look at how ecology, astronomy, social science, medicine, finance, and more can benefit from the next generation of data mining techniques. they examine the algorithms, middleware, infrastructure, and privacy policies associated with ubiquitous, distributed, and high performance data mining. they also discuss the impact of new technologies, such as the semantic web, on data mining and provide recommendations for privacy-preserving mechanisms. the dramatic increase in the availability of massive, complex data from various sources is creating computing, storage, communication, and human-computer interaction challenges for data mining. providing a framework to better understand these fundamental issues, this volume surveys promising approaches to data mining problems that span an array of disciplines. © 2005 by the institute of electrical and electronics engineers, inc. all rights reserved."
